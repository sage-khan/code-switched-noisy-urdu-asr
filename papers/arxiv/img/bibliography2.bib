%!TEX root = ./main.tex

@inproceedings{farooq_improving_2019,
	title = {Improving {Large} {Vocabulary} {Urdu} {Speech} {Recognition} {System} {Using} {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html},
	doi = {10.21437/Interspeech.2019-2629},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Rauf, Sahar and Hussain, Sarmad},
	month = sep,
	year = {2019},
	pages = {2978--2982},
}


@misc{alphacephei_alpha_nodate,
	title = {Alpha {Cephei}},
	url = {https://github.com/alphacep},
	abstract = {Alpha Cephei has 37 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2022-08-16},
	journal = {GitHub},
	author = {Alphacephei},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\C9UHNTWM\\alphacep.html:text/html},
}

@misc{alphacep_vosk_2022,
	title = {Vosk {Speech} {Recognition} {Toolkit}},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-api},
	abstract = {Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C\# and Node},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-09-03T17:48:42Z},
	keywords = {android, asr, deep-learning, deep-neural-networks, deepspeech, google-speech-to-text, ios, kaldi, offline, privacy, python, raspberry-pi, speaker-identification, speaker-verification, speech-recognition, speech-to-text, speech-to-text-android, stt, voice-recognition, vosk},
}

@misc{alphacep_alphacepvosk-server_2022,
	title = {alphacep/vosk-server},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-server},
	abstract = {WebSocket, gRPC and WebRTC speech recognition server based on Vosk and Kaldi libraries},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-05-07T17:24:55Z},
	keywords = {asr, kaldi, python, speech-recognition, vosk, grpc, saas, webrtc, websocket},
}

@misc{alphacep_vosk_nodate,
	title = {{VOSK} {Offline} {Speech} {Recognition} {API}},
	url = {https://alphacephei.com/vosk/},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\H4HQA2XC\\vosk.html:text/html},
}

@misc{alphacep_vosk_nodate-1,
	title = {{VOSK} {Models}},
	url = {https://alphacephei.com/vosk/models},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\J75LJ82E\\models.html:text/html},
}

@misc{daniel_povey_kaldi_nodate,
	title = {Kaldi: {Kaldi}},
	url = {https://kaldi-asr.org/doc/},
	urldate = {2022-08-16},
	author = {Daniel Povey},
	file = {Kaldi\: Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\QYDHVT6P\\doc.html:text/html},
}

@misc{noauthor_openslrorg_nodate,
	title = {openslr.org},
	url = {https://www.openslr.org/resources.php},
	urldate = {2022-08-16},
	file = {openslr.org:C\:\\Users\\DELL\\Zotero\\storage\\EJWY9PRN\\resources.html:text/html},
}

@misc{cmu_cmu_nodate,
	title = {{CMU} {Lexicon} {Tool}},
	url = {http://www.speech.cs.cmu.edu/tools/lextool.html},
	urldate = {2022-08-16},
	author = {CMU},
	file = {CMU Lexicon Tool:C\:\\Users\\DELL\\Zotero\\storage\\BTMXKZQL\\lextool.html:text/html},
}

@misc{coqui-ai_coqui-aistt_2022,
	title = {coqui-ai/{STT}},
	copyright = {MPL-2.0},
	url = {https://github.com/coqui-ai/STT},
	abstract = {STT - The deep learning toolkit for Speech-to-Text. Training and deploying STT models has never been so easy.},
	urldate = {2022-08-16},
	publisher = {coqui},
	author = {coqui-ai},
	month = aug,
	year = {2022},
	note = {original-date: 2021-03-04T04:54:42Z},
	keywords = {asr, deep-learning, speech-recognition, speech-to-text, stt, voice-recognition, automatic-speech-recognition, speech-recognition-api, speech-recognizer, tensorflow},
}

@misc{noauthor_speechbrain_nodate,
	title = {{SpeechBrain}: {A} {PyTorch} {Speech} {Toolkit}},
	url = {https://speechbrain.github.io/},
	urldate = {2022-08-16},
}

@misc{piero_molino_ludwig_nodate,
	title = {Ludwig - code-free deep learning toolbox},
	url = {http://ludwig.ai},
	abstract = {Ludwig is a toolbox for training and testing deep learning models without writing code},
	language = {en},
	urldate = {2022-08-16},
	author = {Piero Molino},
}

@misc{noauthor_espnet_2022,
	title = {{ESPnet}: end-to-end speech processing toolkit},
	copyright = {Apache-2.0},
	shorttitle = {{ESPnet}},
	url = {https://github.com/espnet/espnet},
	abstract = {End-to-End Speech Processing Toolkit},
	urldate = {2022-08-16},
	publisher = {ESPnet},
	month = aug,
	year = {2022},
	note = {original-date: 2017-12-13T00:45:11Z},
	keywords = {deep-learning, kaldi, speech-recognition, chainer, end-to-end, machine-translation, pytorch, speech-enhancement, speech-separation, speech-synthesis, speech-translation, voice-conversion},
}

@misc{noauthor_espnet_nodate,
	title = {{ESPnet}: end-to-end speech processing toolkit — {ESPnet} 202207 documentation},
	url = {https://espnet.github.io/espnet/},
	urldate = {2022-08-16},
}

@article{georgescu_performance_2021,
	title = {Performance vs. hardware requirements in state-of-the-art automatic speech recognition},
	volume = {2021},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00217-4},
	doi = {10.1186/s13636-021-00217-4},
	abstract = {Abstract
            The last decade brought significant advances in automatic speech recognition (ASR) thanks to the evolution of deep learning methods. ASR systems evolved from pipeline-based systems, that modeled hand-crafted speech features with probabilistic frameworks and generated phone posteriors, to end-to-end (E2E) systems, that translate the raw waveform directly into words using one deep neural network (DNN). The transcription accuracy greatly increased, leading to ASR technology being integrated into many commercial applications. However, few of the existing ASR technologies are suitable for integration in embedded applications, due to their hard constrains related to computing power and memory usage. This overview paper serves as a guided tour through the recent literature on speech recognition and compares the most popular ASR implementations. The comparison emphasizes the trade-off between ASR performance and hardware requirements, to further serve decision makers in choosing the system which fits best their embedded application. To the best of our knowledge, this is the first study to provide this kind of trade-off analysis for state-of-the-art ASR systems.},
	language = {en},
	number = {1},
	urldate = {2022-08-16},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Georgescu, Alexandru-Lucian and Pappalardo, Alessandro and Cucu, Horia and Blott, Michaela},
	month = Dec,
	year = {2021},
	pages = {28},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TRA6KAJY\\Georgescu et al. - 2021 - Performance vs. hardware requirements in state-of-.pdf:application/pdf},
}

@misc{cplc_cplc_nodate,
	title = {{CPLC} – {Citizens}-{Police} {Liaison} {Committee}},
	url = {http://www.cplc.org.pk/},
	urldate = {2022-08-16},
	author = {CPLC},
}

@misc{british_broadcast_bbc_nodate,
	title = {{BBC} - {Languages} - {Urdu} - {A} {Guide} to {Urdu} - 10 facts about the {Urdu} language},
	url = {https://www.bbc.co.uk/languages/other/urdu/guide/facts.shtml},
	abstract = {Discover surprising and revealing facts about Urdu, including Urdu words used in the English language and Urdu jokes and quotes.},
	language = {en},
	urldate = {2022-08-16},
	author = {British Broadcast},
	note = {Last Modified: 2008-01-30T12:35:00Z},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4CXS8LEU\\facts.html:text/html},
}

@misc{ethnologue_urdu_nodate,
	title = {Urdu {Language} - {Ethnologue} {Report}},
	url = {https://www.ethnologue.com/language/urdu},
	abstract = {A language profile for Urdu. Get a detailed look at the language, from population to dialects and usage.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Ethnologue},
	author = {Ethnologue},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\WGWY3RRQ\\urd.html:text/html},
}

@article{ali_automatic_2015,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-16},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@phdthesis{sehar_gul_detecting_2020,
	address = {Karachi},
	title = {{DETECTING} {MALICIOUS} {ACTIVITIES} {OVER} {TELEPHONE} {NETWORK} {FOR} {URDU} {SPEAKER}},
	abstract = {Telephone is one of most important invention in the fields of communication it is because on this invention that we are able to connect with our friends and families without hassle of travelling and going to their places,but some people are also using it for negative purpose therefore to secure this medium of communication is one of the most important issue of today as many malicious activities are taking place on this channel.Humanely it is not possible to tap each and every phone call so that one could find malicious activities that are being done.In order to find such malicious activities we need an automatic system that can automatically detect malicious voice activities, for that we have decided to develop an automatic speech recognition system that will detect malicious sentences in Urdu Language from the telephonic conversation which will then be processed further.},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Sehar Gul},
	month = jun,
	year = {2020},
}

@incollection{hutchison_speaker_2010,
	address = {Berlin, Heidelberg},
	title = {Speaker {Independent} {Urdu} {Speech} {Recognition} {Using} {HMM}},
	volume = {6177},
	isbn = {978-3-642-13880-5},
	url = {http://link.springer.com/10.1007/978-3-642-13881-2_14},
	urldate = {2022-08-16},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ashraf, Javed and Iqbal, Naveed and Khattak, Naveed Sarfraz and Zaidi, Ather Mohsin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Hopfe, Christina J. and Rezgui, Yacine and Métais, Elisabeth and Preece, Alun and Li, Haijiang},
	year = {2010},
	doi = {10.1007/978-3-642-13881-2_14},
	note = {Series Title: Lecture Notes in Computer Science ISBN2: 978-3-642-13881-2},
	pages = {140--148},
}

@inproceedings{sarfraz_large_2010,
	address = {Islamabad, Pakistan},
	title = {Large vocabulary continuous speech recognition for {Urdu}},
	isbn = {978-1-4503-0342-2},
	url = {http://portal.acm.org/citation.cfm?doid=1943628.1943629},
	doi = {10.1145/1943628.1943629},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Frontiers} of {Information} {Technology} - {FIT} '10},
	publisher = {ACM Press},
	author = {Sarfraz, Huda and Parveen, Rahila and Hussain, Sarmad and Bokhari, Riffat and Raza, Agha Ali and Ullah, Inam and Sarfraz, Zahid and Pervez, Sophia and Mustafa, Asad and Javed, Iqra},
	year = {2010},
	pages = {1--5},
}

@inproceedings{qasim_urdu_2016,
	address = {Bali, Indonesia},
	title = {Urdu speech recognition system for district names of {Pakistan}: {Development}, challenges and solutions},
	isbn = {978-1-5090-3516-8},
	shorttitle = {Urdu speech recognition system for district names of {Pakistan}},
	url = {http://ieeexplore.ieee.org/document/7918979/},
	doi = {10.1109/ICSDA.2016.7918979},
	urldate = {2022-08-16},
	booktitle = {2016 {Conference} of {The} {Oriental} {Chapter} of {International} {Committee} for {Coordination} and {Standardization} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Qasim, Muhammad and Nawaz, Sohaib and Hussain, Sarmad and Habib, Tania},
	month = oct,
	year = {2016},
	pages = {28--32},
}

@article{aguiar_de_lima_survey_2020,
	title = {A survey on automatic speech recognition systems for {Portuguese} language and its variations},
	volume = {62},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230819302992},
	doi = {10.1016/j.csl.2019.101055},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Aguiar de Lima, Thales and Da Costa-Abreu, Márjory},
	month = jul,
	year = {2020},
	pages = {101055},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\5YG7YNP8\\Aguiar de Lima and Da Costa-Abreu - 2020 - A survey on automatic speech recognition systems f.pdf:application/pdf},
}

@inproceedings{dash_automatic_2018,
	title = {Automatic {Speech} {Recognition} with {Articulatory} {Information} and a {Unified} {Dictionary} for {Hindi}, {Marathi}, {Bengali} and {Oriya}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/dash18_interspeech.html},
	doi = {10.21437/Interspeech.2018-2122},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Dash, Debadatta and Kim, Myungjong and Teplansky, Kristin and Wang, Jun},
	month = sep,
	year = {2018},
	pages = {1046--1050},
}

@inproceedings{patil_automatic_2016,
	address = {Pune, India},
	title = {Automatic {Speech} {Recognition} of isolated words in {Hindi} language using {MFCC}},
	isbn = {978-1-5090-1338-8},
	url = {http://ieeexplore.ieee.org/document/7915008/},
	doi = {10.1109/CAST.2016.7915008},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Computing}, {Analytics} and {Security} {Trends} ({CAST})},
	publisher = {IEEE},
	author = {Patil, U. G. and Shirbahadurkar, S. D. and Paithane, A. N.},
	month = dec,
	year = {2016},
	pages = {433--438},
}

@article{a_n_mishra_robust_2011,
	title = {Robust {Features} for {Connected} {Hindi} {Digits} {Recognition}},
	volume = {4},
	number = {2},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {A. N. Mishra and Mahesh Chandra and Astik Biswas and S. N. Sharan},
	month = jun,
	year = {2011},
}

@inproceedings{aggarwal_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	abstract = {The goal of automatic speech recognition (ASR) system is to accurately and efficiently convert a speech signal into a text message independent of the device, speaker or the environment. In general the speech signal is captured and pre-processed at front-end for feature extraction and evaluated at back-end using the Gaussian mixture hidden Markov model. In this statistical approach since the evaluation of Gaussian likelihoods dominate the total computational load, the appropriate selection of Gaussian mixtures is very important depending upon the amount of training data. As the small databases are available to train the Indian languages ASR system, the higher range of Gaussian mixtures (i.e. 64 and above), normally used for European languages, cannot be applied for them. This paper reviews the statistical framework and presents an iterative procedure to select an optimum number of Gaussian mixtures that exhibits maximum accuracy in the context of Hindi speech recognition system.},
	booktitle = {International {Journal} of {Signal} {Processing}, {Image} {Processing} and {Pattern} {Recognition}},
	author = {Aggarwal, R. K. and Dave, M.},
	year = {2011},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\FVSZRDG9\\Aggarwal and Dave - 2011 - Using Gaussian Mixtures for Hindi Speech Recogniti.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\NYC9U7U3\\summary.html:text/html},
}

@inproceedings{b_venkataraman_sopc-based_2006,
	title = {{SOPC}-based speech-to-text conversion},
	booktitle = {Embedded {Processor} {Design} {Contest} {Outstanding} {Designs}},
	author = {B. Venkataraman},
	year = {2006},
}

@incollection{tanveer_continuous_2019,
	address = {Singapore},
	title = {Continuous {Hindi} {Speech} {Recognition} {Using} {Kaldi} {ASR} {Based} on {Deep} {Neural} {Network}},
	volume = {748},
	isbn = {9789811309229},
	url = {http://link.springer.com/10.1007/978-981-13-0923-6_26},
	urldate = {2022-08-16},
	booktitle = {Machine {Intelligence} and {Signal} {Analysis}},
	publisher = {Springer Singapore},
	author = {Upadhyaya, Prashant and Mittal, Sanjeev Kumar and Farooq, Omar and Varshney, Yash Vardhan and Abidi, Musiur Raza},
	editor = {Tanveer, M. and Pachori, Ram Bilas},
	year = {2019},
	doi = {10.1007/978-981-13-0923-6_26},
	note = {Series Title: Advances in Intelligent Systems and Computing
ISBN2: 9789811309236},
	pages = {303--311},
}

@inproceedings{karel_vesel_sequence-discriminative_2013,
	title = {Sequence-discriminative training of deep neural networks},
	abstract = {Luk Burget},
	author = {Karel Vesel and Arnab Ghoshal and Daniel Povey},
	year = {2013},
}

@inproceedings{k_v_s_parsad_and_s_m_virk_computational_2012,
	title = {Computational evidence that {Hindi} and {Urdu} share a grammar but not the lexicon},
	booktitle = {3rd {Workshop} on {South} and {Southeast} {Asian} {NLP}},
	author = {K. V. S. Parsad {and} S. M. Virk},
	year = {2012},
}

@article{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1402.1128},
	doi = {10.48550/ARXIV.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2022-08-16},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{abdel-hamid_exploring_2013,
	title = {Exploring {Convolutional} {Neural} {Network} {Structures} and {Optimization} {Techniques} for {Speech} {Recognition}},
	url = {https://www.microsoft.com/en-us/research/publication/exploring-convolutional-neural-network-structures-and-optimization-techniques-for-speech-recognition/},
	abstract = {Recently, convolutional neural networks (CNNs) have been shown to outperform the standard fully connected deep neural networks within the hybrid deep neural network / hidden Markov model (DNN/HMM) framework on the phone recognition task. In this paper, we extend the earlier basic form of the CNN and explore it in multiple ways. We first investigate several CNN architectures, including full and limited weight sharing, convolution along frequency and time axes, and stacking of several convolution layers. We then develop a novel weighted softmax pooling layer so that the size in the pooling layer can be automatically learned. Further, we evaluate the effect of CNN pretraining, which is achieved by using a convolutional version of the RBM. We show that all CNN architectures we have investigated outperform the earlier basic form of the DNN on both the phone recognition and large vocabulary speech recognition tasks. The architecture with limited weight sharing provides additional gains over the full weight sharing architecture. The softmax pooling layer performs as well as the best CNN with the manually tuned fixed-pooling size, and has a potential for further improvement. Finally, we show that CNN pretraining produces significantly better results on a large vocabulary speech recognition task.},
	booktitle = {Interspeech 2013},
	publisher = {ISCA},
	author = {Abdel-Hamid, Ossama and Deng, Li and Yu, Dong},
	month = aug,
	year = {2013},
	note = {Edition: Interspeech 2013},
}

@misc{noauthor_gram_nodate,
	title = {{GRAM} {VAANI} {ASR} {Challenge} 2022},
	url = {https://sites.google.com/view/gramvaaniasrchallenge/home},
	abstract = {A challenge on Automatic Speech Recognition for Hindi is being organized as part of INTERSPEECH 2022 by sharing the spontaneous telephone speech recordings collected by a social technology enterprise Gram Vaani. The regional variations of Hindi together with spontaneity of speech, natural},
	language = {en},
	urldate = {2022-08-16},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P4IHK3L5\\home.html:text/html},
}

@misc{noauthor_gramvaaniorg_nodate,
	title = {gramvaani.org},
	url = {https://gramvaani.org/},
	urldate = {2022-08-16},
}

@inproceedings{pop_towards_2021,
	address = {Bucharest, Romania},
	title = {Towards {Detection} of {Synthetic} {Utterances} in {Romanian} {Language} {Speech} {Forensics}},
	isbn = {978-1-66542-786-9},
	url = {https://ieeexplore.ieee.org/document/9587393/},
	doi = {10.1109/SpeD53181.2021.9587393},
	urldate = {2023-02-19},
	booktitle = {2021 {International} {Conference} on {Speech} {Technology} and {Human}-{Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Pop, Gheorghe and Burileanu, Dragos},
	month = oct,
	year = {2021},
	pages = {80--84},
}

@inproceedings{latif_cross_2018,
	address = {Islamabad, Pakistan},
	title = {Cross {Lingual} {Speech} {Emotion} {Recognition}: {Urdu} vs. {Western} {Languages}},
	isbn = {978-1-5386-9355-1},
	shorttitle = {Cross {Lingual} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/8616972/},
	doi = {10.1109/FIT.2018.00023},
	urldate = {2022-08-16},
	booktitle = {2018 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	publisher = {IEEE},
	author = {Latif, Siddique and Qayyum, Adnan and Usman, Muhammad and Qadir, Junaid},
	month = dec,
	year = {2018},
	pages = {88--93},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\NLRUGBJI\\Latif et al. - 2018 - Cross Lingual Speech Emotion Recognition Urdu vs..pdf:application/pdf},
}

@article{lakshmi_sri_kaldi_2020,
	title = {Kaldi recipe in {Hindi} for word level recognition and phoneme level transcription},
	volume = {171},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050920312606},
	doi = {10.1016/j.procs.2020.04.268},
	language = {en},
	urldate = {2022-08-16},
	journal = {Procedia Computer Science},
	author = {Lakshmi Sri, Karra Venkata and Srinivasan, Mayuka and Nair, Radhika Rajeev and Priya, K. Jeeva and Gupta, Deepa},
	year = {2020},
	pages = {2476--2485},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5J9VQKIM\\Lakshmi Sri et al. - 2020 - Kaldi recipe in Hindi for word level recognition a.pdf:application/pdf},
}

@misc{qureshi_urdu_2021,
	title = {Urdu {Speech} {Recognition}},
	copyright = {MIT},
	url = {https://github.com/ZoraizQ/urdu-speech-recognition},
	abstract = {Urdu Speech Recognition using Kaldi ASR, by training Triphone Acoustic GMMs using the PRUS dataset.},
	urldate = {2022-08-16},
	author = {Qureshi, Zoraiz},
	month = oct,
	year = {2021},
	note = {original-date: 2021-04-21T10:11:17Z},
	keywords = {speech-recognition, kaldi-asr, multi-speaker, prus, urdu},
}

@inproceedings{asadullah_automatic_2016,
	address = {Portsmouth},
	title = {Automatic {Urdu} {Speech} {Recognition} using {Hidden} {Markov} {Model}},
	isbn = {978-1-5090-3755-1},
	url = {https://ieeexplore.ieee.org/document/7571287/},
	doi = {10.1109/ICIVC.2016.7571287},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Image}, {Vision} and {Computing} ({ICIVC})},
	publisher = {IEEE},
	author = {{Asadullah} and Shaukat, Arslan and Ali, Hazrat and Akram, Usman},
	month = aug,
	year = {2016},
	pages = {135--139},
}

@misc{chodroff_corpus_2018,
	title = {Corpus {Phonetics} {Tutorial}},
	url = {http://arxiv.org/abs/1811.05553},
	abstract = {Corpus phonetics has become an increasingly popular method of research in linguistic analysis. With advances in speech technology and computational power, large scale processing of speech data has become a viable technique. This tutorial introduces the speech scientist and engineer to various automatic speech processing tools. These include acoustic model creation and forced alignment using the Kaldi Automatic Speech Recognition Toolkit (Povey et al., 2011), forced alignment using FAVE-align (Rosenfelder et al., 2014), the Montreal Forced Aligner (McAuliffe et al., 2017), and the Penn Phonetics Lab Forced Aligner (Yuan \& Liberman, 2008), as well as stop consonant burst alignment using AutoVOT (Keshet et al., 2014). The tutorial provides a general overview of each program, step-by-step instructions for running the program, as well as several tips and tricks.},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Chodroff, Eleanor},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05553 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3GAPGJTY\\Chodroff - 2018 - Corpus Phonetics Tutorial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5EDZSUNT\\1811.html:text/html},
}

@article{alharbi_automatic_2021,
	title = {Automatic {Speech} {Recognition}: {Systematic} {Literature} {Review}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Automatic {Speech} {Recognition}},
	doi = {10.1109/ACCESS.2021.3112535},
	abstract = {A huge amount of research has been done in the field of speech signal processing in recent years. In particular, there has been increasing interest in the automatic speech recognition (ASR) technology field. ASR began with simple systems that responded to a limited number of sounds and has evolved into sophisticated systems that respond fluently to natural language. This systematic review of automatic speech recognition is provided to help other researchers with the most significant topics published in the last six years. This research will also help in identifying recent major ASR challenges in real-world environments. In addition, it discusses current research gaps in ASR. This review covers articles available in five research databases that were completed according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) protocol. The search strategy yielded 82 conferences and articles related to the study’s scope for the period 2015–2020. The results presented in this review shed light on research trends in the area of ASR and also suggest new research directions.},
	journal = {IEEE Access},
	author = {Alharbi, Sadeen and Alrazgan, Muna and Alrashed, Alanoud and Alnomasi, Turkiayh and Almojel, Raghad and Alharbi, Rimah and Alharbi, Saja and Alturki, Sahar and Alshehri, Fatimah and Almojil, Maha},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {ASR challenges, ASR systematic review, automatic speech recognition, Automatic speech recognition, Databases, Licenses, Nails, Quality assessment, Software, Speech recognition, Systematics},
	pages = {131858--131876},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\TLVJTS37\\9536732.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\7LCUHWDI\\Alharbi et al. - 2021 - Automatic Speech Recognition Systematic Literatur.pdf:application/pdf},
}

@article{alsayadi_arabic_2021,
	title = {Arabic speech recognition using end-to-end deep learning},
	volume = {15},
	issn = {1751-9683},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12057},
	doi = {10.1049/sil2.12057},
	abstract = {Arabic automatic speech recognition (ASR) methods with diacritics have the ability to be integrated with other systems better than Arabic ASR methods without diacritics. In this work, the application of state-of-the-art end-to-end deep learning approaches is investigated to build a robust diacritised Arabic ASR. These approaches are based on the Mel-Frequency Cepstral Coefficients and the log Mel-Scale Filter Bank energies as acoustic features. To the best of our knowledge, end-to-end deep learning approach has not been used in the task of diacritised Arabic automatic speech recognition. To fill this gap, this work presents a new CTC-based ASR, CNN-LSTM, and an attention-based end-to-end approach for improving diacritisedArabic ASR. In addition, a word-based language model is employed to achieve better results. The end-to-end approaches applied in this work are based on state-of-the-art frameworks, namely ESPnet and Espresso. Training and testing of these frameworks are performed based on the Standard Arabic Single Speaker Corpus (SASSC), which contains 7 h of modern standard Arabic speech. Experimental results show that the CNN-LSTM with an attention framework outperforms conventional ASR and the Joint CTC-attention ASR framework in the task of Arabic speech recognition. The CNN-LSTM with an attention framework could achieve a word error rate better than conventional ASR and the Joint CTC-attention ASR by 5.24\% and 2.62\%, respectively.},
	language = {en},
	number = {8},
	urldate = {2022-08-16},
	journal = {IET Signal Processing},
	author = {Alsayadi, Hamzah A. and Abdelhamid, Abdelaziz A. and Hegazy, Islam and Fayed, Zaki T.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/sil2.12057},
	pages = {521--534},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8MD9EIN6\\Alsayadi et al. - 2021 - Arabic speech recognition using end-to-end deep le.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8NYWWX8A\\sil2.html:text/html},
}

@inproceedings{raza_rapid_2018,
	title = {Rapid {Collection} of {Spontaneous} {Speech} {Corpora} {Using} {Telephonic} {Community} {Forums}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/raza18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1139},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Raza, Agha Ali and Athar, Awais and Randhawa, Shan and Tariq, Zain and Saleem, Muhammad Bilal and Bin Zia, Haris and Saif, Umar and Rosenfeld, Roni},
	month = sep,
	year = {2018},
	pages = {1021--1025},
}


@article{bochner_effects_2022,
	title = {Effects of {Sound} {Quality} on the {Accuracy} of {Telephone} {Captions} {Produced} by {Automatic} {Speech} {Recognition}: {A} {Preliminary} {Investigation}},
	issn = {1059-0889, 1558-9137},
	shorttitle = {Effects of {Sound} {Quality} on the {Accuracy} of {Telephone} {Captions} {Produced} by {Automatic} {Speech} {Recognition}},
	url = {http://pubs.asha.org/doi/10.1044/2022_AJA-22-00102},
	doi = {10.1044/2022_AJA-22-00102},
	abstract = {Purpose:
              Automatic speech recognition (ASR) is commonly used to produce telephone captions to provide telecommunication access for individuals who are d/Deaf and hard of hearing (DHH). However, little is known about the effects of degraded telephone audio on the intelligibility of ASR captioning. This research note investigates the accuracy of telephone captions produced by ASR under degraded audio conditions.


              Method:
              Packet loss, delay, and repetition are common sources of degradation in sound quality for telephone audio. Eleven sets of wideband filtered sentences were degraded by high and low levels of simulated packet loss, delay, and repetition. These sets, along with a clean set of sentences, were submitted to ASR, and the accuracy of the resulting output was evaluated using three metrics: a word recognition score, word error rate, and word information loss.


              Results:
              The resulting pattern of data indicated the relative impact of each degraded condition on message intelligibility. The high and low packet loss conditions had the largest effect on message intelligibility. This finding was interpreted to indicate that packet loss can have a substantial impact on the accuracy of telephone captions produced with ASR.


              Conclusions:
              The results of this investigation point to a potential area of improvement in service quality that could have a substantial impact on telecommunication services for consumers who are DHH. Further research in this area is needed to provide additional information concerning the scope and impact of packet loss on the accuracy of telephone captioning produced by ASR.


              Supplemental Material:

                https://doi.org/10.23641/asha.21699557},
	language = {en},
	urldate = {2023-02-16},
	journal = {American Journal of Audiology},
	author = {Bochner, Joseph and Indelicato, Mark and Konnur, Pralhad},
	month = dec,
	year = {2022},
	pages = {1--8},
}


@inproceedings{naeem_subspace_2020,
	title = {Subspace {Gaussian} {Mixture} {Model} for {Continuous} {Urdu} {Speech} {Recognition} using {Kaldi}},
	doi = {10.1109/ICOSST51357.2020.9333026},
	abstract = {Automatic Speech Recognition Systems (ASR) have significantly improved in recent years, where deep learning is playing an important role in the development of end to end ASR's. ASR is the task of converting spoken language into computer readable text. ASRs are becoming ever more prevalent way to interact with technology, thereby significantly closing the gap in terms of how humans interact with computers, making it more natural. Urdu is an under resourced language, for which training such a system requires a huge amount of data that is not readily available. In this paper we present improvements to the architecture of a statistical automatic speech recognition system for which the components involved in a statistical ASR have been explored in great detail. We also present the results on various statistical models that are trained for Urdu language. We choose the Kaldi toolkit for training the Urdu ASR using approximately 100 hours of transcribed data. The refined Subspace Gaussian Model gives a word error rate of 9\% on the test set.},
	booktitle = {2020 14th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Naeem, Saad and Iqbal, Majid and Saqib, Muhammad and Saad, Muhammad and Raza, Muhammad Soban and Ali, Zaid and Akhtar, Naveed and Beg, Mirza Omer and Shahzad, Waseem and Arshad, Muhhamad Umair},
	month = dec,
	year = {2020},
	keywords = {Adaptation models, Analytical models, Automatic Speech Recognition, Computational modeling, Context modeling, Hidden Markov models, Hidden Markov Models, Mel-frequency Cepstrum, Probabilistic logic, Subspace Gaussian Mixture Models, Training},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\3KACECHI\\9333026.html:text/html},
}


@article{fish_using_2023,
	title = {{USING} {AUDIO} {QUALITY} {TO} {PREDICT} {WORD} {ERROR} {RATE} {IN} {AN} {AUTOMATIC} {SPEECH} {RECOGNITION} {SYSTEM}},
	abstract = {Faced with a backlog of audio recordings, users of automatic speech recognition (ASR) systems would benefit from the ability to predict which files would result in useful output transcripts in order to prioritize processing resources. ASR systems used in non-research environments typically run in "real time". In other words, one hour of speech requires one hour of processing. These systems produce transcripts with widely varying Word Error Rates (WER) depending upon the actual words spoken and the quality of the recording. Existing correlations between WER and the ability to perform tasks such as information retrieval or machine translation could be leveraged if one could predict WER before processing an audio file. We describe here a method for estimating the quality of the ASR output transcript by predicting the portion of the total WER in a transcript attributable to the quality of the audio recording.},
	author = {Fish, Randall and Hu, Qian and Boykin, Stanley},
	month = feb,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\4N5J2QXR\\Fish et al. - 2023 - USING AUDIO QUALITY TO PREDICT WORD ERROR RATE IN .pdf:application/pdf},
}

@inproceedings{khan_multi-genre_2021,
	address = {Singapore, Singapore},
	title = {A {Multi}-{Genre} {Urdu} {Broadcast} {Speech} {Recognition} {System}},
	isbn = {978-1-66540-870-7},
	url = {https://ieeexplore.ieee.org/document/9660552/},
	doi = {10.1109/O-COCOSDA202152914.2021.9660552},
	urldate = {2022-08-16},
	booktitle = {2021 24th {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Khan, Erbaz and Rauf, Sahar and Adeeba, Farah and Hussain, Sarmad},
	month = nov,
	year = {2021},
	pages = {25--30},
}

@incollection{somogyi_automatic_2021,
	address = {Cham},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-3-030-60031-0 978-3-030-60032-7},
	url = {http://link.springer.com/10.1007/978-3-030-60032-7_5},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Application} of {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Somogyi, Zoltán},
	collaborator = {Somogyi, Zoltán},
	year = {2021},
	doi = {10.1007/978-3-030-60032-7_5},
	pages = {145--171},
}

@incollection{chapelle_automatic_2020,
	edition = {1},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4051-9473-0 978-1-4051-9843-1},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781405198431.wbeal0066.pub2},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Encyclopedia} of {Applied} {Linguistics}},
	publisher = {Wiley},
	author = {Levis, John and Suvorov, Ruslan},
	editor = {Chapelle, Carol A.},
	month = dec,
	year = {2020},
	doi = {10.1002/9781405198431.wbeal0066.pub2},
	pages = {1--8},
}

@incollection{gold_brief_2011,
	address = {Hoboken, NJ, USA},
	title = {Brief {History} of {Automatic} {Speech} {Recognition}},
	isbn = {978-1-118-14288-2 978-0-470-19536-9},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118142882.ch4},
	urldate = {2022-08-16},
	booktitle = {Speech and {Audio} {Signal} {Processing}},
	publisher = {John Wiley \& Sons, Inc.},
	collaborator = {Gold, Ben and Morgan, Nelson and Ellis, Dan},
	month = oct,
	year = {2011},
	doi = {10.1002/9781118142882.ch4},
	pages = {40--58},
}

@misc{kincaid_brief_2018,
	title = {A {Brief} {History} of {ASR}: {Automatic} {Speech} {Recognition}},
	shorttitle = {A {Brief} {History} of {ASR}},
	url = {https://medium.com/descript/a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5},
	abstract = {This is the first post in a series on Automatic Speech Recognition, the foundational technology that makes Descript possible. We’ll be…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FHD2LASC\\a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5.html:text/html},
}

@article{suma_swamy_evolution_2013,
	title = {Evolution of {Speech} {Recognition} – {A} {Brief} {History} of {Technology} {Development}},
	volume = {60},
	journal = {Elixir Adv. Engg. Info.},
	author = {Suma Swamy and Ramakrishnan, Kollengode},
	month = jul,
	year = {2013},
}

@article{smit_advances_2021,
	title = {Advances in subword-based {HMM}-{DNN} speech recognition across languages},
	volume = {66},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300917},
	doi = {10.1016/j.csl.2020.101158},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Smit, Peter and Virpioja, Sami and Kurimo, Mikko},
	month = mar,
	year = {2021},
	pages = {101158},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC6HIS7B\\Smit et al. - 2021 - Advances in subword-based HMM-DNN speech recogniti.pdf:application/pdf},
}

@misc{kincaid_state_2018,
	title = {The {State} of {Automatic} {Speech} {Recognition}: {Q}\&{A} with {Kaldi}’s {Dan} {Povey}},
	shorttitle = {The {State} of {Automatic} {Speech} {Recognition}},
	url = {https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85},
	abstract = {This article continues our series on Automatic Speech Recognition, including our recent piece on the History of ASR.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AI8J5D9U\\the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85.html:text/html},
}

@misc{blog_machine_2022,
	title = {Machine {Learning} {Models} {Are} {Only} as {Good} as the {Data} {They} {Are} {Trained} {On}},
	url = {https://deepchecks.com/machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on/},
	abstract = {Learn about the importance of data validation in machine learning, look into the various tools for different sets of data validation techniques \& procedures.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Deepchecks},
	author = {Blog, Deepchecks Community},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5T5L9UMP\\machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on.html:text/html},
}

@misc{noauthor_machine_2018,
	title = {“{A} machine learning model is only as good as the data it is fed”},
	url = {https://devm.io/machine-learning/apache-spark-machine-learning-interview-143122},
	abstract = {Apache Spark 2.3 was released earlier this year; it marked a major milestone for Structured Streaming but there are a lot of other interesting features that deserve your attention. We talked with Reynold Xin, co-founder and Chief Architect at Databricks about the Databricks Runtime and other enhancements introduced in Apache Spark 2.3.},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {devmio - expand your knowledge},
	month = jun,
	year = {2018},
	note = {Section: Artikel},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HS46J6C4\\apache-spark-machine-learning-interview-143122.html:text/html},
}

@misc{brownlee_why_2020,
	title = {Why {Data} {Preparation} {Is} {So} {Important} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/data-preparation-is-important/},
	abstract = {On a predictive modeling project, machine learning algorithms learn a mapping from input variables to a target variable. The most common form of predictive modeling project involves so-called structured data or tabular data. This is data as it looks in a spreadsheet or a matrix, with rows of examples and columns of features for each […]},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HGMCR7CK\\data-preparation-is-important.html:text/html},
}

@book{ilyas_data_2019,
	address = {New York, NY},
	title = {Data {Cleaning}},
	isbn = {978-1-4503-7153-7},
	language = {English},
	publisher = {ACM Books},
	author = {Ilyas, Ihab F. and Chu, Xu},
	month = jun,
	year = {2019},
}

@misc{khan_introduction_2021,
	title = {An {Introduction} to {Classification} {Using} {Mislabeled} {Data}},
	url = {https://towardsdatascience.com/an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5},
	abstract = {The performance of any classifier, or for that matter any machine learning task, depends crucially on the quality of the available data…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Medium},
	author = {Khan, Shihab Shahriar},
	month = mar,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SY93L99I\\an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5.html:text/html},
}

@article{cao_joint_2019,
	title = {Joint {Prostate} {Cancer} {Detection} and {Gleason} {Score} {Prediction} in mp-{MRI} via {FocalNet}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8653866/},
	doi = {10.1109/TMI.2019.2901928},
	number = {11},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Cao, Ruiming and Mohammadian Bajgiran, Amirhossein and Afshari Mirak, Sohrab and Shakeri, Sepideh and Zhong, Xinran and Enzmann, Dieter and Raman, Steven and Sung, Kyunghyun},
	month = nov,
	year = {2019},
	pages = {2496--2506},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WK7STA45\\Cao et al. - 2019 - Joint Prostate Cancer Detection and Gleason Score .pdf:application/pdf},
}

@article{fan_impact_2021,
	title = {The {Impact} of {Mislabeled} {Changes} by {SZZ} on {Just}-in-{Time} {Defect} {Prediction}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2019.2929761},
	abstract = {Just-in-Time (JIT) defect prediction-a technique which aims to predict bugs at change level-has been paid more attention. JIT defect prediction leverages the SZZ approach to identify bug-introducing changes. Recently, researchers found that the performance of SZZ (including its variants) is impacted by a large amount of noise. SZZ may considerably mislabel changes that are used to train a JIT defect prediction model, and thus impact the prediction accuracy. In this paper, we investigate the impact of the mislabeled changes by different SZZ variants on the performance and interpretation of JIT defect prediction models. We analyze four SZZ variants (i.e., B-SZZ, AG-SZZ, MA-SZZ, and RA-SZZ) that are proposed by prior studies. We build the prediction models using the labeled data by these four SZZ variants. Among the four SZZ variants, RA-SZZ is least likely to generate mislabeled changes, and we construct the testing set by using RA-SZZ. All of the four prediction models are then evaluated on the same testing set. We choose the prediction model built on the labeled data by RA-SZZ as the baseline model, and we compare the performance and metric importance of the models trained using the labeled data by the other three SZZ variants with the baseline model. Through a large-scale empirical study on a total of 126,526 changes from ten Apache open source projects, we find that in terms of various performance measures (AUC, F1-score, G-mean and Recall@20\%), the mislabeled changes by B-SZZ and MA-SZZ are not likely to cause a considerable performance reduction, while the mislabeled changes by AG-SZZ cause a statistically significant performance reduction with an average difference of 1-5 percent. When considering developers' inspection effort (measured by LOC) in practice, the changes mislabeled B-SZZ and AG-SZZ lead to 9-10 and 1-15 percent more wasted inspection effort, respectively. And the mislabeled changes by B-SZZ lead to significantly more wasted effort. The mislabeled changes by MA-SZZ do not cause considerably more wasted effort. We also find that the top-most important metric for identifying bug-introducing changes (i.e., number of files modified in a change) is robust to the mislabeling noise generated by SZZ. But the second- and third-most important metrics are more likely to be impacted by the mislabeling noise, unless random forest is used as the underlying classifier.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fan, Yuanrui and Xia, Xin and da Costa, Daniel Alencar and Lo, David and Hassan, Ahmed E. and Li, Shanping},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Analytical models, Computer bugs, Data models, Inspection, Just-in-time defect prediction, Measurement, mining software repositories, noisy data, Predictive models, SZZ, Testing},
	pages = {1559--1586},
}

@inproceedings{van_rooyen_learning_2015,
	title = {Learning with {Symmetric} {Label} {Noise}: {The} {Importance} of {Being} {Unhinged}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {van Rooyen, Brendan and Menon, Aditya and Williamson, Robert C},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{natarajan_learning_2013,
	title = {Learning with {Noisy} {Labels}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@article{frenay_classification_2014,
	title = {Classification in the {Presence} of {Label} {Noise}: {A} {Survey}},
	volume = {25},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Classification in the {Presence} of {Label} {Noise}},
	url = {http://ieeexplore.ieee.org/document/6685834/},
	doi = {10.1109/TNNLS.2013.2292894},
	number = {5},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Frenay, Benoit and Verleysen, Michel},
	month = may,
	year = {2014},
	pages = {845--869},
}

@misc{zhang_uberi_speechrecognition_nodate,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-08-16},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\9RSTEWBT\\SpeechRecognition.html:text/html},
}

@inproceedings{panayotov_librispeech_2015,
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	shorttitle = {Librispeech},
	doi = {10.1109/ICASSP.2015.7178964},
	abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {Bioinformatics, Blogs, Corpus, Electronic publishing, Genomics, Information services, LibriVox, Resource description framework, Speech Recognition},
	pages = {5206--5210},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\2AK7VZ3R\\7178964.html:text/html},
}

@article{backstrom_introduction_2022,
	title = {Introduction to {Speech} {Processing}: 2nd {Edition}},
	copyright = {Open Access},
	shorttitle = {Introduction to {Speech} {Processing}},
	url = {https://zenodo.org/record/6821775},
	doi = {10.5281/ZENODO.6821775},
	abstract = {This release is primarily about migrating all content to jupyter-books and git. The published version is now hosted at https://speechprocessingbook.aalto.fi. In addition to github, the release has long-term storage location at Zenodo, which also assigns a DOI to the release. We have some entirely new sections, such as Forensic speaker recognition. There are also plenty of small improvements everywhere.},
	language = {en},
	urldate = {2022-08-16},
	author = {Bäckström, Tom and Räsänen, Okko and Zewoudie, Abraham and Zarazaga, Pablo Pérez and Koivusalo, Liisa and Das, Sneha and Mellado, Esteban Gómez and Mariem Bouafif Mansali and Ramos, Daniel},
	month = jul,
	year = {2022},
	note = {Publisher: Zenodo
Version Number: v2},
	keywords = {speech processing},
}

@book{yu_automatic_2015,
	address = {London},
	series = {Signals and {Communication} {Technology}},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4471-5778-6},
	url = {http://link.springer.com/10.1007/978-1-4471-5779-3},
	urldate = {2022-08-16},
	publisher = {Springer London},
	author = {Yu, Dong and Deng, Li},
	year = {2015},
	note = {ISBN2: 978-1-4471-5779-3},
}

@article{park_review_2021,
	title = {A {Review} of {Speaker} {Diarization}: {Recent} {Advances} with {Deep} {Learning}},
	volume = {72},
	copyright = {arXiv.org perpetual, non-exclusive license},
	issn = {101317},
	shorttitle = {A {Review} of {Speaker} {Diarization}},
	url = {https://arxiv.org/abs/2101.09624},
	doi = {10.48550/ARXIV.2101.09624},
	abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
	urldate = {2022-08-17},
	journal = {Computer Speech \& Language},
	author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
This article is a preprint version of the article published in Computer Speech \& Language, Volume 72, March 2022, 101317},
}

@misc{patel_data-centric_2021,
	title = {Data-{Centric} {Approach} vs {Model}-{Centric} {Approach} in {Machine} {Learning}},
	url = {https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning},
	abstract = {Code and data are the foundations of the AI system. Both of these components play an important role in the development of a robust model but which one should you focus on more? In this article, we’ll go through the data-centric vs model-centric approaches, and see which one is better, we would also talk about […]},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {neptune.ai},
	author = {Patel, Harshil},
	month = dec,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ZT7NNEUA\\data-centric-vs-model-centric-machine-learning.html:text/html},
}

@misc{radecic_data-centric_2022,
	title = {Data-centric vs. {Model}-centric {AI}? {The} {Answer} is {Clear}},
	shorttitle = {Data-centric vs. {Model}-centric {AI}?},
	url = {https://towardsdatascience.com/data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67},
	abstract = {There’s something wrong with the current approach to AI. But there’s a solution.},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Radečić, Dario},
	month = mar,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\X4DU4GPV\\data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67.html:text/html},
}

@misc{noauthor_chat_nodate,
	title = {A {Chat} with {Andrew} on {MLOps}: {From} {Model}-centric to {Data}-centric {AI} - {YouTube}},
	url = {https://www.youtube.com/watch?v=06-AZXmwHjo&t=69s},
	urldate = {2022-08-17},
}

@misc{noauthor_data-centric_nodate,
	title = {Data-centric {Machine} {Learning}: {Making} customized {ML} solutions production-ready},
	shorttitle = {Data-centric {Machine} {Learning}},
	url = {https://dida.do/blog/data-centric-machine-learning},
	abstract = {In this article, we will see why many ML Projects do not make it into production, introduce the concepts of model- and data-centric ML, and give examples how we at dida improve projects by applying data-centric techniques.},
	language = {en},
	urldate = {2022-08-17},
	journal = {dida Machine Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\F2569RZH\\data-centric-machine-learning.html:text/html},
}

@misc{noauthor_significance_nodate,
	title = {The {Significance} of {Data}-centric {AI}},
	url = {https://www.kdnuggets.com/the-significance-of-data-centric-ai.html/},
	abstract = {How a systematic way of maintaining data quality can do wonders to your model performance.},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {KDnuggets},
	note = {Section: KDnuggets Originals},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SGUDCHX6\\significance-data-centric-ai.html:text/html},
}

@misc{deeplearningai_data-centric_2021,
	title = {Data-centric {AI}: {Real} {World} {Approaches}},
	shorttitle = {Data-centric {AI}},
	url = {https://www.youtube.com/watch?v=Yqj7Kyjznh4},
	urldate = {2022-08-17},
	author = {{DeepLearningAI}},
	month = aug,
	year = {2021},
}

@article{creutz_morph-based_2007,
	title = {Morph-based speech recognition and modeling of out-of-vocabulary words across languages},
	volume = {5},
	issn = {1550-4875, 1550-4883},
	url = {https://dl.acm.org/doi/10.1145/1322391.1322394},
	doi = {10.1145/1322391.1322394},
	abstract = {We explore the use of morph-based language models in large-vocabulary continuous-speech recognition systems across four so-called morphologically rich languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. The morphs are subword units discovered in an unsupervised, data-driven way using the
              Morfessor
              algorithm. By estimating
              n
              -gram language models over sequences of morphs instead of words, the quality of the language model is improved through better vocabulary coverage and reduced data sparsity. Standard word models suffer from high out-of-vocabulary (OOV) rates, whereas the morph models can recognize previously unseen word forms by concatenating morphs. It is shown that the morph models do perform fairly well on OOVs without compromising the recognition accuracy on in-vocabulary words. The Arabic experiment constitutes the only exception since here the standard word model outperforms the morph model. Differences in the datasets and the amount of data are discussed as a plausible explanation.},
	language = {en},
	number = {1},
	urldate = {2022-08-18},
	journal = {ACM Transactions on Speech and Language Processing},
	author = {Creutz, Mathias and Hirsimäki, Teemu and Kurimo, Mikko and Puurula, Antti and Pylkkönen, Janne and Siivola, Vesa and Varjokallio, Matti and Arisoy, Ebru and Saraçlar, Murat and Stolcke, Andreas},
	month = dec,
	year = {2007},
	pages = {1--29},
}

@misc{noauthor_mfcc_nodate,
	title = {{MFCC} vs {FBANK} for chain models ?},
	url = {https://groups.google.com/g/kaldi-help/c/_7hB74HKhC4},
	urldate = {2022-08-18},
	file = {MFCC vs FBANK for chain models ?:C\:\\Users\\DELL\\Zotero\\storage\\5FKKZNEQ\\_7hB74HKhC4.html:text/html},
}

@book{reithaug_orchestrating_2002,
	title = {Orchestrating {Success} in {Reading}},
	isbn = {978-0-9694974-4-8},
	url = {https://books.google.com.pk/books?id=\_YGZMQAACAAJ},
	publisher = {Stirling Head Enterprises},
	author = {Reithaug, D.},
	year = {2002},
}


@article{zia_pronouncur_2018,
	title = {{PronouncUR}: {An} {Urdu} {Pronunciation} {Lexicon} {Generator}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{PronouncUR}},
	url = {https://arxiv.org/abs/1801.00409},
	doi = {10.48550/ARXIV.1801.00409},
	abstract = {State-of-the-art speech recognition systems rely heavily on three basic components: an acoustic model, a pronunciation lexicon and a language model. To build these components, a researcher needs linguistic as well as technical expertise, which is a barrier in low-resource domains. Techniques to construct these three components without having expert domain knowledge are in great demand. Urdu, despite having millions of speakers all over the world, is a low-resource language in terms of standard publically available linguistic resources. In this paper, we present a grapheme-to-phoneme conversion tool for Urdu that generates a pronunciation lexicon in a form suitable for use with speech recognition systems from a list of Urdu words. The tool predicts the pronunciation of words using a LSTM-based model trained on a handcrafted expert lexicon of around 39,000 words and shows an accuracy of 64\% upon internal evaluation. For external evaluation on a speech recognition task, we obtain a word error rate comparable to one achieved using a fully handcrafted expert lexicon.},
	urldate = {2022-08-18},
	author = {Zia, Haris Bin and Raza, Agha Ali and Athar, Awais},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
5 pages, LREC 2018},
}

@book{jurafsky_speech_2009,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-187321-6},
	shorttitle = {Speech and language processing},
	publisher = {Pearson Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
	note = {OCLC: 213375806},
	keywords = {Automatic speech recognition, Computational linguistics},
}

@article{likhomanenko_rethinking_2020,
	title = {Rethinking {Evaluation} in {ASR}: {Are} {Our} {Models} {Robust} {Enough}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Rethinking {Evaluation} in {ASR}},
	url = {https://arxiv.org/abs/2010.11745},
	doi = {10.48550/ARXIV.2010.11745},
	abstract = {Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets - in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. We show that, in general, reverberative and additive noise augmentation improves generalization performance across domains. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world noisy data. Finally, we show that training a single acoustic model on the most widely-used datasets - combined - reaches competitive performance on both research and real-world benchmarks.},
	urldate = {2022-08-19},
	author = {Likhomanenko, Tatiana and Xu, Qiantong and Pratap, Vineel and Tomasello, Paden and Kahn, Jacob and Avidov, Gilad and Collobert, Ronan and Synnaeve, Gabriel},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD), 68T07, 68T10, I.2.6; I.5.4},
}

@incollection{hutchison_illustrated_2013,
	address = {Berlin, Heidelberg},
	title = {An {Illustrated} {Methodology} for {Evaluating} {ASR} {Systems}},
	volume = {7836},
	isbn = {978-3-642-37424-1},
	url = {http://link.springer.com/10.1007/978-3-642-37425-8_3},
	urldate = {2022-08-19},
	booktitle = {Adaptive {Multimedia} {Retrieval}. {Large}-{Scale} {Multimedia} {Retrieval} and {Evaluation}},
	publisher = {Springer Berlin Heidelberg},
	author = {González, María and Moreno, Julián and Martínez, José Luis and Martínez, Paloma},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Detyniecki, Marcin and García-Serrano, Ana and Nürnberger, Andreas and Stober, Sebastian},
	year = {2013},
	doi = {10.1007/978-3-642-37425-8_3},
	note = {Series Title: Lecture Notes in Computer Science
ISBN2: 978-3-642-37425-8},
	pages = {33--42},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\XVRNLJ7F\\González et al. - 2013 - An Illustrated Methodology for Evaluating ASR Syst.pdf:application/pdf},
}

@phdthesis{zhang_strategies_2019,
	type = {Thesis},
	title = {Strategies for {Handling} {Out}-of-{Vocabulary} {Words} in {Automatic} {Speech} {Recognition}},
	url = {https://jscholarship.library.jhu.edu/handle/1774.2/62275},
	abstract = {Nowadays, most ASR (automatic speech recognition) systems deployed in  industry are closed-vocabulary systems, meaning we have a limited vocabulary of words the system can recognize, and where pronunciations are provided to the system. Words out of this vocabulary are called out-of-vocabulary (OOV) words, for which either pronunciations or both spellings and pronunciations are not known to the system. The basic motivations of developing strategies to handle OOV words are: First, in the training phase, missing or wrong pronunciations of words in training data results in poor acoustic models. Second, in the test phase, words out of the vocabulary cannot be recognized at all, and mis-recognition of OOV words may affect recognition performance of its in-vocabulary neighbors as well. Therefore, this dissertation is dedicated to exploring strategies of handling OOV words in closed-vocabulary ASR.

First, we investigate dealing with OOV words in ASR training data, by introducing an acoustic-data driven pronunciation learning framework using a likelihood-reduction based criterion for selecting pronunciation candidates from multiple sources, i.e. standard grapheme-to-phoneme algorithms (G2P) and phonetic decoding, in a greedy fashion. This framework effectively expands a small hand-crafted pronunciation lexicon to cover OOV words, for which the learned pronunciations have higher quality than approaches using G2P alone or using other baseline pruning criteria. Furthermore, applying the proposed framework to generate alternative pronunciations for in-vocabulary (IV) words improves both recognition performance on relevant words and overall acoustic model performance.

Second, we investigate dealing with OOV words in ASR test data, i.e. OOV detection and recovery. We first conduct a comparative study of a hybrid lexical model (HLM) approach for OOV detection, and several baseline approaches, with the conclusion that the HLM approach outperforms others in both OOV detection and first pass OOV recovery performance. Next, we introduce a grammar-decoding framework for efficient second pass OOV recovery, showing that with properly designed schemes of estimating OOV unigram probabilities, the framework significantly improves OOV recovery and overall decoding performance compared to first pass decoding.

Finally we propose an open-vocabulary word-level recurrent neural network language model (RNNLM) re-scoring framework, making it possible to re-score lattices containing recovered OOVs using a single word-level RNNLM, that was ignorant of OOVs when it was trained. Above all, the whole OOV recovery pipeline shows the potential of a highly efficient open-vocabulary word-level ASR decoding framework, tightly integrated into a standard WFST decoding pipeline.},
	language = {en\_US},
	urldate = {2022-08-19},
	school = {Johns Hopkins University},
	author = {Zhang, Xiaohui},
	month = oct,
	year = {2019},
	note = {Accepted: 2020-02-06T04:08:11Z},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\6E6BV6R6\\Zhang - 2019 - Strategies for Handling Out-of-Vocabulary Words in.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\QLFGJMPH\\62275.html:text/html},
}

@article{morgan_continuous_1995,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {1558-0792},
	doi = {10.1109/79.382443},
	abstract = {The authors focus on a tutorial description of the hybrid HMM/ANN method. The approach has been applied to large vocabulary continuous speech recognition, and variants are in use by many researchers, The method provides a mechanism for incorporating a range of sources of evidence without strong assumptions about their joint statistics, and may have applicability to much more complex systems that can incorporate deep acoustic and linguistic context. The method is inherently discriminant and conservative of parameters. Despite these potential advantages, the hybrid method has focused on implementing fairly simple systems, which do surprisingly well on large continuous speech recognition tasks, Researchers are only beginning to explore the use of more complex structures with this paradigm. In particular, they are just beginning to look at the connectionist inference of language models (including phonology) from data, which may be required in order to take advantage of locally discriminant probabilities rather than simply translating to likelihoods. Finally, the authors' current intuition is that more advanced versions of the hybrid method can greatly benefit from a perceptual perspective.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Speech recognition, Hidden Markov models, Statistics, Vocabulary},
	pages = {24--42},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\SIWUUI5M\\382443.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\99KFFZGH\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{nautsch_gdpr_2019,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\V8ZYWF7E\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@misc{noauthor_spsc_nodate,
	title = {{SPSC} {\textbar} {ISCA} {SIG}-{SPSC}},
	url = {https://www.spsc-sig.org/},
	urldate = {2022-08-19},
}

@misc{noauthor_mozilla_nodate,
	title = {Mozilla {Common} {Voice}},
	url = {https://commonvoice.mozilla.org/},
	language = {en},
	urldate = {2022-08-19},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2ZVA6IM8\\datasets.html:text/html},
}

@article{schertz_acoustic_2020,
	title = {Acoustic cues in production and perception of the four-way stop laryngeal contrast in {Hindi} and {Urdu}},
	volume = {81},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009544702030070X},
	doi = {10.1016/j.wocn.2020.100979},
	language = {en},
	urldate = {2022-08-19},
	journal = {Journal of Phonetics},
	author = {Schertz, Jessamyn and Khan, Sarah},
	month = jul,
	year = {2020},
	pages = {100979},
}

@incollection{dua_urdu_2006,
	title = {Urdu},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022446},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02244-6},
	pages = {269--275},
}

@incollection{dua_hindustani_2006,
	title = {Hindustani},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022203},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02220-3},
	pages = {309--312},
}

@incollection{annamalai_india_2006,
	title = {India: {Language} {Situation}},
	isbn = {978-0-08-044854-1},
	shorttitle = {India},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542046113},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Annamalai, E.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/04611-3},
	pages = {610--613},
}

@misc{noauthor_morfessor_nodate,
	title = {Morfessor 2.0 documentation — {Morfessor} 2.0.4 documentation},
	url = {https://morfessor.readthedocs.io/en/latest/},
	urldate = {2022-08-19},
}

@misc{noauthor_morpho_nodate,
	title = {Morpho project},
	url = {http://morpho.aalto.fi/projects/morpho/},
	urldate = {2022-08-19},
	file = {Morpho project:C\:\\Users\\DELL\\Zotero\\storage\\435PUDQA\\morpho.html:text/html},
}

@inproceedings{farooq_enhancing_2020,
	title = {Enhancing {Large} {Vocabulary} {Continuous} {Speech} {Recognition} {System} for {Urdu}-{English} {Conversational} {Code}-{Switched} {Speech}},
	doi = {10.1109/O-COCOSDA50338.2020.9295036},
	abstract = {This paper presents first step towards Large Vocabulary Continuous Speech Recognition (LVCSR) system for Urdu-English code-switched conversational speech. Urdu is the national language and lingua franca of Pakistan, with 100 million speakers worldwide. English, on the other hand, is official language of Pakistan and commonly mixed with Urdu in daily communication. Urdu, being under-resourced language, have no substantial Urdu-English code-switched corpus in hand to develop speech recognition system. In this research, readily available spontaneous Urdu speech corpus (25 hours) is revised to use it for enhancement of read speech Urdu LVCSR to recognize code-switched speech. This data set is split into 20 hours of train and 5 hours of test set. 10 hours of Urdu BroadCast (BC) data are collected and annotated in a semi-supervised way to enhance the system further. For acoustic modeling, state-of-the-art DNN-HMM modeling technique is used without any prior GMM-HMM training and alignments. Various techniques to improve language model using monolingual data are investigated. The overall percent Word Error Rate (WER) is reduced from 40.71\% to 26.95\% on test set.},
	booktitle = {2020 23rd {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Hussain, Sarmad and Rauf, Sahar and Khalid, Maryam},
	month = nov,
	year = {2020},
	note = {ISSN: 2472-7695},
	keywords = {Speech recognition, Data models, Vocabulary, Acoustics, Speech coding, Speech enhancement, Switches, under-resourced language, Urdu speech recognition, Urdu-English code-switching},
	pages = {155--159},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\88SVPLUM\\9295036.html:text/html},
}

@inproceedings{watanabe_-line_2009,
	address = {Taipei, Taiwan},
	title = {On-line adaptation and {Bayesian} detection of environmental changes based on a macroscopic time evolution system},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4960598/},
	doi = {10.1109/ICASSP.2009.4960598},
	urldate = {2022-08-19},
	booktitle = {2009 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Watanabe, Shinji and Nakamura, Atsushi},
	month = apr,
	year = {2009},
	pages = {4373--4376},
}

@article{davis_automatic_1952,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@article{s_review_2016,
	title = {A {Review} on {Automatic} {Speech} {Recognition} {Architecture} and {Approaches}},
	volume = {9},
	issn = {20054254, 20054254},
	url = {http://article.nadiapub.com/IJSIP/vol9_no4/34.pdf},
	doi = {10.14257/ijsip.2016.9.4.34},
	number = {4},
	urldate = {2022-08-19},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {S, Karpagavalli and E, Chandra},
	month = apr,
	year = {2016},
	pages = {393--404},
}

@incollection{maglogiannis__2020,
	address = {Cham},
	title = {Α {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology
ISBN2: 978-3-030-49161-1},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\79KCL3R7\\Filippidou and Moussiades - 2020 - Α Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@misc{amodei_deep_2015,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	shorttitle = {Deep {Speech} 2},
	url = {http://arxiv.org/abs/1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	month = dec,
	year = {2015},
	note = {arXiv:1512.02595 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\T5Y77XAY\\Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5HBNU4V7\\1512.html:text/html},
}

@article{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech}},
	url = {https://arxiv.org/abs/1412.5567},
	doi = {10.48550/ARXIV.1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	urldate = {2022-08-19},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{xiong_microsoft_2018,
	title = {The {Microsoft} 2017 {Conversational} {Speech} {Recognition} {System}},
	url = {https://www.microsoft.com/en-us/research/publication/conference-paper-microsoft-2017-conversational-speech-recognition-system/},
	abstract = {We describe the latest version of Microsoft's conversational speech recognition system for the Switchboard and CallHome domains. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby acoustic model posteriors are first combined at the senone/frame level,followed by a word-level voting via confusion networks. We also added another language model rescoring step following the confusion network combination. The resulting system yields a 5.1\% word error rate on the NIST 2000 Switchboard test set, and 9.8\% on the CallHome subset.},
	booktitle = {Proc. {IEEE} {ICASSP}},
	publisher = {IEEE},
	author = {Xiong, Wayne and Wu, Lingfeng and Droppo, Jasha and Huang, Xuedong and Stolcke, Andreas},
	month = apr,
	year = {2018},
	pages = {5934--5938},
}

@incollection{juang_speech_2006,
	address = {Oxford},
	title = {Speech {Recognition}, {Automatic}: {History}},
	isbn = {978-0-08-044854-1},
	shorttitle = {Speech {Recognition}, {Automatic}},
	url = {https://www.sciencedirect.com/science/article/pii/B0080448542009068},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (Dudley, 1939; Dudley et al., 1939), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface. Examples are automatic call processing in the telephone network and query-based information systems that provide updated travel information, stock price quotations, weather reports, etc. In this article we review some major highlights in the research and development of automatic speech recognition during the last few decades to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Juang, B. -H. and Rabiner, L. R.},
	editor = {Brown, Keith},
	month = jan,
	year = {2006},
	doi = {10.1016/B0-08-044854-2/00906-8},
	keywords = {Speech recognition, acoustic modeling, automatic transcription, dialog systems, finite state network, hidden Markov models, keyword spotting, language modeling, neural networks, office automation, pattern recognition, spectral analysis, speech understanding, statistical modeling, time normalization},
	pages = {806--819},
	file = {ScienceDirect Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VWPXBBVS\\B0080448542009068.html:text/html},
}

@book{brown_encyclopedia_2006,
	address = {Amsterdam},
	edition = {2nd ed},
	title = {Encyclopedia of language \& linguistics},
	isbn = {978-0-08-044854-1},
	language = {eng},
	publisher = {Elsevier},
	author = {Brown, E. K. and Anderson, Anne},
	year = {2006},
}

@article{upton_speech_1984,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages — this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-19},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@inproceedings{morris_wer_2004,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@inproceedings{wu_deep_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Deep neural networks employing {Multi}-{Task} {Learning} and stacked bottleneck features for speech synthesis},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178814/},
	doi = {10.1109/ICASSP.2015.7178814},
	urldate = {2022-08-19},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Zhizheng and Valentini-Botinhao, Cassia and Watts, Oliver and King, Simon},
	month = apr,
	year = {2015},
	pages = {4460--4464},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\GFDVBE7Z\\Wu et al. - 2015 - Deep neural networks employing Multi-Task Learning.pdf:application/pdf},
}

@article{pohjalainen_feature_2015,
	title = {Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits},
	volume = {29},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230813001113},
	doi = {10.1016/j.csl.2013.11.004},
	language = {en},
	number = {1},
	urldate = {2022-08-19},
	journal = {Computer Speech \& Language},
	author = {Pohjalainen, Jouni and Räsänen, Okko and Kadioglu, Serdar},
	month = jan,
	year = {2015},
	pages = {145--171},
}

@inproceedings{eyben_opensmile_2010,
	address = {Firenze, Italy},
	title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
	isbn = {978-1-60558-933-6},
	shorttitle = {Opensmile},
	url = {http://dl.acm.org/citation.cfm?doid=1873951.1874246},
	doi = {10.1145/1873951.1874246},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the international conference on {Multimedia} - {MM} '10},
	publisher = {ACM Press},
	author = {Eyben, Florian and Wöllmer, Martin and Schuller, Björn},
	year = {2010},
	pages = {1459},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4F8ACZKL\\Eyben et al. - 2010 - Opensmile the munich versatile and fast open-sour.pdf:application/pdf},
}

@article{chung_unsupervised_2019,
	title = {An {Unsupervised} {Autoregressive} {Model} for {Speech} {Representation} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1904.03240},
	doi = {10.48550/ARXIV.1904.03240},
	abstract = {This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.},
	urldate = {2022-08-19},
	author = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Accepted to Interspeech 2019. Code available at: https://github.com/iamyuanchung/Autoregressive-Predictive-Coding},
}

@inproceedings{boser_training_1992,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {A training algorithm for optimal margin classifiers},
	isbn = {978-0-89791-497-0},
	url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
	doi = {10.1145/130385.130401},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the fifth annual workshop on {Computational} learning theory  - {COLT} '92},
	publisher = {ACM Press},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	year = {1992},
	pages = {144--152},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\29LCGTE4\\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf:application/pdf},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {wav2vec 2.0},
	url = {https://arxiv.org/abs/2006.11477},
	doi = {10.48550/ARXIV.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2022-08-19},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@article{soldz_big_1999,
	title = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}: {A} 45-{Year} {Longitudinal} {Study}},
	volume = {33},
	issn = {00926566},
	shorttitle = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656699922432},
	doi = {10.1006/jrpe.1999.2243},
	language = {en},
	number = {2},
	urldate = {2022-08-19},
	journal = {Journal of Research in Personality},
	author = {Soldz, Stephen and Vaillant, George E.},
	month = jun,
	year = {1999},
	pages = {208--232},
}

@inproceedings{zhou_security_2010,
	address = {Beijing, China},
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	isbn = {978-1-4244-8125-5},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	url = {http://ieeexplore.ieee.org/document/5663489/},
	doi = {10.1109/SKG.2010.19},
	urldate = {2022-08-19},
	booktitle = {2010 {Sixth} {International} {Conference} on {Semantics}, {Knowledge} and {Grids}},
	publisher = {IEEE},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = nov,
	year = {2010},
	pages = {105--112},
}

@inproceedings{karimov_cloud_2021,
	title = {Cloud {Computing} {Security} {Challenges} and {Solutions}},
	doi = {10.1109/ICISCT52966.2021.9670220},
	abstract = {in this paper focuses on development of cloud computing, privacy security issues have become increasingly prominent, which is of concern to industry and academia. We review the research progress on privacy security issues from the perspective of several privacy security protection technologies in cloud computing. First, we introduce some privacy security risks of cloud computing and propose a comprehensive privacy security protection framework.},
	booktitle = {2021 {International} {Conference} on {Information} {Science} and {Communications} {Technologies} ({ICISCT})},
	author = {Karimov, Abdukodir and Olimov, Iskandar and Berdiyev, Khusniddin and Tojiakbarova, Umida and Tursunov, Otabek},
	month = nov,
	year = {2021},
	keywords = {Computational modeling, Access control, Attribute-based encryption, Cloud computing, Communications technology, Industries, Information science, Privacy, Privacy security},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\6B73HGPD\\9670220.html:text/html},
}

@incollection{takabi_introduction_2019,
	title = {Introduction to the {Cloud} and {Fundamental} {Security} and {Privacy} {Issues} of the {Cloud}},
	isbn = {978-1-119-05340-8},
	url = {https://ieeexplore.ieee.org/document/9821151},
	abstract = {Cloud Computing is the most important solution to extend Information Technology's (IT) capabilities. However, Cloud is still vulnerable to a variety of threats and attacks that affects the growth of cloud computing in recent years. Therefore, the security concerns should be considered to improve the assurance of required security for the cloud customers. The cloud security consists of different aspects such as: infrastructure, information, and identity. In this chapter, we provide an introduction to the Cloud and its fundamental security and privacy issues, investigate security issues in different cloud services delivery models and introduce Cloud security standards.},
	urldate = {2022-08-19},
	booktitle = {Security, {Privacy}, and {Digital} {Forensics} in the {Cloud}},
	publisher = {Wiley},
	author = {Takabi, Hassan and GhasemiGol, Mohammad},
	year = {2019},
	doi = {10.1002/9781119053385.ch1},
	note = {Conference Name: Security, Privacy, and Digital Forensics in the Cloud},
	keywords = {Computational modeling, Cloud computing, Privacy, Operating systems, Organizations, Security, Software as a service},
	pages = {1--22},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\NNG85SBV\\9821151.html:text/html},
}

@inproceedings{kumar_systematic_2020,
	title = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}: {Data} {Integrity}, {Confidentiality} and {Availability}},
	shorttitle = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}},
	doi = {10.1109/GUCON48875.2020.9231255},
	abstract = {The cloud computing plays the prominent role in many organizations and researchers were focus on securing the cloud computing. The privacy preserving is the major challenge that grows exponentially with increases in user. In this paper, the depth survey is conducted on the recent methodologies of the cloud storage security related with the cloud computing. The overview of the cloud computing and security issues is analyzed in this paper. The key security requirements such as data integrity, availability and confidentiality. Security issues in the recent methodologies of cloud security is analyzed. The challenges in the cloud security is analyzed and possible future scope of the method is discussed. The paper involves in analyzing the state-of-art method to investigate the advantages and limitations.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Computing}, {Power} and {Communication} {Technologies} ({GUCON})},
	author = {Kumar, Rajeev and Bhatia, M P S},
	month = oct,
	year = {2020},
	keywords = {Systematics, Privacy, Organizations, Cloud Computing, Cloud computing security, Cloud Storage Security, Conferences, Confidentiality, Data integrity, Data Integrity, Data transfer, Privacy Preserving},
	pages = {334--337},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\MT8C247U\\9231255.html:text/html},
}


@article{chen_audio_2009,
	title = {Audio {Quality} {Issue} for {Automatic} {Speech} {Assessment}},
	abstract = {Recently, in the language testing field, automatic speech recog-nition (ASR) technology has been used to automatically score speaking tests. This paper investigates the impact of audio quality on ASR-based automatic speaking assessment. Using the read speech data in the International English Speaking Test (IEST) practice test, we annotated audio quality and compared scores rated by humans, speech recognition accuracy, and the quality of features used for the automatic assessment under high and low audio quality conditions. Our investigation suggests that human raters can cope with low-quality audio files well, but speech recognition and the features extracted for the automatic assessment perform worse on the low audio quality condition.},
	author = {Chen, Lei},
	month = jan,
	year = {2009},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\MV935Q6L\\Chen - 2009 - Audio Quality Issue for Automatic Speech Assessmen.pdf:application/pdf},
}


@inproceedings{godfrey_switchboard_1992,
	address = {San Francisco, CA, USA},
	title = {{SWITCHBOARD}: telephone speech corpus for research and development},
	isbn = {978-0-7803-0532-8},
	shorttitle = {{SWITCHBOARD}},
	url = {http://ieeexplore.ieee.org/document/225858/},
	doi = {10.1109/ICASSP.1992.225858},
	urldate = {2022-08-20},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
	year = {1992},
	pages = {517--520 vol.1},
}

@article{zhang_hello_2017,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-20},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@inproceedings{lin_self-attentive_2020,
	title = {Self-{Attentive} {Similarity} {Measurement} {Strategies} in {Speaker} {Diarization}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1908},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Lin, Qingjian and Hou, Yu and Li, Ming},
	month = oct,
	year = {2020},
	pages = {284--288},
}

@article{schuller_interspeech_2013,
	title = {The interspeech 2013 computational paralinguistics challenge: {Social} signals, conflict, emotion, autism},
	shorttitle = {The interspeech 2013 computational paralinguistics challenge},
	journal = {Proceedings of Interspeech},
	author = {Schuller, Björn and Steidl, S. and Batliner, Anton and Vinciarelli, Alessandro and Scherer, K. and Ringeval, Fabien and Chetouani, Mohamed and Weninger, F. and Eyben, Florian and Marchi, Erik and Mortillaro, Marcello and Salamin, H. and Polychroniou, Anna and Valente, F. and Kim, S.},
	month = jan,
	year = {2013},
	pages = {148--152},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IJ9QFIG3\\Schuller et al. - 2013 - The interspeech 2013 computational paralinguistics.pdf:application/pdf},
}

@ARTICLE{Li2014robustoverview,
  author={Li, Jinyu and Deng, Li and Gong, Yifan and Haeb-Umbach, Reinhold},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title={An Overview of Noise-Robust Automatic Speech Recognition},
  year={2014},
  volume={22},
  number={4},
  pages={745-777},
  doi={10.1109/TASLP.2014.2304637}}

@INPROCEEDINGS{bou1994duration,
  author={Bou-Ghazale, S.E. and Hansen, J.H.L.},
  booktitle={Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
  title={Duration and spectral based stress token generation for HMM speech recognition under stress},
  year={1994},
  volume={i},
  number={},
  pages={I/413-I/416 vol.1},
  doi={10.1109/ICASSP.1994.389268}}


@inproceedings{misra_spectral_2004,
	address = {Montreal, Que., Canada},
	title = {Spectral entropy based feature for robust {ASR}},
	volume = {1},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1325955/},
	doi = {10.1109/ICASSP.2004.1325955},
	urldate = {2022-08-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Misra, H. and Ikbal, S. and Bourlard, H. and Hermansky, H.},
	year = {2004},
	pages = {I--193--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\YLKP828K\\Misra et al. - 2004 - Spectral entropy based feature for robust ASR.pdf:application/pdf},
}

@article{hunt_spectral_2000,
	title = {Spectral {Signal} {Processing} for {ASR}},
	abstract = {The paper begins by discussing the difficulties in obtaining repeatable results in speech recognition. Theoretical arguments are presented for and against copying human auditory properties in automatic speech recognition. The "standard" acoustic analysis for automatic speech recognition, consisting of melscale cepstrum coefficients and their temporal derivatives, is described. Some variations and extensions of the standard analysis --- PLP, cepstrum correlation methods, LDA, and variants on log power --- are then discussed. These techniques pass the test of having been found useful at multiple sites, especially with noisy speech. The extent to which auditory properties can account for the advantage found for particular techniques is considered. It is concluded that the advantages do not in fact stem from auditory properties, and that there is so far little or no evidence that the study of the human auditory system has contributed to advances in automatic speech recognition. Contributio...},
	author = {Hunt, Melvyn},
	month = aug,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\Z9XRFNJG\\Hunt - 2000 - Spectral Signal Processing for ASR.pdf:application/pdf},
}

@article{biswas_speaker_2021,
	title = {Speaker recognition: an enhanced approach to identify singer voice using neural network},
	volume = {24},
	issn = {1381-2416, 1572-8110},
	shorttitle = {Speaker recognition},
	url = {http://link.springer.com/10.1007/s10772-020-09698-8},
	doi = {10.1007/s10772-020-09698-8},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {International Journal of Speech Technology},
	author = {Biswas, Sharmila and Solanki, Sandeep Singh},
	month = mar,
	year = {2021},
	pages = {9--21},
}

@incollection{penn_computational_2012,
	title = {Computational {Linguistics}},
	isbn = {978-0-444-51747-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444517470500056},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Philosophy of {Linguistics}},
	publisher = {Elsevier},
	author = {Penn, Gerald},
	year = {2012},
	doi = {10.1016/B978-0-444-51747-0.50005-6},
	pages = {143--173},
}

@inproceedings{brill_improved_2000,
	address = {Hong Kong},
	title = {An improved error model for noisy channel spelling correction},
	url = {http://portal.acm.org/citation.cfm?doid=1075218.1075255},
	doi = {10.3115/1075218.1075255},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Proceedings of the 38th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '00},
	publisher = {Association for Computational Linguistics},
	author = {Brill, Eric and Moore, Robert C.},
	year = {2000},
	pages = {286--293},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YP8NG3XW\\Brill and Moore - 2000 - An improved error model for noisy channel spelling.pdf:application/pdf},
}

@misc{noauthor_type_2017,
	title = {Type less, talk more},
	url = {https://blog.google/products/search/type-less-talk-more/},
	abstract = {We’re bringing voice typing (aka talking to your phone instead of typing) to 30 new languages and locales around the world, covering more than a billion people.},
	language = {en-us},
	urldate = {2022-08-20},
	journal = {Google},
	month = aug,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L9T2I37L\\type-less-talk-more.html:text/html},
}

@article{noauthor_amazon_2019,
	title = {Amazon {Workers} {Are} {Listening} to {What} {You} {Tell} {Alexa}},
	url = {https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio},
	abstract = {A global team reviews audio clips in an effort to help the voice-activated assistant respond to commands.},
	language = {en},
	urldate = {2022-08-20},
	journal = {Bloomberg.com},
	month = apr,
	year = {2019},
	keywords = {Software, Privacy, ALPHABET INC-CL A, AMAZON.COM INC, APPLE INC, Boston, business, Costa Rica, India, markets, Policy, Romania, technology},
}

@misc{noauthor_amazon_nodate,
	title = {Amazon {Sends} 1,700 {Alexa} {Voice} {Recordings} to a {Random} {Person}},
	url = {https://threatpost.com/amazon-1700-alexa-voice-recordings/140201/},
	abstract = {The intimate recordings paint a detailed picture of a man's life.},
	language = {en},
	urldate = {2022-08-20},
}

@article{wolfson_amazons_2018,
	chapter = {Technology},
	title = {Amazon's {Alexa} recorded private conversation and sent it to random contact},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2018/may/24/amazon-alexa-recorded-conversation},
	abstract = {The company, which has insisted its Echo devices aren’t always recording, has confirmed the audio was sent},
	language = {en-GB},
	urldate = {2022-08-20},
	journal = {The Guardian},
	author = {Wolfson, Sam},
	month = may,
	year = {2018},
	keywords = {Privacy, Amazon, Amazon Alexa, Internet, Surveillance, Technology, US news},
}

@article{stupp_fraudsters_2019,
	chapter = {WSJ Pro},
	title = {Fraudsters {Used} {AI} to {Mimic} {CEO}’s {Voice} in {Unusual} {Cybercrime} {Case}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402},
	abstract = {Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of funds in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.},
	language = {en-US},
	urldate = {2022-08-20},
	journal = {Wall Street Journal},
	author = {Stupp, Catherine},
	month = aug,
	year = {2019},
	keywords = {artificial intelligence, Artificial Intelligence/Machine Learning, Bobby Filar, business in europe, Business in Europe, business in the u.k., Business in the U.K., c\&e executive news filter, C\&E Executive News Filter, c\&e industry news filter, C\&E Industry News Filter, computer science, Computer Science, content types, Content Types, corporate, corporate crime, Corporate Crime/Legal Action, Corporate/Industrial News, crime, Crime/Legal Action, cybercrime, Cybercrime/Hacking, Euler Hermes Group, factiva filters, Factiva Filters, financial services, Financial Services, fraud, Fraud, general news, hacking, humanities, industrial news, insurance, Insurance, Irakli Beridze, legal action, machine learning, management, Management, non-life insurance, Non-life Insurance, Philipp Amann, political, Political/General News, PRO, Rüdiger Kirsch, sciences, Sciences/Humanities, senior level management, Senior Level Management, trade credit insurance, Trade Credit Insurance, WSJ-PRO-CYBER, WSJ-PRO-WSJ.com},
}

@book{petronio_boundaries_2002,
	address = {Albany},
	series = {{SUNY} series in communication studies},
	title = {Boundaries of privacy: dialectics of disclosure},
	isbn = {978-0-7914-5515-9},
	shorttitle = {Boundaries of privacy},
	publisher = {State University of New York Press},
	author = {Petronio, Sandra Sporbert},
	year = {2002},
	note = {ISBN2: 978-0-7914-5516-6},
	keywords = {Privacy, Interpersonal communication, Secrecy, Self-disclosure},
}

@article{xie_how_2009,
	title = {How to repair customer trust after negative publicity: {The} roles of competence, integrity, benevolence, and forgiveness},
	volume = {26},
	issn = {07426046, 15206793},
	shorttitle = {How to repair customer trust after negative publicity},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mar.20289},
	doi = {10.1002/mar.20289},
	language = {en},
	number = {7},
	urldate = {2022-08-20},
	journal = {Psychology and Marketing},
	author = {Xie, Yi and Peng, Siqing},
	month = jul,
	year = {2009},
	pages = {572--589},
}

@article{shi_edge_2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	url = {http://ieeexplore.ieee.org/document/7488250/},
	doi = {10.1109/JIOT.2016.2579198},
	number = {5},
	urldate = {2022-08-20},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	month = oct,
	year = {2016},
	pages = {637--646},
}

@misc{noauthor_mydata_2015,
	type = {Muut julkaisut},
	title = {{MyData} – {A} {Nordic} {Model} for human-centered personal data management and processing},
	copyright = {This publication is copyrighted. You may download, display and print it for Your own personal use. Commercial use is prohibited.},
	url = {https://julkaisut.valtioneuvosto.fi/handle/10024/78439},
	abstract = {This white paper presents a framework, principles, and a model for a human-centric approach to the managing and processing of personal information. The approach – defined as MyData – is based on the right of individuals to access the data collected about them. The core idea is that individuals should be in control of their own data. The MyData approach aims at strengthening digital human rights while opening new opportunities for businesses to develop innovative personal data based services built on mutual trust.},
	language = {en},
	urldate = {2022-08-20},
	year = {2015},
	note = {Accepted: 2016-11-11T10:03:41Z
ISBN: 9789522434555
Publisher: liikenne- ja viestintäministeriö},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\UBFJDIUP\\2015 - MyData – A Nordic Model for human-centered persona.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KEV9QSG2\\78439.html:text/html},
}

@book{european_data_protection_supervisor_edps_2019,
	address = {LU},
	series = {{EDPS} {TechDispatch}},
	title = {{EDPS} {TechDispatch}},
	url = {https://data.europa.eu/doi/10.2804/004275},
	language = {eng},
	urldate = {2022-08-20},
	publisher = {Publications Office},
	author = {{European Data Protection Supervisor}},
	year = {2019},
}

@article{konig_automatic_2015,
	title = {Automatic speech analysis for the assessment of patients with predementia and {Alzheimer}'s disease},
	volume = {1},
	issn = {2352-8729, 2352-8729},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/j.dadm.2014.11.012},
	doi = {10.1016/j.dadm.2014.11.012},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	author = {König, Alexandra and Satt, Aharon and Sorin, Alexander and Hoory, Ron and Toledo‐Ronen, Orith and Derreumaux, Alexandre and Manera, Valeria and Verhey, Frans and Aalten, Pauline and Robert, Phillipe H. and David, Renaud},
	month = mar,
	year = {2015},
	pages = {112--124},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WZVVDNX2\\König et al. - 2015 - Automatic speech analysis for the assessment of pa.pdf:application/pdf},
}

@incollection{gutwirth_seven_2013,
	address = {Dordrecht},
	title = {Seven {Types} of {Privacy}},
	isbn = {978-94-007-5184-2 978-94-007-5170-5},
	url = {http://link.springer.com/10.1007/978-94-007-5170-5_1},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {European {Data} {Protection}: {Coming} of {Age}},
	publisher = {Springer Netherlands},
	author = {Finn, Rachel L. and Wright, David and Friedewald, Michael},
	editor = {Gutwirth, Serge and Leenes, Ronald and de Hert, Paul and Poullet, Yves},
	year = {2013},
	doi = {10.1007/978-94-007-5170-5_1},
	pages = {3--32},
}

@article{chen_no_2003,
	title = {[{No} title found]},
	volume = {4},
	issn = {1385951X},
	url = {http://link.springer.com/10.1023/A:1022962631249},
	doi = {10.1023/A:1022962631249},
	number = {2/3},
	urldate = {2022-08-20},
	journal = {Information Technology and Management},
	author = {Chen, Sandy C. and Dhillon, Gurpreet S.},
	year = {2003},
	pages = {303--318},
}

@misc{reuter_guide_2015,
	title = {A {Guide} to {Fully} {Homomorphic} {Encryption}},
	url = {https://eprint.iacr.org/2015/1192},
	abstract = {Fully homomorphic encryption (FHE) has been dubbed the holy grail of cryptography, an elusive goal which could solve the IT world's problems of security and trust. Research in the area exploded after 2009 when Craig Gentry showed that FHE can be realised in principle. Since that time considerable progress has been made in finding more practical and more efficient solutions. Whilst research quickly developed, terminology and concepts became diverse and confusing so that today it can be difficult to understand what the achievements of different works actually are. The purpose of this paper is to address three fundamental questions: What is FHE? What can FHE be used for? What is the state of FHE today? As well as surveying the field, we clarify different terminology in use and prove connections between different FHE notions.Updated the acknowledgements.},
	urldate = {2022-08-20},
	author = {Reuter, Colin Boyd, Christopher Carr, Kristian Gjøsteen, Angela Jäschke, Christian A., Frederik Armknecht and Strand, Martin},
	year = {2015},
	note = {Report Number: 1192},
	keywords = {Fully Homomorphic Encryption},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PE6U2WJV\\Reuter and Strand - 2015 - A Guide to Fully Homomorphic Encryption.pdf:application/pdf},
}

@book{jessica_gasiorek_message_2018,
	title = {Message {Processing}: {The} {Science} of {Creating} {Understanding}},
	copyright = {Creative Commons Attribution 4.0 International License},
	url = {http://pressbooks-dev.oer.hawaii.edu/messageprocessing/},
	publisher = {UH Mānoa Outreach College},
	author = {Jessica Gasiorek and R. Kelly Aune},
	year = {2018},
	note = {https://pressbooks.oer.hawaii.edu/messageprocessing/\#:{\textasciitilde}:text=Book\%20Title\%3A\%20Message\%20Processing\%3A\%20The\%20Science\%20of\%20Creating\%20Understanding\&text=Book\%20Description\%3A\%20The\%20text\%20provides,on\%20how\%20people\%20create\%20understanding.},
}

@misc{noauthor_kunstliche_nodate,
	title = {Künstliche {Intelligenz} kommt in {Unternehmen} allmählich voran {\textbar} {Bitkom} e.{V}.},
	url = {https://www.bitkom.org/Presse/Presseinformation/Kuenstliche-Intelligenz-kommt-in-Unternehmen-allmaehlich-voran},
	abstract = {Zwei Drittel halten KI für die wichtigste Zukunftstechnologie Bislang nutzen 8 Prozent KI-Anwendungen, jedes vierte Unternehmen will investieren Bitkom-Präsident Berg: „KI braucht noch mehr Schwung“},
	language = {de},
	urldate = {2022-08-21},
}

@misc{noauthor_scaling_nodate,
	title = {Scaling {AI}: {From} {Experimental} to {Exponential}},
	shorttitle = {Scaling {AI}},
	url = {https://www.accenture.com/us-en/insights/artificial-intelligence/ai-investments},
	abstract = {Most businesses deploy pilot \#AI programs, but they struggle when it comes to scaling it. A new Accenture report explains the 3 critical factors for scaling AI.},
	language = {en},
	urldate = {2022-08-21},
}

@article{ardila_end--end_2019,
	title = {End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-019-0447-x},
	doi = {10.1038/s41591-019-0447-x},
	language = {en},
	number = {6},
	urldate = {2022-08-21},
	journal = {Nature Medicine},
	author = {Ardila, Diego and Kiraly, Atilla P. and Bharadwaj, Sujeeth and Choi, Bokyung and Reicher, Joshua J. and Peng, Lily and Tse, Daniel and Etemadi, Mozziyar and Ye, Wenxing and Corrado, Greg and Naidich, David P. and Shetty, Shravya},
	month = jun,
	year = {2019},
	pages = {954--961},
}

@article{fukushima_neocognitron_1988,
	title = {Neocognitron: {A} hierarchical neural network capable of visual pattern recognition},
	volume = {1},
	issn = {08936080},
	shorttitle = {Neocognitron},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
	doi = {10.1016/0893-6080(88)90014-7},
	language = {en},
	number = {2},
	urldate = {2022-08-21},
	journal = {Neural Networks},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {1988},
	pages = {119--130},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2022-08-21},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@inproceedings{ghahremani_acoustic_2016,
	title = {Acoustic {Modelling} from the {Signal} {Domain} {Using} {CNNs}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/ghahremani16_interspeech.html},
	doi = {10.21437/Interspeech.2016-1495},
	language = {en},
	urldate = {2022-08-21},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Ghahremani, Pegah and Manohar, Vimal and Povey, Daniel and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {3434--3438},
}

@misc{hou_audio-visual_2018,
	title = {Audio-{Visual} {Speech} {Enhancement} {Using} {Multimodal} {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.10893},
	abstract = {Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus only on addressing audio information. In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. We also propose a multi-task learning framework for reconstructing audio and visual signals at the output layer. Precisely speaking, the proposed AVDCNN model is structured as an audio-visual encoder-decoder network, in which audio and visual data are first processed using individual CNNs, and then fused into a joint network to generate enhanced speech (the primary task) and reconstructed images (the secondary task) at the output layer. The model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five instrumental criteria. Results show that the AVDCNN model yields a notably superior performance compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process. In addition, the AVDCNN model also outperforms an existing audio-visual SE model, confirming its capability of effectively combining audio and visual information in SE.},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Hou, Jen-Cheng and Wang, Syu-Siang and Lai, Ying-Hui and Tsao, Yu and Chang, Hsiu-Wen and Wang, Hsin-Min},
	month = jan,
	year = {2018},
	note = {arXiv:1703.10893 [cs, stat]},
	keywords = {Computer Science - Multimedia, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: To appear in IEEE Transactions on Emerging Topics in Computational Intelligence. Some audio samples can be reached in this link: https://sites.google.com/view/avse2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\RZW9TSJU\\Hou et al. - 2018 - Audio-Visual Speech Enhancement Using Multimodal D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\W8FSCKT6\\1703.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2022-08-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{errattahi_automatic_2018,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-21},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\HJ993EDS\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@article{morgan_continuous_1995-1,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {10535888},
	url = {http://ieeexplore.ieee.org/document/382443/},
	doi = {10.1109/79.382443},
	number = {3},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	pages = {24--42},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\8AZVI7RK\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{li_hybrid_2013,
	address = {Geneva, Switzerland},
	title = {Hybrid {Deep} {Neural} {Network}--{Hidden} {Markov} {Model} ({DNN}-{HMM}) {Based} {Speech} {Emotion} {Recognition}},
	isbn = {978-0-7695-5048-0},
	url = {http://ieeexplore.ieee.org/document/6681449/},
	doi = {10.1109/ACII.2013.58},
	urldate = {2022-08-21},
	booktitle = {2013 {Humaine} {Association} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction}},
	publisher = {IEEE},
	author = {Li, Longfei and Zhao, Yong and Jiang, Dongmei and Zhang, Yanning and Wang, Fengna and Gonzalez, Isabel and Valentin, Enescu and Sahli, Hichem},
	month = sep,
	year = {2013},
	pages = {312--317},
}

@article{ochiai_speaker_2016,
	title = {Speaker {Adaptive} {Training} {Localizing} {Speaker} {Modules} in {DNN} for {Hybrid} {DNN}-{HMM} {Speech} {Recognizers}},
	volume = {E99.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E99.D/10/E99.D_2016SLP0010/_article},
	doi = {10.1587/transinf.2016SLP0010},
	language = {en},
	number = {10},
	urldate = {2022-08-21},
	journal = {IEICE Transactions on Information and Systems},
	author = {Ochiai, Tsubasa and Matsuda, Shigeki and Watanabe, Hideyuki and Lu, Xugang and Hori, Chiori and Kawai, Hisashi and Katagiri, Shigeru},
	year = {2016},
	pages = {2431--2443},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\8EY8WZF7\\Ochiai et al. - 2016 - Speaker Adaptive Training Localizing Speaker Modul.pdf:application/pdf},
}

@book{muller_fundamentals_2021,
	address = {Cham, Switzerland},
	edition = {Second edition},
	title = {Fundamentals of music processing: using {Python} and {Jupyter} notebooks},
	isbn = {978-3-030-69807-2},
	shorttitle = {Fundamentals of music processing},
	language = {eng},
	publisher = {Springer},
	author = {Müller, Meinard},
	year = {2021},
}

@inproceedings{seide_feature_2011,
	address = {Waikoloa, HI, USA},
	title = {Feature engineering in {Context}-{Dependent} {Deep} {Neural} {Networks} for conversational speech transcription},
	isbn = {978-1-4673-0367-5},
	url = {http://ieeexplore.ieee.org/document/6163899/},
	doi = {10.1109/ASRU.2011.6163899},
	urldate = {2022-08-21},
	booktitle = {2011 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} \& {Understanding}},
	publisher = {IEEE},
	author = {Seide, Frank and Li, Gang and Chen, Xie and Yu, Dong},
	month = dec,
	year = {2011},
	note = {ISBN2: 978-1-4673-0365-1
ISBN3: 978-1-4673-0366-8},
	pages = {24--29},
}

@article{dua_developing_2022,
	title = {Developing a {Speech} {Recognition} {System} for {Recognizing} {Tonal} {Speech} {Signals} {Using} a {Convolutional} {Neural} {Network}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/12/6223},
	doi = {10.3390/app12126223},
	abstract = {Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15\% accuracy rate and a 10.56\% WER for continuous and extensive vocabulary sentences of speech signals with different tones.},
	language = {en},
	number = {12},
	urldate = {2022-08-21},
	journal = {Applied Sciences},
	author = {Dua, Sakshi and Kumar, Sethuraman Sambath and Albagory, Yasser and Ramalingam, Rajakumar and Dumka, Ankur and Singh, Rajesh and Rashid, Mamoon and Gehlot, Anita and Alshamrani, Sultan S. and AlGhamdi, Ahmed Saeed},
	month = jun,
	year = {2022},
	pages = {6223},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\97ILRS3G\\Dua et al. - 2022 - Developing a Speech Recognition System for Recogni.pdf:application/pdf},
}

@misc{garofolo_john_s_timit_1993,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST).{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}ul{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.phn" rel="nofollow"{\textgreater}phonemes{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.txt" rel="nofollow"{\textgreater}transcripts{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wav" rel="nofollow"{\textgreater}audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wrd" rel="nofollow"{\textgreater}word list{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}/ul{\textgreater}{\textless}/br{\textgreater}
Portions © 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	year = {1993},
	doi = {10.35111/17GK-BN40},
	note = {Artwork Size: 715776 KB
Pages: 715776 KB
Type: dataset},
}

@article{bell_adaptation_2021,
	title = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}: {An} {Overview}},
	volume = {2},
	issn = {2644-1322},
	shorttitle = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9296327/},
	doi = {10.1109/OJSP.2020.3045349},
	urldate = {2022-08-21},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
	year = {2021},
	pages = {33--66},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\X557RF2Q\\Bell et al. - 2021 - Adaptation Algorithms for Neural Network-Based Spe.pdf:application/pdf},
}

@misc{garofolo_john_s_csr-i_2007,
	title = {{CSR}-{I} ({WSJ0}) {Complete}},
	url = {https://catalog.ldc.upenn.edu/LDC93S6A},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}LDC93S6A - Complete CSR-I corpus {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6B" rel="nofollow"{\textgreater}LDC93S6B{\textless}/a{\textgreater} - CSR-I Sennheiser speech {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6C" rel="nofollow"{\textgreater}LDC93S6C{\textless}/a{\textgreater} - CSR-I other speech{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}During 1991, the DARPA Spoken Language Program initiated efforts to build a new corpus to support research on large-vocabulary Continuous Speech Recognition (CSR) systems.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The first two CSR Corpora consist primarily of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news text and are thus often known as WSJ0 and WSJ1. (Later sections of the CSR set of corpora, however, will consist of read texts from other sources of North American business news and eventually from other news domains).{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The texts to be read were selected to fall within either a 5,000-word or a 20,000-word subset of the WSJ text corpus. (See the documentation for details). Some spontaneous dictation is included in addition to the read speech. The dictation portion was collected using journalists who dictated hypothetical news articles.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}Two microphones are used throughout: a close-talking Sennheiser HMD414 and a secondary microphone, which may vary. The corpora are thus offered in three configurations: the speech from the Sennheiser, the speech from the other microphone and the speech from both; all three sets include all transcriptions, tests, documentation, etc.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}In general, transcriptions of the speech, test data from ARPA evaluations, scores achieved by various speech recognition systems and software used in scoring are included on separate discs from the waveform data.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}Please listen to this {\textless}a href="desc/addenda/LDC93S6A.wav"{\textgreater}audio sample{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}/br{\textgreater}
Portions © 1987-1989 Dow Jones \& Company, Inc., © 1992, 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Graff, David and Paul, Doug and Pallett, David},
	month = may,
	year = {2007},
	doi = {10.35111/EWKM-CG47},
	note = {Artwork Size: 9542041 KB
Pages: 9542041 KB
Type: dataset},
}

@article{li_deng_mnist_2012,
	title = {The {MNIST} {Database} of {Handwritten} {Digit} {Images} for {Machine} {Learning} {Research} [{Best} of the {Web}]},
	volume = {29},
	issn = {1053-5888},
	url = {http://ieeexplore.ieee.org/document/6296535/},
	doi = {10.1109/MSP.2012.2211477},
	number = {6},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {{Li Deng}},
	month = nov,
	year = {2012},
	pages = {141--142},
}

@inproceedings{deng_imagenet_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	urldate = {2022-08-21},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
}

@misc{noauthor_center_nodate,
	title = {Center for {Language} {Engineering}},
	url = {https://cle.org.pk/},
	urldate = {2022-08-22},
	file = {Center for Language Engineering:C\:\\Users\\DELL\\Zotero\\storage\\LV3V3DMG\\cle.org.pk.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub}: {Where} the world builds software},
	shorttitle = {{GitHub}},
	url = {https://github.com/},
	abstract = {GitHub is where over 83 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositories, review code like a pro, track bugs and feat...},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {citation-265913669.bib:C\:\\Users\\DELL\\Zotero\\storage\\YSPPY2BA\\citation-265913669.bib:application/x-bibtex;citation.bib:C\:\\Users\\DELL\\Zotero\\storage\\6H4SBI5D\\citation.bib:application/x-bibtex},
}

@misc{noauthor_kaggle_nodate,
	title = {Kaggle: {Your} {Machine} {Learning} and {Data} {Science} {Community}},
	shorttitle = {Kaggle},
	url = {https://www.kaggle.com/},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	language = {en},
	urldate = {2022-08-22},
}

@article{verma_i-vectors_2015,
	title = {i-{Vectors} in speech processing applications: a survey},
	volume = {18},
	issn = {1381-2416, 1572-8110},
	shorttitle = {i-{Vectors} in speech processing applications},
	url = {http://link.springer.com/10.1007/s10772-015-9295-3},
	doi = {10.1007/s10772-015-9295-3},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {International Journal of Speech Technology},
	author = {Verma, Pulkit and Das, Pradip K.},
	month = dec,
	year = {2015},
	pages = {529--546},
}

@article{joshi_modified_2016,
	title = {Modified {Mean} and {Variance} {Normalization}: {Transforming} to {Utterance}-{Specific} {Estimates}},
	volume = {35},
	issn = {0278-081X, 1531-5878},
	shorttitle = {Modified {Mean} and {Variance} {Normalization}},
	url = {http://link.springer.com/10.1007/s00034-015-0129-y},
	doi = {10.1007/s00034-015-0129-y},
	language = {en},
	number = {5},
	urldate = {2022-08-22},
	journal = {Circuits, Systems, and Signal Processing},
	author = {Joshi, Vikas and Prasad, N. Vishnu and Umesh, S.},
	month = may,
	year = {2016},
	pages = {1593--1609},
}

@book{institute_of_electrical_and_electronics_engineers_2013_2013,
	address = {Piscataway, NJ},
	title = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013): {Olomouc}, {Czech} {Republic}, 8 - 12 {December} 2013},
	isbn = {978-1-4799-2756-2 978-1-4799-2757-9},
	shorttitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013)},
	language = {eng},
	publisher = {IEEE},
	editor = {Institute of Electrical {and} Electronics Engineers},
	year = {2013},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\K3IX7I3E\\Institute of Electrical and Electronics Engineers - 2013 - 2013 IEEE Workshop on Automatic Speech Recognition.pdf:application/pdf},
}

@misc{noauthor_urdu_nodate,
	title = {Urdu {Speech} {Dataset}},
	url = {https://www.kaggle.com/datasets/hazrat/urdu-speech-dataset},
	abstract = {2,500 Urdu audio samples},
	language = {en},
	urldate = {2022-08-22},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\G6KKGYW6\\urdu-speech-dataset.html:text/html},
}

@misc{noauthor_gramvaani_hindi_asrkaldiasr_nodate,
	title = {gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	url = {https://github.com/anish9208/gramvaani_hindi_asr},
	abstract = {This repo contains the baseline model recipes and pre-trained model for GramVanni hindi ASR challenge  - gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\MXCR2LTK\\asr.html:text/html},
}

@inproceedings{peinl_open_2020,
	address = {Deggendorf, Germany},
	title = {Open {Source} {Speech} {Recognition} on {Edge} {Devices}},
	isbn = {978-1-72816-759-6 978-1-72816-760-2},
	url = {https://ieeexplore.ieee.org/document/9208978/},
	doi = {10.1109/ACIT49673.2020.9208978},
	urldate = {2022-08-22},
	booktitle = {2020 10th {International} {Conference} on {Advanced} {Computer} {Information} {Technologies} ({ACIT})},
	publisher = {IEEE},
	author = {Peinl, Rene and Rizk, Basem and Szabad, Robert},
	month = sep,
	year = {2020},
	pages = {441--445},
}

@inproceedings{christian_gaida_comparing_2014,
	title = {Comparing {Open}-{Source} {Speech} {Recognition} {Toolkits}},
	author = {Christian Gaida and P. Lange and Rico Petrick and Patrick Proba and Ahmed Malatawy and David Suendermann-Oeft},
	year = {2014},
}

@misc{noauthor_tdnn_nodate,
	title = {{TDNN} --{\textgreater} {CNN}},
	url = {https://groups.google.com/g/kaldi-help/c/jsg1Oo4bNGQ/m/uwvFw5PtBwAJ},
	urldate = {2022-08-22},
	file = {TDNN --> CNN:C\:\\Users\\DELL\\Zotero\\storage\\X6FFGDN4\\uwvFw5PtBwAJ.html:text/html},
}

@inproceedings{kreyssig_improved_2018,
	address = {Calgary, AB},
	title = {Improved {Tdnns} {Using} {Deep} {Kernels} and {Frequency} {Dependent} {Grid}-{RNNS}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462523/},
	doi = {10.1109/ICASSP.2018.8462523},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kreyssig, F. L. and Zhang, C. and Woodland, P. C.},
	month = apr,
	year = {2018},
	pages = {4864--4868},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\MDGVZMYB\\Kreyssig et al. - 2018 - Improved Tdnns Using Deep Kernels and Frequency De.pdf:application/pdf},
}

@inproceedings{biswas_semi-supervised_2019,
	title = {Semi-{Supervised} {Acoustic} {Model} {Training} for {Five}-{Lingual} {Code}-{Switched} {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html},
	doi = {10.21437/Interspeech.2019-1325},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Biswas, Astik and Yılmaz, Emre and Wet, Febe de and Westhuizen, Ewald van der and Niesler, Thomas},
	month = sep,
	year = {2019},
	pages = {3745--3749},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5DDQYLKT\\Biswas et al. - 2019 - Semi-Supervised Acoustic Model Training for Five-L.pdf:application/pdf},
}

@inproceedings{zorila_investigation_2019,
	address = {SG, Singapore},
	title = {An {Investigation} into the {Effectiveness} of {Enhancement} in {ASR} {Training} and {Test} for {Chime}-5 {Dinner} {Party} {Transcription}},
	isbn = {978-1-72810-306-8},
	url = {https://ieeexplore.ieee.org/document/9003785/},
	doi = {10.1109/ASRU46091.2019.9003785},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Zorila, Catalin and Boeddeker, Christoph and Doddipatla, Rama and Haeb-Umbach, Reinhold},
	month = dec,
	year = {2019},
	pages = {47--53},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\IE9HPYPR\\Zorila et al. - 2019 - An Investigation into the Effectiveness of Enhance.pdf:application/pdf},
}

@article{abdel-hamid_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Speech} {Recognition}},
	volume = {22},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/6857341/},
	doi = {10.1109/TASLP.2014.2339736},
	number = {10},
	urldate = {2022-08-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
	month = oct,
	year = {2014},
	pages = {1533--1545},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\MQX94C3D\\Abdel-Hamid et al. - 2014 - Convolutional Neural Networks for Speech Recogniti.pdf:application/pdf},
}

@article{eeckt_continual_2021,
	title = {Continual {Learning} for {Monolingual} {End}-to-{End} {Automatic} {Speech} {Recognition}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2112.09427},
	doi = {10.48550/ARXIV.2112.09427},
	abstract = {Adapting Automatic Speech Recognition (ASR) models to new domains results in a deterioration of performance on the original domain(s), a phenomenon called Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to new accents, dialects, topics, etc. without suffering from CF, making them unable to be continually enhanced without storing all past data. Fortunately, Continual Learning (CL) methods, which aim to enable continual adaptation while overcoming CF, can be used. In this paper, we implement an extensive number of CL methods for End-to-End ASR and test and compare their ability to extend a monolingual Hybrid CTC-Transformer model across four new tasks. We find that the best performing CL method closes the gap between the fine-tuned model (lower bound) and the model trained jointly on all tasks (upper bound) by more than 40\%, while requiring access to only 0.6\% of the original data.},
	urldate = {2022-08-22},
	author = {Eeckt, Steven Vander and Van hamme, Hugo},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering},
	annote = {Other
Accepted at EUSIPCO 2022. 5 pages, 1 figure},
}

@article{ali_automatic_2015-1,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-22},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@inproceedings{shaik_riyaz_automatic_2019,
	title = {Automatic {Speaker} {Recognition} {System} in {Urdu} using {MFCC} {\textbackslash}\& {HMM}},
	author = {Shaik Riyaz and Bathula Lakshmi Bhavani and S. Venkatrama Phani Kumar},
	year = {2019},
}

@misc{noauthor_federated_nodate,
	title = {Federated {Learning}: {Collaborative} {Machine} {Learning} without {Centralized} {Training} {Data}},
	shorttitle = {Federated {Learning}},
	url = {http://ai.googleblog.com/2017/04/federated-learning-collaborative.html},
	abstract = {Posted by Brendan McMahan and Daniel Ramage, Research Scientists Standard machine learning approaches require centralizing the training data...},
	language = {en},
	urldate = {2022-08-22},
	journal = {Google AI Blog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\CPB7S2RU\\federated-learning-collaborative.html:text/html},
}

@inproceedings{andreas_stolcke_srilm_2002,
	title = {{SRILM} -- {An} xtensible language modeling toolkit},
	url = {https://www.sri.com/platform/srilm/},
	author = {Andreas Stolcke},
	year = {2002},
}

@inproceedings{ali_complete_2014,
	address = {South Lake Tahoe, NV, USA},
	title = {A complete {KALDI} recipe for building {Arabic} speech recognition systems},
	isbn = {978-1-4799-7129-9},
	url = {http://ieeexplore.ieee.org/document/7078629/},
	doi = {10.1109/SLT.2014.7078629},
	urldate = {2022-08-23},
	booktitle = {2014 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Ali, Ahmed and Zhang, Yifan and Cardinal, Patrick and Dahak, Najim and Vogel, Stephan and Glass, James},
	month = dec,
	year = {2014},
	pages = {525--529},
}

@article{amodei_deep_2015-1,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech} 2},
	url = {https://arxiv.org/abs/1512.02595},
	doi = {10.48550/ARXIV.1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-23},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{alhanai_development_2016,
	address = {San Diego, CA},
	title = {Development of the {MIT} {ASR} system for the 2016 {Arabic} {Multi}-genre {Broadcast} {Challenge}},
	isbn = {978-1-5090-4903-5},
	url = {http://ieeexplore.ieee.org/document/7846280/},
	doi = {10.1109/SLT.2016.7846280},
	urldate = {2022-08-23},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {AlHanai, Tuka and Hsu, Wei-Ning and Glass, James},
	month = dec,
	year = {2016},
	pages = {299--304},
}

@phdthesis{meyerjosh_multi-task_2019,
	title = {Multi-{Task} and {Transfer} {Learning} in {Low}-{Resource} {Speech} {Recognition}},
	copyright = {Pro Quest LLC},
	url = {https://www.proquest.com/openview/0d416c7a0cb00a3f6069c467ad545db5/1?pq-origsite=gscholar&cbl=18750&diss=y},
	abstract = {This thesis investigates methods for Acoustic Modeling in Automatic Speech Recognition, assuming limited access to training data in the target domain. The Acoustic
Models of interest are Deep Neural Network Acoustic Models (in both the Hybrid
and End-to-End approaches), and the target domains in question are either different
languages or different speakers. Inductive bias is transfered from a source domain
during training, via Multi-Task Learning or Transfer Learning.
With regards to Multi-Task Learning, Chapter (5) presents experiments which
explicitly incorporate linguistic knowledge (i.e. phonetics and phonology) into an
auxiliary task during neural Acoustic Model training. In Chapter (6), I investigate
Multi-Task methods which do not rely on expert knowledge (linguistic or otherwise),
by re-using existing parts of the Hybrid training pipeline. In Chapter (7), new tasks
are discovered using unsupervised learning. In Chapter (8), using the “copy-paste”
Transfer Learning approach, I demonstrate that with an appropriate early-stopping
criteria, cross-lingual transfer is possible to both large and small target datasets.
The methods and intuitions which rely on linguistic knowledge are of interest to
the Speech Recognition practitioner working in low-resource domains. These same
sections may be of interest to the theoretical linguist, as a study of the relative import
of phonetic categories in classification. To the Machine Learning practitioner, I hope
to offer approaches which can be easily ported over to other classification tasks. To
the Machine Learning researcher, I hope to inspire new ideas on addressing the small
data problem.},
	language = {en},
	urldate = {2022-08-23},
	school = {University of Arizona},
	author = {Meyer,Josh},
	year = {2019},
	note = {http://jrmeyer.github.io/misc/MEYER\_dissertation\_2019.pdf
Published by Pro Quest LLC},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\IWJEWKS8\\1.html:text/html},
}

@book{international_speech_communication_association_speech_2016,
	address = {Red Hook, NY},
	title = {Speech beyond speech towards a better understanding of the most important biosignal: 16th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({INTERSPEECH} 2015): {Dresden}, {Germany}, 6-10 {September} 2015. {Volume} 5},
	volume = {5},
	isbn = {978-1-5108-1790-6},
	shorttitle = {Speech beyond speech towards a better understanding of the most important biosignal},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {International Speech Communication Association},
	year = {2016},
	note = {Meeting Name: International Speech Communication Association},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\QHEKNZCX\\International Speech Communication Association - 2016 - Speech beyond speech towards a better understandin.pdf:application/pdf},
}

@inproceedings{shaik_improvements_2015,
	title = {Improvements in {RWTH} {LVCSR} evaluation systems for {Polish}, {Portuguese}, {English}, urdu, and {Arabic}},
	url = {https://www.isca-speech.org/archive/interspeech_2015/shaik15_interspeech.html},
	doi = {10.21437/Interspeech.2015-635},
	language = {en},
	urldate = {2022-08-23},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Shaik, M. Ali Basha and Tüske, Zoltán and Tahir, M. Ali and Nußbaum-Thom, Markus and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2015},
	pages = {3154--3158},
}

@article{kumar_large-vocabulary_2004,
	title = {A large-vocabulary continuous speech recognition system for {Hindi}},
	volume = {48},
	issn = {0018-8646, 0018-8646},
	url = {http://ieeexplore.ieee.org/document/5388836/},
	doi = {10.1147/rd.485.0703},
	number = {5.6},
	urldate = {2022-08-23},
	journal = {IBM Journal of Research and Development},
	author = {Kumar, M. and Rajput, N. and Verma, A.},
	month = sep,
	year = {2004},
	pages = {703--715},
}

@article{ming_speech_2017,
	title = {Speech {Enhancement} {Based} on {Full}-{Sentence} {Correlation} and {Clean} {Speech} {Recognition}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7814226/},
	doi = {10.1109/TASLP.2017.2651406},
	number = {3},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Ming, Ji and Crookes, Danny},
	month = mar,
	year = {2017},
	pages = {531--543},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\FKM8TYUH\\Ming and Crookes - 2017 - Speech Enhancement Based on Full-Sentence Correlat.pdf:application/pdf},
}

@article{ganapathy_multivariate_2017,
	title = {Multivariate {Autoregressive} {Spectrogram} {Modeling} for {Noisy} {Speech} {Recognition}},
	volume = {24},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7973047/},
	doi = {10.1109/LSP.2017.2724561},
	number = {9},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Ganapathy, Sriram},
	month = sep,
	year = {2017},
	pages = {1373--1377},
}

@article{lee_dnn-based_2016,
	title = {{DNN}-{Based} {Feature} {Enhancement} {Using} {DOA}-{Constrained} {ICA} for {Robust} {Speech} {Recognition}},
	volume = {23},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7497454/},
	doi = {10.1109/LSP.2016.2583658},
	number = {8},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Lee, Ho-Yong and Cho, Ji-Won and Kim, Minook and Park, Hyung-Min},
	month = aug,
	year = {2016},
	pages = {1091--1095},
}

@article{lee_threshold-based_2018,
	title = {Threshold-{Based} {Noise} {Detection} and {Reduction} for {Automatic} {Speech} {Recognition} {System} in {Human}-{Robot} {Interactions}},
	volume = {18},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/18/7/2068},
	doi = {10.3390/s18072068},
	language = {en},
	number = {7},
	urldate = {2022-08-23},
	journal = {Sensors},
	author = {Lee, Sheng-Chieh and Wang, Jhing-Fa and Chen, Miao-Hia},
	month = jun,
	year = {2018},
	pages = {2068},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TB36Q4AT\\Lee et al. - 2018 - Threshold-Based Noise Detection and Reduction for .pdf:application/pdf},
}

@article{gosztolya_domain_2016,
	title = {Domain {Adaptation} of {Deep} {Neural} {Networks} for {Automatic} {Speech} {Recognition} via {Wireless} {Sensors}},
	volume = {67},
	issn = {1339-309X},
	url = {https://www.sciendo.com/article/10.1515/jee-2016-0017},
	doi = {10.1515/jee-2016-0017},
	abstract = {Abstract
            Wireless sensors are recent, portable, low-powered devices, designed to record and transmit observations of their environment such as speech. To allow portability they are designed to have a small size and weight; this, however, along with their low power consumption, usually means that they have only quite basic recording equipment (e.g. microphone) installed. Recent speech technology applications typically require several dozen hours of audio recordings (nowadays even hundreds of hours is common), which is usually not available as recorded material by such sensors. Since systems trained with studio-level utterances tend to perform suboptimally for such recordings, a sensible idea is to adapt models which were trained on existing, larger, noise-free corpora. In this study, we experimented with adapting Deep Neural Network-based acoustic models trained on noise-free speech data to perform speech recognition on utterances recorded by wireless sensors. In the end, we were able to achieve a 5\% gain in terms of relative error reduction compared to training only on the sensor-recorded, restricted utterance subset.},
	language = {en},
	number = {2},
	urldate = {2022-08-23},
	journal = {Journal of Electrical Engineering},
	author = {Gosztolya, Gábor and Grósz, Tamás},
	month = apr,
	year = {2016},
	pages = {124--130},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CSYDKLJL\\Gosztolya and Grósz - 2016 - Domain Adaptation of Deep Neural Networks for Auto.pdf:application/pdf},
}

@article{chen_progressive_2018,
	title = {Progressive {Joint} {Modeling} in {Unsupervised} {Single}-{Channel} {Overlapped} {Speech} {Recognition}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/8080252/},
	doi = {10.1109/TASLP.2017.2765834},
	number = {1},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Chen, Zhehuai and Droppo, Jasha and Li, Jinyu and Xiong, Wayne},
	month = jan,
	year = {2018},
	pages = {184--196},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U75U9AZK\\Chen et al. - 2018 - Progressive Joint Modeling in Unsupervised Single-.pdf:application/pdf},
}

@misc{dautume_episodic_2019,
	title = {Episodic {Memory} in {Lifelong} {Language} {Learning}},
	url = {http://arxiv.org/abs/1906.01076},
	abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\textasciitilde}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {d'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
	month = nov,
	year = {2019},
	note = {arXiv:1906.01076 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Proceedings of NeurIPS 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\F68HIZ99\\d'Autume et al. - 2019 - Episodic Memory in Lifelong Language Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\U9A7WITC\\1906.html:text/html},
}

@misc{chang_towards_2021,
	title = {Towards {Lifelong} {Learning} of {End}-to-end {ASR}},
	url = {http://arxiv.org/abs/2104.01616},
	abstract = {Automatic speech recognition (ASR) technologies today are primarily optimized for given datasets; thus, any changes in the application environment (e.g., acoustic conditions or topic domains) may inevitably degrade the performance. We can collect new data describing the new environment and fine-tune the system, but this naturally leads to higher error rates for the earlier datasets, referred to as catastrophic forgetting. The concept of lifelong learning (LLL) aiming to enable a machine to sequentially learn new tasks from new datasets describing the changing real world without forgetting the previously learned knowledge is thus brought to attention. This paper reports, to our knowledge, the first effort to extensively consider and analyze the use of various approaches of LLL in end-to-end (E2E) ASR, including proposing novel methods in saving data for past domains to mitigate the catastrophic forgetting problem. An overall relative reduction of 28.7\% in WER was achieved compared to the fine-tuning baseline when sequentially learning on three very different benchmark corpora. This can be the first step toward the highly desired ASR technologies capable of synchronizing with the continuously changing real world.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Chang, Heng-Jui and Lee, Hung-yi and Lee, Lin-shan},
	month = jul,
	year = {2021},
	note = {arXiv:2104.01616 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Interspeech 2021. We acknowledge the support of Salesforce Research Deep Learning Grant},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\NZP72WVC\\Chang et al. - 2021 - Towards Lifelong Learning of End-to-end ASR.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\PTMWQGE5\\2104.html:text/html},
}

@misc{yang_online_2022,
	title = {Online {Continual} {Learning} of {End}-to-{End} {Speech} {Recognition} {Models}},
	url = {http://arxiv.org/abs/2207.05071},
	abstract = {Continual Learning, also known as Lifelong Learning, aims to continually learn from new data as it becomes available. While prior research on continual learning in automatic speech recognition has focused on the adaptation of models across multiple different speech recognition tasks, in this paper we propose an experimental setting for {\textbackslash}textit\{online continual learning\} for automatic speech recognition of a single task. Specifically focusing on the case where additional training data for the same task becomes available incrementally over time, we demonstrate the effectiveness of performing incremental model updates to end-to-end speech recognition models with an online Gradient Episodic Memory (GEM) method. Moreover, we show that with online continual learning and a selective sampling strategy, we can maintain an accuracy that is similar to retraining a model from scratch while requiring significantly lower computation costs. We have also verified our method with self-supervised learning (SSL) features.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Yang, Muqiao and Lane, Ian and Watanabe, Shinji},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05071 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at InterSpeech 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D5PLRPCB\\Yang et al. - 2022 - Online Continual Learning of End-to-End Speech Rec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Q2KHNKDV\\2207.html:text/html},
}

@article{kumar_leveraging_2020,
	title = {Leveraging {Linguistic} {Context} in {Dyadic} {Interactions} to {Improve} {Automatic} {Speech} {Recognition} for {Children}},
	volume = {63},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300346},
	doi = {10.1016/j.csl.2020.101101},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Lyon, Thomas D. and Narayanan, Shrikanth},
	month = sep,
	year = {2020},
	pages = {101101},
}

@article{pironkov_hybrid-task_2020,
	title = {Hybrid-task learning for robust automatic speech recognition},
	volume = {64},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S088523082030036X},
	doi = {10.1016/j.csl.2020.101103},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Pironkov, Gueorgui and Wood, Sean UN and Dupont, Stéphane},
	month = nov,
	year = {2020},
	pages = {101103},
}

@article{wang_wavenet_2020,
	title = {{WaveNet} {With} {Cross}-{Attention} for {Audiovisual} {Speech} {Recognition}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9197622/},
	doi = {10.1109/ACCESS.2020.3024218},
	urldate = {2022-08-23},
	journal = {IEEE Access},
	author = {Wang, Hui and Gao, Fei and Zhao, Yue and Wu, Licheng},
	year = {2020},
	pages = {169160--169168},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\FRECEY55\\Wang et al. - 2020 - WaveNet With Cross-Attention for Audiovisual Speec.pdf:application/pdf},
}

@inproceedings{sarfraz_huda_speech_2016,
	address = {Khatmandu, Nepal},
	title = {Speech {Corpus} {Development} for a {Speaker} {Independent} {Spontaneous} {Urdu} {Speech} {Recognition} {System}},
	booktitle = {Proceedings of the {O}-{COCOSDA}},
	author = {Sarfraz, Huda and Hussain, Sarmad and Bokhari, Riffat and Agha, Ali and Agha Ali Raza and Inamullah and Sarfaraz, Zahid and Parvez, Sophia and Mustafa, Asad and Javed, Iqra and Parveen, Raheela},
	month = mar,
	year = {2016},
}

@misc{mozilla_deep_nodate,
	title = {Deep {Speech} {Documentation}},
	url = {https://deepspeech.readthedocs.io/en/r0.9/?badge=latest},
	urldate = {2022-08-23},
	author = {Mozilla},
}

@inproceedings{wang_application_2015,
	address = {Beijing},
	title = {The {Application} of {Data} {Mining} {Technology} for the {Judgment} of {Poisoning} {Cases}},
	isbn = {978-1-4673-7211-4},
	url = {https://ieeexplore.ieee.org/document/7518465/},
	doi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.284},
	urldate = {2022-08-24},
	booktitle = {2015 {IEEE} 12th {Intl} {Conf} on {Ubiquitous} {Intelligence} and {Computing} and 2015 {IEEE} 12th {Intl} {Conf} on {Autonomic} and {Trusted} {Computing} and 2015 {IEEE} 15th {Intl} {Conf} on {Scalable} {Computing} and {Communications} and {Its} {Associated} {Workshops} ({UIC}-{ATC}-{ScalCom})},
	publisher = {IEEE},
	author = {Wang, Jiong and Zhang, Yunfeng and Wang, Fanglin and Gao, Bin},
	month = aug,
	year = {2015},
	pages = {1567--1571},
}

@article{zhao_data_2022,
	title = {Data {Poisoning} {Attacks} and {Defenses} in {Dynamic} {Crowdsourcing} with {Online} {Data} {Quality} {Learning}},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/9640529/},
	doi = {10.1109/TMC.2021.3133365},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Zhao, Yuxi and Gong, Xiaowen and Lin, Fuhong and Chen, Xu},
	year = {2022},
	pages = {1--1},
}

@inproceedings{hu_data_2020,
	title = {Data {Poisoning} on {Deep} {Learning} {Models}},
	doi = {10.1109/CSCI51800.2020.00111},
	abstract = {Deep learning is a form of artificial intelligence (AI) that has seen rapid development and deployment in computer software as a means to implementing AI functionality with greater efficiency and ease as compared to other alternative AI solutions, with usage seen in systems varying from search and recommendation engines to autonomous vehicles. With the demand for deep learning algorithms that can perform increasingly complex tasks in a shorter time frame growing at an exponential pace, the developments in the efficiency and productivity of algorithms has far outpaced that of the security of such algorithms, drawing concerns over the many unaddressed vulnerabilities that may be exploited to compromise the integrity of these software. This study investigated the ability of poisoning attacks, a form of attack targeting the vulnerability of deep learning training data, to compromise the integrity of a deep learning model's classificational functionality. Experimentation involved the processing of training data sets with varying deep learning models and the incremental introduction of poisoned data sets to view the efficacy of a poisoning attack under multiple circumstances and correlate such with aspects of the model's design conditions. Analysis of results showed evidence of a decrease of classificational ability correlating with an increase of poison percentage in the training data sets, but the scale of which the decrease occurred varied with the specified parameters in the model design. Based on this, it was concluded that poisoning can provide varying levels of damage to deep learning classificational ability depending on the parameters utilized in the model design, and methods to countermeasure such were proposed, such as increasing epoch count, implementing mechanisms bolstering model fit, and integrating input level filtration systems.},
	booktitle = {2020 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Hu, Charles and Hu, Yen-Hung Frank},
	month = dec,
	year = {2020},
	keywords = {Software, Computational modeling, Data models, artificial intelligence, machine learning, data poisoning, deep learning, Deep learning, Software algorithms, Toxicology, Training data},
	pages = {628--632},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\EU5RHS82\\9457922.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\2MNURGVH\\Hu and Hu - 2020 - Data Poisoning on Deep Learning Models.pdf:application/pdf},
}

@inproceedings{uprety_mitigating_2021,
	title = {Mitigating {Poisoning} {Attack} in {Federated} {Learning}},
	doi = {10.1109/SSCI50451.2021.9659839},
	abstract = {Adversarial machine learning (AML) has emerged as one of the significant research areas in machine learning (ML) because models we train lack robustness and trustworthiness. Federated learning (FL) trains models over distributed devices and model parameters are shared instead of actual data in a privacy-preserving manner. Unfortunately, FL is also vulnerable to attacks including parameter/data poisoning attacks. In this paper, we first analyze the impact of the data poisoning attack on this training method with a label-flipping attack. We propose a poisoning attack mitigation technique based on the reputation of nodes' involved in the training process. The reputation score for each client is calculated using the beta probability distribution method. This is the first work to show the removal of malicious nodes with the poisoned dataset from the training environment based on the calculated reputation score. The improvement in model performance after filtering malicious nodes is validated using the benchmark MNIST dataset. At the same time, our work contributes to preventing denial of service attacks by considering a blockchain-based server network. Our results hold for two different attack settings with different proportions of poisoned data samples.},
	booktitle = {2021 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Uprety, Aashma and Rawat, Danda B.},
	month = dec,
	year = {2021},
	keywords = {Computational modeling, Training, Data models, Collaborative work, Data poisoning attack, Data privacy, Distance learning, Filtering, reputation model, secure federated learning},
	pages = {01--07},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PM3PPZV8\\Uprety and Rawat - 2021 - Mitigating Poisoning Attack in Federated Learning.pdf:application/pdf},
}

@inproceedings{seetharaman_influence_2022,
	title = {Influence {Based} {Defense} {Against} {Data} {Poisoning} {Attacks} in {Online} {Learning}},
	doi = {10.1109/COMSNETS53615.2022.9668557},
	abstract = {Data poisoning is a type of adversarial attack on training data where an attacker manipulates a fraction of data to degrade the performance of machine learning model. There are several known defensive mechanisms for handling offline attacks, however defensive measures for online learning, where data points arrive sequentially, have not garnered similar interest. In this work, we propose a defense mechanism to minimize the degradation caused by the poisoned training data on a learner's model in an online setup. Our proposed method utilizes an influence function which is a classic technique in robust statistics. Further, we supplement it with the existing data sanitization methods for filtering out some of the poisoned data points. We study the effectiveness of our defense mechanism on multiple datasets and across multiple attack strategies against an online learner.},
	booktitle = {2022 14th {International} {Conference} on {COMmunication} {Systems} \& {NETworkS} ({COMSNETS})},
	author = {Seetharaman, Sanjay and Malaviya, Shubham and Vasu, Rosni and Shukla, Manish and Lodha, Sachin},
	month = jan,
	year = {2022},
	note = {ISSN: 2155-2509},
	keywords = {Data models, Data integrity, Training data, Filtering, Adversarial Machine Learning, Data Poisoning, Degradation, Influence Function, Linear programming, Machine learning, Online Learning},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\QI6QPWYV\\Seetharaman et al. - 2022 - Influence Based Defense Against Data Poisoning Att.pdf:application/pdf},
}

@article{zhao_garbage_2021,
	title = {Garbage {In}, {Garbage} {Out}: {Poisoning} {Attacks} {Disguised} {With} {Plausible} {Mobility} in {Data} {Aggregation}},
	volume = {8},
	issn = {2327-4697, 2334-329X},
	shorttitle = {Garbage {In}, {Garbage} {Out}},
	url = {https://ieeexplore.ieee.org/document/9511094/},
	doi = {10.1109/TNSE.2021.3103919},
	number = {3},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Zhao, Ping and Jiang, Hongbo and Li, Jie and Xiao, Zhu and Liu, Daibo and Ren, Ju and Guo, Deke},
	month = jul,
	year = {2021},
	pages = {2679--2693},
}

@inproceedings{franci_influence-driven_2022,
	title = {Influence-{Driven} {Data} {Poisoning} in {Graph}-{Based} {Semi}-{Supervised} {Classifiers}},
	abstract = {Graph-based Semi-Supervised Learning (GSSL) is a practical solution to learn from a limited amount of labelled data together with a vast amount of unlabelled data. However, due to their reliance on the known labels to infer the unknown labels, these algorithms are sensitive to data quality. It is therefore essential to study the potential threats related to the labelled data, more specifically, label poisoning. In this paper, we propose a novel data poisoning method which efficiently approximates the result of label inference to identify the inputs which, if poisoned, would produce the highest number of incorrectly inferred labels. We extensively evaluate our approach on three classification problems under 24 different experimental settings each. Compared to the state of the art, our influence-driven attack produces an average increase of error rate 50\% higher, while being faster by multiple orders of magnitude. Moreover, our method can inform engineers of inputs that deserve investigation (relabelling them) before training the learning model. We show that relabelling one-third of the poisoned inputs (selected based on their influence) reduces the poisoning effect by 50\%. ACM Reference Format: Adriano Franci, Maxime Cordy, Martin Gubri, Mike Papadakis, and Yves Le Traon. 2022. Influence-Driven Data Poisoning in Graph-Based Semi-Supervised Classifiers. In 1st Conference on AI Engineering - Software Engineering for AI (CAIN’22), May 16–24, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3522664.3528606},
	booktitle = {2022 {IEEE}/{ACM} 1st {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Franci, Adriano and Cordy, Maxime and Gubri, Martin and Papadakis, Mike and Traon, Yves Le},
	month = may,
	year = {2022},
	keywords = {Training, Measurement, Data integrity, data poisoning, Machine learning, Approximation algorithms, Error analysis, Inference algorithms, semi-supervised learning, Semisupervised learning},
	pages = {77--87},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\J9JGMGKF\\Franci et al. - 2022 - Influence-Driven Data Poisoning in Graph-Based Sem.pdf:application/pdf},
}

@article{zhang_poisongan_2021,
	title = {{PoisonGAN}: {Generative} {Poisoning} {Attacks} {Against} {Federated} {Learning} in {Edge} {Computing} {Systems}},
	volume = {8},
	issn = {2327-4662, 2372-2541},
	shorttitle = {{PoisonGAN}},
	url = {https://ieeexplore.ieee.org/document/9194010/},
	doi = {10.1109/JIOT.2020.3023126},
	number = {5},
	urldate = {2022-08-24},
	journal = {IEEE Internet of Things Journal},
	author = {Zhang, Jiale and Chen, Bing and Cheng, Xiang and Binh, Huynh Thi Thanh and Yu, Shui},
	month = mar,
	year = {2021},
	pages = {3310--3322},
}

@inproceedings{doku_mitigating_2021,
	address = {Las Vegas, NV, USA},
	title = {Mitigating {Data} {Poisoning} {Attacks} {On} a {Federated} {Learning}-{Edge} {Computing} {Network}},
	isbn = {978-1-72819-794-4},
	url = {https://ieeexplore.ieee.org/document/9369581/},
	doi = {10.1109/CCNC49032.2021.9369581},
	urldate = {2022-08-24},
	booktitle = {2021 {IEEE} 18th {Annual} {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	publisher = {IEEE},
	author = {Doku, Ronald and Rawat, Danda B.},
	month = jan,
	year = {2021},
	pages = {1--6},
}

@article{wen_great_2021,
	title = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}: {Efficient} {Poisoning} {Attacks} and {Defenses} for {Linear} {Regression} {Models}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}},
	url = {https://ieeexplore.ieee.org/document/9448089/},
	doi = {10.1109/TIFS.2021.3087332},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Wen, Jialin and Zhao, Benjamin Zi Hao and Xue, Minhui and Oprea, Alina and Qian, Haifeng},
	year = {2021},
	pages = {3709--3723},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\PWEN89AG\\Wen et al. - 2021 - With Great Dispersion Comes Greater Resilience Ef.pdf:application/pdf},
}

@article{chen_-pois_2021,
	title = {De-{Pois}: {An} {Attack}-{Agnostic} {Defense} against {Data} {Poisoning} {Attacks}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {De-{Pois}},
	url = {https://ieeexplore.ieee.org/document/9431105/},
	doi = {10.1109/TIFS.2021.3080522},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Chen, Jian and Zhang, Xuxin and Zhang, Rui and Wang, Chen and Liu, Ling},
	year = {2021},
	pages = {3412--3425},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\AK6SW628\\Chen et al. - 2021 - De-Pois An Attack-Agnostic Defense against Data P.pdf:application/pdf},
}

@inproceedings{kwon_selective_2019,
	address = {Sardinia, Italy},
	title = {Selective {Poisoning} {Attack} on {Deep} {Neural} {Network} to {Induce} {Fine}-{Grained} {Recognition} {Error}},
	isbn = {978-1-72811-488-0},
	url = {https://ieeexplore.ieee.org/document/8791700/},
	doi = {10.1109/AIKE.2019.00033},
	urldate = {2022-08-24},
	booktitle = {2019 {IEEE} {Second} {International} {Conference} on {Artificial} {Intelligence} and {Knowledge} {Engineering} ({AIKE})},
	publisher = {IEEE},
	author = {Kwon, Hyun and Yoon, Hyunsoo and Park, Ki-Woong},
	month = jun,
	year = {2019},
	pages = {136--139},
}

@inproceedings{kontopoulos_countering_2018,
	address = {Athens},
	title = {Countering {Real}-{Time} {Stream} {Poisoning}: {An} {Architecture} for {Detecting} {Vessel} {Spoofing} in {Streams} of {AIS} {Data}},
	isbn = {978-1-5386-7518-2},
	shorttitle = {Countering {Real}-{Time} {Stream} {Poisoning}},
	url = {https://ieeexplore.ieee.org/document/8512006/},
	doi = {10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00139},
	urldate = {2022-08-24},
	booktitle = {2018 {IEEE} 16th {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, 16th {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, 4th {Intl} {Conf} on {Big} {Data} {Intelligence} and {Computing} and {Cyber} {Science} and {Technology} {Congress}({DASC}/{PiCom}/{DataCom}/{CyberSciTech})},
	publisher = {IEEE},
	author = {Kontopoulos, Ioannis and Spiliopoulos, Giannis and Zissis, Dimitrios and Chatzikokolakis, Konstantinos and Artikis, Alexander},
	month = aug,
	year = {2018},
	pages = {981--986},
}

@misc{noauthor_adoption_nodate,
	title = {Adoption of {AI} advances, but foundational barriers remain {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/featured-insights/artificial-intelligence/ai-adoption-advances-but-foundational-barriers-remain},
	urldate = {2022-08-24},
	file = {Adoption of AI advances, but foundational barriers remain | McKinsey:C\:\\Users\\DELL\\Zotero\\storage\\7BXXNTS6\\ai-adoption-advances-but-foundational-barriers-remain.html:text/html},
}

@misc{noauthor_exclusive_nodate,
	title = {Exclusive: {What} is data poisoning and why should we be concerned? - {International} {Security} {Journal} ({ISJ})},
	shorttitle = {Exclusive},
	url = {https://internationalsecurityjournal.com/what-is-data-poisoning/},
	abstract = {Machine learning could be one of the most disruptive technologies the world has seen in decades. Virtually every industry can benefit from these artificial},
	language = {en-GB},
	urldate = {2022-08-24},
	note = {Section: AI \& Deep Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VYNPR2KK\\what-is-data-poisoning.html:text/html},
}

@misc{vincent_twitter_2016,
	title = {Twitter taught {Microsoft}’s friendly {AI} chatbot to be a racist asshole in less than a day},
	url = {https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist},
	abstract = {It took less than 24 hours for Twitter to corrupt an innocent AI chatbot. Yesterday, Microsoft unveiled Tay — a Twitter bot that the company described as an experiment in "conversational...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Vincent, James},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5ZCPBDM9\\tay-microsoft-chatbot-racist.html:text/html},
}

@misc{kastrenakes_microsoft_2016,
	title = {Microsoft made a chatbot that tweets like a teen},
	url = {https://www.theverge.com/2016/3/23/11290200/tay-ai-chatbot-released-microsoft},
	abstract = {Microsoft is trying to create AI that can pass for a teen. Its research team launched a chatbot this morning called Tay, which is meant to test and improve Microsoft's understanding of...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Kastrenakes, Jacob},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SJ6QZ3R3\\tay-ai-chatbot-released-microsoft.html:text/html},
}

@article{steinhardt_certified_2017,
	title = {Certified {Defenses} for {Data} {Poisoning} {Attacks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03691},
	doi = {10.48550/ARXIV.1706.03691},
	abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.},
	urldate = {2022-08-24},
	author = {Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
	annote = {Other
Appeared at NIPS 2017},
}

@article{newman_ai_nodate,
	title = {{AI} {Can} {Help} {Cybersecurity}—{If} {It} {Can} {Fight} {Through} the {Hype}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/ai-machine-learning-cybersecurity/},
	abstract = {There are a ton of claims around AI and cybersecurity that don't quite add up. Here's what's really going on.},
	language = {en-US},
	urldate = {2022-08-24},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {artificial intelligence, machine learning, ai, cybersecurity},
}

@misc{ilmoi_evasion_2019,
	title = {Evasion attacks on {Machine} {Learning} (or “{Adversarial} {Examples}”)},
	url = {https://towardsdatascience.com/evasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1},
	abstract = {Your ML model is easier to fool than you think},
	language = {en},
	urldate = {2022-08-24},
	journal = {Medium},
	author = {ilmoi},
	month = jul,
	year = {2019},
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	copyright = {Creative Commons Attribution 3.0 Unported},
	url = {https://arxiv.org/abs/1312.6199},
	doi = {10.48550/ARXIV.1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2022-08-24},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV)},
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1412.6572},
	doi = {10.48550/ARXIV.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2022-08-24},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{bursztein_attacks_nodate,
	title = {Attacks against machine learning — an overview},
	url = {https://elie.net/blog/ai/attacks-against-machine-learning-an-overview/},
	abstract = {This blog post surveys the attacks techniques that target AI (Artificial Intelligence) systems and how to protect against them.},
	language = {en},
	urldate = {2022-08-24},
	journal = {Elie Bursztein's site},
	author = {Bursztein, Elie},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P5FGBPU5\\attacks-against-machine-learning-an-overview.html:text/html},
}

@article{goh_comprehensive_2015,
	title = {Comprehensive {Literature} {Review} on {Machine} {Learning} {Structures} for {Web} {Spam} {Classification}},
	volume = {70},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915032330},
	doi = {10.1016/j.procs.2015.10.069},
	language = {en},
	urldate = {2022-08-24},
	journal = {Procedia Computer Science},
	author = {Goh, Kwang Leng and Singh, Ashutosh Kumar},
	year = {2015},
	pages = {434--441},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ZZY6ZDLK\\Goh and Singh - 2015 - Comprehensive Literature Review on Machine Learnin.pdf:application/pdf},
}

@article{jo_measuring_2017,
	title = {Measuring the tendency of {CNNs} to {Learn} {Surface} {Statistical} {Regularities}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.11561},
	doi = {10.48550/ARXIV.1711.11561},
	abstract = {Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset. We are concerned with the following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities. For the SVHN and CIFAR-10 datasets, we present two Fourier filtered variants: a low frequency variant and a randomly filtered variant. Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects. Our main finding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap across the various test sets. Moreover, we observe that significantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap. Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts.},
	urldate = {2022-08-24},
	author = {Jo, Jason and Bengio, Yoshua},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	annote = {Other
Submitted},
}

@inproceedings{florian_tramer_stealing_2016,
	address = {Austin, Texas, USA},
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	isbn = {978-1-931971-32-4},
	url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer},
	urldate = {2022-08-24},
	publisher = {USENIX Association},
	author = {Florian Tramer and Fan Zhang and Ari Juels and Michael K. Reiter and Thomas Ristenpart},
	month = aug,
	year = {2016},
	pages = {601--618},
	file = {Stealing Machine Learning Models via Prediction APIs | USENIX:C\:\\Users\\DELL\\Zotero\\storage\\KYVFLJL7\\tramer.html:text/html},
}

@misc{papernot_semi-supervised_2017,
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	url = {http://arxiv.org/abs/1610.05755},
	abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
	month = mar,
	year = {2017},
	note = {arXiv:1610.05755 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted to ICLR 17 as an oral},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\B2VJHRWF\\Papernot et al. - 2017 - Semi-supervised Knowledge Transfer for Deep Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\JK48PTLF\\1610.html:text/html},
}

@misc{papernot_scalable_2018,
	title = {Scalable {Private} {Learning} with {PATE}},
	url = {http://arxiv.org/abs/1802.08908},
	abstract = {The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a "student" model the knowledge of an ensemble of "teacher" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy (\${\textbackslash}varepsilon\$ {\textless} 1.0).},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Úlfar},
	month = feb,
	year = {2018},
	note = {arXiv:1802.08908 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Published as a conference paper at ICLR 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Y9TDCBWP\\Papernot et al. - 2018 - Scalable Private Learning with PATE.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L8FAK7HV\\1802.html:text/html},
}

@article{wang_poisoning_2022,
	title = {Poisoning attacks and countermeasures in intelligent networks: {Status} quo and prospects},
	volume = {8},
	issn = {23528648},
	shorttitle = {Poisoning attacks and countermeasures in intelligent networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S235286482100050X},
	doi = {10.1016/j.dcan.2021.07.009},
	language = {en},
	number = {2},
	urldate = {2022-08-24},
	journal = {Digital Communications and Networks},
	author = {Wang, Chen and Chen, Jian and Yang, Yang and Ma, Xiaoqiang and Liu, Jiangchuan},
	month = apr,
	year = {2022},
	pages = {225--234},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5924FJWC\\Wang et al. - 2022 - Poisoning attacks and countermeasures in intellige.pdf:application/pdf},
}

@misc{noauthor_google_nodate,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/emergency-response/},
	urldate = {2022-08-24},
	file = {Google - Site Reliability Engineering:C\:\\Users\\DELL\\Zotero\\storage\\27RWN7KZ\\emergency-response.html:text/html},
}

@misc{noauthor_google_nodate-1,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/managing-incidents/},
	urldate = {2022-08-24},
}

@misc{gaudesi_channelaugment_2021,
	title = {{ChannelAugment}: {Improving} generalization of multi-channel {ASR} by training with input channel randomization},
	shorttitle = {{ChannelAugment}},
	url = {http://arxiv.org/abs/2109.11225},
	abstract = {End-to-end (E2E) multi-channel ASR systems show state-of-the-art performance in far-field ASR tasks by joint training of a multi-channel front-end along with the ASR model. The main limitation of such systems is that they are usually trained with data from a fixed array geometry, which can lead to degradation in accuracy when a different array is used in testing. This makes it challenging to deploy these systems in practice, as it is costly to retrain and deploy different models for various array configurations. To address this, we present a simple and effective data augmentation technique, which is based on randomly dropping channels in the multi-channel audio input during training, in order to improve the robustness to various array configurations at test time. We call this technique ChannelAugment, in contrast to SpecAugment (SA) which drops time and/or frequency components of a single channel input audio. We apply ChannelAugment to the Spatial Filtering (SF) and Minimum Variance Distortionless Response (MVDR) neural beamforming approaches. For SF, we observe 10.6\% WER improvement across various array configurations employing different numbers of microphones. For MVDR, we achieve a 74\% reduction in training time without causing degradation of recognition accuracy.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Gaudesi, Marco and Weninger, Felix and Sharma, Dushyant and Zhan, Puming},
	month = sep,
	year = {2021},
	note = {arXiv:2109.11225 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	annote = {Comment: To appear in ASRU 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D2RMIMH8\\Gaudesi et al. - 2021 - ChannelAugment Improving generalization of multi-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4F2SMWT4\\2109.html:text/html},
}

@article{lin_ml_2021,
	title = {{ML} {Attack} {Models}: {Adversarial} {Attacks} and {Data} {Poisoning} {Attacks}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{ML} {Attack} {Models}},
	url = {https://arxiv.org/abs/2112.02797},
	doi = {10.48550/ARXIV.2112.02797},
	abstract = {Many state-of-the-art ML models have outperformed humans in various tasks such as image classification. With such outstanding performance, ML models are widely used today. However, the existence of adversarial attacks and data poisoning attacks really questions the robustness of ML models. For instance, Engstrom et al. demonstrated that state-of-the-art image classifiers could be easily fooled by a small rotation on an arbitrary image. As ML systems are being increasingly integrated into safety and security-sensitive applications, adversarial attacks and data poisoning attacks pose a considerable threat. This chapter focuses on the two broad and important areas of ML security: adversarial attacks and data poisoning attacks.},
	urldate = {2022-08-24},
	author = {Lin, Jing and Dang, Long and Rahouti, Mohamed and Xiong, Kaiqi},
	year = {2021},
	note = {Publisher: arXiv Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
}

@misc{noauthor_why_nodate,
	title = {Why does kaldi require mono channel audio for training instead of stereo or surround?},
	url = {https://groups.google.com/g/kaldi-help/c/92-jEzqyNb4},
	urldate = {2022-08-24},
	file = {Why does kaldi require mono channel audio for training instead of stereo or surround?:C\:\\Users\\DELL\\Zotero\\storage\\8P3Y6TTD\\92-jEzqyNb4.html:text/html},
}

@misc{noauthor_kaldi_nodate,
	title = {Kaldi: {Data} preparation},
	url = {https://kaldi-asr.org/doc/data_prep.html},
	urldate = {2022-08-24},
	file = {Kaldi\: Data preparation:C\:\\Users\\DELL\\Zotero\\storage\\QLKDHK6G\\data_prep.html:text/html},
}

@article{kocon_offensive_2021,
	title = {Offensive, aggressive, and hate speech analysis: {From} data-centric to human-centered approach},
	volume = {58},
	issn = {03064573},
	shorttitle = {Offensive, aggressive, and hate speech analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457321001333},
	doi = {10.1016/j.ipm.2021.102643},
	language = {en},
	number = {5},
	urldate = {2022-08-26},
	journal = {Information Processing \& Management},
	author = {Kocoń, Jan and Figas, Alicja and Gruza, Marcin and Puchalska, Daria and Kajdanowicz, Tomasz and Kazienko, Przemysław},
	month = sep,
	year = {2021},
	pages = {102643},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\NSSERT5Y\\Kocoń et al. - 2021 - Offensive, aggressive, and hate speech analysis F.pdf:application/pdf},
}

@inproceedings{ghahremani_investigation_2017,
	address = {Okinawa},
	title = {Investigation of transfer learning for {ASR} using {LF}-{MMI} trained neural networks},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268947/},
	doi = {10.1109/ASRU.2017.8268947},
	urldate = {2022-09-04},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Ghahremani, Pegah and Manohar, Vimal and Hadian, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = dec,
	year = {2017},
	pages = {279--286},
}

@inproceedings{wallington_learning_2021,
	title = {On the {Learning} {Dynamics} of {Semi}-{Supervised} {Training} for {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/wallington21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1777},
	language = {en},
	urldate = {2022-09-04},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Wallington, Electra and Kershenbaum, Benji and Klejch, Ondřej and Bell, Peter},
	month = aug,
	year = {2021},
	pages = {716--720},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\F2TLM7QF\\Wallington et al. - 2021 - On the Learning Dynamics of Semi-Supervised Traini.pdf:application/pdf},
}

@inproceedings{sarkar_novel_2014,
	title = {A novel boosting algorithm for improved i-vector based speaker verification in noisy environments},
	booktitle = {Proceedings of the {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {INTERSPEECH}},
	author = {Sarkar, Sourjya and Rao, K.},
	month = sep,
	year = {2014},
}

@inproceedings{sarkar_study_2012,
	title = {Study of the {Effect} of {I}-vector {Modeling} on {Short} and {Mismatch} {Utterance} {Duration} for {Speaker} {Verification}},
	booktitle = {{INTERSPEECH}},
	author = {Sarkar, Achintya Kumar and Matrouf, Driss and Bousquet, Pierre-Michel and Bonastre, Jean-François},
	year = {2012},
}

@misc{zhang_uberi_speechrecognition_nodate-1,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-09-18},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4MV996BJ\\SpeechRecognition.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What {Is} {Automatic} {Speech} {Recognition}? - {Alexa} {Skills} {Kit} {Official} {Site}},
	shorttitle = {What {Is} {Automatic} {Speech} {Recognition}?},
	url = {https://developer.amazon.com/en-US/alexa/alexa-skills-kit/asr.html},
	abstract = {Automatic speech recognition (ASR) is technology that converts spoken words into text. Explore the topic of ASR and learn about building for voice.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Amazon (Alexa)},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\EA9BYMAB\\asr.html:text/html},
}

@misc{noauthor_speech--text_nodate,
	title = {Speech-to-{Text}: {Automatic} {Speech} {Recognition}},
	shorttitle = {Speech-to-{Text}},
	url = {https://cloud.google.com/speech-to-text},
	abstract = {Accurately convert voice to text in over 125 languages and variants by applying Google’s powerful machine learning models with an easy-to-use API.},
	language = {en},
	urldate = {2022-09-19},
	journal = {Google Cloud},
}

@misc{noauthor_siri_nodate,
	title = {Siri},
	url = {https://www.apple.com/siri/},
	abstract = {Siri is an easy way to make calls, send texts, use apps, and get things done with just your voice. And Siri is the most private intelligent assistant.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Apple},
}

@misc{noauthor_cortana_nodate,
	title = {Cortana - {Your} personal productivity assistant},
	url = {https://www.microsoft.com/en-us/cortana},
	abstract = {Cortana helps you achieve more with less effort. Your personal productivity assistant helps you stay on top of what matters, follow through, and do your best work.},
	language = {en-us},
	urldate = {2022-09-19},
	journal = {Cortana - Your personal productivity assistant},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\APLZ98WM\\cortana.html:text/html},
}

@misc{noauthor_jarvis_nodate,
	title = {Jarvis {\textbar} {NVIDIA} {NGC}},
	url = {https://catalog.ngc.nvidia.com/orgs/nvidia/collections/jarvis},
	abstract = {NVIDIA Jarvis is a framework for production-grade conversational AI inference. The Jarvis Collection on NGC includes all the resources required for getting started with Jarvis.},
	language = {en},
	urldate = {2022-09-19},
	journal = {NVIDIA NGC Catalog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8U4FAMTW\\jarvis.html:text/html},
}

@misc{noauthor_sox_nodate,
	title = {{SoX} - {Sound} {eXchange} {\textbar} {HomePage}},
	url = {http://sox.sourceforge.net/},
	urldate = {2022-09-19},
	file = {SoX - Sound eXchange | HomePage:C\:\\Users\\DELL\\Zotero\\storage\\Y5XJDGHX\\sox.sourceforge.net.html:text/html},
}

@misc{raj_note_nodate,
	title = {A note on {MFCCs} and delta features},
	url = {https://desh2608.github.io/2019-07-26-delta-feats/},
	abstract = {What are MFCCs and how are they computed? Feature extraction is the first step in any automatic speech recognition (ASR) pipeline. The objective is to compute features from speech waveforms which contain relevant information about the linguistic content of the speech, and ignore information about the background noise, emotions, etc....},
	language = {en},
	urldate = {2022-09-19},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\R4WGBL8Q\\2019-07-26-delta-feats.html:text/html},
}

@inproceedings{menne_analysis_2019,
	title = {Analysis of {Deep} {Clustering} as {Preprocessing} for {Automatic} {Speech} {Recognition} of {Sparsely} {Overlapping} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html},
	doi = {10.21437/Interspeech.2019-1728},
	language = {en},
	urldate = {2022-09-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Menne, Tobias and Sklyar, Ilya and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2019},
	pages = {2638--2642},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5X2I92JR\\Menne et al. - 2019 - Analysis of Deep Clustering as Preprocessing for A.pdf:application/pdf},
}

@article{li_tenet_2019,
	title = {{TEnet}: target speaker extraction network with accumulated speaker embedding for automatic speech recognition},
	volume = {55},
	issn = {0013-5194, 1350-911X},
	shorttitle = {{TEnet}},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/el.2019.1228},
	doi = {10.1049/el.2019.1228},
	language = {en},
	number = {14},
	urldate = {2022-09-19},
	journal = {Electronics Letters},
	author = {Li, Wenjie and Zhang, Pengyuan and Yan, Yonghong},
	month = jul,
	year = {2019},
	pages = {816--819},
}

@article{van_wyk_multivaluedness_2021,
	title = {Multivaluedness in {Networks}: {Shannon}’s {Noisy}-{Channel} {Coding} {Theorem}},
	volume = {68},
	issn = {1549-7747, 1558-3791},
	shorttitle = {Multivaluedness in {Networks}},
	url = {https://ieeexplore.ieee.org/document/9410598/},
	doi = {10.1109/TCSII.2021.3074925},
	number = {10},
	urldate = {2022-09-19},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {van Wyk, Michael Antonie and Ping, Li and Chen, Guanrong},
	month = oct,
	year = {2021},
	pages = {3234--3235},
}

@inproceedings{mohamed_understanding_2012,
	address = {Kyoto, Japan},
	title = {Understanding how {Deep} {Belief} {Networks} perform acoustic modelling},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	url = {http://ieeexplore.ieee.org/document/6288863/},
	doi = {10.1109/ICASSP.2012.6288863},
	urldate = {2022-09-19},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Mohamed, Abdel-rahman and Hinton, Geoffrey and Penn, Gerald},
	month = mar,
	year = {2012},
	pages = {4273--4276},
}

@article{benesty_springer_2009,
	title = {Springer {Handbook} of {Speech} {Processing}},
	volume = {126},
	issn = {00014966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/126/4/10.1121/1.3203918},
	doi = {10.1121/1.3203918},
	language = {en},
	number = {4},
	urldate = {2022-09-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Benesty, Jacob and Sondhi, Mohan M. and Huang, Yiteng and Greenberg, Steven},
	year = {2009},
	pages = {2130},
}

@misc{marvel_jrvis_nodate,
	title = {J.{A}.{R}.{V}.{I}.{S}.},
	url = {https://ironman.fandom.com/wiki/J.A.R.V.I.S.},
	abstract = {Just A Rather Very Intelligent System (J.A.R.V.I.S.) was originally Tony Stark's natural-language user interface computer system, named after Edwin Jarvis, the butler who worked for Howard Stark. Over time, he was upgraded into an artificially intelligent system, tasked with running business for Stark Industries as well as security for Tony Stark's Mansion and Stark Tower. After creating the Mark II armor, Stark uploaded J.A.R.V.I.S. into all of the Iron Man Armors, as well as allowing him to in},
	language = {en},
	urldate = {2022-09-25},
	journal = {Iron Man Wiki},
	author = {Marvel, Fandom},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2AT6NTE6\\J.A.R.V.I.S..html:text/html},
}

@misc{audacity_linux_nodate,
	title = {Audacity for Linux},
	url = {https://www.audacityteam.org/download/linux/},
	abstract = {Thank you for downloading Audacity

Your download will start in 5 seconds.
  Problems with the download? Please use this direct link


AppImage
Audacity 3.2.0 is available as an AppImage. The AppImage should run on most modern Linux distributions. To run AppImage:

Left click the link below.
Make t},
	language = {en-US},
	urldate = {2022-09-27},
	journal = {Audacity ®},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\TZSQDL3M\\linux.html:text/html},
}

@article{hussein_arabic_2022,
	title = {Arabic speech recognition by end-to-end, modular systems and human},
	volume = {71},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230821000760},
	doi = {10.1016/j.csl.2021.101272},
	language = {en},
	urldate = {2022-10-02},
	journal = {Computer Speech \& Language},
	author = {Hussein, Amir and Watanabe, Shinji and Ali, Ahmed},
	month = jan,
	year = {2022},
	pages = {101272},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\WJXMYF47\\Hussein et al. - 2022 - Arabic speech recognition by end-to-end, modular s.pdf:application/pdf},
}

@article{dahl_context-dependent_2012,
	title = {Context-{Dependent} {Pre}-{Trained} {Deep} {Neural} {Networks} for {Large}-{Vocabulary} {Speech} {Recognition}},
	volume = {20},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5740583/},
	doi = {10.1109/TASL.2011.2134090},
	number = {1},
	urldate = {2022-10-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Dahl, G. E. and {Dong Yu} and {Li Deng} and Acero, A.},
	month = jan,
	year = {2012},
	pages = {30--42},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CT4BGLDU\\Dahl et al. - 2012 - Context-Dependent Pre-Trained Deep Neural Networks.pdf:application/pdf},
}

@article{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1303.5778},
	doi = {10.48550/ARXIV.1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates {\textbackslash}emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	urldate = {2022-10-02},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},
	annote = {Other
To appear in ICASSP 2013},
}

@article{hifny_unified_2015,
	title = {Unified {Acoustic} {Modeling} using {Deep} {Conditional} {Random} {Fields}},
	issn = {20547390},
	url = {http://scholarpublishing.org/index.php/TMLAI/article/view/1124},
	doi = {10.14738/tmlai.32.1124},
	urldate = {2022-10-02},
	journal = {Transactions on Machine Learning and Artificial Intelligence},
	author = {Hifny, Yasser},
	month = apr,
	year = {2015},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\V8U7GBTW\\Hifny - 2015 - Unified Acoustic Modeling using Deep Conditional R.pdf:application/pdf},
}

@inproceedings{povey_purely_2016,
	title = {Purely {Sequence}-{Trained} {Neural} {Networks} for {ASR} {Based} on {Lattice}-{Free} {MMI}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/povey16_interspeech.html},
	doi = {10.21437/Interspeech.2016-595},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Povey, Daniel and Peddinti, Vijayaditya and Galvez, Daniel and Ghahremani, Pegah and Manohar, Vimal and Na, Xingyu and Wang, Yiming and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {2751--2755},
}

@inproceedings{vijayaditya_time_2015,
	title = {A time delay neural network architecture for efficient modeling of long temporal contexts},
	url = {https://www.semanticscholar.org/paper/A-time-delay-neural-network-architecture-for-of-Peddinti-Povey/3a79ac688f2558b2d9693e434f010e041eba0fae},
	author = {Vijayaditya and Peddinti and Sanjeev Khudanpur and Daniel Povey},
	year = {2015},
}

@article{ali_speech_2017,
	title = {Speech {Recognition} {Challenge} in the {Wild}: {Arabic} {MGB}-3},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Speech {Recognition} {Challenge} in the {Wild}},
	url = {https://arxiv.org/abs/1709.07276},
	doi = {10.48550/ARXIV.1709.07276},
	abstract = {This paper describes the Arabic MGB-3 Challenge - Arabic Speech Recognition in the Wild. Unlike last year's Arabic MGB-2 Challenge, for which the recognition task was based on more than 1,200 hours broadcast TV news recordings from Aljazeera Arabic TV programs, MGB-3 emphasises dialectal Arabic using a multi-genre collection of Egyptian YouTube videos. Seven genres were used for the data collection: comedy, cooking, family/kids, fashion, drama, sports, and science (TEDx). A total of 16 hours of videos, split evenly across the different genres, were divided into adaptation, development and evaluation data sets. The Arabic MGB-Challenge comprised two tasks: A) Speech transcription, evaluated on the MGB-3 test set, along with the 10 hour MGB-2 test set to report progress on the MGB-2 evaluation; B) Arabic dialect identification, introduced this year in order to distinguish between four major Arabic dialects - Egyptian, Levantine, North African, Gulf, as well as Modern Standard Arabic. Two hours of audio per dialect were released for development and a further two hours were used for evaluation. For dialect identification, both lexical features and i-vector bottleneck features were shared with participants in addition to the raw audio recordings. Overall, thirteen teams submitted ten systems to the challenge. We outline the approaches adopted in each system, and summarise the evaluation results.},
	urldate = {2022-10-02},
	author = {Ali, Ahmed and Vogel, Stephan and Renals, Steve},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{khurana_qcri_2016,
	address = {San Diego, CA},
	title = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition: {MGB}-2 challenge},
	isbn = {978-1-5090-4903-5},
	shorttitle = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition},
	url = {http://ieeexplore.ieee.org/document/7846279/},
	doi = {10.1109/SLT.2016.7846279},
	urldate = {2022-10-02},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Khurana, Sameer and Ali, Ahmed},
	month = dec,
	year = {2016},
	pages = {292--298},
}

@inproceedings{smit_aalto_2017,
	address = {Okinawa},
	title = {Aalto system for the 2017 {Arabic} multi-genre broadcast challenge},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268955/},
	doi = {10.1109/ASRU.2017.8268955},
	urldate = {2022-10-02},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Smit, Peter and Gangireddy, Siva Reddy and Enarvi, Seppo and Virpioja, Sami and Kurimo, Mikko},
	month = dec,
	year = {2017},
	pages = {338--345},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\YUJC9CV7\\Smit et al. - 2017 - Aalto system for the 2017 Arabic multi-genre broad.pdf:application/pdf},
}

@inproceedings{snyder_speaker_2019,
	address = {Brighton, United Kingdom},
	title = {Speaker {Recognition} for {Multi}-speaker {Conversations} {Using} {X}-vectors},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683760/},
	doi = {10.1109/ICASSP.2019.8683760},
	urldate = {2022-10-02},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and McCree, Alan and Povey, Daniel and Khudanpur, Sanjeev},
	month = may,
	year = {2019},
	pages = {5796--5800},
}

@article{dutta_performance_2021,
	title = {Performance analysis of {ASR} system in hybrid {DNN}-{HMM} framework using a {PWL} euclidean activation function},
	volume = {15},
	issn = {2095-2228, 2095-2236},
	url = {https://link.springer.com/10.1007/s11704-020-9419-z},
	doi = {10.1007/s11704-020-9419-z},
	language = {en},
	number = {4},
	urldate = {2022-10-02},
	journal = {Frontiers of Computer Science},
	author = {Dutta, Anirban and Ashishkumar, Gudmalwar and Rao, Ch V. Rama},
	month = aug,
	year = {2021},
	pages = {154705},
}

@inproceedings{georgescu_kaldi-based_2019,
	address = {Timisoara, Romania},
	title = {Kaldi-based {DNN} {Architectures} for {Speech} {Recognition} in {Romanian}},
	isbn = {978-1-72810-984-8},
	url = {https://ieeexplore.ieee.org/document/8906555/},
	doi = {10.1109/SPED.2019.8906555},
	urldate = {2022-10-04},
	booktitle = {2019 {International} {Conference} on {Speech} {Technology} and {Human}-{Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Georgescu, Alexandru-Lucian and Cucu, Horia and Burileanu, Corneliu},
	month = oct,
	year = {2019},
	pages = {1--6},
}

@incollection{ekstein_cnn-tdnn-based_2021,
	address = {Cham},
	title = {{CNN}-{TDNN}-{Based} {Architecture} for {Speech} {Recognition} {Using} {Grapheme} {Models} in {Bilingual} {Czech}-{Slovak} {Task}},
	volume = {12848},
	isbn = {978-3-030-83526-2 978-3-030-83527-9},
	url = {https://link.springer.com/10.1007/978-3-030-83527-9_45},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {Text, {Speech}, and {Dialogue}},
	publisher = {Springer International Publishing},
	author = {Psutka, Josef V. and Švec, Jan and Pražák, Aleš},
	editor = {Ekštein, Kamil and Pártl, František and Konopík, Miloslav},
	year = {2021},
	doi = {10.1007/978-3-030-83527-9_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {523--533},
}

@misc{noauthor_lattice_nodate,
	title = {On lattice free {MMI} and {Chain} models in {Kaldi}},
	url = {https://desh2608.github.io/2019-05-21-chain/},
	urldate = {2022-10-04},
	file = {On lattice free MMI and Chain models in Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\WVI67A8B\\2019-05-21-chain.html:text/html},
}

@misc{raj_experiments_nodate,
	title = {Experiments with {Subword} {Modeling}},
	url = {https://desh2608.github.io/2018-11-22-subword-segmentation/},
	abstract = {Think about tasks such as machine translation (MT), automatic speech recognition (ASR), or handwriting recognition (HWR). While these appear very distinct, on abstraction they share the same pipeline wherein given an input signal, we are required to predict some text. The difference only lies in the form of the input...},
	language = {en},
	urldate = {2022-10-04},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ID8BRFHZ\\2018-11-22-subword-segmentation.html:text/html},
}

@inproceedings{tian_consistent_2022,
	address = {Singapore, Singapore},
	title = {Consistent {Training} and {Decoding} for {End}-to-{End} {Speech} {Recognition} {Using} {Lattice}-{Free} {MMI}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9746579/},
	doi = {10.1109/ICASSP43922.2022.9746579},
	urldate = {2022-10-05},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Tian, Jinchuan and Yu, Jianwei and Weng, Chao and Zhang, Shi-Xiong and Su, Dan and Yu, Dong and Zou, Yuexian},
	month = may,
	year = {2022},
	pages = {7782--7786},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\UFAHYT8M\\Tian et al. - 2022 - Consistent Training and Decoding for End-to-End Sp.pdf:application/pdf},
}

@misc{wiesner_lattice_2020,
	title = {Lattice {Free} {Maximum} {Mutual} {Information} ({LF}-{MMI})},
	url = {https://m-wiesner.github.io/LF-MMI/},
	abstract = {Everything about LF-MMI},
	language = {en},
	urldate = {2022-10-08},
	journal = {Matthew Wiesner},
	author = {Wiesner, Matthew and MatthewWiesner},
	month = jan,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ELAZNE4P\\LF-MMI.html:text/html},
}

@article{liu_time_2019,
	title = {Time {Delay} {Recurrent} {Neural} {Network} for {Speech} {Recognition}},
	volume = {1229},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1229/1/012078},
	doi = {10.1088/1742-6596/1229/1/012078},
	abstract = {Abstract
            In Automatic Speech Recognition(ASR), Time Delay Neural Network (TDNN) has been proven to be an efficient network structure for its strong ability in context modeling. In addition, as a feed-forward neural architecture, it is faster to train TDNN, compared with recurrent neural networks such as Long Short-Term Memory (LSTM). However, different from recurrent neural networks, the context in TDNN is carefully designed and is limited. Although stacking Long Short-Term Memory (LSTM) together with TDNN in order to extend the context information have been proven to be useful, it is too complex and is hard to train. In this paper, we focus on directly extending the context modeling capability of TDNNs by adding recurrent connections. Several new network architectures were investigated. The results on the Switchboard show that the best model significantly outperforms the base line TDNN system and is comparable with TDNN-LSTM architecture. In addition, the training process is much simpler than that of TDNN-LSTM.},
	number = {1},
	urldate = {2022-10-12},
	journal = {Journal of Physics: Conference Series},
	author = {Liu, Boji and Zhang, Weibin and Xu, Xiangming and Chen, Dongpeng},
	month = may,
	year = {2019},
	pages = {012078},
}

@phdthesis{ritter_neural_2019,
	title = {Neural {Architecture} {Search} for {Finding} the {Best} {Time} {Delay} {Neural} {Network} {Acoustic} {Model} for {Speech} {Recognition}},
	abstract = {Time Delay Neural Network (TDNN) is a popular type of acoustic model used for speech recognition applications. Their popularity is mainly due to their faster training and decoding times, with word error rates (WER) comparable to a long-short term memory (LSTM) based acoustic model. The popularity of TDNNs picked up in 2015 when a more efficient setup was proposed, which is known as a TDNN with sub-sampling scheme. However, the 2015 proposal with sub-sampling scheme does not offer any details on the sub-sampling used. Neural Architecture Search (NAS) is a new research field that has garnered a lot of attention with successful results in computer vision research. Despite this, NAS has received little attention in speech recognition, where design architectures for acoustic models is crucial. TDNNs are provided in the Kaldi speech recognition toolkit to be used for research or deployment purposes. From the literature, we have observed that it is common for a TDNN to be used in Kaldi as is, with no additional tuning of its hyperparameters. For this reason, this project aims to investigate if the Kaldi baseline TDNN is actually the best configuration to be used for a speech recognition application. To do so, we have made use of a recently proposed algorithm which integrates reinforcement learning in its training process. Specifically, we have used Neural Architecture Search (NAS) to target and improve automatically the sub-sampling scheme of the Kaldi TDNN. For reproducibility we have based all our results on the standard Wall Street Journal database. We performed experiments by setting a RNN based TDNN architecture generator, with the added constraint that the TDNNs evaluated have the same number of frames as the one provided with Kaldi. Our results show that NAS is able to sample a better TDNN architecture than the one provided by Kaldi in less than 40,000 iterations, achieving a 0.19 WER reduction. i},
	author = {Ritter, Fabian},
	month = aug,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8Y4PMA9Y\\Ritter - 2019 - Neural Architecture Search for Finding the Best Ti.pdf:application/pdf},
}

@misc{fayek_speech_2016,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}’s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2022-10-13},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\GNNWTTYV\\speech-processing-for-machine-learning.html:text/html},
}

@misc{shrawankar_adverse_2013,
	title = {Adverse {Conditions} and {ASR} {Techniques} for {Robust} {Speech} {User} {Interface}},
	url = {http://arxiv.org/abs/1303.5515},
	abstract = {The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Shrawankar, Urmila and Thakare, V. M.},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	annote = {Comment: 10 pages 2 Tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Q3SPV23K\\Shrawankar and Thakare - 2013 - Adverse Conditions and ASR Techniques for Robust S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\82368FRG\\1303.html:text/html},
}

@article{vipperla_ageing_2010,
	title = {Ageing {Voices}: {The} {Effect} of {Changes} in {Voice} {Parameters} on {ASR} {Performance}},
	volume = {2010},
	issn = {1687-4714, 1687-4722},
	shorttitle = {Ageing {Voices}},
	url = {http://asmp.eurasipjournals.com/content/2010/1/525783},
	doi = {10.1155/2010/525783},
	language = {en},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	year = {2010},
	pages = {1--10},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\G6TNUYYM\\Vipperla et al. - 2010 - Ageing Voices The Effect of Changes in Voice Para.pdf:application/pdf},
}

@techreport{markus_forsberg_why_2003,
	title = {Why is {Speech} {Recognition} {Difficult}?},
	url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.3677},
	institution = {Chalmers University of Technology},
	author = {Markus Forsberg},
	month = feb,
	year = {2003},
}

@article{schuller_recognition_2009,
	title = {Recognition of {Noisy} {Speech}: {A} {Comparative} {Survey} of {Robust} {Model} {Architecture} and {Feature} {Enhancement}},
	volume = {2009},
	issn = {1687-4722},
	shorttitle = {Recognition of {Noisy} {Speech}},
	url = {http://asmp.eurasipjournals.com/content/2009/1/942617},
	doi = {10.1155/2009/942617},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Schuller, Björn and Wöllmer, Martin and Moosmayr, Tobias and Rigoll, Gerhard},
	year = {2009},
	pages = {942617},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9UQKPJZ2\\Schuller et al. - 2009 - Recognition of Noisy Speech A Comparative Survey .pdf:application/pdf},
}

@article{hoge_basic_2007,
	title = {Basic parameters in speech processing. {The} need for evaluation},
	volume = {32},
	copyright = {Copyright on any open access article in the Archives of Acoustics published by Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society is retained by the author(s).   Authors grant Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society a license to publish the article and identify itself as the original publisher.   Authors also grant any user the right to use the article freely as long as its integrity is maintained and its original authors, citation details and publisher are identified.           The Creative Commons Attribution License-ShareAlike 4.0 formalizes these and other terms and conditions of publishing articles.    Exceptions to copyright policy    For the articles which were previously published, before year 2019, policies that are different from the above. In all such, access to these articles is free from fees or any other access restrictions.   Permissions for the use of the texts published in that journal may be sought directly from the Editorial Office of Archives of Acoustics},
	issn = {2300-262X},
	url = {https://acoustics.ippt.pan.pl/index.php/aa/article/view/766},
	abstract = {As basic parameters in speech processing we regard pitch, duration, intensity, voice quality, signal to noise ratio, voice activity detection and strength of Lombard effect. Taking in account also adverse conditions the performance of many published algorithms to extract those parameters from the speech signal automatically is not known. A framework based on competitive evaluation is proposed to push algorithmic research and to make progress comparable.},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {Archives of Acoustics},
	author = {Höge, Harald},
	year = {2007},
	note = {Number: 1},
	pages = {67},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\9Z4BXXSK\\Höge - 2007 - Basic parameters in speech processing. The need fo.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FSTAYK66\\766.html:text/html},
}

@article{venkatagiri_speech_2002,
	title = {Speech {Recognition} {Technology} {Applications} in {Communication} {Disorders}},
	volume = {11},
	issn = {1058-0360, 1558-9110},
	url = {http://pubs.asha.org/doi/10.1044/1058-0360%282002/037%29},
	doi = {10.1044/1058-0360(2002/037)},
	abstract = {Speech recognition (SR) is the process whereby a microprocessor-based system, typically a computer with sound processing hardware and speech recognition software, responds in predictable ways to spoken commands and/or converts speech into text. This tutorial describes the types and the general uses of SR and provides an explanation of the technology behind it. The emerging applications of SR technology for dictation, articulation training, language and literacy development, environmental control, and communication augmentation are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-10-13},
	journal = {American Journal of Speech-Language Pathology},
	author = {Venkatagiri, H. S.},
	month = nov,
	year = {2002},
	pages = {323--332},
}

@techreport{noauthor_alternative_nodate,
	title = {Alternative {Control} {Technologies}: {Human} {Factors} {Issues}},
	shorttitle = {Alternative {Control} {Technologies}},
	url = {https://apps.dtic.mil/sti/citations/ADA355911},
	abstract = {With the increasing intelligence of computer systems, it is becoming more desirable to have an operator communicate with machines rather than simply operate them. In combat aircraft, this need to communicate is made quite crucial due to high temporal pressure and workload during critical phases of the flight ingress, engagement, deployment of self-defense. The HOTAS concept, with manual controls fitted on the stick and throttle, has been widely used in modern fighters such as F16, F18, EFA and Rafale. This concept allows pilots to input real time commands to the aircraft system. However, it increases the complexity of the pilot task due to inflation of real time controls, with some controls being multifunction. It is therefore desirable, in the framework of ecological interfaces, to introduce alternative input channels in order to reduce the complexity of manual control in the HOTAS concept and allow more direct and natural access to the aircraft systems. Control and display technologies are the critical enablers for these advanced interfaces. There are a variety of novel alternative control technologies that when integrated usefully with critical mission tasks can make natural use of the innate potential of human sensory and motor systems. Careful design and integration of candidate control technologies will result in human-machine interfaces which are natural, easier to learn, easier to use, and less prone to error. Significant progress is being made on using signals from the brain, muscles, voice, lip, head position, eye position and gestures for the control of computers and other devices. Judicious application of alternative control technologies has the potential to increase the bandwidth of operator-system interaction, improve the effectiveness of military systems, and realize cost savings. Alternative controls can reduce workload and improve efficiency within the cockpit, directly supporting the warfighter.},
	language = {en},
	urldate = {2022-10-13},
	note = {Section: Technical Reports},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\96Z76MGM\\ADA355911.html:text/html},
}

@book{vasilescu_cross-lingual_2011,
	title = {Cross-{Lingual} {Study} of {ASR} {Errors}: {On} the {Role} of the {Context} in {Human} {Perception} of {Near}-{Homophones}.},
	shorttitle = {Cross-{Lingual} {Study} of {ASR} {Errors}},
	author = {Vasilescu, Ioana and Yahia, Dahbia and Snoeren, Natalie and Adda-Decker, Martine and Lamel, Lori},
	month = aug,
	year = {2011},
	note = {Pages: 1952},
}

@inproceedings{povey_semi-orthogonal_2018,
	title = {Semi-{Orthogonal} {Low}-{Rank} {Matrix} {Factorization} for {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/povey18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1417},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Povey, Daniel and Cheng, Gaofeng and Wang, Yiming and Li, Ke and Xu, Hainan and Yarmohammadi, Mahsa and Khudanpur, Sanjeev},
	month = sep,
	year = {2018},
	pages = {3743--3747},
}

@inproceedings{yeh_taiwanese_2020,
	address = {Taipei, Taiwan},
	title = {Taiwanese {Speech} {Recognition} {Based} on {Hybrid} {Deep} {Neural} {Network} {Architecture}},
	url = {https://aclanthology.org/2020.rocling-1.11},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 32nd {Conference} on {Computational} {Linguistics} and {Speech} {Processing} ({ROCLING} 2020)},
	publisher = {The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)},
	author = {Yeh, Yu-Fu and Su, Bo-Hao and Ou, Yang-Yen and Wang, Jhing-Fa and Tsai, An-Chao},
	month = sep,
	year = {2020},
	pages = {102--113},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\MTSZB5PS\\Yeh et al. - 2020 - Taiwanese Speech Recognition Based on Hybrid Deep .pdf:application/pdf},
}

@misc{hannun_speech_nodate,
	title = {Speech {Recognition} {Is} {Not} {Solved}},
	url = {https://awni.github.io/speech-recognition/},
	abstract = {Ever since Deep Learning hit the scene in speech recognition, word error rates
have fallen dramatically. But despite articles you may have read, we still
don’t have human-level speech recognition. Speech recognizers have many failure
modes. Acknowledging these and taking steps towards solving them is critical to
progress. It’s the only way to go from ASR
which works for some people, most of the time to ASR which works for all
people, all of the time.},
	urldate = {2022-10-23},
	author = {Hannun, Awni},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HK2ZJCFX\\speech-recognition.html:text/html},
}

@misc{rev_speech_2021,
	title = {Speech {Recognition} {Challenges} and {How} to {Solve} {Them}},
	url = {https://www.rev.com/blog/speech-to-text-technology/speech-recognition-challenges-and-how-to-solve-them},
	abstract = {Cutting edge tech is always a challenge. That’s one of the reasons we love it. The breakthrough discovery, the moment when we figure out how to solve the},
	language = {en-US},
	urldate = {2022-10-27},
	journal = {Rev},
	author = {Rev},
	month = sep,
	year = {2021},
}

@phdthesis{ander_gonzalez_docasal_noisy_2018,
	title = {Noisy speech recognition using {Kaldi} and neural architectures},
	url = {http://hdl.handle.net/10810/27865},
	abstract = {Noisy Speech Recognition using Kaldi and Neural Architectures ABSTRACT The goal of an Automatic Speech Recognition (ASR) system is to transform a set of acoustic features into a sequence of words. It mainly consists of various parts: the feature extraction part which extracts information from a speech signal; the acoustic model, in charge of the conversion from speech to phonemes; and the language model that transforms the detected phonemes into the most probable sequence of words. Throughout their history, these systems were built with statistical methods, mainly Hidden Markov Models (HMM) and Gaussian Mixture Models (GMM). However, in recent years the use of neural architectures such as Deep, Convolutional and Recurrent Neural Networks (DNN, CNN and RNN), have improved the achieved results significantly. Moreover, freely available tools made ASR research develop quickly. Kaldi is one of the most known and widely used ASR systems. It includes a set of neural network packages —nnet1, nnet2 and nnet3— which can be used for implementing the acoustic model. These are fast, accurate and able to handle huge databases since they distribute the load on clusters of machines. However, Kaldi’s slow development cycle implies that new neural architectures may be introduced many years after their publications. Therefore, in this work we substitute the neural acoustic model of Kaldi by our own implementations written in TensorFlow. TensorFlow has the largest community of users and the best support among the available deep learning libraries. By substituting the Acoustic Model of Kaldi with different architectures and testing their performance on the well-known database Aurora-4, we managed to reduce Word Error Rate (WER) by 3.17 \% (baseline 15.14 \%) when using a CNN architecture. Also, focusing on just the clean subset of the Test part of the database, a further improvement has been achieved once implementing a CNN + RNN structure, from a 4.54 \% WER with the CNNs alone to a 4.13 \% with this architecture. This work is therefore believed to improve the results on obtained by one of the widely used ASR tools simply by implementing more advanced deep learning techniques, which could be executed by more powerful and dedicated external programs. For future work, a further analysis on more complex convolutional networks could lead to a better performance in this particular database and, in general, in noisy environments. Finally, further improvement of convolutional and recurrent architectures is suggested in clean and noise-free conditions, since they have been shown to obtain the best results in this specific circumstances.},
	school = {University of Crete},
	author = {Ander González Docasal},
	month = feb,
	year = {2018},
}

@inproceedings{mirzaei_errors_2015,
	title = {Errors in automatic speech recognition versus difficulties in second language listening},
	isbn = {978-1-908416-29-2},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2015.000367},
	doi = {10.14705/rpnet.2015.000367},
	urldate = {2022-10-27},
	booktitle = {Critical {CALL} – {Proceedings} of the 2015 {EUROCALL} {Conference}, {Padova}, {Italy}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Meshgi, Kourosh and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2015},
	pages = {410--415},
}

@book{farghaly_handbook_2003,
	address = {Stanford, Calif},
	series = {{CSLI} lecture notes},
	title = {Handbook for language engineers},
	isbn = {978-1-57586-395-5 978-1-57586-396-2},
	url = {https://web.stanford.edu/group/cslipublications/cslipublications/site/1575863960.shtml},
	number = {no. 164},
	publisher = {CSLI Publications},
	author = {Farghaly, Ali Ahmed Sabry},
	year = {2003},
	keywords = {Computational linguistics, Applied linguistics},
}

@techreport{kardome_how_2022,
	title = {How {Kardome}'s {Advanced} {Speech} {Recognition} {Technology} {Improves} {Voice} {Enabled} {Devices}},
	url = {https://uploads-ssl.webflow.com/5f6ccdcd2e8b3e529677788d/6306b0cf32d2742c5a0e8db7_kardome-wake-word-detection-and-speech-recognition-study-for-consumer-voice-devices_6306ab45.pdf},
	urldate = {2022-10-27},
	author = {Kardome},
	year = {2022},
}

@misc{noauthor_solving_nodate,
	title = {Solving {Automatic} {Speech} {Recognition} {Deployment} {Challenges} {\textbar} {NVIDIA} {Technical} {Blog}},
	url = {https://developer.nvidia.com/blog/solving-automatic-speech-recognition-deployment-challenges/},
	urldate = {2022-10-29},
	file = {Solving Automatic Speech Recognition Deployment Challenges | NVIDIA Technical Blog:C\:\\Users\\DELL\\Zotero\\storage\\4ZD2CTIL\\solving-automatic-speech-recognition-deployment-challenges.html:text/html},
}

@misc{noauthor_global_nodate,
	title = {Global voice recognition market 2026},
	url = {https://www.statista.com/statistics/1133875/global-voice-recognition-market-size/},
	abstract = {The global voice recognition market size was forecast to grow from 10.7 billion U.S.},
	language = {en},
	urldate = {2022-10-29},
	journal = {Statista},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2MV2NQEG\\global-voice-recognition-market-size.html:text/html},
}

@misc{noauthor_speech_nodate,
	title = {Speech {Recognition} {Software} {Market} 2022 by {Size}, {Share}, international business analysis, {Key} firms {Profile} and {Forecast} to 2029 {\textbar} 104 {Pages} {Report}},
	url = {https://www.marketwatch.com/press-release/speech-recognition-software-market-2022-by-size-share-international-business-analysis-key-firms-profile-and-forecast-to-2029-104-pages-report-2022-09-29},
	abstract = {Sep 29, 2022 (The Expresswire) --
[Premium 104 Pages Report] Speech Recognition Software market trend that pertains to the world market for ICT Industry. The...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KKRL736U\\speech-recognition-software-market-2022-by-size-share-international-business-analysis-key-firms.html:text/html},
}

@misc{noauthor_speech_nodate-1,
	title = {Speech and {Voice} {Recognition} {Market} {Size} {And} {Opportunities} for {New} {Players}, {Forecast} from 2022 {To} 2029 with {Top} {Countries} {Data} {\textbar} 127 {Pages} {Report}},
	url = {https://www.marketwatch.com/press-release/speech-and-voice-recognition-market-size-and-opportunities-for-new-players-forecast-from-2022-to-2029-with-top-countries-data-127-pages-report-2022-10-13},
	abstract = {Oct 13, 2022 (The Expresswire) --
According to this latest study, In 2022 the growth of Speech and Voice Recognition Market is projected to reach...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\CPRRIETA\\speech-and-voice-recognition-market-size-and-opportunities-for-new-players-forecast-from-2022-t.html:text/html},
}

@misc{noauthor_speech_nodate-2,
	title = {Speech and {Voice} {Recognition} {Technology} {Market} {Size} 2022, {Demand}, {Share}, {Global} {Trend}, {Business} {Growth}, {Top} {Key} {Players} {Update}, {Business} {Statistics} and {Research} {Methodology} by {Forecast} to 2028},
	url = {https://www.marketwatch.com/press-release/speech-and-voice-recognition-technology-market-size-2022-demand-share-global-trend-business-growth-top-key-players-update-business-statistics-and-research-methodology-by-forecast-to-2028-2022-10-14},
	abstract = {Oct 14, 2022 (The Expresswire) --
"Final Report will add the analysis of the impact of Pre and Post COVID-19 on this Speech and Voice Recognition Technology...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
}

@misc{noauthor_speech_nodate-3,
	title = {Speech {Recognition} {Software} {Market} {Size}, {Share} and {Forecast} till 2028},
	url = {https://www.marketwatch.com/press-release/speech-recognition-software-market-size-share-and-forecast-till-2028-2022-09-16},
	abstract = {Sep 16, 2022 (Reportmines via Comtex) --
Pre and Post Covid is covered and Report Customization is available. It provides a detailed market research report...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\GF4FZHDT\\speech-recognition-software-market-size-share-and-forecast-till-2028-2022-09-16.html:text/html},
}

@misc{noauthor_pwc_2018,
	title = {{PwC}: {Lack} of trust in {AI} assistants like {Alexa} could hinder adoption},
	shorttitle = {{PwC}},
	url = {https://venturebeat.com/ai/pwc-lack-of-trust-in-ai-assistants-like-alexa-could-hinder-adoption/},
	abstract = {Register now for your free virtual pass to the Low-Code/No-Code Summit this November 9. Hear from executives from Service Now, Credit Karma, Stitch Fix, Appian, and more. Learn more. Tech leaders like Microsoft CEO Satya Nadella argue that voice and conversational AI represent a new age of computing, but a survey and report released today by […]},
	language = {en-US},
	urldate = {2022-10-29},
	journal = {VentureBeat},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KIY5FN7G\\pwc-lack-of-trust-in-ai-assistants-like-alexa-could-hinder-adoption.html:text/html},
}

@misc{noauthor_rise_2022,
	title = {The {Rise} and {Stall} of the {U}.{S}. {Smart} {Speaker} {Market} - {New} {Report}},
	url = {https://voicebot.ai/2022/03/02/the-rise-and-stall-of-the-u-s-smart-speaker-market-new-report/},
	abstract = {Smart speakers powered by Alexa, Google Assistant, and Siri represented a white-hot consumer device market in the 2016-2019 period. Surging..},
	language = {en-US},
	urldate = {2022-10-29},
	journal = {Voicebot.ai},
	month = mar,
	year = {2022},
	note = {Section: Ai},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\LNTBT9F4\\the-rise-and-stall-of-the-u-s-smart-speaker-market-new-report.html:text/html},
}

@inproceedings{k_e_mats_blomberg_automatisk_1997,
	title = {Automatisk igenk¨anning av tal},
	author = {K. E. Mats Blomberg},
	year = {1997},
}

@phdthesis{lowerre_harpy_1976,
	title = {The {Harpy} speech recognition system},
	url = {https://ui.adsabs.harvard.edu/abs/1976PhDT........81L},
	abstract = {The Harpy connected speech recognition system is the result of an attempt to understand the relative importance of various design choices of two earlier speech recognition systems developed at Carnegie-Mellon University: The Hearsay-1 system and the Dragon system. Knowledge is represented in the Hearsay-1 system as procedures and in the Dragon system as a Markov network with a-priori transition probabilities between states. Systematic performance analysis of various design choices of these two systems resulted in the HARPY system, in which knowledge is represented as a finite state transition network but without the a-priori transition probabilities. Harpy searches only a few 'best' syntactic (and acoustic) paths in parallel to determine the optimal path, and uses segmentation to effectively reduce the utterance length, thereby reducing the number of state probability updates that must be done. Several new heuristics have been added to the HARPY system to improve its performance and speed: detection of common sub-nets and collapsing them to reduce overall network size and complexity, eliminating the need for doing an acoustic match for all phonemic types at every time sample, and semi-automatic techniques for learning the lexical representations (that are needed for a steady-state system of this type) and the phonemic templates from training data, thus automatically accounting for the commonly occurring intra-word coarticulation and juncture phenomena. Inter-word phenomena are handled by the use of juncture rules which are applied at network generation time, thereby eliminating the need for repetitive and time consuming application of phonological rules during the recognition phase.},
	urldate = {2022-10-30},
	author = {Lowerre, B. T.},
	month = apr,
	year = {1976},
	note = {Publication Title: Ph.D. Thesis
ADS Bibcode: 1976PhDT........81L},
	keywords = {Speech Recognition, Communications and Radar, Dictionaries, Grammars, Heuristic Methods, Knowledge, Performance Tests},
}

@misc{noauthor_ibm_2003,
	type = {{TS200}},
	title = {{IBM} {Archives}: {IBM} {Shoebox}},
	copyright = {© Copyright IBM Corp. 2011},
	shorttitle = {{IBM} {Archives}},
	url = {//www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html},
	abstract = {IBM Archives: Exhibits: IBM special products (vol. 1): IBM Shoebox},
	language = {en-US},
	urldate = {2022-10-30},
	month = jan,
	year = {2003},
}

@misc{noauthor_audrey_2021,
	title = {Audrey, {Alexa}, {Hal}, and {More}},
	url = {https://computerhistory.org/blog/audrey-alexa-hal-and-more/},
	abstract = {Star Trek got speech recognition right. How did this science fiction fantasy of only a few decades ago come true? What's the history of speech recognition and where is it headed?},
	language = {en},
	urldate = {2022-10-30},
	journal = {CHM},
	month = jun,
	year = {2021},
	note = {Section: Curatorial Insights},
}

@article{levinson_continuously_1986,
	title = {Continuously variable duration hidden {Markov} models for automatic speech recognition},
	volume = {1},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230886800092},
	doi = {10.1016/S0885-2308(86)80009-2},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {Computer Speech \& Language},
	author = {Levinson, S.E.},
	month = mar,
	year = {1986},
	pages = {29--45},
}

@article{lee_speech_1990,
	title = {Speech recognition using hidden {Markov} models: {A} {CMU} perspective},
	volume = {9},
	issn = {01676393},
	shorttitle = {Speech recognition using hidden {Markov} models},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167639390900255},
	doi = {10.1016/0167-6393(90)90025-5},
	language = {en},
	number = {5-6},
	urldate = {2022-10-30},
	journal = {Speech Communication},
	author = {Lee, Kai-Fu and Hon, Hsiao-Wuen and Hwang, Mei-Yuh and Huang, Xuedong},
	month = dec,
	year = {1990},
	pages = {497--508},
}

@article{baker_dragon_1975,
	title = {The {DRAGON} system--{An} overview},
	volume = {23},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1162650/},
	doi = {10.1109/TASSP.1975.1162650},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Baker, J.},
	month = feb,
	year = {1975},
	pages = {24--29},
}

@misc{kikel_history_2022,
	title = {History of {Voice} {Recognition} {Technology}},
	url = {https://www.totalvoicetech.com/a-brief-history-of-voice-recognition-technology/},
	abstract = {Learn about the history of voice recognition technology. It has come a long way since the 1950's, and will continue to evolve.},
	language = {en-US},
	urldate = {2022-10-30},
	journal = {Total Voice Technologies},
	author = {Kikel, Chris},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Y55S4SD6\\a-brief-history-of-voice-recognition-technology.html:text/html},
}

@article{shneiderman_limits_2000,
	title = {The limits of speech recognition},
	volume = {43},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/348941.348990},
	doi = {10.1145/348941.348990},
	language = {en},
	number = {9},
	urldate = {2022-10-30},
	journal = {Communications of the ACM},
	author = {Shneiderman, Ben},
	month = sep,
	year = {2000},
	pages = {63--65},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC9Z8S9X\\Shneiderman - 2000 - The limits of speech recognition.pdf:application/pdf},
}

@article{humes_speech-recognition_1990,
	title = {Speech-{Recognition} {Difficulties} of the {Hearing}-{Impaired} {Elderly}: {The} {Contributions} of {Audibility}},
	volume = {33},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Speech-{Recognition} {Difficulties} of the {Hearing}-{Impaired} {Elderly}},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3304.726},
	doi = {10.1044/jshr.3304.726},
	abstract = {The role that sensorineural hearing loss plays in the speech-recognition difficulties of the hearing-impaired elderly is examined. One approach to this issue was to make between-group comparisons of performance for three groups of subjects: (a) young normal-hearing adults; (b) elderly hearing-impaired adults; and (c) young normal-hearing adults with simulated sensorineural hearing loss equivalent to that of the elderly subjects produced by a spectrally shaped masking noise. Another approach to this issue employed correlational analyses to examine the relation between audibility and speech recognition within the group of elderly hearing-impaired subjects. An additional approach was pursued in which an acoustical index incorporating adjustments for threshold elevation was used to examine the role audibility played in the speech-recognition performance of the hearing-impaired elderly. A wide range of listening conditions was sampled in this experiment. The conclusion was that the primary determiner of speech-recognition performance in the elderly hearing-impaired subjects was their threshold elevation.},
	language = {en},
	number = {4},
	urldate = {2022-10-30},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Humes, Larry E. and Roberts, Lisa},
	month = dec,
	year = {1990},
	pages = {726--735},
}

@techreport{meyer_bernd_whats_2013,
	title = {What's the difference? {Comparing} humans and machines on the {Aurora} 2 speech recognition task},
	url = {https://www.researchgate.net/publication/290246535_What's_the_difference_Comparing_humans_and_machines_on_the_Aurora_2_speech_recognition_task/references},
	abstract = {The comparison of human speech recognition (HSR) and machine performance allows to learn from the differences between HSR and automatic speech recognition (ASR) and serves as motivation for using auditory-inspired strategies in ASR. The recognition of noisy digit strings from the Aurora 2 framework is one of the most widely used tasks in the ASR community. This paper establishes a baseline with a close-to-optimal classifier, i.e., our auditory system by comparing results from 10 normal-hearing listeners to the Aurora 2 reference system using identical speech material. The baseline ASR system reaches the human performance level only when the signal-to-noise ratio is increased by 10 or 21 dB depending on the training condition. The recognition of 1-digit recordings was found to be considerably better for HSR, indicating that onset detection is an important feature neglected in standard ASR systems. Results of recent studies are considered in the light of these findings to measure how far we have come on the way to human speech recognition performance.},
	institution = {Medical Physics, Carl-von-Ossietzky Universit¨at Oldenburg, Germany},
	author = {Meyer, Bernd},
	month = jan,
	year = {2013},
	pages = {2634--2638},
}

@article{lippmann_speech_1997,
	title = {Speech recognition by machines and humans},
	volume = {22},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639397000216},
	doi = {10.1016/S0167-6393(97)00021-6},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {Speech Communication},
	author = {Lippmann, Richard P.},
	month = jul,
	year = {1997},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\D25IW8PV\\Lippmann - 1997 - Speech recognition by machines and humans.pdf:application/pdf},
}

@article{meyer_effect_2011,
	title = {Effect of speech-intrinsic variations on human and automatic recognition of spoken phonemes},
	volume = {129},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3514525},
	doi = {10.1121/1.3514525},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Meyer, Bernd T. and Brand, Thomas and Kollmeier, Birger},
	month = jan,
	year = {2011},
	pages = {388--403},
}

@book{gold_speech_2011,
	address = {Hoboken, N.J},
	edition = {2nd ed},
	title = {Speech and audio signal processing: processing and perception of speech and music},
	isbn = {978-0-470-19536-9},
	shorttitle = {Speech and audio signal processing},
	publisher = {Wiley},
	author = {Gold, Bernard and Morgan, Nelson and Ellis, Dan},
	year = {2011},
	keywords = {Digital techniques, Electronic music, Signal processing, Speech processing systems},
}

@inproceedings{xu_semantic_2010,
	address = {Berkeley, CA, USA},
	title = {Semantic understanding by combining extended {CFG} parser with {HMM} model},
	isbn = {978-1-4244-7904-7},
	url = {http://ieeexplore.ieee.org/document/5700824/},
	doi = {10.1109/SLT.2010.5700824},
	urldate = {2022-10-30},
	booktitle = {2010 {IEEE} {Spoken} {Language} {Technology} {Workshop}},
	publisher = {IEEE},
	author = {Xu, Yushi and Seneff, Stephanie and Li, Alice and Polifroni, Joe},
	month = dec,
	year = {2010},
	pages = {67--72},
}

@inproceedings{ye-yi_wang_is_2003,
	address = {St Thomas, VI, USA},
	title = {Is word error rate a good indicator for spoken language understanding accuracy},
	isbn = {978-0-7803-7980-0},
	url = {http://ieeexplore.ieee.org/document/1318504/},
	doi = {10.1109/ASRU.2003.1318504},
	urldate = {2022-10-30},
	booktitle = {2003 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({IEEE} {Cat}. {No}.{03EX721})},
	publisher = {IEEE},
	author = {{Ye-Yi Wang} and Acero, A. and Chelba, C.},
	year = {2003},
	pages = {577--582},
}

@inproceedings{riccardi_stochastic_1998,
	title = {Stochastic language models for speech recognition and understanding},
	url = {https://www.isca-speech.org/archive/icslp_1998/riccardi98b_icslp.html},
	doi = {10.21437/ICSLP.1998-502},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {5th {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1998)},
	publisher = {ISCA},
	author = {Riccardi, Giuseppe and Gorin, Allen L.},
	month = nov,
	year = {1998},
	pages = {paper 0111--0},
}

@inproceedings{esteve_conceptual_2003,
	title = {Conceptual decoding for spoken dialog systems},
	url = {https://www.isca-speech.org/archive/eurospeech_2003/esteve03_eurospeech.html},
	doi = {10.21437/Eurospeech.2003-260},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {8th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 2003)},
	publisher = {ISCA},
	author = {Esteve, Yannick and Raymond, Christian and Bechet, Frédéric and Mori, Renato De},
	month = sep,
	year = {2003},
	pages = {617--620},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\KSIVHEB4\\Esteve et al. - 2003 - Conceptual decoding for spoken dialog systems.pdf:application/pdf},
}

@phdthesis{muhammad_danyal_khan_automatic_2022,
	address = {Karachi, Pakistan},
	title = {Automatic {Speech} {Recognition} to {Detect} {Malicious}/{Mal}-intended {Speech}},
	url = {https://www.researchgate.net/publication/365349074_Automatic_Speech_Recognition_to_Detect_MaliciousMal-intended_Speech},
	abstract = {Speech Recognition is a growing field since last five decades and there have been many advancements which has led to its applications like Speech to Text. This has allowed a possibility of Transcription of audio files to text and much of work is available on this in English, Arabic and Cantonese Languages.

However, Urdu is a low-resource language in field of ASR although it is the world's \$11{\textasciicircum}\{th\}\$ most widely spoken language, with 232 Million speakers worldwide. There are no applicable models available which can be readily deployed for Speech To Text in a noisy scenario which is why Urdu Community is devoid of all the benefits of ASR.

Apart from noise problems in normal telephonic or call-center conversations in Urdu, people tend to spontaneously use words from other language since Pakistan is a multi-cultural society, which presents a code-switching problem.

Hence, we proposed an implementation of Automatic Speech Recognition/ Speech to Text System in a noisy/ call center environment with less labelled training data available using Hybrid HMM-DNN in a Resource constraint environment in terms of time, budget, computation power, HR etc.

We were able to access to call center audio files, thanks to CPLC {\textbackslash}cite\{cplc\_cplc\_nodate\} (a semi-government Law Enforcement Agency), some of which was labelled manually. We further integrated various open source data-sets to include more variety in data-set. The data comprised of mix of noisy and clean audio as well as single utterances and long sentences (1-20 second audios). It was split into 6.5 hours and 3.5 hours of train and test data-set respectively.

The Language Model was developed from the training data-set and for acoustic modelling we used HMM (Monophone and Triphone) based on which we trained a Neural Network based model using Chain CNN-TDNN, achieving up to 5.2{\textbackslash}\% WER with noisy and clean data-set as well as on single word to spontaneous speech data as well.


    {\textbackslash}textbf\{{\textbackslash}{\textbackslash} Keywords:\} {\textbackslash}textit\{{\textbackslash}{\textbackslash} Speech Recognition, ASR, Call Center, audio transcription, Urdu language, Code-switched Urdu ASR, Speech to Text, AI, Cyber Security, ASR for Resource Constrained Environment, ASR for Noisy Environment, ASR for Low Resource Languages, Under Resourced Languages\}},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Muhammad Danyal Khan},
	month = oct,
	year = {2022},
	doi = {10.13140/RG.2.2.34319.05281},
}

@article{wang_overview_2019,
	title = {Overview of end-to-end speech recognition},
	volume = {1187},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1187/5/052068},
	doi = {10.1088/1742-6596/1187/5/052068},
	number = {5},
	urldate = {2022-11-05},
	journal = {Journal of Physics: Conference Series},
	author = {Wang, Song and Li, Guanyu},
	month = apr,
	year = {2019},
	pages = {052068},
}

@techreport{timothy_r_anderson_applications_1998,
	address = {Bre’tigny, France},
	title = {{APPLICATIONS} {OF} {SPEECH}-{BASED} {CONTROL}},
	author = {Timothy R. Anderson},
	month = oct,
	year = {1998},
	note = {Ohio, USA, 14-15 October 1998
Published in RTO EN-3},
}

@article{haton_problems_1994,
	title = {Problems and solutions for noisy speech recognition},
	volume = {04},
	issn = {1155-4339},
	url = {http://www.edpsciences.org/10.1051/jp4:1994592},
	doi = {10.1051/jp4:1994592},
	number = {C5},
	urldate = {2022-11-05},
	journal = {Le Journal de Physique IV},
	author = {Haton, J.-P.},
	month = may,
	year = {1994},
	pages = {C5--439--C5--448},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5UM3AEJV\\Haton - 1994 - Problems and solutions for noisy speech recognitio.pdf:application/pdf},
}


@article{boothroyd_statistical_1968,
	title = {Statistical {Theory} of the {Speech} {Discrimination} {Score}},
	volume = {43},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1910787},
	doi = {10.1121/1.1910787},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Boothroyd, A.},
	month = feb,
	year = {1968},
	pages = {362--367},
}

@techreport{l_lamel_quaero_2010,
	address = {Limsi Orsay},
	title = {Quaero program - ctc project - progress report on task 5.1: {Speech} to text},
	url = {https://www.linkedin.com/company/quaero/},
	urldate = {2022-11-05},
	author = {L. Lamel},
	year = {2010},
	note = {http://www.quaero.org/developpement-scientifique-onglet-synthese/
Cd.ctc.5.6., Quaero Program, .},
}

@inproceedings{lawrence_r_rabiner_and_sorin_dusan_can_2005,
	title = {{CAN} {AUTOMATIC} {SPEECH} {RECOGNITION} {LEARN} {MORE} {FROM} {HUMAN} {SPEECH} {PERCEPTION}},
	author = {Lawrence R. Rabiner {and} Sorin Dusan},
	year = {2005},
}

@article{benzeghiba_automatic_2007,
	title = {Automatic speech recognition and speech variability: {A} review},
	volume = {49},
	issn = {01676393},
	shorttitle = {Automatic speech recognition and speech variability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000404},
	doi = {10.1016/j.specom.2007.02.006},
	language = {en},
	number = {10-11},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Benzeghiba, M. and De Mori, R. and Deroo, O. and Dupont, S. and Erbes, T. and Jouvet, D. and Fissore, L. and Laface, P. and Mertins, A. and Ris, C. and Rose, R. and Tyagi, V. and Wellekens, C.},
	month = oct,
	year = {2007},
	pages = {763--786},
}

@inproceedings{rk_moore_comparison_2003,
	address = {Gen`eve, Suisse},
	title = {A comparison of the data requirements of automatic speech recognition systems and human listeners},
	booktitle = {In {Proceedings} of {Eurospeech}},
	author = {R.K. Moore},
	year = {2003},
	pages = {2582--2584},
}

@inproceedings{rk_moore_and_a_cutler_constraints_2001,
	title = {Constraints on theories of humans vs. machine recognition of speech.},
	booktitle = {{SPRAAC} {Workshop} on {HSR} as {Pattern} {Classification}},
	author = {R.K. Moore {and} A. Cutler},
	year = {2001},
}


@inproceedings{n_deshmuk_benchmarking_nodate,
	title = {Benchmarking human performance for continuous speech recognition},
	booktitle = {{ICSLP} 1996.},
	author = {N. Deshmuk and R.J. Duncan, and J. Picone and A. Ganapathiraju},
}


@inproceedings{n_deshmuk_rj_duncan_a_ganapathraju_and_j_picone_response_nodate,
	address = {Philadelphia, USA},
	title = {Response time as metric for comparison of speech recognition by humans and machines},
	booktitle = {{ICSLP}},
	author = {N. Deshmuk, R.J. Duncan, A. Ganapathraju, {and} J. Picone},
}

@inproceedings{lcw_pols_flexible_1997,
	title = {Flexible human speech recognition},
	booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}},
	author = {L.C.W. Pols},
	year = {1997},
}

@article{scharenborg_how_2005,
	title = {How {Should} a {Speech} {Recognizer} {Work}?},
	volume = {29},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1207/s15516709cog0000_37},
	doi = {10.1207/s15516709cog0000_37},
	language = {en},
	number = {6},
	urldate = {2022-11-05},
	journal = {Cognitive Science},
	author = {Scharenborg, Odette and Norris, Dennis and ten Bosch, Louis and McQueen, James M.},
	month = nov,
	year = {2005},
	pages = {867--918},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\8MAVAN8F\\Scharenborg et al. - 2005 - How Should a Speech Recognizer Work.pdf:application/pdf},
}

@article{scharenborg_reaching_2007,
	title = {Reaching over the gap: {A} review of efforts to link human and automatic speech recognition research},
	volume = {49},
	issn = {01676393},
	shorttitle = {Reaching over the gap},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000106},
	doi = {10.1016/j.specom.2007.01.009},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Scharenborg, Odette},
	month = may,
	year = {2007},
	pages = {336--347},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\A8IJ4SXY\\Scharenborg - 2007 - Reaching over the gap A review of efforts to link.pdf:application/pdf},
}

@inproceedings{furui_robust_2003,
	title = {Robust methods in automatic speech recognition and understanding},
	url = {https://www.isca-speech.org/archive/eurospeech_2003/furui03_eurospeech.html},
	doi = {10.21437/Eurospeech.2003-575},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {8th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 2003)},
	publisher = {ISCA},
	author = {Furui, Sadaoki},
	month = sep,
	year = {2003},
	pages = {1993--1998},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\BJUNT7HU\\Furui - 2003 - Robust methods in automatic speech recognition and.pdf:application/pdf},
}

@inproceedings{takahiro_shinozaki_and_sadaoki_furui_assessment_2003,
	title = {An assessment of automatic recognition techniques for spontaneous speech in comparison with human performance},
	url = {https://www.isca-speech.org/archive/sspr_2003/shinozaki03_sspr.html},
	booktitle = {Proc. {ISCA}/{IEEE} {Workshop} on {Spontaneous} {Speech} {Processing} and {Recognition}},
	author = {Takahiro Shinozaki {and} Sadaoki Furui},
	year = {2003},
	pages = {paper MAP15},
}

@inproceedings{a_cutler_and_t_robinson_1992_1992,
	address = {Banff, Canada.},
	title = {1992. {Response} time as metric for comparison of speech recognition by humans and machines},
	booktitle = {In {Proc}. of {ICSLP}},
	author = {A. Cutler {and} T. Robinson},
	year = {1992},
	pages = {189--192},
}

@inproceedings{da_van_leeuwen_l-g_van_den_berg_and_hjmsteeneken_human_1995,
	address = {Madrid, Spain},
	title = {Human benchmarks for speaker independent large vocabulary recognition performance},
	booktitle = {In {Proc}. of {Eurospeech}},
	author = {D.A. Van Leeuwen, L.-G. Van den Berg, {and} H.J.M.Steeneken},
	year = {1995},
	pages = {1461--1464},
}

@inproceedings{b_meyer_and_twesker_human-machine_2006,
	address = {Toulouse, France},
	title = {A human-machine comparison in speech recognition based on a logatome corpus},
	booktitle = {In {Proc}. of {Workshop} on {Speech} {Recognition} and {Intrinsic} {Variation}},
	author = {B. Meyer {and} T.Wesker},
	year = {2006},
}

@article{sroka_and_ld_braida_human_2005,
	title = {Human and machine consonant recognition.},
	journal = {Speech Communication, 45(2005)},
	author = {Sroka {and} L.D. Braida},
	year = {2005},
	pages = {401--423},
}

@inproceedings{w_shen_j_olive_and_d_jones_two_2008,
	address = {Brisbane, Australia},
	title = {Two protocols comparing human and machine phonetic discrimination performance in conversational speech},
	booktitle = {In {Proc}. of {Interspeech}},
	author = {W. Shen, J. Olive, {and} D. Jones},
	year = {2008},
}

@article{rost_michael_introducing_1994,
	title = {Introducing listening},
	author = {Rost, Michael},
	month = jan,
	year = {1994},
}

@book{rost_michael_teaching_2016,
	title = {Teaching and researching listening: {Third} edition},
	author = {Rost, Michael},
	month = jan,
	year = {2016},
}

@article{webb_vocabulary_2009,
	title = {Vocabulary {Demands} of {Television} {Programs}},
	volume = {59},
	issn = {00238333, 14679922},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9922.2009.00509.x},
	doi = {10.1111/j.1467-9922.2009.00509.x},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Language Learning},
	author = {Webb, Stuart and Rodgers, Michael P. H.},
	month = jun,
	year = {2009},
	pages = {335--366},
}

@article{webb_lexical_2009,
	title = {The {Lexical} {Coverage} of {Movies}},
	volume = {30},
	issn = {0142-6001, 1477-450X},
	url = {https://academic.oup.com/applij/article-lookup/doi/10.1093/applin/amp010},
	doi = {10.1093/applin/amp010},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Applied Linguistics},
	author = {Webb, S. and Rodgers, M. P. H.},
	month = sep,
	year = {2009},
	pages = {407--427},
}

@article{webb_stuart_using_2010,
	title = {Using glossaries to increase the lexical coverage of television programs},
	volume = {22},
	journal = {Reading in a Foreign Language},
	author = {Webb, Stuart},
	month = jan,
	year = {2010},
}

@article{goldwater_which_2010,
	title = {Which words are hard to recognize? {Prosodic}, lexical, and disfluency factors that increase speech recognition error rates},
	volume = {52},
	issn = {01676393},
	shorttitle = {Which words are hard to recognize?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639309001599},
	doi = {10.1016/j.specom.2009.10.001},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Goldwater, Sharon and Jurafsky, Dan and Manning, Christopher D.},
	month = mar,
	year = {2010},
	pages = {181--200},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\W4MWM6JM\\Goldwater et al. - 2010 - Which words are hard to recognize Prosodic, lexic.pdf:application/pdf},
}

@article{field_bricks_2008,
	title = {Bricks or {Mortar}: {Which} {Parts} of the {Input} {Does} a {Second} {Language} {Listener} {Rely} on?},
	volume = {42},
	issn = {0039-8322, 1545-7249},
	shorttitle = {Bricks or {Mortar}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.1545-7249.2008.tb00139.x},
	doi = {10.1002/j.1545-7249.2008.tb00139.x},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {TESOL Quarterly},
	author = {Field, John},
	month = sep,
	year = {2008},
	pages = {411--432},
}

@article{webb_learning_2011,
	title = {Learning {Collocations}: {Do} the {Number} of {Collocates}, {Position} of the {Node} {Word}, and {Synonymy} {Affect} {Learning}?},
	volume = {32},
	issn = {1477-450X, 0142-6001},
	shorttitle = {Learning {Collocations}},
	url = {https://academic.oup.com/applij/article-lookup/doi/10.1093/applin/amq051},
	doi = {10.1093/applin/amq051},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Applied Linguistics},
	author = {Webb, Stuart and Kagimoto, Eve},
	month = jul,
	year = {2011},
	pages = {259--276},
}

@article{webb_pre-learning_2010,
	title = {Pre-learning low-frequency vocabulary in second language television programmes},
	volume = {14},
	issn = {1362-1688, 1477-0954},
	url = {http://journals.sagepub.com/doi/10.1177/1362168810375371},
	doi = {10.1177/1362168810375371},
	abstract = {This study investigated the potential of pre-learning frequently occurring low-frequency vocabulary as a means to increase comprehension of television and incidental vocabulary learning through watching television. Eight television programmes, each representing different television genres, were analysed using the RANGE program to determine the 10 most frequent low-frequency word-families in each programme and the coverage that they represented. The results showed that coverage of the 10 most frequent low-frequency word-families ranged from 0.70\% to 3.91\%, coverage of the most frequent 3,000 to 3,999 word-families ranged from 0.22\% to 2.58\%, and coverage of the 4,000 to 4,999 word-families ranged from 0.35\% to 1.96\%. This result shows the relative value of pre-learning vocabulary in television programmes and provides a strong argument for pre-learning vocabulary. The findings also suggested that if learners knew the most frequent 3,000 word-families and pre-learned low-frequency vocabulary, comprehension and incidental vocabulary learning may increase.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Language Teaching Research},
	author = {Webb, Stuart},
	month = oct,
	year = {2010},
	pages = {501--515},
}

@article{webb_corpus_2010,
	title = {A corpus driven study of the potential for vocabulary learning through watching movies},
	volume = {15},
	issn = {1384-6655, 1569-9811},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.4.03web},
	doi = {10.1075/ijcl.15.4.03web},
	abstract = {In this corpus driven study, the scripts of 143 movies consisting of 1,267,236 running words were analyzed using the RANGE program (Heatley et al. 2002) to determine the number of encounters with low frequency words. Low frequency words were operationalized as items from Nation’s (2004) 4th to 14th 1,000-word BNC lists. The results showed that in a single movie, few words were encountered 10 or more times indicating that only a small number of words may be learned through watching one movie. However, as the number of movies analyzed increased, the number of words encountered 10 or more times increased. Twenty-three percent of the word families from Nation’s (2004) 4th 1,000-word list were encountered 10 or more times in a set of 70 movies. This indicates that if learners watch movies regularly over a long period of time, there is the potential for significant incidental learning to occur},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {International Journal of Corpus Linguistics},
	author = {Webb, Stuart},
	month = nov,
	year = {2010},
	pages = {497--519},
}

@article{webb_selecting_2011,
	title = {Selecting {Television} {Programs} for {Language} {Learning}: {Investigating} {Television} {Programs} from the {Same} {Genre}},
	volume = {11},
	shorttitle = {Selecting {Television} {Programs} for {Language} {Learning}},
	doi = {10.6018/ijes.11.1.137131},
	abstract = {The scripts of 288 television episodes were analysed to determine the extent to which vocabulary reoccurs in television programs from the same subgenres and unrelated television programs from different genres. Episodes from two programs from each of the following three subgenres of the American drama genre: medical, spy/action, and criminal forensic investigation were compared with different sets of random episodes. The results showed that although there were an equivalent number of running words in each set of episodes, the episodes from programs within the same subgenre contained fewer word families than random programs. The findings also showed that low frequency word families (4000-14,000 levels) reoccur more often in programs within the same subgenre. Together the results indicate that watching programs within the same subgenre may be an effective approach to language learning with television because it reduces the lexical demands of viewing and increases the potential for vocabulary learning. RESUMEN Los guiones de 288 episodios televisivos se analizaron para determinar el alcance de la recursividad del vocabulario en programas de televisión del mismo subgénero y en programas no relacionados de géneros diferentes. Se compararon episodios de tres subgéneros del drama americano: médico, de espías/acción y de investigación forense, con varios grupos de episodios elegidos al azar. Los resultados muestran que, aunque el número de palabras en cada grupo de episodios era equivalente, los episodios del mismo subgénero contienen menos familias de palabras que aquellos elegidos al azar. Los hallazgos mostraron que las familias de baja frecuencia (niveles de 4.000-14.000) se repiten con más frecuencia en los programas del mismo subgénero. En conjunto, los resultados indican que el visionado de programas del mismo subgénero puede ser un método efectivo para aprender el lenguaje por medio de la televisión porque reduce la demanda léxica de la proyección y aumenta el potencial de aprendizaje de vocabulario. PALABRAS CLAVE: Comprensión, Lingüística de corpus, género, aprendizaje incidental, televisión, cobertura del vocabulario, frecuencia léxica.},
	journal = {International Journal of English Studies (IJES)},
	author = {Webb, Stuart},
	month = jun,
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IBB7BUSU\\Webb - 2011 - Selecting Television Programs for Language Learnin.pdf:application/pdf},
}

@book{holmes_introduction_2013,
	edition = {0},
	title = {An {Introduction} to {Sociolinguistics}},
	isbn = {978-1-292-00506-5},
	url = {https://www.taylorfrancis.com/books/9781317860723},
	language = {en},
	urldate = {2022-11-05},
	publisher = {Routledge},
	author = {Holmes, Janet},
	month = oct,
	year = {2013},
	doi = {10.4324/9781315833057},
}

@inproceedings{mirzaei_partial_2014,
	title = {Partial and synchronized captioning: {A} new tool for second language listening development},
	isbn = {978-1-908416-20-9},
	shorttitle = {Partial and synchronized captioning},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2014.000223},
	doi = {10.14705/rpnet.2014.000223},
	urldate = {2022-11-05},
	booktitle = {{CALL} {Design}: {Principles} and {Practice} - {Proceedings} of the 2014 {EUROCALL} {Conference}, {Groningen}, {The} {Netherlands}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2014},
	pages = {230--236},
}

@inproceedings{shimogori_automatically_2010,
	address = {Copenhagen, Denmark},
	title = {Automatically generated captions: will they help non-native speakers communicate in english?},
	isbn = {978-1-4503-0108-4},
	shorttitle = {Automatically generated captions},
	url = {http://portal.acm.org/citation.cfm?doid=1841853.1841865},
	doi = {10.1145/1841853.1841865},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 3rd international conference on {Intercultural} collaboration - {ICIC} '10},
	publisher = {ACM Press},
	author = {Shimogori, Nobuhiro and Ikeda, Tomoo and Tsuboi, Sougo},
	year = {2010},
	pages = {79},
}

@inproceedings{shinozaki_error_2001,
	address = {Madonna di Campiglio, Italy},
	title = {Error analysis using decision trees in spontaneous presentation speech recognition},
	isbn = {978-0-7803-7343-3},
	url = {http://ieeexplore.ieee.org/document/1034621/},
	doi = {10.1109/ASRU.2001.1034621},
	urldate = {2022-11-05},
	booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}, 2001. {ASRU} '01.},
	publisher = {IEEE},
	author = {Shinozaki, T. and Furui, S.},
	year = {2001},
	pages = {198--201},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AYTJH6KH\\Shinozaki and Furui - 2001 - Error analysis using decision trees in spontaneous.pdf:application/pdf},
}

@article{thomson_computer_2011,
	title = {Computer {Assisted} {Pronunciation} {Training}: {Targeting} {Second} {Language} {Vowel} {Perception} {Improves} {Pronunciation}},
	volume = {28},
	issn = {07427778},
	shorttitle = {Computer {Assisted} {Pronunciation} {Training}},
	url = {http://www.equinoxpub.com/journals/index.php/CALICO/article/view/22985},
	doi = {10.11139/cj.28.3.744-765},
	number = {3},
	urldate = {2022-11-05},
	journal = {CALICO Journal},
	author = {Thomson, Ron I.},
	month = may,
	year = {2011},
	pages = {744--765},
}

@article{yuan_pauses_2021,
	title = {Pauses for {Detection} of {Alzheimer}’s {Disease}},
	volume = {2},
	issn = {2624-9898},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2020.624488/full},
	doi = {10.3389/fcomp.2020.624488},
	abstract = {Pauses, disfluencies and language problems in Alzheimer’s disease can be naturally modeled by fine-tuning Transformer-based pre-trained language models such as BERT and ERNIE. Using this method with pause-encoded transcripts, we achieved 89.6\% accuracy on the test set of the ADReSS (
              A
              lzheimer’s
              D
              ementia
              Re
              cognition through
              S
              pontaneous
              S
              peech) Challenge. The best accuracy was obtained with ERNIE, plus an encoding of pauses. Robustness is a challenge for large models and small training sets. Ensemble over many runs of BERT/ERNIE fine-tuning reduced variance and improved accuracy. We found that
              um
              was used much less frequently in Alzheimer’s speech, compared to
              uh
              . We discussed this interesting finding from linguistic and cognitive perspectives.},
	urldate = {2022-11-05},
	journal = {Frontiers in Computer Science},
	author = {Yuan, Jiahong and Cai, Xingyu and Bian, Yuchen and Ye, Zheng and Church, Kenneth},
	month = jan,
	year = {2021},
	pages = {624488},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\67RYRPW8\\Yuan et al. - 2021 - Pauses for Detection of Alzheimer’s Disease.pdf:application/pdf},
}

@article{bucks_analysis_2000,
	title = {Analysis of spontaneous, conversational speech in dementia of {Alzheimer} type: {Evaluation} of an objective technique for analysing lexical performance},
	volume = {14},
	issn = {0268-7038, 1464-5041},
	shorttitle = {Analysis of spontaneous, conversational speech in dementia of {Alzheimer} type},
	url = {http://www.tandfonline.com/doi/abs/10.1080/026870300401603},
	doi = {10.1080/026870300401603},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Aphasiology},
	author = {Bucks, R. S. and Singh, S. and Cuerden, J. M. and Wilcock, G. K.},
	month = jan,
	year = {2000},
	pages = {71--91},
}

@article{singh_evaluation_2001,
	title = {Evaluation of an objective technique for analysing temporal variables in {DAT} spontaneous speech},
	volume = {15},
	issn = {0268-7038, 1464-5041},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02687040143000041},
	doi = {10.1080/02687040143000041},
	language = {en},
	number = {6},
	urldate = {2022-11-05},
	journal = {Aphasiology},
	author = {Singh, Sameer and Bucks, Romola S. and Cuerden, Joanne M.},
	month = jun,
	year = {2001},
	pages = {571--583},
}

@article{pulido_alzheimers_2020,
	title = {Alzheimer's disease and automatic speech analysis: {A} review},
	volume = {150},
	issn = {09574174},
	shorttitle = {Alzheimer's disease and automatic speech analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420300397},
	doi = {10.1016/j.eswa.2020.113213},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Pulido, María Luisa Barragán and Hernández, Jesús Bernardino Alonso and Ballester, Miguel Ángel Ferrer and González, Carlos Manuel Travieso and Mekyska, Jiří and Smékal, Zdeněk},
	month = jul,
	year = {2020},
	pages = {113213},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4YMA25IC\\Pulido et al. - 2020 - Alzheimer's disease and automatic speech analysis.pdf:application/pdf},
}

@inproceedings{li_comparative_2021,
	address = {Toronto, ON, Canada},
	title = {A {Comparative} {Study} of {Acoustic} and {Linguistic} {Features} {Classification} for {Alzheimer}'s {Disease} {Detection}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414147/},
	doi = {10.1109/ICASSP39728.2021.9414147},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	pages = {6423--6427},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\54WJ2NZX\\Li et al. - 2021 - A Comparative Study of Acoustic and Linguistic Fea.pdf:application/pdf},
}

@inproceedings{yuan_pause-encoded_2021,
	address = {Toronto, ON, Canada},
	title = {Pause-{Encoded} {Language} {Models} for {Recognition} of {Alzheimer}’s {Disease} and {Emotion}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413548/},
	doi = {10.1109/ICASSP39728.2021.9413548},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Yuan, Jiahong and Cai, Xingyu and Church, Kenneth},
	month = jun,
	year = {2021},
	pages = {7293--7297},
}

@inproceedings{koo_exploiting_2020,
	title = {Exploiting {Multi}-{Modal} {Features} from {Pre}-{Trained} {Networks} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/koo20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3153},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Koo, Junghyun and Lee, Jie Hwan and Pyo, Jaewoo and Jo, Yujin and Lee, Kyogu},
	month = oct,
	year = {2020},
	pages = {2217--2221},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\P9SR4M7P\\Koo et al. - 2020 - Exploiting Multi-Modal Features from Pre-Trained N.pdf:application/pdf},
}

@article{szatloczki_speaking_2015,
	title = {Speaking in {Alzheimer}’s {Disease}, is {That} an {Early} {Sign}? {Importance} of {Changes} in {Language} {Abilities} in {Alzheimer}’s {Disease}},
	volume = {7},
	issn = {1663-4365},
	shorttitle = {Speaking in {Alzheimer}’s {Disease}, is {That} an {Early} {Sign}?},
	url = {http://journal.frontiersin.org/Article/10.3389/fnagi.2015.00195/abstract},
	doi = {10.3389/fnagi.2015.00195},
	urldate = {2022-11-05},
	journal = {Frontiers in Aging Neuroscience},
	author = {Szatloczki, Greta and Hoffmann, Ildiko and Vincze, Veronika and Kalman, Janos and Pakaski, Magdolna},
	month = oct,
	year = {2015},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ZFBPTM9I\\Szatloczki et al. - 2015 - Speaking in Alzheimer’s Disease, is That an Early .pdf:application/pdf},
}

@article{meilan_speech_2014,
	title = {Speech in {Alzheimer}'s {Disease}: {Can} {Temporal} and {Acoustic} {Parameters} {Discriminate} {Dementia}?},
	volume = {37},
	issn = {1420-8008, 1421-9824},
	shorttitle = {Speech in {Alzheimer}'s {Disease}},
	url = {https://www.karger.com/Article/FullText/356726},
	doi = {10.1159/000356726},
	abstract = {\textbf{\textit{Aims:}} The study explores how speech measures may be linked to language profiles in participants with Alzheimer's disease (AD) and how these profiles could distinguish AD from changes associated with normal aging. \textbf{\textit{Methods:}} We analysed simple sentences spoken by older adults with and without AD. Spectrographic analysis of temporal and acoustic characteristics was carried out using the Praat software. \textbf{\textit{Results:}} We found that measures of speech, such as variations in the percentage of voice breaks, number of periods of voice, number of voice breaks, shimmer (amplitude perturbation quotient), and noise-to-harmonics ratio, characterise people with AD with an accuracy of 84.8\%. \textbf{\textit{Discussion:}} These measures offer a sensitive method of assessing spontaneous speech output in AD, and they discriminate well between people with AD and healthy older adults. This method of evaluation is a promising tool for AD diagnosis and prognosis, and it could be used as a dependent measure in clinical trials.},
	language = {en},
	number = {5-6},
	urldate = {2022-11-05},
	journal = {Dementia and Geriatric Cognitive Disorders},
	author = {Meilán, Juan José G. and Martínez-Sánchez, Francisco and Carro, Juan and López, Dolores E. and Millian-Morell, Lymarie and Arana, José M.},
	year = {2014},
	pages = {327--334},
}

@inproceedings{meghanani_exploration_2021,
	address = {Shenzhen, China},
	title = {An {Exploration} of {Log}-{Mel} {Spectrogram} and {MFCC} {Features} for {Alzheimer}’s {Dementia} {Recognition} from {Spontaneous} {Speech}},
	isbn = {978-1-72817-066-4},
	url = {https://ieeexplore.ieee.org/document/9383491/},
	doi = {10.1109/SLT48900.2021.9383491},
	urldate = {2022-11-05},
	booktitle = {2021 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Meghanani, Amit and C. S., Anoop and Ramakrishnan, A. G.},
	month = jan,
	year = {2021},
	pages = {670--677},
}

@inproceedings{raghavendra_pappagari_jaejin_cho_laureano_moro-velazquez_et_al_using_2020,
	title = {Using state of the art speaker recognition and natural language processing technologies to detect alzheimer’s disease and assess its severity},
	author = {Raghavendra Pappagari, Jaejin Cho, Laureano Moro-Velazquez, et al.},
	year = {2020},
	pages = {2177--2181},
}


@inproceedings{cummins_comparison_2020,
	title = {A {Comparison} of {Acoustic} and {Linguistics} {Methodologies} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/cummins20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2635},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Cummins, Nicholas and Pan, Yilin and Ren, Zhao and Fritsch, Julian and Nallanthighal, Venkata Srikanth and Christensen, Heidi and Blackburn, Daniel and Schuller, Björn W. and Magimai-Doss, Mathew and Strik, Helmer and Härmä, Aki},
	month = oct,
	year = {2020},
	pages = {2182--2186},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\B32UPSNS\\Cummins et al. - 2020 - A Comparison of Acoustic and Linguistics Methodolo.pdf:application/pdf},
}

@inproceedings{eyben_openear_2009,
	address = {Amsterdam},
	title = {{OpenEAR} — {Introducing} the munich open-source emotion and affect recognition toolkit},
	isbn = {978-1-4244-4800-5},
	url = {http://ieeexplore.ieee.org/document/5349350/},
	doi = {10.1109/ACII.2009.5349350},
	urldate = {2022-11-05},
	booktitle = {2009 3rd {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} and {Workshops}},
	publisher = {IEEE},
	author = {Eyben, Florian and Wollmer, Martin and Schuller, Bjorn},
	month = sep,
	year = {2009},
	pages = {1--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\MU67XKJM\\Eyben et al. - 2009 - OpenEAR — Introducing the munich open-source emoti.pdf:application/pdf},
}

@inproceedings{rohanian_multi-modal_2020,
	title = {Multi-{Modal} {Fusion} with {Gating} {Using} {Audio}, {Lexical} and {Disfluency} {Features} for {Alzheimer}’s {Dementia} {Recognition} from {Spontaneous} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/rohanian20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2721},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Rohanian, Morteza and Hough, Julian and Purver, Matthew},
	month = oct,
	year = {2020},
	pages = {2187--2191},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\956T2T7L\\Rohanian et al. - 2020 - Multi-Modal Fusion with Gating Using Audio, Lexica.pdf:application/pdf},
}

@inproceedings{balagopalan_impact_2020,
	address = {Online},
	title = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}: {All} {Errors} are {Equal}, but {Deletions} are {More} {Equal} than {Others}},
	shorttitle = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}},
	url = {https://www.aclweb.org/anthology/2020.wnut-1.21},
	doi = {10.18653/v1/2020.wnut-1.21},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Noisy} {User}-generated {Text} ({W}-{NUT} 2020)},
	publisher = {Association for Computational Linguistics},
	author = {Balagopalan, Aparna and Shkaruta, Ksenia and Novikova, Jekaterina},
	year = {2020},
	pages = {159--164},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\459E52FL\\Balagopalan et al. - 2020 - Impact of ASR on Alzheimer’s Disease Detection Al.pdf:application/pdf},
}

@inproceedings{liu_detecting_2021,
	address = {Toronto, ON, Canada},
	title = {Detecting {Alzheimer}’s {Disease} from {Speech} {Using} {Neural} {Networks} with {Bottleneck} {Features} and {Data} {Augmentation}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413566/},
	doi = {10.1109/ICASSP39728.2021.9413566},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Liu, Zhaoci and Guo, Zhiqiang and Ling, Zhenhua and Li, Yunxia},
	month = jun,
	year = {2021},
	pages = {7323--7327},
}

@inproceedings{toth_automatic_2015,
	title = {Automatic detection of mild cognitive impairment from spontaneous speech using {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2015/toth15_interspeech.html},
	doi = {10.21437/Interspeech.2015-568},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Tóth, László and Gosztolya, Gábor and Vincze, Veronika and Hoffmann, Ildikó and Szatlóczki, Gréta and Biró, Edit and Zsura, Fruzsina and Pákáski, Magdolna and Kálmán, János},
	month = sep,
	year = {2015},
	pages = {2694--2698},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\3FGE8JIU\\Tóth et al. - 2015 - Automatic detection of mild cognitive impairment f.pdf:application/pdf},
}

@misc{tits_asr-based_2018,
	title = {{ASR}-based {Features} for {Emotion} {Recognition}: {A} {Transfer} {Learning} {Approach}},
	shorttitle = {{ASR}-based {Features} for {Emotion} {Recognition}},
	url = {http://arxiv.org/abs/1805.09197},
	abstract = {During the last decade, the applications of signal processing have drastically improved with deep learning. However areas of affecting computing such as emotional speech synthesis or emotion recognition from spoken language remains challenging. In this paper, we investigate the use of a neural Automatic Speech Recognition (ASR) as a feature extractor for emotion recognition. We show that these features outperform the eGeMAPS feature set to predict the valence and arousal emotional dimensions, which means that the audio-to-text mapping learning by the ASR system contain information related to the emotional dimensions in spontaneous speech. We also examine the relationship between first layers (closer to speech) and last layers (closer to text) of the ASR and valence/arousal.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Tits, Noé and Haddad, Kevin El and Dutoit, Thierry},
	month = jun,
	year = {2018},
	note = {arXiv:1805.09197 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to be published in the First Workshop on Computational Modeling of Human Multimodal Language - ACL 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\RIXSMVYF\\Tits et al. - 2018 - ASR-based Features for Emotion Recognition A Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\DDGNTFFZ\\1805.html:text/html},
}

@misc{zhang_unified_2021,
	title = {Unified {Streaming} and {Non}-streaming {Two}-pass {End}-to-end {Model} for {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2012.05481},
	abstract = {In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60\% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42\% CER with 640ms latency in a streaming ASR system.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Zhang, Binbin and Wu, Di and Yao, Zhuoyuan and Wang, Xiong and Yu, Fan and Yang, Chao and Guo, Liyong and Hu, Yaguang and Xie, Lei and Lei, Xin},
	month = dec,
	year = {2021},
	note = {arXiv:2012.05481 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3YGQUSLL\\Zhang et al. - 2021 - Unified Streaming and Non-streaming Two-pass End-t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Y9XSYRY3\\2012.html:text/html},
}

@article{sun_ernie_2020,
	title = {{ERNIE} 2.0: {A} {Continual} {Pre}-{Training} {Framework} for {Language} {Understanding}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{ERNIE} 2.0},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6428},
	doi = {10.1609/aaai.v34i05.6428},
	abstract = {Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
	number = {05},
	urldate = {2022-11-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
	month = apr,
	year = {2020},
	pages = {8968--8975},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WVPE3NJM\\Sun et al. - 2020 - ERNIE 2.0 A Continual Pre-Training Framework for .pdf:application/pdf},
}

@inproceedings{schuller_interspeech_2010,
	title = {The {INTERSPEECH} 2010 paralinguistic challenge},
	url = {https://www.isca-speech.org/archive/interspeech_2010/schuller10b_interspeech.html},
	doi = {10.21437/Interspeech.2010-739},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2010},
	publisher = {ISCA},
	author = {Schuller, Björn and Steidl, Stefan and Batliner, Anton and Burkhardt, Felix and Devillers, Laurence and Müller, Christian and Narayanan, Shrikanth S.},
	month = sep,
	year = {2010},
	pages = {2794--2797},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3QAQXZVB\\Schuller et al. - 2010 - The INTERSPEECH 2010 paralinguistic challenge.pdf:application/pdf},
}

@misc{noauthor_ad2021_2022,
	title = {{AD2021}: {Alzheimer}'s {Disease} {Recognition} {Evaluation} 2021},
	copyright = {Apache-2.0},
	shorttitle = {{AD2021}},
	url = {https://github.com/THUsatlab/AD2021},
	abstract = {Alzheimer's Disease Recognition Evaluation 2021},
	urldate = {2022-11-05},
	publisher = {THUsatlab},
	month = aug,
	year = {2022},
	note = {original-date: 2021-07-08T04:10:11Z},
}

@inproceedings{gui_end--end_2022,
	address = {Singapore, Singapore},
	title = {End-to-{End} {ASR}-{Enhanced} {Neural} {Network} for {Alzheimer}’s {Disease} {Diagnosis}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9747856/},
	doi = {10.1109/ICASSP43922.2022.9747856},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Gui, Jiancheng and Li, Yikai and Chen, Kai and Siebert, Joanna and Chen, Qingcai},
	month = may,
	year = {2022},
	pages = {8562--8566},
}

@inproceedings{garnerin_investigating_2021,
	address = {Online},
	title = {Investigating the {Impact} of {Gender} {Representation} in {ASR} {Training} {Data}: a {Case} {Study} on {Librispeech}},
	shorttitle = {Investigating the {Impact} of {Gender} {Representation} in {ASR} {Training} {Data}},
	url = {https://aclanthology.org/2021.gebnlp-1.10},
	doi = {10.18653/v1/2021.gebnlp-1.10},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 3rd {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	year = {2021},
	pages = {86--92},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9ZB3LE4W\\Garnerin et al. - 2021 - Investigating the Impact of Gender Representation .pdf:application/pdf},
}

@inproceedings{abushariah_mohammad_and_sawalha_majdi_effects_2013,
	address = {Lancaster, UK},
	title = {The effects of speakers' gender, age, and region on overall performance of {Arabic} automatic speech recognition systems using the phonetically rich and balanced {Modern} {Standard} {Arabic} speech corpus},
	url = {http://eprints.whiterose.ac.uk/81859/},
	booktitle = {Proceedings of the 2nd {Workshop} of {Arabic} {Corpus} {Linguistics}},
	author = {Abushariah, Mohammad {and} Sawalha, Majdi},
	month = jul,
	year = {2013},
	note = {University of Leeds},
}

@inproceedings{shah_predictive_2020,
	address = {Online},
	title = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}: {A} {Conceptual} {Framework} and {Overview}},
	shorttitle = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.468},
	doi = {10.18653/v1/2020.acl-main.468},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shah, Deven Santosh and Schwartz, H. Andrew and Hovy, Dirk},
	year = {2020},
	pages = {5248--5264},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\C44TW2PL\\Shah et al. - 2020 - Predictive Biases in Natural Language Processing M.pdf:application/pdf},
}

@inproceedings{hovy_social_2016,
	address = {Berlin, Germany},
	title = {The {Social} {Impact} of {Natural} {Language} {Processing}},
	url = {http://aclweb.org/anthology/P16-2096},
	doi = {10.18653/v1/P16-2096},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Spruit, Shannon L.},
	year = {2016},
	pages = {591--598},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\Z3PQLK4Y\\Hovy and Spruit - 2016 - The Social Impact of Natural Language Processing.pdf:application/pdf},
}

@article{garg_word_2018,
	title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1720347115},
	doi = {10.1073/pnas.1720347115},
	abstract = {Significance
            Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science.
          ,
            Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.},
	language = {en},
	number = {16},
	urldate = {2022-11-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	month = apr,
	year = {2018},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\RQYPG4XA\\Garg et al. - 2018 - Word embeddings quantify 100 years of gender and e.pdf:application/pdf},
}

@book{kutuzov_diachronic_2018,
	title = {Diachronic word embeddings and semantic shifts: a survey},
	shorttitle = {Diachronic word embeddings and semantic shifts},
	abstract = {Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.},
	author = {Kutuzov, Andrei and Øvrelid, Lilja and Szymanski, Terrence and Velldal, Erik},
	month = jun,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5PQKNLMN\\Kutuzov et al. - 2018 - Diachronic word embeddings and semantic shifts a .pdf:application/pdf},
}

@inproceedings{sap_risk_2019,
	address = {Florence, Italy},
	title = {The {Risk} of {Racial} {Bias} in {Hate} {Speech} {Detection}},
	url = {https://www.aclweb.org/anthology/P19-1163},
	doi = {10.18653/v1/P19-1163},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
	year = {2019},
	pages = {1668--1678},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\85P2NR4F\\Sap et al. - 2019 - The Risk of Racial Bias in Hate Speech Detection.pdf:application/pdf},
}

@article{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	month = jul,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5YSE3CSF\\Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf},
}

@inproceedings{zhao_gender_2019,
	address = {Minneapolis, Minnesota},
	title = {Gender {Bias} in {Contextualized} {Word} {Embeddings}},
	url = {http://aclweb.org/anthology/N19-1064},
	doi = {10.18653/v1/N19-1064},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
	year = {2019},
	pages = {629--634},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\JX738BCA\\Zhao et al. - 2019 - Gender Bias in Contextualized Word Embeddings.pdf:application/pdf},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	abstract = {Machines learn what people know implicitly

              AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan
              et al.
              now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.


              Science
              , this issue p.
              183
              ; see also p.
              133

          ,
            Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.
          ,
            Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	language = {en},
	number = {6334},
	urldate = {2022-11-05},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	pages = {183--186},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\ER9DI443\\Caliskan et al. - 2017 - Semantics derived automatically from language corp.pdf:application/pdf},
}

@inproceedings{adda-decker_speech_2005,
	title = {Do speech recognizers prefer female speakers?},
	url = {https://www.isca-speech.org/archive/interspeech_2005/addadecker05_interspeech.html},
	doi = {10.21437/Interspeech.2005-699},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2005},
	publisher = {ISCA},
	author = {Adda-Decker, Martine and Lamel, Lori},
	month = sep,
	year = {2005},
	pages = {2205--2208},
}

@misc{feng_quantifying_2021,
	title = {Quantifying {Bias} in {Automatic} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2103.15122},
	abstract = {Automatic speech recognition (ASR) systems promise to deliver objective interpretation of human speech. Practice and recent evidence suggests that the state-of-the-art (SotA) ASRs struggle with the large variation in speech due to e.g., gender, age, speech impairment, race, and accents. Many factors can cause the bias of an ASR system. Our overarching goal is to uncover bias in ASR systems to work towards proactive bias mitigation in ASR. This paper is a first step towards this goal and systematically quantifies the bias of a Dutch SotA ASR system against gender, age, regional accents and non-native accents. Word error rates are compared, and an in-depth phoneme-level error analysis is conducted to understand where bias is occurring. We primarily focus on bias due to articulation differences in the dataset. Based on our findings, we suggest bias mitigation strategies for ASR development.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Feng, Siyuan and Kudina, Olya and Halpern, Bence Mark and Scharenborg, Odette},
	month = apr,
	year = {2021},
	note = {arXiv:2103.15122 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	annote = {Comment: Submitted to INTERSPEECH (IS) 2021. This preprint version differs slightly from the version submitted to IS 2021: Figure 1 is not included in IS 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\GJK7YX95\\Feng et al. - 2021 - Quantifying Bias in Automatic Speech Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\XRIQKM99\\2103.html:text/html},
}

@inproceedings{tatman_gender_2017,
	address = {Valencia, Spain},
	title = {Gender and {Dialect} {Bias} in {YouTube}'s {Automatic} {Captions}},
	url = {http://aclweb.org/anthology/W17-1606},
	doi = {10.18653/v1/W17-1606},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tatman, Rachael},
	year = {2017},
	pages = {53--59},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\QHGSA22L\\Tatman - 2017 - Gender and Dialect Bias in YouTube's Automatic Cap.pdf:application/pdf},
}

@inproceedings{tatman_effects_2017,
	title = {Effects of {Talker} {Dialect}, {Gender} \& {Race} on {Accuracy} of {Bing} {Speech} and {YouTube} {Automatic} {Captions}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/tatman17_interspeech.html},
	doi = {10.21437/Interspeech.2017-1746},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Tatman, Rachael and Kasten, Conner},
	month = aug,
	year = {2017},
	pages = {934--938},
}

@techreport{frank_wilcoxon_sk_katti_and_roberta_a_wilcox_critical_1963,
	address = {Pearl River, NY, USA},
	title = {Critical values and probability levels for the {Wilcoxon} rank sum test and the {Wilcoxon} signed rank test},
	institution = {American Cyanamid Company},
	author = {Frank Wilcoxon, SK Katti, {and} Roberta A Wilcox},
	year = {1963},
}

@inproceedings{garnerin_gender_2019,
	address = {Nice, France},
	title = {Gender {Representation} in {French} {Broadcast} {Corpora} and {Its} {Impact} on {ASR} {Performance}},
	isbn = {978-1-4503-6917-6},
	url = {http://dl.acm.org/citation.cfm?doid=3347449.3357480},
	doi = {10.1145/3347449.3357480},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {AI} for {Smart} {TV} {Content} {Production}, {Access} and {Delivery} - {AI4TV} '19},
	publisher = {ACM Press},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	year = {2019},
	pages = {3--9},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GWSDNHSG\\Garnerin et al. - 2019 - Gender Representation in French Broadcast Corpora .pdf:application/pdf},
}

@inproceedings{garnerin_gender_2020,
	address = {Marseille, France},
	title = {Gender {Representation} in {Open} {Source} {Speech} {Resources}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.813},
	abstract = {With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited/non elicited speech, low/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.},
	language = {English},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	month = may,
	year = {2020},
	pages = {6599--6605},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\HW5HZXVR\\Garnerin et al. - 2020 - Gender Representation in Open Source Speech Resour.pdf:application/pdf},
}

@article{adamek_whose_2018,
	title = {Whose {Artifacts}? {Whose} {Stories}? {Public} {History} and {Representation} of {Women} at the {Canada} {Science} and {Technology} {Museum}},
	issn = {0121-1617, 1900-6152},
	shorttitle = {Whose {Artifacts}?},
	url = {https://revistas.uniandes.edu.co/doi/10.7440/histcrit68.2018.03},
	doi = {10.7440/histcrit68.2018.03},
	language = {pt},
	number = {68},
	urldate = {2022-11-05},
	journal = {Historia Crítica},
	author = {Adamek, Anna and Gann, Emily},
	month = apr,
	year = {2018},
	pages = {47--66},
}

@incollection{goos_classification_2001,
	address = {Berlin, Heidelberg},
	title = {Classification on {Data} with {Biased} {Class} {Distribution}},
	volume = {2167},
	isbn = {978-3-540-42536-6 978-3-540-44795-5},
	url = {http://link.springer.com/10.1007/3-540-44795-4_45},
	urldate = {2022-11-05},
	booktitle = {Machine {Learning}: {ECML} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Vucetic, Slobodan and Obradovic, Zoran},
	editor = {Goos, G. and Hartmanis, J. and van Leeuwen, J. and De Raedt, Luc and Flach, Peter},
	year = {2001},
	doi = {10.1007/3-540-44795-4_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {527--538},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WWEYIB8Z\\Vucetic and Obradovic - 2001 - Classification on Data with Biased Class Distribut.pdf:application/pdf},
}

@article{haibo_he_learning_2009,
	title = {Learning from {Imbalanced} {Data}},
	volume = {21},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5128907/},
	doi = {10.1109/TKDE.2008.239},
	number = {9},
	urldate = {2022-11-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {{Haibo He} and Garcia, E.A.},
	month = sep,
	year = {2009},
	pages = {1263--1284},
}


@inproceedings{noauthor_trouble_2017,
	address = {Long Beach, CA, USA},
	title = {The trouble with bias},
	url = {https://www.youtube.com/watch?v=fMym_BKWQzk},
	booktitle = {31st {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	year = {2017},
	note = {Keynote at the 31st Annual Conference on Neural Information Processing Systems},
}

@article{d_a_mahler_r_a_rosiello_and_j_loke_aging_1986,
	title = {The {Aging} {Lung}},
	volume = {2},
	number = {2},
	journal = {Clinics in Geriatric Medicine},
	author = {D. A. Mahler, R. A. Rosiello, {and} J. Loke},
	year = {1986},
	pages = {215--225},
}

@article{karam_anatomic_2013,
	title = {Anatomic and {Physiologic} {Changes} of the {Aging} {Kidney}},
	volume = {29},
	issn = {07490690},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749069013000396},
	doi = {10.1016/j.cger.2013.05.006},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Clinics in Geriatric Medicine},
	author = {Karam, Zeina and Tuazon, Jennifer},
	month = aug,
	year = {2013},
	pages = {555--564},
}

@article{tolep_comparison_1995,
	title = {Comparison of diaphragm strength between healthy adult elderly and young men.},
	volume = {152},
	issn = {1073-449X, 1535-4970},
	url = {https://www.atsjournals.org/doi/10.1164/ajrccm.152.2.7633725},
	doi = {10.1164/ajrccm.152.2.7633725},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {American Journal of Respiratory and Critical Care Medicine},
	author = {Tolep, K and Higgins, N and Muza, S and Criner, G and Kelsen, S G},
	month = aug,
	year = {1995},
	pages = {677--682},
}

@article{linville_vocal_1995,
	title = {Vocal aging:},
	volume = {3},
	issn = {1068-9508},
	shorttitle = {Vocal aging},
	url = {http://journals.lww.com/00020840-199506000-00006},
	doi = {10.1097/00020840-199506000-00006},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Current Opinion in Otolaryngology \& Head and Neck Surgery},
	author = {Linville, Sue Ellen},
	month = jun,
	year = {1995},
	pages = {183--187},
}

@article{ramig_aging_2001,
	title = {The {Aging} {Voice}: {A} {Review}, {Treatment} {Data} and {Familial} and {Genetic} {Perspectives}},
	volume = {53},
	issn = {1021-7762, 1421-9972},
	shorttitle = {The {Aging} {Voice}},
	url = {https://www.karger.com/Article/FullText/52680},
	doi = {10.1159/000052680},
	abstract = {This paper will provide a review of aspects of vocal aging within the context of general body aging and describe two data sets related to the aging voice. Data will be presented which document pre- to posttreatment improvement in select voice characteristics (sound pressure level, subglottal air pressure, thyroarytenoid laryngeal muscle activity and voice quality) following application of an intensive voice treatment program (the LSVT$^{\textrm{®}}$) to 3 individuals with aged voice. Additionally, physiological data (forced expiratory volume, visual accommodation, bone density, taste discrimination, white blood count and resting heart rate) and select perceptual (perceived age) and acoustic measures (reflecting both cycle-to-cycle and longer-term intensity and frequency stability) from 67 subjects will be reviewed from the work of Gray and colleagues to document the differential impact of the global aging process across organ systems including the aging voice.},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {Folia Phoniatrica et Logopaedica},
	author = {Ramig, Lorraine Olson and Gray, Steven and Baker, Kristin and Corbin-Lewis, Kim and Buder, Eugene and Luschei, Erich and Coon, Hillary and Smith, Marshall},
	year = {2001},
	pages = {252--265},
}

@article{paulsen_degenerative_1998,
	title = {Degenerative {Changes} in the {Human} {Cricoarytenoid} {Joint}},
	volume = {124},
	issn = {0886-4470},
	url = {http://archotol.jamanetwork.com/article.aspx?doi=10.1001/archotol.124.8.903},
	doi = {10.1001/archotol.124.8.903},
	language = {en},
	number = {8},
	urldate = {2022-11-05},
	journal = {Archives of Otolaryngology–Head \& Neck Surgery},
	author = {Paulsen, Friedrich P. and Tillmann, Bernhard N.},
	month = aug,
	year = {1998},
	pages = {903},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\VHRPKC7W\\Paulsen and Tillmann - 1998 - Degenerative Changes in the Human Cricoarytenoid J.pdf:application/pdf},
}

@article{rodeno_histochemical_1993,
	title = {Histochemical and {Morphometrical} {Ageing} {Changes} in {Human} {Vocal} {Cord} {Muscles}},
	volume = {113},
	issn = {0001-6489, 1651-2251},
	url = {http://www.tandfonline.com/doi/full/10.3109/00016489309135842},
	doi = {10.3109/00016489309135842},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Acta Oto-Laryngologica},
	author = {Rodeño, M. T. and Sánchez-fernández, J. M. and Rivera-pomar, J. M.},
	month = jan,
	year = {1993},
	pages = {445--449},
}

@article{hirano_ageing_1989,
	title = {Ageing of the {Vibratory} {Tissue} of {Human} {Vocal} {Folds}},
	volume = {107},
	issn = {0001-6489, 1651-2251},
	url = {http://www.tandfonline.com/doi/full/10.3109/00016488909127535},
	doi = {10.3109/00016488909127535},
	language = {en},
	number = {5-6},
	urldate = {2022-11-05},
	journal = {Acta Oto-Laryngologica},
	author = {Hirano, Minoru and Kurita, Shigejiro and Sakaguchi, Shinji},
	month = jan,
	year = {1989},
	pages = {428--433},
}

@article{sato_age-related_1997,
	title = {Age-{Related} {Changes} of {Elastic} {Fibers} in the {Superficial} {Layer} of the {Lamina} {Propria} of {Vocal} {Folds}},
	volume = {106},
	issn = {0003-4894, 1943-572X},
	url = {http://journals.sagepub.com/doi/10.1177/000348949710600109},
	doi = {10.1177/000348949710600109},
	abstract = {An investigation was carried out to determine the morphologic characteristics of elastic fibers in the superficial layer of the lamina propria of aged vocal folds (EFAVFs). Excised human adult vocal folds served as the material for this study. Scanning and transmission electron microscopic observations were made. The results can be summarized as follows. First, the EFAVFs were composed of amorphous substances and microfibrils. The amorphous substances increased in amount and the microfibrils became less numerous. Second, the EFAVFs ran in various directions, were branched, and formed a complicated network. The surface of the fibers was rough, and the fibers appeared to vary in size. Some EFAVFs united to form a sheet with a rough surface. Third, the EFAVFs could not be easily digested by elastase compared with those of younger adults. We conclude that the morphologic and metabolic changes of elastic fibers in the most important vibrating portion (superficial layer of the lamina propria) of the aged vocal folds contribute partially to aging of the voice.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Annals of Otology, Rhinology \& Laryngology},
	author = {Sato, Kiminori and Hirano, Minoru},
	month = jan,
	year = {1997},
	pages = {44--48},
}

@article{rother_morphometrically_2002,
	title = {Morphometrically observable aging changes in the human tongue},
	volume = {184},
	issn = {09409602},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0940960202800115},
	doi = {10.1016/S0940-9602(02)80011-5},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Annals of Anatomy - Anatomischer Anzeiger},
	author = {Rother, Paul and Wohlgemuth, Balthasar and Wolff, Werner and Rebentrost, Ines},
	month = mar,
	year = {2002},
	pages = {159--164},
}

@article{b_weinstein_biology_nodate,
	title = {The biology of aging},
	journal = {Geriatric Audiology},
	author = {B. Weinstein},
	pages = {pp. 15--40},
}

@book{weinstein_geriatric_2013,
	address = {New York},
	edition = {Second edition},
	title = {Geriatric audiology},
	isbn = {978-1-60406-174-1 978-1-60406-775-0},
	publisher = {Thieme},
	author = {Weinstein, Barbara E.},
	year = {2013},
	note = {“The biology of aging,” in Geriatric Audiology,
pp. 15–40, Georg Thieme, Stuttgart, Germany, 2000},
	keywords = {Aged, Aging, Health Services for the Aged, Hearing, Hearing Disorders, physiology},
}

@article{xue_changes_2003,
	title = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}: {A} {Pilot} {Study}},
	volume = {46},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282003/054%29},
	doi = {10.1044/1092-4388(2003/054)},
	abstract = {This investigation used a derivation of acoustic reflection (AR) technology to make cross-sectional measurements of changes due to aging in the oral and pharyngeal lumina of male and female speakers. The purpose of the study was to establish preliminary normative data for such changes and to obtain acoustic measurements of changes due to aging in the formant frequencies of selected spoken vowels and their long-term average spectra (LTAS) analysis. Thirty- eight young men and women and 38 elderly men and women were involved in the study. The oral and pharyngeal lumina of the participants were measured with AR technology, and their formant frequencies were analyzed using the Kay Elemetrics Computerized Speech Lab. The findings have delineated specific and similar patterns of aging changes in human vocal tract configurations in speakers of both genders. Namely, the oral cavity length and volume of elderly speakers increased significantly compared to their young cohorts. The total vocal tract volume of elderly speakers also showed a significant increment, whereas the total vocal tract length of elderly speakers did not differ significantly from their young cohorts. Elderly speakers of both genders also showed similar patterns of acoustic changes of speech production, that is, consistent lowering of formant frequencies (especially F1) across selected vowel productions. Although new research models are still needed to succinctly account for the speech acoustic changes of the elderly, especially for their specific patterns of human vocal tract dimensional changes, this study has innovatively applied the noninvasive and cost-effective AR technology to monitor age-related human oral and pharyngeal lumina changes that have direct consequences for speech production.},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Xue, Steve An and Hao, Grace Jianping},
	month = jun,
	year = {2003},
	pages = {689--701},
}

@article{baba_acoustic_2004,
	title = {Acoustic models of the elderly for large-vocabulary continuous speech recognition},
	volume = {87},
	issn = {8756-663X, 1520-6432},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ecjb.20101},
	doi = {10.1002/ecjb.20101},
	language = {en},
	number = {7},
	urldate = {2022-11-05},
	journal = {Electronics and Communications in Japan (Part II: Electronics)},
	author = {Baba, Akira and Yoshizawa, Shinichi and Yamada, Miichi and Lee, Akinobu and Shikano, Kiyohiro},
	month = jul,
	year = {2004},
	pages = {49--57},
}

@book{vipperla_longitudinal_2008,
	title = {Longitudinal study of {ASR} performance on ageing voices},
	abstract = {This paper presents the results of a longitudinal study of ASR performance on ageing voices. Experiments were conducted on the audio recordings of the proceedings of the Supreme Court Of The United States (SCOTUS). Results show that the Au- tomatic Speech Recognition (ASR) Word Error Rates (WERs) for elderly voices are significantly higher than those of adult voices. The word error rate increases gradually as the age of the elderly speakers increase. Use of maximum likelihood linear regression (MLLR) based speaker adaptation on ageing voices improves the WER though the performance is still considerably lower compared to adult voices. Speaker adaptation however reduces the increase in WER with age during old age. IndexTerms: Ageing Voices, longitudinal study, SCOTUScor- pus, MLLR},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	month = jan,
	year = {2008},
	note = {Pages: 2553},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\YEYS3FPY\\Vipperla et al. - 2008 - Longitudinal study of ASR performance on ageing vo.pdf:application/pdf},
}

@inproceedings{wilpon_study_1996,
	address = {Atlanta, GA, USA},
	title = {A study of speech recognition for children and the elderly},
	volume = {1},
	isbn = {978-0-7803-3192-1},
	url = {http://ieeexplore.ieee.org/document/541104/},
	doi = {10.1109/ICASSP.1996.541104},
	urldate = {2022-11-05},
	booktitle = {1996 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing} {Conference} {Proceedings}},
	publisher = {IEEE},
	author = {Wilpon, J.G. and Jacobsen, C.N.},
	year = {1996},
	pages = {349--352},
}

@book{moller_corpus_2008,
	title = {Corpus {Analysis} of {Spoken} {Smart}-{Home} {Interactions} with {Older} {Users}.},
	abstract = {In this paper, we present the collection and analysis of a spoken dialogue corpus obtained from interactions of older and younger users with a smart-home system. Our aim is to identify the amount and the origin of linguistic differences in the way older and younger users address the system. In addition, we investigate changes in the users' linguistic behaviour after exposure to the system. The results show that the two user groups differ in their speaking style as well as their vocabulary. In contrast to younger users, who adapt their speaking style to the expected limitations of the system, older users tend to use a speaking style that is closer to human-human communication in terms of sentence complexity and politeness. However, older users are far less easy to stereotype than younger users.},
	author = {Möller, Sebastian and Wolters, Maria},
	month = jan,
	year = {2008},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\SHJVDLVP\\Möller and Wolters - 2008 - Corpus Analysis of Spoken Smart-Home Interactions .pdf:application/pdf},
}

@article{wolters_being_2009,
	title = {Being {Old} {Doesn}’t {Mean} {Acting} {Old}: {How} {Older} {Users} {Interact} with {Spoken} {Dialog} {Systems}},
	volume = {2},
	issn = {1936-7228, 1936-7236},
	shorttitle = {Being {Old} {Doesn}’t {Mean} {Acting} {Old}},
	url = {https://dl.acm.org/doi/10.1145/1525840.1525842},
	doi = {10.1145/1525840.1525842},
	abstract = {Most studies on adapting voice interfaces to older users work top-down by comparing the interaction behavior of older and younger users. In contrast, we present a bottom-up approach. A statistical cluster analysis of 447 appointment scheduling dialogs between 50 older and younger users and 9 simulated spoken dialog systems revealed two main user groups, a “social” group and a “factual” group. “Factual” users adapted quickly to the systems and interacted efficiently with them. “Social” users, on the other hand, were more likely to treat the system like a human, and did not adapt their interaction style. While almost all “social” users were older, over a third of all older users belonged in the “factual” group. Cognitive abilities and gender did not predict group membership. We conclude that spoken dialog systems should adapt to users based on observed behavior, not on age.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {ACM Transactions on Accessible Computing},
	author = {Wolters, Maria and Georgila, Kallirroi and Moore, Johanna D. and MacPherson, Sarah E.},
	month = may,
	year = {2009},
	pages = {1--39},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\7MBYCCBB\\Wolters et al. - 2009 - Being Old Doesn’t Mean Acting Old How Older Users.pdf:application/pdf},
}

@incollection{stephanidis_speech_2009,
	address = {Berlin, Heidelberg},
	title = {Speech {Input} from {Older} {Users} in {Smart} {Environments}: {Challenges} and {Perspectives}},
	volume = {5615},
	isbn = {978-3-642-02709-3 978-3-642-02710-9},
	shorttitle = {Speech {Input} from {Older} {Users} in {Smart} {Environments}},
	url = {http://link.springer.com/10.1007/978-3-642-02710-9_14},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Intelligent} and {Ubiquitous} {Interaction} {Environments}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vipperla, Ravichander and Wolters, Maria and Georgila, Kallirroi and Renals, Steve},
	editor = {Stephanidis, Constantine},
	year = {2009},
	doi = {10.1007/978-3-642-02710-9_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {117--126},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\SHIF9M32\\Vipperla et al. - 2009 - Speech Input from Older Users in Smart Environment.pdf:application/pdf},
}

@book{mller_combining_2007,
	title = {Combining short-term cepstral and long-term pitch features for automatic recognition of speaker age.},
	abstract = {The most successful systems in previous comparative studies on speaker age recognition used short-term cepstral features modeled with Gaussian Mixture Models (GMMs) or applied multiple phone recognizers trained with the data of speakers of the respective class. Acoustic analyses, however, indic ate that certain features such as pitch extracted from a longer s pan of speech correlate clearly with the speaker age although the systems based on those features have been inferior to the be- fore mentioned approaches. In this paper, three novel systems combining short-term cepstral features and long-term features for speaker age recognition are compared to each other. A sys- tem combining GMMs using frame-based MFCCs and Support- Vector-Machines using long-term pitch performs best. The re- sults indicate that the combination of the two feature types is a promising approach, which corresponds to findings in relat ed fields like speaker recognition. Index Terms: speaker classifi- cation, age recognition, GMM, SVM},
	author = {Mller, Christian and Burkhardt, Felix},
	month = jan,
	year = {2007},
	note = {Pages: 2280},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PUAFDFIY\\Mller and Burkhardt - 2007 - Combining short-term cepstral and long-term pitch .pdf:application/pdf},
}

@book{wolters_age_2009,
	title = {Age recognition for spoken dialogue systems: do we need it?},
	shorttitle = {Age recognition for spoken dialogue systems},
	abstract = {When deciding whether to adapt relevant aspects of the system to the particular needs of older users, spoken dialogue systems often rely on automatic detection of chronological age. In this paper, we show that vocal age- ing as measured by acoustic features is an unreliable indi- cator of the need for adaptation. Simple lexical features greatly improve the prediction of both relevant aspects of cognition and interactions style. Lexical features also boost age group prediction. We suggest that adaptation should be based on observed behaviour, not on chrono- logical age, unless it is not feasible to build classifiers for relevant adaptation decisions. Index Terms :a ge recognition, pitch, keyword spotting, cognitive ageing},
	author = {Wolters, Maria and Vipperla, Ravichander and Renals, Steve},
	month = jan,
	year = {2009},
	note = {Pages: 1438},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\S7NGAXUA\\Wolters et al. - 2009 - Age recognition for spoken dialogue systems do we.pdf:application/pdf},
}

@article{hillenbrand_acoustic_1996,
	title = {Acoustic correlates of breathy vocal quality: {Dysphonic} voices and continuous speech},
	volume = {39},
	shorttitle = {Acoustic correlates of breathy vocal quality},
	abstract = {In an earlier study, we evaluated the effectiveness of several acoustic measures in predicting breathiness ratings for sustained vowels spoken by nonpathological talkers who were asked to produce nonbreathy, moderately breathy, and very breathy phonation (Hillenbrand, Cleveland, \& Erickson, 1994). The purpose of the present study was to extend these results to speakers with laryngeal pathologies and to conduct tests using connected speech in addition to sustained vowels. Breathiness ratings were obtained from a sustained vowel and a 12-word sentence spoken by 20 pathological and 5 nonpathological talkers. Acoustic measures were made of (a) signal periodicity, (b) first harmonic amplitude, and (c) spectral tilt. For the sustained vowels, a frequency domain measure of periodicity provided the most accurate predictions of perceived breathiness, accounting for 92\% of the variance in breathiness ratings. The relative amplitude of the first harmonic and two measures of spectral tilt correlated moderately with breathiness ratings. For the sentences, both signal periodicity and spectral tilt provided accurate predictions of breathiness ratings, accounting for 70\%-85\% of the variance.},
	journal = {Journal of speech and hearing research},
	author = {Hillenbrand, James and Houde, Robert},
	month = apr,
	year = {1996},
	pages = {311--21},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\445N846Q\\Hillenbrand and Houde - 1996 - Acoustic correlates of breathy vocal quality Dysp.pdf:application/pdf},
}

@article{klich_relationships_1982,
	title = {Relationships of {Vowel} {Characteristics} to {Listener} {Ratings} of {Breathiness}},
	volume = {25},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.2504.574},
	doi = {10.1044/jshr.2504.574},
	abstract = {This experiment was designed to investigate the relationships of listener ratings of breathiness to vowel duration, speaking rate, and the relative energy in three frequency ranges (100–500, 1500–2500, and 3500–4500 Hz) in vowel spectra. The effects of vowel SPL also were considered. Listeners used a seven-point equal-appearing interval scale to rate a sentence spoken by each of 10 young adult females in each of four voice qualities: normal speech, mildly breathy, severely breathy, and whisper. Significant Pearson correlations to the ratings were found only for mean SPL and the relative energy in the 100–500 and 3500–4500 Hz ranges. After the effects of mean SPL were accounted for in partial correlation and multiple regression analyses, all vowel parameters were related significantly to the mean ratings. The partial correlations for vowel duration were as high as those for the three frequency ranges. Vowel duration may be as important as spectral characteristics of vowels when breathiness is judged from samples of connected discourse.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Klich, Richard J.},
	month = dec,
	year = {1982},
	pages = {574--580},
}

@article{hillenbrand_acoustic_1994,
	title = {Acoustic {Correlates} of {Breathy} {Vocal} {Quality}},
	volume = {37},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3704.769},
	doi = {10.1044/jshr.3704.769},
	abstract = {The purpose of this study was to evaluate the effectiveness of several acoustic measures in predicting breathiness ratings. Recordings were made of eight normal men and seven normal women producing normally phonated, moderately breathy, and very breathy sustained vowels. Twenty listeners rated the degree of breathiness using a direct magnitude estimation procedure. Acoustic measures were made of: (a) signal periodicity, (b) first harmonic amplitude, and (c) spectral tilt. Periodicity measures provided the most accurate predictions of perceived breathiness, accounting for approximately 80\% of the variance in breathiness ratings. The relative amplitude of the first harmonic correlated moderately with breathiness ratings, and two measures of spectral tilt correlated weakly with perceived breathiness.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Hillenbrand, James and Cleveland, Ronald A. and Erickson, Robert L.},
	month = aug,
	year = {1994},
	pages = {769--778},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\SCZ928SH\\Hillenbrand et al. - 1994 - Acoustic Correlates of Breathy Vocal Quality.pdf:application/pdf},
}

@article{boersma_accurate_2000,
	title = {Accurate {Short}-{Term} {Analysis} {Of} {The} {Fundamental} {Frequency} {And} {The} {Harmonics}-{To}-{Noise} {Ratio} {Of} {A} {Sampled} {Sound}},
	volume = {17},
	abstract = {We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods. By definition, the best candidate for the acoustic pitch period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for p...},
	journal = {Proceedings of the Institute of Phonetic Sciences},
	author = {Boersma, Paul},
	month = jan,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\YHEXVA6R\\Boersma - 2000 - Accurate Short-Term Analysis Of The Fundamental Fr.pdf:application/pdf},
}

@article{deliyski_effects_2001,
	title = {{EFFECTS} {OF} {AGING} {ON} {SELECTED} {ACOUSTIC} {VOICE} {PARAMETERS}: {PRELIMINARY} {NORMATIVE} {DATA} {AND} {EDUCATIONAL} {IMPLICATIONS}},
	volume = {27},
	issn = {0360-1277, 1521-0472},
	shorttitle = {{EFFECTS} {OF} {AGING} {ON} {SELECTED} {ACOUSTIC} {VOICE} {PARAMETERS}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03601270151075561},
	doi = {10.1080/03601270151075561},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Educational Gerontology},
	author = {Deliyski, Dimitar, Steve An Xue},
	month = mar,
	year = {2001},
	pages = {159--168},
}

@misc{noauthor_lombard_2022,
	title = {Lombard effect},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Lombard_effect&oldid=1100217835},
	abstract = {The Lombard effect or Lombard reflex is the involuntary tendency of speakers to increase their vocal effort when speaking in loud noise to enhance the audibility of their voice. This change includes not only loudness but also other acoustic features such as pitch, rate, and duration of syllables. This compensation effect maintains the auditory signal-to-noise ratio of the speaker's spoken words.
The effect links to the needs of effective communication, as there is a reduced effect when words are repeated or lists are read where communication intelligibility is not important. Since the effect is involuntary it is used as a means to detect malingering in those simulating hearing loss. Research on birds and monkeys find that the effect also occurs in the vocalizations of animals.
The effect was discovered in 1909 by Étienne Lombard, a French otolaryngologist.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Wikipedia},
	month = jul,
	year = {2022},
	note = {Page Version ID: 1100217835},
}

@article{lane_lombard_1971,
	title = {The {Lombard} {Sign} and the {Role} of {Hearing} in {Speech}},
	volume = {14},
	issn = {0022-4685},
	url = {http://pubs.asha.org/doi/10.1044/jshr.1404.677},
	doi = {10.1044/jshr.1404.677},
	abstract = {Lombard noted in 1911 that a speaker changes his voice level similarly when the ambient noise level increases, on the one hand, and when the level at which he hears his own voice (his sidetone) decreases, on the other. We can now state the form of these two functions, show that they are related to each other and to the equal-sensation function for imitating speech or noise loudness, and account for their form in terms of the underlying sensory scales and the hypothesis that the speaker tries to maintain a speech-to-noise ratio favorable for communication.
            Perturbations in the timing and spectrum of sidetone also lead the speaker to compensate for the apparent deterioration in his intelligibility. Such compensations reflect direct and indirect audience control of speech, rather than its autoregulation by sidetone. When not harassed by prying experimenters or an unfavorable acoustic environment, the speaker need no more listen to himself while speaking than he need speak to himself while listening.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech and Hearing Research},
	author = {Lane, Harlan and Tranel, Bernard},
	month = dec,
	year = {1971},
	pages = {677--709},
}

@article{lane_lombard_1971-1,
	title = {The {Lombard} {Sign} and the {Role} of {Hearing} in {Speech}},
	volume = {14},
	issn = {0022-4685},
	url = {http://pubs.asha.org/doi/10.1044/jshr.1404.677},
	doi = {10.1044/jshr.1404.677},
	abstract = {Lombard noted in 1911 that a speaker changes his voice level similarly when the ambient noise level increases, on the one hand, and when the level at which he hears his own voice (his sidetone) decreases, on the other. We can now state the form of these two functions, show that they are related to each other and to the equal-sensation function for imitating speech or noise loudness, and account for their form in terms of the underlying sensory scales and the hypothesis that the speaker tries to maintain a speech-to-noise ratio favorable for communication.
            Perturbations in the timing and spectrum of sidetone also lead the speaker to compensate for the apparent deterioration in his intelligibility. Such compensations reflect direct and indirect audience control of speech, rather than its autoregulation by sidetone. When not harassed by prying experimenters or an unfavorable acoustic environment, the speaker need no more listen to himself while speaking than he need speak to himself while listening.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech and Hearing Research},
	author = {Lane, Harlan and Tranel, Bernard},
	month = dec,
	year = {1971},
	pages = {677--709},
}

@article{derryberry_singing_2020,
	title = {Singing in a silent spring: {Birds} respond to a half-century soundscape reversion during the {COVID}-19 shutdown},
	volume = {370},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Singing in a silent spring},
	url = {https://www.science.org/doi/10.1126/science.abd5777},
	doi = {10.1126/science.abd5777},
	abstract = {Songbirds reclaim favored frequencies

              When severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic lockdowns were instituted across entire countries, human activities ceased in an unprecedented way. Derryberry
              et al.
              found that the reduction in traffic sound in the San Francisco Bay Area of California to levels not seen for half a century led to a shift in song frequency in white-crowned sparrows (see the Perspective by Halfwerk). This shift was especially notable because the frequency of human-produced traffic noise occurs within a range that interferes with the highest performance and most effective song. Thus, our “quiet” allowed the birds to quickly fill the most effective song space.


              Science
              , this issue p.
              575
              ; see also p.
              523

          ,
            Reductions in noise pollution during the pandemic shutdown allowed for more effective song production in white-crowned sparrows.
          ,
            Actions taken to control the coronavirus disease 2019 (COVID-19) pandemic have conspicuously reduced motor vehicle traffic, potentially alleviating auditory pressures on animals that rely on sound for survival and reproduction. Here, by comparing soundscapes and songs across the San Francisco Bay Area before and during the recent statewide shutdown, we evaluated whether a common songbird responsively exploited newly emptied acoustic space. We show that noise levels in urban areas were substantially lower during the shutdown, characteristic of traffic in the mid-1950s. We also show that birds responded by producing higher performance songs at lower amplitudes, effectively maximizing communication distance and salience. These findings illustrate that behavioral traits can change rapidly in response to newly favorable conditions, indicating an inherent resilience to long-standing anthropogenic pressures such as noise pollution.},
	language = {en},
	number = {6516},
	urldate = {2022-11-05},
	journal = {Science},
	author = {Derryberry, Elizabeth P. and Phillips, Jennifer N. and Derryberry, Graham E. and Blum, Michael J. and Luther, David},
	month = oct,
	year = {2020},
	pages = {575--579},
}

@article{summers_effects_1988,
	title = {Effects of noise on speech production: {Acoustic} and perceptual analyses},
	volume = {84},
	issn = {0001-4966},
	shorttitle = {Effects of noise on speech production},
	url = {http://asa.scitation.org/doi/10.1121/1.396660},
	doi = {10.1121/1.396660},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Summers, W. Van and Pisoni, David B. and Bernacki, Robert H. and Pedlow, Robert I. and Stokes, Michael A.},
	month = sep,
	year = {1988},
	pages = {917--928},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\HVCCM58H\\Summers et al. - 1988 - Effects of noise on speech production Acoustic an.pdf:application/pdf},
}

@article{summers_effects_1988-1,
	title = {Effects of noise on speech production: {Acoustic} and perceptual analyses},
	volume = {84},
	issn = {0001-4966},
	shorttitle = {Effects of noise on speech production},
	url = {http://asa.scitation.org/doi/10.1121/1.396660},
	doi = {10.1121/1.396660},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Summers, W. Van and Pisoni, David B. and Bernacki, Robert H. and Pedlow, Robert I. and Stokes, Michael A.},
	month = sep,
	year = {1988},
	pages = {917--928},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\P8ZUXZP7\\Summers et al. - 1988 - Effects of noise on speech production Acoustic an.pdf:application/pdf},
}

@article{manabe_control_1998,
	title = {Control of vocal intensity in budgerigars ( \textit{{Melopsittacus} undulatus} ): {Differential} reinforcement of vocal intensity and the {Lombard} effect},
	volume = {103},
	issn = {0001-4966},
	shorttitle = {Control of vocal intensity in budgerigars ( \textit{{Melopsittacus} undulatus} )},
	url = {http://asa.scitation.org/doi/10.1121/1.421227},
	doi = {10.1121/1.421227},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Manabe, Kazuchika and Sadr, Ehsanee I. and Dooling, Robert J.},
	month = feb,
	year = {1998},
	pages = {1190--1198},
}

@article{brumm_causes_2004,
	title = {Causes and consequences of song amplitude adjustment in a territorial bird: a case study in nightingales},
	volume = {76},
	issn = {0001-3765},
	shorttitle = {Causes and consequences of song amplitude adjustment in a territorial bird},
	url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0001-37652004000200017&lng=en&tlng=en},
	doi = {10.1590/S0001-37652004000200017},
	abstract = {Vocal amplitude, one of the crucial factors for the exchange of acoustic signals, has been neglected in studies of animal communication, but recent studies on song variation in Common Nightingales Luscinia megarhynchos have revealed new insights into its importance in the singing behavior of territorial birds. In nightingales song amplitude is not maximized per se, but is individually regulated according to the level of masking background noise. Also, birds adjust their vocal intensity according to social variables, as in male-male interactions. Moreover, during such interactions, males exploited the directionality of their songs to broadcast them in the direction of the intended receivers ensuring the most effective signal transmission. Studies of the development of this typical long-range signaling suggest that sound level is highly interrelated with overall developmental progression and learning, and thus should be viewed as an integral part of song ontogeny. I conclude that song amplitude is a dynamic feature of the avian signal system, which is individually regulated according to the ecological demands of signal transmission and the social context of communication.
          ,
            A amplitude vocal, um dos fatores cruciais para a troca de sinais acústicos, tem sido negligenciada nos estudos da comunicação animal, mas trabalhos recentes sobre a variação do canto do Rouxinol-comum Luscinia megarhynchos evidenciaram sua importância no comportamento de canto das aves territoriais. No rouxinol a amplitude do canto não é aumentada ao máximo per se, mas é regulada individualmente de acordo com o nível de ruído de fundo que mascara o sinal. As aves também ajustam sua intensidade vocal às variáveis sociais, tais como nas interações entre machos. Além disso, durante essas interações, os machos tiram proveito da direcionalidade de seus cantos para emiti-los em direção aos receptores desejados no intuito de garantir a mais eficiente transmissão do sinal. Estudos do desenvolvimento desta sinalização típica de longo alcance sugerem que o nível sonoro seja altamente relacionado com o desenvolvimento geral e a aprendizagem, e deveria portanto ser visto como parte integrante da ontogenia do canto. Concluímos que a amplitude do canto é um parâmetro dinâmico do sistema de sinalização em aves, que é regulado individualmente de acordo com as exigências ecológicas da transmissão do sinal e o contexto social da comunicação.},
	number = {2},
	urldate = {2022-11-05},
	journal = {Anais da Academia Brasileira de Ciências},
	author = {Brumm, Henrik},
	month = jun,
	year = {2004},
	pages = {289--295},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TL5VXJ8V\\Brumm - 2004 - Causes and consequences of song amplitude adjustme.pdf:application/pdf},
}

@article{sinnott_regulation_1975,
	title = {Regulation of voice amplitude by the monkey},
	volume = {58},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.380685},
	doi = {10.1121/1.380685},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Sinnott, Joan M. and Stebbins, William C. and Moody, David B.},
	month = aug,
	year = {1975},
	pages = {412--414},
}

@article{patel_influence_2008,
	title = {The {Influence} of {Linguistic} {Content} on the {Lombard} {Effect}},
	volume = {51},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282008/016%29},
	doi = {10.1044/1092-4388(2008/016)},
	abstract = {Purpose

                The
                Lombard effect
                describes the tendency for speakers to increase pitch, intensity, and duration in the presence of noise. It is unclear whether these modifications are uniformly applied across all words within an utterance or whether information-bearing content words are further enhanced compared with function words. In the present study, the authors investigated the influence of linguistic content on acoustic modifications made to speech in noise.



              Method

                Sixteen speaker–listener pairs engaged in an interactive cooperative game in quiet, 60 dB of multitalker noise, and 90 dB of multitalker noise. Speaker productions were analyzed to examine differences in fundamental frequency (F
                0
                ), intensity, and duration of target words in sentences across noise conditions.



              Results

                Proportional increases in F
                0,
                intensity, and duration were noted for all word types as noise increased from quiet to 60 dB. From quiet to 90 dB, content words that referred to agents, objects, and locations were disproportionately elongated compared with function words. Additionally, agents were further enhanced by increased F
                0
                .



              Conclusions

                At moderate noise levels, most word types appear to be uniformly boosted in F
                0
                , intensity, and duration. As noise increases, linguistic content shapes the extent of the Lombard effect, with F
                0
                and duration serving as primary cues for marking information-bearing word types.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Patel, Rupal and Schell, Kevin W.},
	month = feb,
	year = {2008},
	pages = {209--220},
}

@article{winkworth_speech_1997,
	title = {Speech {Breathing} and the {Lombard} {Effect}},
	volume = {40},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jslhr.4001.159},
	doi = {10.1044/jslhr.4001.159},
	abstract = {Respiratory measurements were made using linearized magnetometers placed antero-posteriorly over the rib cages and abdomens of five healthy young women. Background noise was introduced over headphones simultaneously as "babble" presented binaurally at 55 dB ("moderate noise") and 70 dB ("high noise"). Speech during oral reading and spontaneous monologue was transduced with a microphone positioned near the lips, from which a speaking intensity signal (dBA) was derived. Subjects were instructed to speak during the noise conditions, but no instruction was given to alter speaking intensity. Compared with a "no noise" condition, the speaking intensities of all the subjects increased significantly for both speech tasks in the moderate and high noise conditions, thereby replicating the well-documented Lombard effect. No consistent trend of lung volume change was observed, in contrast to the linear increases in speech intensity as the noise level increased. For the higher speech intensities during the moderate and high noise conditions both initiation and termination lung volumes either increased or decreased. These preliminary findings suggest that when speech intensity is increased following the introduction of noise via headphones rather than by specific instructions to speak more loudly, speakers employ variable lung volume strategies for intensity control.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Winkworth, Alison L. and Davis, Pamela J.},
	month = feb,
	year = {1997},
	pages = {159--169},
}

@article{vatikiotisbateson_auditory_2006,
	title = {Auditory, but perhaps not visual, processing of {Lombard} speech},
	volume = {119},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4786950},
	doi = {10.1121/1.4786950},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Vatikiotis‐Bateson, Eric and Chung, Victor and Lutz, Kevin and Mirante, Nicole and Otten, Jolien and Tan, Johanna},
	month = may,
	year = {2006},
	pages = {3444--3444},
}

@article{pick_inhibiting_1989,
	title = {Inhibiting the {Lombard} effect},
	volume = {85},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.397561},
	doi = {10.1121/1.397561},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Pick, Herbert L. and Siegel, Gerald M. and Fox, Paul W. and Garber, Sharon R. and Kearney, Joseph K.},
	month = feb,
	year = {1989},
	pages = {894--900},
}

@inproceedings{choudhury_comparitive_2015,
	address = {Silchar, India},
	title = {A comparitive study on classifiers to classify languages into {Tonal} and {Non}-{Tonal} {Languages}},
	isbn = {978-1-4673-6707-3 978-1-4673-6708-0},
	url = {http://ieeexplore.ieee.org/document/7377329/},
	doi = {10.1109/ISACC.2015.7377329},
	urldate = {2022-11-05},
	booktitle = {2015 {International} {Symposium} on {Advanced} {Computing} and {Communication} ({ISACC})},
	publisher = {IEEE},
	author = {Choudhury, Biplav and Choudhury, Tameem Salman},
	month = sep,
	year = {2015},
	pages = {132--135},
}

@incollection{ide_bruce_2000,
	address = {Dordrecht},
	title = {Bruce, {Pierrehumbert}, and the {Elements} of {Intonational} {Phonology}},
	volume = {14},
	isbn = {978-90-481-5562-0 978-94-015-9413-4},
	url = {http://link.springer.com/10.1007/978-94-015-9413-4_3},
	urldate = {2022-11-05},
	booktitle = {Prosody: {Theory} and {Experiment}},
	publisher = {Springer Netherlands},
	author = {Ladd, D. Robert},
	editor = {Ide, Nancy and Véronis, Jean and Horne, Merle},
	year = {2000},
	doi = {10.1007/978-94-015-9413-4_3},
	note = {Series Title: Text, Speech and Language Technology},
	pages = {37--50},
}

@article{werker_cross-language_1984,
	title = {Cross-language speech perception: {Evidence} for perceptual reorganization during the first year of life},
	volume = {7},
	issn = {01636383},
	shorttitle = {Cross-language speech perception},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0163638384800223},
	doi = {10.1016/S0163-6383(84)80022-3},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Infant Behavior and Development},
	author = {Werker, Janet F. and Tees, Richard C.},
	month = jan,
	year = {1984},
	pages = {49--63},
}

@article{greenberg_temporal_2003,
	title = {Temporal properties of spontaneous speech—a syllable-centric perspective},
	volume = {31},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447003000603},
	doi = {10.1016/j.wocn.2003.09.005},
	language = {en},
	number = {3-4},
	urldate = {2022-11-05},
	journal = {Journal of Phonetics},
	author = {Greenberg, Steven and Carvey, Hannah and Hitchcock, Leah and Chang, Shuangyu},
	month = jul,
	year = {2003},
	pages = {465--485},
}

@incollection{van_oostendorp_stress-timed_2011,
	address = {Oxford, UK},
	title = {Stress-{Timed} \textit{vs} . {Syllable}-{Timed} {Languages}: {Stress}-timed \textit{vs} . {Syllable}-timed {Languages}},
	isbn = {978-1-4443-3526-2},
	shorttitle = {Stress-{Timed} \textit{vs} . {Syllable}-{Timed} {Languages}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781444335262.wbctp0048},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {The {Blackwell} {Companion} to {Phonology}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Nespor, Marina and Shukla, Mohinish and Mehler, Jacques},
	editor = {van Oostendorp, Marc and Ewen, Colin J. and Hume, Elizabeth and Rice, Keren},
	month = apr,
	year = {2011},
	doi = {10.1002/9781444335262.wbctp0048},
	pages = {1--13},
}

@book{pike_intonation_1963,
	series = {Linguistics},
	title = {The {Intonation} of {American} {English}},
	url = {https://books.google.com.pk/books?id=Jz3iAAAAMAAJ},
	publisher = {University of Michigan Press},
	author = {Pike, K.L.},
	year = {1963},
	lccn = {45004529},
}

@inproceedings{adda-decker_m_reconnaissance_2006,
	title = {De la reconnaissance automatique de la parole `a l’analyse linguistique de corpus oraux},
	author = {Adda-Decker, M},
	year = {2006},
}

@article{avendano_properties_1998,
	title = {On the {Properties} of {Temporal} {Processing} for {Speech} in {Adverse} {Environments}},
	url = {https://www.researchgate.net/publication/2466964_Carlos_Avendano_and_Hynek_Hermansky_On_the_Properties_of_Temporal_Processing_for_Speech_in_Adverse_Environments_On_the_Properties_of_Temporal_Processing_for_Speech_in_Adverse_Environments},
	abstract = {In this paper we report on the results that we have obtained in the application of temporal processing to speech signals. We describe what are the properties that make temporal processing an interesting and useful technique to alleviate the harmful effects that environmental factors have on speech. Though temporal processing has been used in the past, its analysis and properties have not been studied in detail. We summarize some results that we obtained in a detailed analysis, and describe a data-driven design technique to design the processing. We demonstrate a speech enhancementsystem which illustrates some properties, advantages, and short-comings of the technique. 1. Introduction The performance of speech communication systems often degrades under realistic environmental conditions. Adverse environmental factors include additive noise sources, room reverberation, and transmission channel distortions. This work discusses the processing of speech in the temporal-feature or modulati...},
	author = {Avendaño, Carlos and Hermansky, Hynek},
	month = sep,
	year = {1998},
}

@inproceedings{melot_analysis_2015,
	address = {Scottsdale, AZ, USA},
	title = {Analysis of factors affecting system performance in the {ASpIRE} challenge},
	isbn = {978-1-4799-7291-3},
	url = {http://ieeexplore.ieee.org/document/7404838/},
	doi = {10.1109/ASRU.2015.7404838},
	urldate = {2022-11-05},
	booktitle = {2015 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU})},
	publisher = {IEEE},
	author = {Melot, Jennifer and Malyska, Nicolas and Ray, Jessica and Shen, Wade},
	month = dec,
	year = {2015},
	pages = {512--517},
}

@misc{noauthor_literature_nodate,
	title = {Literature {Review} - {Speech} {Recognition} for {Noisy} {Environments}},
	url = {https://users.dcc.uchile.cl/~abassi/WWW/Voz/speech-recog.html},
	urldate = {2022-11-08},
	file = {Literature Review - Speech Recognition for Noisy Environments:C\:\\Users\\DELL\\Zotero\\storage\\66WTUREM\\speech-recog.html:text/html},
}

@article{endres_voice_1971,
	title = {Voice {Spectrograms} as a {Function} of {Age}, {Voice} {Disguise}, and {Voice} {Imitation}},
	volume = {49},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1912589},
	doi = {10.1121/1.1912589},
	language = {en},
	number = {6B},
	urldate = {2022-11-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Endres, W. and Bambach, W. and Flösser, G.},
	month = jun,
	year = {1971},
	pages = {1842--1848},
}

@incollection{muller_study_2007,
	address = {Berlin, Heidelberg},
	title = {A {Study} of {Acoustic} {Correlates} of {Speaker} {Age}},
	volume = {4441},
	isbn = {978-3-540-74121-3 978-3-540-74122-0},
	url = {http://link.springer.com/10.1007/978-3-540-74122-0_1},
	language = {en},
	urldate = {2022-11-10},
	booktitle = {Speaker {Classification} {II}},
	publisher = {Springer Berlin Heidelberg},
	author = {Schötz, Susanne and Müller, Christian},
	editor = {Müller, Christian},
	year = {2007},
	doi = {10.1007/978-3-540-74122-0_1},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {1--9},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\WBR85E2H\\Schötz and Müller - 2007 - A Study of Acoustic Correlates of Speaker Age.pdf:application/pdf},
}

@inproceedings{gillick_statistical_1989,
	address = {Glasgow, UK},
	title = {Some statistical issues in the comparison of speech recognition algorithms},
	url = {http://ieeexplore.ieee.org/document/266481/},
	doi = {10.1109/ICASSP.1989.266481},
	urldate = {2022-11-10},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Gillick, L. and Cox, S.J.},
	year = {1989},
	pages = {532--535},
}

@inproceedings{aleksic_improved_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Improved recognition of contact names in voice commands},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178957/},
	doi = {10.1109/ICASSP.2015.7178957},
	urldate = {2022-11-10},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Aleksic, Petar and Allauzen, Cyril and Elson, David and Kracun, Aleksandar and Casado, Diego Melendo and Moreno, Pedro J.},
	month = apr,
	year = {2015},
	pages = {5172--5175},
}

@article{isaacs_rater_2013,
	title = {Rater {Experience}, {Rating} {Scale} {Length}, and {Judgments} of {L2} {Pronunciation}: {Revisiting} {Research} {Conventions}},
	volume = {10},
	issn = {1543-4303, 1543-4311},
	shorttitle = {Rater {Experience}, {Rating} {Scale} {Length}, and {Judgments} of {L2} {Pronunciation}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/15434303.2013.769545},
	doi = {10.1080/15434303.2013.769545},
	language = {en},
	number = {2},
	urldate = {2022-11-10},
	journal = {Language Assessment Quarterly},
	author = {Isaacs, Talia and Thomson, Ron I.},
	month = apr,
	year = {2013},
	pages = {135--159},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\D4D585T7\\Isaacs and Thomson - 2013 - Rater Experience, Rating Scale Length, and Judgmen.pdf:application/pdf},
}

@article{saito_second_2016,
	title = {Second language speech production: {Investigating} linguistic correlates of comprehensibility and accentedness for learners at different ability levels},
	volume = {37},
	issn = {0142-7164, 1469-1817},
	shorttitle = {Second language speech production},
	url = {https://www.cambridge.org/core/product/identifier/S0142716414000502/type/journal_article},
	doi = {10.1017/S0142716414000502},
	abstract = {ABSTRACT
            The current project aimed to investigate the potentially different linguistic correlates of comprehensibility (i.e., ease of understanding) and accentedness (i.e., linguistic nativelikeness) in adult second language (L2) learners’ extemporaneous speech production. Timed picture descriptions from 120 beginner, intermediate, and advanced Japanese learners of English were analyzed using native speaker global judgments based on learners’ comprehensibility and accentedness, and then submitted to segmental, prosodic, temporal, lexical, and grammatical analyses. Results showed that comprehensibility was related to all linguistic domains, and accentedness was strongly tied with pronunciation (specifically segmentals) rather than lexical and grammatical domains. In particular, linguistic correlates of L2 comprehensibility and accentedness were found to vary by learners’ proficiency levels. In terms of comprehensibility, optimal rate of speech, appropriate and rich vocabulary use, and adequate and varied prosody were important for beginner to intermediate levels, whereas segmental accuracy, good prosody, and correct grammar featured strongly for intermediate to advanced levels. For accentedness, grammatical complexity was a feature of intermediate to high-level performance, whereas segmental and prosodic variables were essential to accentedness across all levels. These findings suggest that syllabi tailored to learners’ proficiency level (beginner, intermediate, or advanced) and learning goal (comprehensibility or nativelike accent) would be advantageous for the teaching of L2 speaking.},
	language = {en},
	number = {2},
	urldate = {2022-11-10},
	journal = {Applied Psycholinguistics},
	author = {Saito, Kazuya and Trofimovich, Pavel and Isaacs, Talia},
	month = mar,
	year = {2016},
	pages = {217--240},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\VPJLHZP5\\Saito et al. - 2016 - Second language speech production Investigating l.pdf:application/pdf},
}

@inproceedings{chuchual_inbound_2010,
	address = {Tokyo, Japan},
	title = {Inbound and outbound calls assignment for an efficient call center},
	isbn = {978-1-4244-6485-2},
	url = {http://ieeexplore.ieee.org/document/5530196/},
	doi = {10.1109/ICSSSM.2010.5530196},
	urldate = {2022-11-13},
	booktitle = {2010 7th {International} {Conference} on {Service} {Systems} and {Service} {Management}},
	publisher = {IEEE},
	author = {Chuchual, Panichapat and Chongpravatisakul, Narissa and Kusolmanomai, Teerasarn and Komolavanij, Somrote},
	month = jun,
	year = {2010},
	pages = {1--4},
}

@inproceedings{liston_simulation_2017,
	address = {Las Vegas, NV},
	title = {Simulation based decision support for contact centers},
	isbn = {978-1-5386-3428-8},
	url = {http://ieeexplore.ieee.org/document/8248217/},
	doi = {10.1109/WSC.2017.8248217},
	urldate = {2022-11-13},
	booktitle = {2017 {Winter} {Simulation} {Conference} ({WSC})},
	publisher = {IEEE},
	author = {Liston, Paul and Byrne, James and Keogh, Orla and Byrne, P.J. and Bourke, Joe and Jones, Karl},
	month = dec,
	year = {2017},
	pages = {4586--4587},
}

@inproceedings{nwobodo-anyadiegwu_evaluating_2018,
	address = {Singapore},
	title = {Evaluating variables that affect job satisfaction of bank customer contact centre agents in {South} {Africa}},
	isbn = {978-1-5386-5747-8 978-1-5386-5748-5},
	url = {https://ieeexplore.ieee.org/document/8387081/},
	doi = {10.1109/IEA.2018.8387081},
	urldate = {2022-11-13},
	booktitle = {2018 5th {International} {Conference} on {Industrial} {Engineering} and {Applications} ({ICIEA})},
	publisher = {IEEE},
	author = {Nwobodo-Anyadiegwu, Eveth and Mbohwa, Charles and Ndlovu, Nokukhanya},
	month = apr,
	year = {2018},
	pages = {116--121},
}

@inproceedings{liu_research_2012,
	address = {Hangzhou, China},
	title = {Research on {Forecasting} {Call} {Center} {Traffic} through {PCA} and {BP} {Artificial} {Neural} {Network}},
	isbn = {978-1-4673-2646-9},
	url = {http://ieeexplore.ieee.org/document/6407017/},
	doi = {10.1109/ISCID.2012.117},
	urldate = {2022-11-13},
	booktitle = {2012 {Fifth} {International} {Symposium} on {Computational} {Intelligence} and {Design}},
	publisher = {IEEE},
	author = {Liu, Tao and Liu, Lieli},
	month = oct,
	year = {2012},
	pages = {444--447},
}

@inproceedings{archawaporn_erlang_2013,
	address = {Nakorn Pathom, Thailand},
	title = {Erlang {C} model for evaluate incoming call uncertainty in automotive call centers},
	isbn = {978-1-4673-5324-3 978-1-4673-5322-9},
	url = {http://ieeexplore.ieee.org/document/6694762/},
	doi = {10.1109/ICSEC.2013.6694762},
	urldate = {2022-11-13},
	booktitle = {2013 {International} {Computer} {Science} and {Engineering} {Conference} ({ICSEC})},
	publisher = {IEEE},
	author = {Archawaporn, Laksamon and Wongseree, Waranyu},
	month = sep,
	year = {2013},
	pages = {109--113},
}

@inproceedings{nguyen_development_2017,
	address = {Seoul},
	title = {Development of a {Vietnamese} speech recognition system for {Viettel} call center},
	isbn = {978-1-5386-3333-5},
	url = {https://ieeexplore.ieee.org/document/8384456/},
	doi = {10.1109/ICSDA.2017.8384456},
	urldate = {2022-11-17},
	booktitle = {2017 20th {Conference} of the {Oriental} {Chapter} of the {International} {Coordinating} {Committee} on {Speech} {Databases} and {Speech} {I}/{O} {Systems} and {Assessment} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Nguyen, Quoc Bao and Do, Van Hai and Dam, Ba Quyen and Le, Minh Hung},
	month = nov,
	year = {2017},
	pages = {1--5},
}

@inproceedings{do_agentclient_2020,
	address = {Kuala Lumpur, Malaysia},
	title = {Agent/{Client} {Speech} {Identification} for {Mixed}-{Channel} {Conversation} in {Customer} {Service} {Call} {Centers}},
	isbn = {978-1-72817-689-5},
	url = {https://ieeexplore.ieee.org/document/9310469/},
	doi = {10.1109/IALP51396.2020.9310469},
	urldate = {2022-11-17},
	booktitle = {2020 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	publisher = {IEEE},
	author = {Do, Van Hai and Mai, Van Tuan},
	month = dec,
	year = {2020},
	pages = {197--200},
}

@inproceedings{min_tang_call-type_2003,
	address = {St Thomas, VI, USA},
	title = {Call-type classification and unsupervised training for the call center domain},
	isbn = {978-0-7803-7980-0},
	url = {http://ieeexplore.ieee.org/document/1318429/},
	doi = {10.1109/ASRU.2003.1318429},
	urldate = {2022-11-17},
	booktitle = {2003 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({IEEE} {Cat}. {No}.{03EX721})},
	publisher = {IEEE},
	author = {{Min Tang} and Pellom, B. and Hacioglu, K.},
	year = {2003},
	pages = {204--208},
}

@inproceedings{galanis_classification_2013,
	address = {Budapest, Hungary},
	title = {Classification of emotional speech units in call centre interactions},
	isbn = {978-1-4799-1546-0 978-1-4799-1543-9},
	url = {http://ieeexplore.ieee.org/document/6719279/},
	doi = {10.1109/CogInfoCom.2013.6719279},
	urldate = {2022-11-17},
	booktitle = {2013 {IEEE} 4th {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
	publisher = {IEEE},
	author = {Galanis, Dimitrios and Karabetsos, Sotiris and Koutsombogera, Maria and Papageorgiou, Harris and Esposito, Anna and Riviello, Maria-Teresa},
	month = dec,
	year = {2013},
	pages = {403--406},
}

@inproceedings{draman_malay_2017,
	address = {Malacca City},
	title = {Malay speech corpus of telecommunication call center preparation for {ASR}},
	isbn = {978-1-5090-4912-7},
	url = {https://ieeexplore.ieee.org/document/8074675/},
	doi = {10.1109/ICoICT.2017.8074675},
	urldate = {2022-11-17},
	booktitle = {2017 5th {International} {Conference} on {Information} and {Communication} {Technology} ({ICoIC7})},
	publisher = {IEEE},
	author = {Draman, M. and Tee, D.C. and Lambak, Z. and Yahya, M.R. and Mohd Yusoff, M.I. and Ibrahim, S.H. and Saidon, S. and Abu Haris, N. and Tan, T.P.},
	month = may,
	year = {2017},
	pages = {1--6},
}

@inproceedings{deschamps-berger_end--end_2021,
	address = {Nara, Japan},
	title = {End-to-{End} {Speech} {Emotion} {Recognition}: {Challenges} of {Real}-{Life} {Emergency} {Call} {Centers} {Data} {Recordings}},
	isbn = {978-1-66540-019-0},
	shorttitle = {End-to-{End} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9597419/},
	doi = {10.1109/ACII52823.2021.9597419},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo and Lamel, Lori and Devillers, Laurence},
	month = sep,
	year = {2021},
	pages = {1--8},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\EQSN7492\\Deschamps-Berger et al. - 2021 - End-to-End Speech Emotion Recognition Challenges .pdf:application/pdf},
}

@inproceedings{zweig_automated_2006,
	address = {Toulouse, France},
	title = {Automated {Quality} {Monitoring} in the {Call} {Center} with {ASR} and {Maximum} {Entropy}},
	volume = {1},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1660089/},
	doi = {10.1109/ICASSP.2006.1660089},
	urldate = {2022-11-17},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Zweig, G. and Siohan, O. and Saon, G. and Ramabhadran, B. and Povey, D. and Mangu, L. and Kingsbury, B.},
	year = {2006},
	pages = {I--589--I--592},
}

@inproceedings{tarjan_n-gram_2019,
	address = {Naples, Italy},
	title = {N-gram {Approximation} of {LSTM} {Recurrent} {Language} {Models} for {Single}-pass {Recognition} of {Hungarian} {Call} {Center} {Conversations}},
	isbn = {978-1-72814-793-2},
	url = {https://ieeexplore.ieee.org/document/9089959/},
	doi = {10.1109/CogInfoCom47531.2019.9089959},
	urldate = {2022-11-17},
	booktitle = {2019 10th {IEEE} {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
	publisher = {IEEE},
	author = {Tarjan, Balazs and Szaszak, Gyorgy and Fegyo, Tibor and Mihajlik, Peter},
	month = oct,
	year = {2019},
	pages = {131--136},
}

@inproceedings{tarjan_improved_2013,
	address = {Cluj-Napoca, Romania},
	title = {Improved recognition of {Hungarian} call center conversations},
	isbn = {978-1-4799-1065-6 978-1-4799-1063-2},
	url = {http://ieeexplore.ieee.org/document/6682652/},
	doi = {10.1109/SpeD.2013.6682652},
	urldate = {2022-11-17},
	booktitle = {2013 7th {Conference} on {Speech} {Technology} and {Human} - {Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Tarjan, Balazs and Sarosi, Gellert and Fegyo, Tibor and Mihajlik, Peter},
	month = oct,
	year = {2013},
	pages = {1--6},
}

@article{plaza_call_2021,
	title = {Call {Transcription} {Methodology} for {Contact} {Center} {Systems}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9508438/},
	doi = {10.1109/ACCESS.2021.3102502},
	urldate = {2022-11-17},
	journal = {IEEE Access},
	author = {Plaza, Miroslaw and Pawlik, Lukasz and Deniziak, Stanislaw},
	year = {2021},
	pages = {110975--110988},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\JM7QLB6V\\Plaza et al. - 2021 - Call Transcription Methodology for Contact Center .pdf:application/pdf},
}

@article{fan_gated_2021,
	title = {Gated {Recurrent} {Fusion} {With} {Joint} {Training} {Framework} for {Robust} {End}-to-{End} {Speech} {Recognition}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9265262/},
	doi = {10.1109/TASLP.2020.3039600},
	urldate = {2022-11-17},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Fan, Cunhang and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Liu, Bin and Wen, Zhengqi},
	year = {2021},
	pages = {198--209},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\36B4AJ37\\Fan et al. - 2021 - Gated Recurrent Fusion With Joint Training Framewo.pdf:application/pdf},
}

@inproceedings{deschamps-berger_end--end_2021-1,
	address = {Nara, Japan},
	title = {End-to-{End} {Speech} {Emotion} {Recognition}: {Challenges} of {Real}-{Life} {Emergency} {Call} {Centers} {Data} {Recordings}},
	isbn = {978-1-66540-019-0},
	shorttitle = {End-to-{End} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9597419/},
	doi = {10.1109/ACII52823.2021.9597419},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo and Lamel, Lori and Devillers, Laurence},
	month = sep,
	year = {2021},
	pages = {1--8},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\DJESTVXN\\Deschamps-Berger et al. - 2021 - End-to-End Speech Emotion Recognition Challenges .pdf:application/pdf},
}

@inproceedings{chunwijitra_improving_2021,
	address = {Chiang Mai, Thailand},
	title = {Improving automatic transcription of call center speech using data simulation},
	isbn = {978-1-66540-382-5},
	url = {https://ieeexplore.ieee.org/document/9454803/},
	doi = {10.1109/ECTI-CON51831.2021.9454803},
	urldate = {2022-11-17},
	booktitle = {2021 18th {International} {Conference} on {Electrical} {Engineering}/{Electronics}, {Computer}, {Telecommunications} and {Information} {Technology} ({ECTI}-{CON})},
	publisher = {IEEE},
	author = {Chunwijitra, Vataya and Kurpukdee, Nattapong},
	month = may,
	year = {2021},
	pages = {842--845},
}

@inproceedings{deschamps-berger_emotion_2021,
	address = {Nara, Japan},
	title = {Emotion {Recognition} {In} {Emergency} {Call} {Centers}: {The} challenge of real-life emotions},
	isbn = {978-1-66540-021-3},
	shorttitle = {Emotion {Recognition} {In} {Emergency} {Call} {Centers}},
	url = {https://ieeexplore.ieee.org/document/9666308/},
	doi = {10.1109/ACIIW52867.2021.9666308},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} {Workshops} and {Demos} ({ACIIW})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo},
	month = sep,
	year = {2021},
	pages = {1--5},
}

@inproceedings{seknedy_speech_2021,
	address = {Cairo, Egypt},
	title = {Speech {Emotion} {Recognition} {System} for {Human} {Interaction} {Applications}},
	isbn = {978-1-66544-076-9},
	url = {https://ieeexplore.ieee.org/document/9694246/},
	doi = {10.1109/ICICIS52592.2021.9694246},
	urldate = {2022-11-17},
	booktitle = {2021 {Tenth} {International} {Conference} on {Intelligent} {Computing} and {Information} {Systems} ({ICICIS})},
	publisher = {IEEE},
	author = {Seknedy, Mai El and Fawzi, Sahar},
	month = dec,
	year = {2021},
	pages = {361--368},
}

@article{bai_fast_2021,
	title = {Fast {End}-to-{End} {Speech} {Recognition} {Via} {Non}-{Autoregressive} {Models} and {Cross}-{Modal} {Knowledge} {Transferring} {From} {BERT}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9437636/},
	doi = {10.1109/TASLP.2021.3082299},
	urldate = {2022-11-17},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Wen, Zhengqi and Zhang, Shuai},
	year = {2021},
	pages = {1897--1911},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U8MZQEJI\\Bai et al. - 2021 - Fast End-to-End Speech Recognition Via Non-Autoreg.pdf:application/pdf},
}

@inproceedings{li_comparative_2021-1,
	address = {Toronto, ON, Canada},
	title = {A {Comparative} {Study} of {Acoustic} and {Linguistic} {Features} {Classification} for {Alzheimer}'s {Disease} {Detection}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414147/},
	doi = {10.1109/ICASSP39728.2021.9414147},
	urldate = {2022-11-19},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	pages = {6423--6427},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\XNGARJH9\\Li et al. - 2021 - A Comparative Study of Acoustic and Linguistic Fea.pdf:application/pdf},
}

@incollection{dev_survey_2022,
	address = {Cham},
	title = {Survey on {Automatic} {Speech} {Recognition} {Systems} for {Indic} {Languages}},
	volume = {1546},
	isbn = {978-3-030-95710-0 978-3-030-95711-7},
	url = {https://link.springer.com/10.1007/978-3-030-95711-7_8},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {Artificial {Intelligence} and {Speech} {Technology}},
	publisher = {Springer International Publishing},
	author = {Sethi, Nandini and Dev, Amita},
	editor = {Dev, Amita and Agrawal, S. S. and Sharma, Arun},
	year = {2022},
	doi = {10.1007/978-3-030-95711-7_8},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {85--98},
}

@incollection{joshi_robust_2023,
	address = {Singapore},
	title = {Robust {Feature} {Extraction} and {Recognition} {Model} for {Automatic} {Speech} {Recognition} {System} on {News} {Report} {Dataset}},
	volume = {400},
	isbn = {978-981-19009-4-5 978-981-19009-5-2},
	url = {https://link.springer.com/10.1007/978-981-19-0095-2_56},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {Information and {Communication} {Technology} for {Competitive} {Strategies} ({ICTCS} 2021)},
	publisher = {Springer Nature Singapore},
	author = {Mendiratta, Sunanda and Turk, Neelam and Bansal, Dipali},
	editor = {Joshi, Amit and Mahmud, Mufti and Ragel, Roshan G.},
	year = {2023},
	doi = {10.1007/978-981-19-0095-2_56},
	note = {Series Title: Lecture Notes in Networks and Systems},
	pages = {589--601},
}

@article{alam_bengali_2022,
	title = {Bengali {Common} {Voice} {Speech} {Dataset} for {Automatic} {Speech} {Recognition}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2206.14053},
	doi = {10.48550/ARXIV.2206.14053},
	abstract = {Bengali is one of the most spoken languages in the world with over 300 million speakers globally. Despite its popularity, research into the development of Bengali speech recognition systems is hindered due to the lack of diverse open-source datasets. As a way forward, we have crowdsourced the Bengali Common Voice Speech Dataset, which is a sentence-level automatic speech recognition corpus. Collected on the Mozilla Common Voice platform, the dataset is part of an ongoing campaign that has led to the collection of over 400 hours of data in 2 months and is growing rapidly. Our analysis shows that this dataset has more speaker, phoneme, and environmental diversity compared to the OpenSLR Bengali ASR dataset, the largest existing open-source Bengali speech dataset. We present insights obtained from the dataset and discuss key linguistic challenges that need to be addressed in future versions. Additionally, we report the current performance of a few Automatic Speech Recognition (ASR) algorithms and set a benchmark for future research.},
	urldate = {2023-01-24},
	author = {Alam, Samiul and Sushmit, Asif and Abdullah, Zaowad and Nakkhatra, Shahrin and Ansary, MD. Nazmuddoha and Hossen, Syed Mobassir and Mehnaz, Sazia Morshed and Reasat, Tahsin and Humayun, Ahmed Imtiaz},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@inproceedings{younis_challenges_2021,
	address = {New Orleans, LA, USA},
	title = {Challenges in real-time-embedded {IoT} {Command} {Recognition}},
	isbn = {978-1-66544-431-6},
	url = {https://ieeexplore.ieee.org/document/9595903/},
	doi = {10.1109/WF-IoT51360.2021.9595903},
	urldate = {2023-02-05},
	booktitle = {2021 {IEEE} 7th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	publisher = {IEEE},
	author = {Younis, Hazem and Hansen, John H.L.},
	month = jun,
	year = {2021},
	pages = {848--851},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://aclweb.org/anthology/P16-1162},
	doi = {10.18653/v1/P16-1162},
	language = {en},
	urldate = {2023-02-05},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2016},
	pages = {1715--1725},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AVUDWHZP\\Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@article{chen_audio_2009,
	title = {Audio {Quality} {Issue} for {Automatic} {Speech} {Assessment}},
	abstract = {Recently, in the language testing field, automatic speech recog-nition (ASR) technology has been used to automatically score speaking tests. This paper investigates the impact of audio quality on ASR-based automatic speaking assessment. Using the read speech data in the International English Speaking Test (IEST) practice test, we annotated audio quality and compared scores rated by humans, speech recognition accuracy, and the quality of features used for the automatic assessment under high and low audio quality conditions. Our investigation suggests that human raters can cope with low-quality audio files well, but speech recognition and the features extracted for the automatic assessment perform worse on the low audio quality condition.},
	author = {Chen, Lei},
	month = jan,
	year = {2009},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\VFAQD9MQ\\Chen - 2009 - Audio Quality Issue for Automatic Speech Assessmen.pdf:application/pdf},
}

@article{kumalija_performance_2022,
	title = {Performance evaluation of automatic speech recognition systems on integrated noise-network distorted speech},
	volume = {2},
	issn = {2673-8198},
	url = {https://www.frontiersin.org/articles/10.3389/frsip.2022.999457/full},
	doi = {10.3389/frsip.2022.999457},
	abstract = {In VoIP applications, such as Interactive Voice Response and VoIP-phone conversation transcription, speech signals are degraded not only by environmental noise but also by transmission network quality, and distortions induced by encoding and decoding algorithms. Therefore, there is a need for automatic speech recognition (ASR) systems to handle integrated noise-network distorted speech. In this study, we present a comparative analysis of a speech-to-text system trained on clean speech against one trained on integrated noise-network distorted speech. Training an ASR model on noise-network distorted speech dataset improves its robustness. Although the performance of an ASR model trained on clean speech depends on noise type, this is not the case when noise is further distorted by network transmission. The model trained on noise-network distorted speech exhibited a 60\% improvement rate in the word error rate (WER), word match rate (MER), and word information lost (WIL) over the model trained on clean speech. Furthermore, the ASR model trained with noise-network distorted speech could tolerate a jitter of less than 20\% and a packet loss of less than 15\%, without a decrease in performance. However, WER, MER, and WIL increased in proportion to the jitter and packet loss as they exceeded 20\% and 15\%, respectively. Additionally, the model trained on noise-network distorted speech exhibited higher robustness compared to that trained on clean speech. The ASR model trained on noise-network distorted speech can also tolerate signal-to-noise (SNR) values of 5dB and above, without the loss of performance, independent of noise type.},
	urldate = {2023-02-16},
	journal = {Frontiers in Signal Processing},
	author = {Kumalija, Elhard and Nakamoto, Yukikazu},
	month = sep,
	year = {2022},
	pages = {999457},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\PUD2J8ZQ\\Kumalija and Nakamoto - 2022 - Performance evaluation of automatic speech recogni.pdf:application/pdf},
}

@article{bochner_effects_2022,
	title = {Effects of {Sound} {Quality} on the {Accuracy} of {Telephone} {Captions} {Produced} by {Automatic} {Speech} {Recognition}: {A} {Preliminary} {Investigation}},
	issn = {1059-0889, 1558-9137},
	shorttitle = {Effects of {Sound} {Quality} on the {Accuracy} of {Telephone} {Captions} {Produced} by {Automatic} {Speech} {Recognition}},
	url = {http://pubs.asha.org/doi/10.1044/2022_AJA-22-00102},
	doi = {10.1044/2022_AJA-22-00102},
	abstract = {Purpose:
              Automatic speech recognition (ASR) is commonly used to produce telephone captions to provide telecommunication access for individuals who are d/Deaf and hard of hearing (DHH). However, little is known about the effects of degraded telephone audio on the intelligibility of ASR captioning. This research note investigates the accuracy of telephone captions produced by ASR under degraded audio conditions.


              Method:
              Packet loss, delay, and repetition are common sources of degradation in sound quality for telephone audio. Eleven sets of wideband filtered sentences were degraded by high and low levels of simulated packet loss, delay, and repetition. These sets, along with a clean set of sentences, were submitted to ASR, and the accuracy of the resulting output was evaluated using three metrics: a word recognition score, word error rate, and word information loss.


              Results:
              The resulting pattern of data indicated the relative impact of each degraded condition on message intelligibility. The high and low packet loss conditions had the largest effect on message intelligibility. This finding was interpreted to indicate that packet loss can have a substantial impact on the accuracy of telephone captions produced with ASR.


              Conclusions:
              The results of this investigation point to a potential area of improvement in service quality that could have a substantial impact on telecommunication services for consumers who are DHH. Further research in this area is needed to provide additional information concerning the scope and impact of packet loss on the accuracy of telephone captioning produced by ASR.


              Supplemental Material:

                https://doi.org/10.23641/asha.21699557},
	language = {en},
	urldate = {2023-02-16},
	journal = {American Journal of Audiology},
	author = {Bochner, Joseph and Indelicato, Mark and Konnur, Pralhad},
	month = dec,
	year = {2022},
	pages = {1--8},
}

@article{kumalija_performance_2022-1,
	title = {Performance evaluation of automatic speech recognition systems on integrated noise-network distorted speech},
	volume = {2},
	issn = {2673-8198},
	url = {https://www.frontiersin.org/articles/10.3389/frsip.2022.999457/full},
	doi = {10.3389/frsip.2022.999457},
	abstract = {In VoIP applications, such as Interactive Voice Response and VoIP-phone conversation transcription, speech signals are degraded not only by environmental noise but also by transmission network quality, and distortions induced by encoding and decoding algorithms. Therefore, there is a need for automatic speech recognition (ASR) systems to handle integrated noise-network distorted speech. In this study, we present a comparative analysis of a speech-to-text system trained on clean speech against one trained on integrated noise-network distorted speech. Training an ASR model on noise-network distorted speech dataset improves its robustness. Although the performance of an ASR model trained on clean speech depends on noise type, this is not the case when noise is further distorted by network transmission. The model trained on noise-network distorted speech exhibited a 60\% improvement rate in the word error rate (WER), word match rate (MER), and word information lost (WIL) over the model trained on clean speech. Furthermore, the ASR model trained with noise-network distorted speech could tolerate a jitter of less than 20\% and a packet loss of less than 15\%, without a decrease in performance. However, WER, MER, and WIL increased in proportion to the jitter and packet loss as they exceeded 20\% and 15\%, respectively. Additionally, the model trained on noise-network distorted speech exhibited higher robustness compared to that trained on clean speech. The ASR model trained on noise-network distorted speech can also tolerate signal-to-noise (SNR) values of 5 
              dB
              and above, without the loss of performance, independent of noise type.},
	urldate = {2023-02-16},
	journal = {Frontiers in Signal Processing},
	author = {Kumalija, Elhard and Nakamoto, Yukikazu},
	month = sep,
	year = {2022},
	pages = {999457},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\N27P9CHJ\\Kumalija and Nakamoto - 2022 - Performance evaluation of automatic speech recogni.pdf:application/pdf},
}

@inproceedings{fan_practical_2020,
	address = {Taipei, Taiwan},
	title = {A {Practical} {Black}-{Box} {Attack} {Against} {Autonomous} {Speech} {Recognition} {Model}},
	isbn = {978-1-72818-298-8},
	url = {https://ieeexplore.ieee.org/document/9348184/},
	doi = {10.1109/GLOBECOM42002.2020.9348184},
	urldate = {2023-02-16},
	booktitle = {{GLOBECOM} 2020 - 2020 {IEEE} {Global} {Communications} {Conference}},
	publisher = {IEEE},
	author = {Fan, Wenshu and Li, Hongwei and Jiang, Wenbo and Xu, Guowen and Lu, Rongxing},
	month = dec,
	year = {2020},
	pages = {1--6},
}

@inproceedings{rg_leonard_tidigits_1984,
	address = {San Diego, CA, USA},
	title = {{TIDIGITS} database},
	author = {R.G Leonard},
	year = {1984},
	pages = {Paper 42.11},
}

@article{ming_union_2001,
	title = {Union: a model for partial temporal corruption of speech},
	volume = {15},
	issn = {08852308},
	shorttitle = {Union},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230800901669},
	doi = {10.1006/csla.2000.0166},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Ming, Ji and Jack Smith, F.},
	month = jul,
	year = {2001},
	pages = {217--231},
}

@article{noauthor_proceedings_1984,
	title = {{PROCEEDINGS} - {ICASSP} 84, {IEEE} {INTERNATIONAL} {CONFERENCE} {ON} {ACOUSTICS}, {SPEECH}, {AND} {SIGNAL} {PROCESSING}.},
	abstract = {This conference proceedings contains 294 papers. 286 papers are indexed separately. The topics covered are: narrow band speech coding; algorithms; spectral estimation; speech analysis synthesis; aids for the handicapped and laryngeal analysis; sonar signal processing; speaker recognition; filter design; adaptive processing; transform and convolution methods; speech hardware; DSP applications and techniques; digital audio; medium and wide band speech coding; digital signal processing; speech recognition; audio; system identification; radar and seismology processing; speech enhancement and quality; digital image processing; and time delay and coherence estimation.},
	journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	month = jan,
	year = {1984},
	pages = {var paging},
}

@inproceedings{marcus_overview_1992,
	address = {Harriman, New York},
	title = {Overview of the fifth {DARPA} speech and natural language workshop},
	isbn = {978-1-55860-272-4},
	url = {http://portal.acm.org/citation.cfm?doid=1075527.1075528},
	doi = {10.3115/1075527.1075528},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the workshop on {Speech} and {Natural} {Language}  - {HLT} '91},
	publisher = {Association for Computational Linguistics},
	author = {Marcus, Mitchell P.},
	year = {1992},
	pages = {3--4},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\L4P2XWME\\Marcus - 1992 - Overview of the fifth DARPA speech and natural lan.pdf:application/pdf},
}

@inproceedings{paul_d_overview_1992,
	address = {Harriman, New York},
	title = {Overview of the fifth {DARPA} speech and natural language workshop},
	isbn = {978-1-55860-272-4},
	url = {http://portal.acm.org/citation.cfm?doid=1075527.1075528},
	doi = {10.3115/1075527.1075528},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the workshop on {Speech} and {Natural} {Language}  - {HLT} '91},
	publisher = {Association for Computational Linguistics},
	author = {Paul D.},
	year = {1992},
	pages = {7--14},
}

@inproceedings{lockwood_experiments_1991,
	title = {Experiments with a non-linear spectral subtractor ({NSS}), hidden {Markov} models and the projection, for robust speech recognition in cars},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/lockwood91_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-16},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Lockwood, P. and Boudy, J.},
	month = sep,
	year = {1991},
	pages = {79--82},
}

@article{junqua_lombard_1993,
	title = {The {Lombard} reflex and its role on human listeners and automatic speech recognizers},
	volume = {93},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.405631},
	doi = {10.1121/1.405631},
	language = {en},
	number = {1},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Junqua, Jean‐Claude},
	month = jan,
	year = {1993},
	pages = {510--524},
}

@inproceedings{bateman_spectral_1992,
	address = {San Francisco, CA, USA},
	title = {Spectral contrast normalization and other techniques for speech recognition in noise},
	isbn = {978-0-7803-0532-8},
	url = {http://ieeexplore.ieee.org/document/225927/},
	doi = {10.1109/ICASSP.1992.225927},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Bateman, D.C. and Bye, D.K. and Hunt, M.J.},
	year = {1992},
	pages = {241--244 vol.1},
}

@article{atal_effectiveness_1974,
	title = {Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification},
	volume = {55},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1914702},
	doi = {10.1121/1.1914702},
	language = {en},
	number = {6},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Atal, B. S.},
	month = jun,
	year = {1974},
	pages = {1304--1312},
}

@book{conrads_effect_2014,
	title = {The {Effect} of {Communication} {Channels} on {Lying}},
	abstract = {This paper investigates the effect of different channels of communica- tion on lying behavior. A simple coin flip game with four coin tosses is adapted in which subjects have monetary incentives to misreport their pri- vate information. The treatments differ with respect to the communication channel employed to convey the private information, i.e., face-to face, phone, computer-mediated, and online. Against the hypotheses, the results show that a majority of subjects lies independently of communication channel in use. However, the decision whether to lie either to some or the full extent depends on the communication channel. Compared to more socially- distant communication, direct communication encourages partial lying, but decreases lying to the extreme. Women tend to lie to the full extent only under online communication. Social distance considerations and the prob- ability of being detect lying may drive observed behavioral patterns. The findings highlight the relevance of lying costs in relation to the decision making environment.},
	author = {Conrads, Julian},
	month = nov,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\N5KZFP4M\\Conrads - 2014 - The Effect of Communication Channels on Lying.pdf:application/pdf},
}

@book{conrads_effect_2014-1,
	title = {The {Effect} of {Communication} {Channels} on {Promise}-{Making} and {Promise}-{Keeping}},
	abstract = {This paper investigates the effect of different communication channels on promise-making
and promise-keeping in a helping situation. Four treatments differ with respect to the
communication channel employed to solicit unincentivized cooperation, i.e., face-to-face,
phone call and two different sorts of computer-mediated communication. The less
anonymous (face-to-face, phone) the interpersonal interaction is due to the different
communication channels, the higher the propensity of an agent to make a promise.
Treatment effects, however, vanish if we then look at the actual promise-keeping rates
across treatments as more anonymous channels (computer-mediated) do not perform
relatively worse than more direct channels.

JEL Classification: D02, D83, C91

Keywords: promises, communication, experimental economics, organizational behavior,
behavioral ethics},
	author = {Conrads, Julian and Reggiani, Tommaso},
	month = oct,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\NSVQL7F6\\Conrads and Reggiani - 2014 - The Effect of Communication Channels on Promise-Ma.pdf:application/pdf},
}

@article{herrnansky_compensation_1991,
	title = {Compensation for the effect of the communication channel in auditory-like analysis of speech ({RASTA}-{PLP})},
	journal = {Proc. Eurospeech},
	author = {Herrnansky, H. and Morgan, Nathaniel and Bayya, A. and Kohn, P.},
	month = jan,
	year = {1991},
}

@inproceedings{hermansky_rasta-plp_1992,
	address = {San Francisco, CA, USA},
	title = {{RASTA}-{PLP} speech analysis technique},
	isbn = {978-0-7803-0532-8},
	url = {http://ieeexplore.ieee.org/document/225957/},
	doi = {10.1109/ICASSP.1992.225957},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hermansky, H. and Morgan, N. and Bayya, A. and Kohn, P.},
	year = {1992},
	pages = {121--124 vol.1},
}

@inproceedings{hermansky_recognition_1993,
	address = {Minneapolis, MN, USA},
	title = {Recognition of speech in additive and convolutional noise based on {RASTA} spectral processing},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319236/},
	doi = {10.1109/ICASSP.1993.319236},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hermansky, H. and Morgan, N. and Hirsch, H.-G.},
	year = {1993},
	pages = {83--86 vol.2},
}

@inproceedings{hermansky_compensation_1991,
	title = {Compensation for the effect of the communication channel in auditory-like analysis of speech ({RASTA}-{PLP})},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/hermansky91b_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-312},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Hermansky, Hynek and Morgan, Nelson and Bayya, Aruna and Kohn, Phil},
	month = sep,
	year = {1991},
	pages = {1367--1370},
}

@inproceedings{hirsch_improved_1991,
	title = {Improved speech recognition using high-pass filtering of subband envelopes},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/hirsch91_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-105},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Hirsch, H. G. and Meyer, Peter and Ruehl, Hans-Wilhelm},
	month = sep,
	year = {1991},
	pages = {413--416},
}

@article{lu_sub-band_2011,
	title = {Sub-band temporal modulation envelopes and their normalization for automatic speech recognition in reverberant environments},
	volume = {25},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230810000677},
	doi = {10.1016/j.csl.2010.10.002},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Lu, Xugang and Unoki, Masashi and Nakamura, Satoshi},
	month = jul,
	year = {2011},
	pages = {571--584},
}

@article{vicente-pena_band-pass_2006,
	title = {Band-pass filtering of the time sequences of spectral parameters for robust wireless speech recognition},
	volume = {48},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639306000872},
	doi = {10.1016/j.specom.2006.07.007},
	language = {en},
	number = {10},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Vicente-Peña, J. and Gallardo-Antolín, A. and Peláez-Moreno, C. and Díaz-de-María, F.},
	month = oct,
	year = {2006},
	pages = {1379--1398},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\WQ2H7S55\\Vicente-Peña et al. - 2006 - Band-pass filtering of the time sequences of spect.pdf:application/pdf},
}

@inproceedings{do_improved_2017,
	title = {Improved {Automatic} {Speech} {Recognition} {Using} {Subband} {Temporal} {Envelope} {Features} and {Time}-{Delay} {Neural} {Network} {Denoising} {Autoencoder}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/do17c_interspeech.html},
	doi = {10.21437/Interspeech.2017-1096},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Do, Cong-Thanh and Stylianou, Yannis},
	month = aug,
	year = {2017},
	pages = {3832--3836},
}

@incollection{paliwal_robust_2010,
	title = {Robust {Speech} {Recognition} {Under} {Noisy} {Ambient} {Conditions}},
	isbn = {978-0-12-374708-2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123747082000061},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Human-{Centric} {Interfaces} for {Ambient} {Intelligence}},
	publisher = {Elsevier},
	author = {Paliwal, Kuldip K. and Yao, Kaisheng},
	year = {2010},
	doi = {10.1016/B978-0-12-374708-2.00006-1},
	pages = {135--162},
}

@inproceedings{paliwal_neural_1990,
	address = {Albuquerque, NM, USA},
	title = {Neural net classifiers for robust speech recognition under noisy environments},
	url = {http://ieeexplore.ieee.org/document/115737/},
	doi = {10.1109/ICASSP.1990.115737},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Paliwal, K.K.},
	year = {1990},
	pages = {429--432},
}

@inproceedings{anglade_speech_1993,
	address = {Minneapolis, MN, USA},
	title = {Speech discrimination in adverse conditions using acoustic knowledge and selectively trained neural networks},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319290/},
	doi = {10.1109/ICASSP.1993.319290},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Anglade, Y. and Fohr, D. and Junqua, J.-C.},
	year = {1993},
	pages = {279--282 vol.2},
}

@inproceedings{anglade_selectively_1992,
	title = {Selectively trained neural networks for the discrimination of normal and lombard speech},
	url = {https://www.isca-speech.org/archive/icslp_1992/anglade92_icslp.html},
	doi = {10.21437/ICSLP.1992-175},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Anglade, Yolande and Fohr, Dominique and Junqua, Jean-Claude},
	month = oct,
	year = {1992},
	pages = {595--598},
}

@article{erell_filterbank-energy_1993,
	title = {Filterbank-energy estimation using mixture and {Markov} models for recognition of noisy speech},
	volume = {1},
	issn = {10636676},
	url = {http://ieeexplore.ieee.org/document/221385/},
	doi = {10.1109/89.221385},
	number = {1},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Erell, A. and Weintraub, M.},
	month = jan,
	year = {1993},
	pages = {68--76},
}

@article{green_auditory_1995,
	title = {Auditory {Scene} {Analysis} {And} {Hidden} {Markov} {Model} {Recognition} {Of} {Speech} {In} {Noise}},
	abstract = {We describe a novel paradigm for automatic speech recognition in noisy environments in which an initial stage of auditory scene analysis separates out the evidence for the speech to be recognised from the evidence for other sounds. In general, this evidence will be incomplete, since intruding sound sources will dominate some spectro-temporal regions. We generalise continuous-density hidden Markov model recognition to this `occluded speech' case. The technique is based on estimating the probability that a Gaussian mixture density distribution for an auditory firing rate map will generate an observation such that the separated components are at their observed values and the remaining components are not greater than their values in the acoustic mixture. Experiments on isolated digit recognition in noise demonstrate the potential of the new approach to yield performance comparable to that of listeners. 1. AUDITORY SCENE ANALYSIS AS A PREPROCESSOR FOR SPEECH RECOGNITION Auditory scene an...},
	author = {Green, Phil and Cooke, M.P. and Crawford, M.},
	month = mar,
	year = {1995},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\F5ZA4LBP\\Green et al. - 1995 - Auditory Scene Analysis And Hidden Markov Model Re.pdf:application/pdf},
}

@article{lee_information-theoretic_1991,
	title = {Information-theoretic distortion measures for speech recognition},
	volume = {39},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/80815/},
	doi = {10.1109/78.80815},
	number = {2},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Signal Processing},
	author = {Lee, Y.-T.},
	month = feb,
	year = {1991},
	pages = {330--335},
}

@article{nocerino_comparative_1985,
	title = {Comparative study of several distortion measures for speech recognition},
	volume = {4},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167639385900573},
	doi = {10.1016/0167-6393(85)90057-3},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Nocerino, N. and Soong, F.K. and Rabiner, L.R. and Klatt, D.H.},
	month = dec,
	year = {1985},
	pages = {317--331},
}

@article{nakamura_robust_1995,
	title = {Robust {Word} {Spotting} {In} {Adverse} {Car} {Environments}},
	abstract = {This paper presents a novel word recognition technique which allows hand-free dialing under adverse car environments. The algorithm is based on speaker dependent word spotting. The paper compared and investigated the methods of spectral subtraction, short time modified coherence, multi-microphone, dynamic and accelerated features, weighted distance measures and multi-templates by word recognition experiments. The experiments are carried out using real speech database uttered in adverse car environments. The experiments show that the method using sinusoidal weighted lifter and the method using multi-templates are robust against the adverse environments. Keywords: word spotting, distance measure, adverse environment 1. INTRODUCTION Recent technology realizes a small size mobile cellular telephone. This enables easy communication to anybody at any time on the way. However, it has a problem when we use the telephone while driving. Dialing and a telephone call while driving are very dang...},
	author = {Nakamura, Satoshi and Akabane, Toshio and Hamaguchi, Seiji},
	month = dec,
	year = {1995},
}

@article{nakamura_robust_1993,
	title = {Robust word recognition in adverse car environments},
	abstract = {This paper presents a novel word recognition technique which allows hand-free dialing in adverse car environments. The algorithm is based on speaker dependent word spotting. The paper compared and investigated such methods as spectral subtraction, short time modified coherence, effects of multi-microphone, dynamic and accelerated features, weighted distance measures and multi-templates by word recognition experiments. The experiments which were carried out using real speech database uttered in adverse car environments showed that the proposed method using the cepstrum weighted lifter and multi-templates achieves 98\% word recognition accuracy in 80km/h driving car environments.},
	author = {Nakamura, Satoshi and Akabane, Toshio and Hamaguchi, Seiji and Kitoh, Atsunori},
	month = nov,
	year = {1993},
	pages = {9--14},
}

@inproceedings{nakamura_robust_1993-1,
	title = {Robust word spotting in adverse car environments},
	url = {https://www.isca-speech.org/archive/eurospeech_1993/nakamura93_eurospeech.html},
	doi = {10.21437/Eurospeech.1993-254},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {3rd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1993)},
	publisher = {ISCA},
	author = {Nakamura, Satoshi and Akabane, Toshio and Hamaguchi, Seiji},
	month = sep,
	year = {1993},
	pages = {1045--1048},
}

@article{kanazawa_learning_1989,
	title = {A learning word‐spotting method for speaker‐independent word recognition in noisy environments},
	volume = {86},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2027643},
	doi = {10.1121/1.2027643},
	language = {en},
	number = {S1},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kanazawa, Hiroshi and Takebayashi, Yoichi},
	month = nov,
	year = {1989},
	pages = {S76--S76},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\BAQBJT22\\Kanazawa and Takebayashi - 1989 - A learning word‐spotting method for speaker‐indepe.pdf:application/pdf},
}

@article{mansour_short-time_1989,
	title = {The short-time modified coherence representation and noisy speech recognition},
	volume = {37},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/28053/},
	doi = {10.1109/ASSP.1989.28053},
	language = {en},
	number = {6},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Mansour, D. and Juang, B.H.},
	month = jun,
	year = {1989},
	pages = {795--804},
}

@article{kosaka_noisy_2006,
	title = {Noisy speech recognition based on codebook normalization of discrete‐mixture hidden {Markov} models},
	volume = {120},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4787211},
	doi = {10.1121/1.4787211},
	language = {en},
	number = {5},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kosaka, Tetsuo and Models, hidden Markov and Katoh, Masaharu and Kohda, Masaki},
	month = nov,
	year = {2006},
	pages = {3041--3041},
}

@inproceedings{mansour_short-time_1988,
	address = {New York, NY, USA},
	title = {The short-time modified coherence representation and its application for noisy speech recognition},
	url = {http://ieeexplore.ieee.org/document/196635/},
	doi = {10.1109/ICASSP.1988.196635},
	urldate = {2023-02-18},
	booktitle = {{ICASSP}-88., {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Mansour, D. and Juang, B.H.},
	year = {1988},
	pages = {525--528},
}

@article{bhatt_comparative_2010,
	title = {{COMPARATIVE} {STUDY} {OF} {SPEECH} {RECOGNITION} {SYSTEM} {USING} {VARIOUS} {FEATURE} {EXTRACTION} {TECHNIQUES}},
	abstract = {It is very important to detect the speech endpoints accurately in speech recognition. This paper presents a comparative analysis of various feature extraction techniques of endpoint detection in speech recognition of isolated words in noisy environments. The endpoint detection problem is nontrivial for no stationary backgrounds where artifacts (i.e., no speech events) may be introduced by the speaker, the recording environment, and the transmission system. An optimum set of characteristics is identified by combining parameters from both time domain and frequency domain, in a robust approach for identification when the speech signal is corrupted by additive noise and channel distortion. The cases of colored noises such as babble noise, factory noise at different SNR values in conjunction with distortions due to recording medium were tested. Experimental results identify the optimal algorithm which significantly achieves the highest performance in the recognition task.},
	author = {Bhatt, Kapil and SINHA, RK},
	month = dec,
	year = {2010},
}

@inproceedings{jgmenalssandovalrggomez_comparative_1990,
	address = {Barcelona, Spain},
	title = {A comparative study of feature extraction methods for noisy speech recognition},
	abstract = {L.torres, E.Masgrau, and M.A.Lagunas},
	booktitle = {Signal processing {V} : theories and applications : proceedings of  {Fifth} {EUSIPCO}},
	author = {J.G.Mena,L.S.Sandoval,R.G.Gomez},
	month = sep,
	year = {1990},
	pages = {1191--1194},
}

@article{gao_auditory_1993,
	title = {Auditory model based speech recognition and comparison with other methods},
	volume = {21},
	abstract = {On the basis of the periphery auditory model and partial control auditory neural processing model set up in reference (1), a speech recognizer using an auditory model as the acoustic front-end preprocessor and a monotopical organized neural network as the recognition classifier have been built. The experiments show that the parameters derived from the auditory model are a good representation of speech discrimination, especially in noisy environments. The results of speech recognition show that under the condition of 3dB background noise with the same neural network as the classifier, the recognition rate of 3 confusable consonants(p.t.k) is 80.3\% for auditory model as the front-end processor and 69.2\% for LPC-derived spectrum as speech parameters, respectively.},
	author = {Gao, Yuqing and Huang, Taiyi and Chen, Shaoyan},
	month = oct,
	year = {1993},
	pages = {1--6},
}

@inproceedings{hermansky_perceptually_1985,
	address = {Tampa, FL, USA},
	title = {Perceptually based linear predictive analysis of speech},
	volume = {10},
	url = {http://ieeexplore.ieee.org/document/1168384/},
	doi = {10.1109/ICASSP.1985.1168384},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '85. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Hermansky, H. and Hanson, B. and Wakita, H.},
	year = {1985},
	pages = {509--512},
}

@article{cheng_speech_1991,
	title = {Speech enhancement based conceptually on auditory evidence},
	volume = {39},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/134427/},
	doi = {10.1109/78.134427},
	number = {9},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Signal Processing},
	author = {Cheng, Y.M. and O'Shaughnessy, D.},
	month = sep,
	year = {1991},
	pages = {1943--1954},
}

@inproceedings{cheng_speech_1991-1,
	address = {Toronto, Ont., Canada},
	title = {Speech enhancement based conceptually on auditory evidence},
	isbn = {978-0-7803-0003-3},
	url = {http://ieeexplore.ieee.org/document/150500/},
	doi = {10.1109/ICASSP.1991.150500},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP} 91: 1991 {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Cheng, Y.M. and O'Shaughnessy, D.},
	year = {1991},
	pages = {961--964 vol.2},
}

@book{toner_speech_1993,
	title = {Speech enhancement based conceptually on auditory processingutilising correlation measurements},
	abstract = {An adaptive subband multisensor structure for speech enhancement is proposed, its conceptual basis being the accepted model of the cochlea as a spectrum analyser. Furthermore, the type of subband process implemented is dependent on subband correlation. The convergence of the proposed method is compared with conventional LMS and frequency domain LMS and a dramatic increase in convergence rate is shown using both simulated and real data},
	author = {Toner, E. and Campbell, D.R.},
	month = mar,
	year = {1993},
	note = {Pages: 2/5},
}

@article{arehart_speech_2001,
	title = {Speech enhancement based on aspects of the auditory process},
	volume = {109},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4744642},
	doi = {10.1121/1.4744642},
	language = {en},
	number = {5},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Arehart, Kathryn and Hansen, John},
	month = may,
	year = {2001},
	pages = {2440--2440},
}

@inproceedings{nandkumar_speech_1994,
	address = {Adelaide, SA, Australia},
	title = {Speech enhancement based on a new set of auditory constrained parameters},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389370/},
	doi = {10.1109/ICASSP.1994.389370},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Nandkumar, S. and Hansen, J.H.L.},
	year = {1994},
	pages = {I/1--I/4},
}

@article{ghitza_auditory_1986,
	title = {Auditory nerve representation as a front-end for speech recognition in a noisy environment},
	volume = {1},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230886800183},
	doi = {10.1016/S0885-2308(86)80018-3},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Ghitza, Oded},
	month = dec,
	year = {1986},
	pages = {109--130},
}

@incollection{ghitza_auditory_1992,
	title = {Auditory {Nerve} {Representation} as a {Basis} for {Speech} {Processing}},
	author = {Ghitza, Oded},
	month = jan,
	year = {1992},
}

@inproceedings{dobrin_speech_1995,
	title = {Speech recognition experiments in a noisy environment using auditory system modelling},
	url = {https://www.isca-speech.org/archive/eurospeech_1995/dobrin95_eurospeech.html},
	doi = {10.21437/Eurospeech.1995-36},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {4th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1995)},
	publisher = {ISCA},
	author = {Dobrin, Cristina and Haavisto, Petri and Laurila, Kari and Astola, Jaakko},
	month = sep,
	year = {1995},
	pages = {131--134},
}

@inproceedings{wu_modeling_1990,
	address = {Albuquerque, NM, USA},
	title = {Modeling spectral processing in the central auditory system},
	url = {http://ieeexplore.ieee.org/document/115693/},
	doi = {10.1109/ICASSP.1990.115693},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Wu, Z.L. and Schwartz, J.-L. and Escudier, P.},
	year = {1990},
	pages = {373--376},
}

@article{dautrich_effects_1983,
	title = {On the effects of varying filter bank parameters on isolated word recognition},
	volume = {31},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164172/},
	doi = {10.1109/TASSP.1983.1164172},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Dautrich, B. and Rabiner, L. and Martin, T.},
	month = aug,
	year = {1983},
	pages = {793--807},
}

@article{furui_toward_2005,
	title = {Toward {Robust} {Speech} {Recognition} and {Understanding}},
	volume = {41},
	issn = {0922-5773},
	url = {http://link.springer.com/10.1007/s11265-005-4149-x},
	doi = {10.1007/s11265-005-4149-x},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Journal of VLSI signal processing systems for signal, image and video technology},
	author = {Furui, Sadaoki},
	month = nov,
	year = {2005},
	pages = {245--254},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\VEBF5ANZ\\Furui - 2005 - Toward Robust Speech Recognition and Understanding.pdf:application/pdf},
}

@incollection{goos_toward_2003,
	address = {Berlin, Heidelberg},
	title = {Toward {Robust} {Speech} {Recognition} and {Understanding}},
	volume = {2807},
	isbn = {978-3-540-20024-6 978-3-540-39398-6},
	url = {http://link.springer.com/10.1007/978-3-540-39398-6_2},
	urldate = {2023-02-18},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer Berlin Heidelberg},
	author = {Furui, Sadaoki},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Matoušek, Václav and Mautner, Pavel},
	year = {2003},
	doi = {10.1007/978-3-540-39398-6_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {2--11},
}

@article{stern_signal_1999,
	title = {Signal {Processing} {For} {Robust} {Speech} {Recognition}},
	abstract = {This chapter compares several di\#erent approaches to robust automatic speech recognition. We review ongoing research in the use of acoustical pre-processing to achieve robust speech recognition, discussing and comparing approaches based on direct cepstral comparisons, on parametric models of environmental degradation, and on cepstral high-pass \#ltering. We also describe and compare the e\#ectiveness of two complementary methods of signal processing for robust speech recognition: microphone array processing and the use of physiologically-motivated models of peripheral auditory processing. This chapter includes comparisons of recognition error rates obtained when the various signal processing algorithms considered are used to process inputs to CMU's SPHINX speech recognition system. 1 INTRODUCTION The development of robust speech recognition systems that maintain a high level of recognition accuracy in di\#cult and dynamically-varying acoustical environments is becoming increasingly impo...},
	author = {Stern, Richard and Acero, Ro},
	month = mar,
	year = {1999},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\KE3QDV3H\\Stern and Acero - 1999 - Signal Processing For Robust Speech Recognition.pdf:application/pdf},
}

@inproceedings{sfurui_toward_1992,
	address = {Cannes,France},
	title = {Toward robust speech recognition under adverse conditions},
	booktitle = {Proc. {Speech} {Processing} in {Adverse} {Conditions}},
	author = {S.Furui},
	year = {1992},
	pages = {31--41},
}

@inproceedings{takebayashi_robust_1991,
	address = {Toronto, Ont., Canada},
	title = {A robust speech recognition system using word-spotting with noise immunity learning},
	isbn = {978-0-7803-0003-3},
	url = {http://ieeexplore.ieee.org/document/150486/},
	doi = {10.1109/ICASSP.1991.150486},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP} 91: 1991 {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Takebayashi, Y. and Tsuboi, H. and Kanazawa, H.},
	year = {1991},
	pages = {905--908 vol.2},
}

@inproceedings{takebayashi_adaptive_1994,
	address = {Adelaide, SA, Australia},
	title = {Adaptive noise immunity learning for word spotting},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389259/},
	doi = {10.1109/ICASSP.1994.389259},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Takebayashi, Y. and Kanazawa, H.},
	year = {1994},
	pages = {I/449--I/452},
}

@inproceedings{htsuboihkanazawaytakebayashi_accelerator_1990,
	title = {An accelerator for high-speech spoken word spotting and noise immunity learning system},
	url = {https://www.isca-speech.org/archive/icslp_1990/tsuboi90_icslp.html},
	author = {H.Tsuboi,H.Kanazawa,Y.Takebayashi},
	year = {1990},
	pages = {273--276},
}

@inproceedings{sankar_noise_1994,
	address = {Adelaide, SA, Australia},
	title = {Noise immunization using neural net for speech recognition},
	volume = {ii},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389563/},
	doi = {10.1109/ICASSP.1994.389563},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Sankar, R. and Patravali, S.},
	year = {1994},
	pages = {II/685--II/688},
}

@inproceedings{kitamura_speaker-independent_1992,
	title = {Speaker-independent spoken digit recognition in noisy environments using dynamic spectral features and neural networks},
	url = {https://www.isca-speech.org/archive/icslp_1992/kitamura92_icslp.html},
	doi = {10.21437/ICSLP.1992-237},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Kitamura, Tadashi and Ando, Satoshi and Hayahara, Etsuro},
	month = oct,
	year = {1992},
	pages = {699--702},
}

@inproceedings{wu_deep_2015-1,
	address = {South Brisbane, Queensland, Australia},
	title = {Deep neural networks employing {Multi}-{Task} {Learning} and stacked bottleneck features for speech synthesis},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178814/},
	doi = {10.1109/ICASSP.2015.7178814},
	urldate = {2023-02-18},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Zhizheng and Valentini-Botinhao, Cassia and Watts, Oliver and King, Simon},
	month = apr,
	year = {2015},
	pages = {4460--4464},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\STRY8WPV\\Wu et al. - 2015 - Deep neural networks employing Multi-Task Learning.pdf:application/pdf},
}

@inproceedings{lippmann_multi-style_1987,
	address = {Dallas, TX, USA},
	title = {Multi-style training for robust isolated-word speech recognition},
	volume = {12},
	url = {http://ieeexplore.ieee.org/document/1169544/},
	doi = {10.1109/ICASSP.1987.1169544},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '87. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Lippmann, R. and Martin, E. and Paul, D.},
	year = {1987},
	pages = {705--708},
}

@inproceedings{mizuta_optimal_1992,
	title = {Optimal discriminative training for {HMMs} to recognize noisy speech},
	url = {https://www.isca-speech.org/archive/icslp_1992/mizuta92_icslp.html},
	doi = {10.21437/ICSLP.1992-194},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Mizuta, Shinobu and Nakajima, Kunio},
	month = oct,
	year = {1992},
	pages = {1519--1522},
}

@article{lee_study_1991,
	title = {A study on speaker adaptation of the parameters of continuous density hidden {Markov} models},
	volume = {39},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/80902/},
	doi = {10.1109/78.80902},
	number = {4},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Signal Processing},
	author = {Lee, C.-H. and Lin, C.-H. and Juang, B.-H.},
	month = apr,
	year = {1991},
	pages = {806--814},
}

@article{ferguson_variable_1979,
	title = {Variable duration models for speech},
	journal = {Symposium on the Application of Hidden Markov Models to Text and Speech},
	author = {Ferguson, J.},
	month = nov,
	year = {1979},
}

@inproceedings{russell_experimental_1987,
	address = {Dallas, TX, USA},
	title = {Experimental evaluation of duration modelling techniques for automatic speech recognition},
	url = {http://ieeexplore.ieee.org/document/1169918/},
	doi = {10.1109/ICASSP.1987.1169918},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '87. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Russell, M. and Cook, A.},
	year = {1987},
	pages = {2376--2379},
}

@inproceedings{nnicolseulermfalkhausenhreiningerdwolfjzinke_improving_1992,
	title = {Improving the robustness of automatic speech recognisers using state duration information},
	booktitle = {Proc. {Speech} {Processing} in {Adverse} {Conditions}},
	author = {N.Nicol,S.Euler,M.Falkhausen,H.Reininger,D.Wolf,J.Zinke},
	year = {1992},
	pages = {183--186},
}

@article{noyes_automatic_1989,
	title = {Automatic speech recognition for disabled people},
	volume = {20},
	issn = {00036870},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0003687089901932},
	doi = {10.1016/0003-6870(89)90193-2},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Applied Ergonomics},
	author = {Noyes, J.M. and Haigh, R. and Starr, A.F.},
	month = dec,
	year = {1989},
	pages = {293--298},
}

@inproceedings{mosner_improving_2019,
	address = {Brighton, United Kingdom},
	title = {Improving {Noise} {Robustness} of {Automatic} {Speech} {Recognition} via {Parallel} {Data} and {Teacher}-student {Learning}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683422/},
	doi = {10.1109/ICASSP.2019.8683422},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Mosner, Ladislav and Wu, Minhua and Raju, Anirudh and Krishnan Parthasarathi, Sree Hari and Kumatani, Kenichi and Sundaram, Shiva and Maas, Roland and Hoffmeister, Bjorn},
	month = may,
	year = {2019},
	pages = {6475--6479},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\K3BKCUP5\\Mosner et al. - 2019 - Improving Noise Robustness of Automatic Speech Rec.pdf:application/pdf},
}

@inproceedings{varga_hidden_1990,
	address = {Albuquerque, NM, USA},
	title = {Hidden {Markov} model decomposition of speech and noise},
	url = {http://ieeexplore.ieee.org/document/115970/},
	doi = {10.1109/ICASSP.1990.115970},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Varga, A.P. and Moore, R.K.},
	year = {1990},
	pages = {845--848},
}

@book{gales_improved_1992,
	title = {An {Improved} {Approach} to the {Hidden} {Markov} {Model} {Decomposition} of {Speech} and {Noise}},
	abstract = {This paper addresses the problem of automatic speech recognition in the presence of interfering noise. The new approach described decomposes the contaminated speech signal using a generalisation of standard Hidden Markov Modelling, whilst utilising a compact and effective parametrisation of the speech signal. The technique is compared to some existing noise compensation techniques, using data recorded in noise, and is found to have improved performance compared to existing model decomposition techniques. Performance is comparable to existing noise subtraction techniques, but the technique is applicable to a wider range of noise environments and is not dependent on an accurate end pointing of the speech.},
	author = {Gales, M.J.F. and Young, Steve},
	month = mar,
	year = {1992},
	note = {Pages: 236},
}

@article{gales_cepstral_1993,
	title = {Cepstral parameter compensation for {HMM} recognition in noise},
	volume = {12},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016763939390093Z},
	doi = {10.1016/0167-6393(93)90093-Z},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Gales, M.J.F. and Young, S.J.},
	month = jul,
	year = {1993},
	pages = {231--239},
}

@inproceedings{kobayashi_markov_1994,
	address = {Adelaide, SA, Australia},
	title = {Markov model based noise modeling and its application to noisy speech recognition using dynamical features of speech},
	volume = {ii},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389719/},
	doi = {10.1109/ICASSP.1994.389719},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Kobayashi, T. and Mine, R. and Shirai, K.},
	year = {1994},
	pages = {II/57--II/60},
}

@inproceedings{vaseghi_noisy_1994,
	address = {Adelaide, SA, Australia},
	title = {Noisy speech recognition using cepstral-time features and spectral-time filters},
	volume = {ii},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389717/},
	doi = {10.1109/ICASSP.1994.389717},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Vaseghi, S.V. and Milner, B.P. and Humphries, J.J.},
	year = {1994},
	pages = {II/65--II/68},
}

@inproceedings{mokbel_word_1992,
	title = {Word recognition in the car: adapting recognizers to new environments},
	shorttitle = {Word recognition in the car},
	url = {https://www.isca-speech.org/archive/icslp_1992/mokbel92_icslp.html},
	doi = {10.21437/ICSLP.1992-239},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Mokbel, C. and Barbier, L. and Kerlou, Y. and Chollet, Gérard},
	month = oct,
	year = {1992},
	pages = {707--710},
}

@inproceedings{tamura_improvements_1990,
	address = {Albuquerque, NM, USA},
	title = {Improvements to the noise reduction neural network},
	url = {http://ieeexplore.ieee.org/document/115957/},
	doi = {10.1109/ICASSP.1990.115957},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Tamura, S. and Nakamura, M.},
	year = {1990},
	pages = {825--828},
}

@article{van_compernolle_noise_1989,
	title = {Noise adaptation in a hidden {Markov} model speech recognition system},
	volume = {3},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0885230889900272},
	doi = {10.1016/0885-2308(89)90027-2},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Van Compernolle, Dirk},
	month = apr,
	year = {1989},
	pages = {151--167},
}

@inproceedings{berouti_enhancement_1979,
	address = {Washington, DC, USA},
	title = {Enhancement of speech corrupted by acoustic noise},
	volume = {4},
	url = {http://ieeexplore.ieee.org/document/1170788/},
	doi = {10.1109/ICASSP.1979.1170788},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '79. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Berouti, M. and Schwartz, R. and Makhoul, J.},
	year = {1979},
	pages = {208--211},
}

@article{boll_suppression_1979,
	title = {Suppression of acoustic noise in speech using spectral subtraction},
	volume = {27},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1163209/},
	doi = {10.1109/TASSP.1979.1163209},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Boll, S.},
	month = apr,
	year = {1979},
	pages = {113--120},
}

@inproceedings{lockwood_experiments_1991-1,
	title = {Experiments with a non-linear spectral subtractor ({NSS}), hidden {Markov} models and the projection, for robust speech recognition in cars},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/lockwood91_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-16},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Lockwood, P. and Boudy, J.},
	month = sep,
	year = {1991},
	pages = {79--82},
}

@inproceedings{flores_continuous_1994,
	address = {Adelaide, SA, Australia},
	title = {Continuous speech recognition in noise using spectral subtraction and {HMM} adaptation},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389269/},
	doi = {10.1109/ICASSP.1994.389269},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Flores, J.A.N. and Young, S.J.},
	year = {1994},
	pages = {I/409--I/412},
}

@inproceedings{klatt_digital_1976,
	address = {Philadelphia, PA, USA},
	title = {A digital filter bank for spectral matching},
	volume = {1},
	url = {http://ieeexplore.ieee.org/document/1170107/},
	doi = {10.1109/ICASSP.1976.1170107},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '76. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Klatt, D.},
	year = {1976},
	pages = {573--576},
}

@inproceedings{jholmes_noise_1986,
	title = {Noise compensation for speech recognition using probabalistic models},
	author = {J.Holmes and N.Sedgewick},
	year = {1986},
	pages = {741--744},
}

@inproceedings{varga_control_1989,
	title = {Control experiments on noise compensation in hidden {Markov} model based continuous word recognition},
	url = {https://www.isca-speech.org/archive/eurospeech_1989/varga89_eurospeech.html},
	doi = {10.21437/Eurospeech.1989-53},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {First {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1989)},
	publisher = {ISCA},
	author = {Varga, Andrew and Ponting, Keith},
	month = sep,
	year = {1989},
	pages = {1167--1170},
}

@inproceedings{mellor_noise_1993,
	address = {Minneapolis, MN, USA},
	title = {Noise masking in a transform domain},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319237/},
	doi = {10.1109/ICASSP.1993.319237},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Mellor, B.A. and Varga, A.P.},
	year = {1993},
	pages = {87--90 vol.2},
}

@inproceedings{graf_dynamic_1993,
	address = {Minneapolis, MN, USA},
	title = {Dynamic time warping comb filter for the enhancement of speech degraded by white {Gaussian} noise},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319307/},
	doi = {10.1109/ICASSP.1993.319307},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Graf, J.T. and Hubing, N.},
	year = {1993},
	pages = {339--342 vol.2},
}

@article{jae_lim_evaluation_1978,
	title = {Evaluation of an adaptive comb filtering method for enhancing speech degraded by white noise addition},
	volume = {26},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1163117/},
	doi = {10.1109/TASSP.1978.1163117},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {{Jae Lim} and Oppenheim, A. and Braida, L.},
	month = aug,
	year = {1978},
	pages = {354--358},
}

@inproceedings{oshaughnessy_speech_1988,
	address = {New York, NY, USA},
	title = {Speech enhancement using vector quantization and a formant distance measure},
	url = {http://ieeexplore.ieee.org/document/196642/},
	doi = {10.1109/ICASSP.1988.196642},
	urldate = {2023-02-18},
	booktitle = {{ICASSP}-88., {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {O'Shaughnessy, D.},
	year = {1988},
	pages = {549--552},
}

@inproceedings{ygong_base_1993,
	address = {Berlin, Germany},
	title = {Base transformation for environment adaption in continuous speech recognition},
	volume = {3},
	booktitle = {Prococeedings of {European} {Conf}. {Speech} {Communication} and {Technology} 1993},
	author = {Y.Gong},
	year = {1993},
	pages = {2227--2230},
}

@inproceedings{treurniet_noise_1994,
	address = {Adelaide, SA, Australia},
	title = {Noise independent speech recognition for a variety of noise types},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389262/},
	doi = {10.1109/ICASSP.1994.389262},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Treurniet, W.C. and Gong, Y.},
	year = {1994},
	pages = {I/437--I/440},
}

@inproceedings{van_compernolle_spectral_1989,
	address = {Glasgow, UK},
	title = {Spectral estimation using a log-distance error criterion applied to speech recognition},
	url = {http://ieeexplore.ieee.org/document/266414/},
	doi = {10.1109/ICASSP.1989.266414},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {van Compernolle, D.},
	year = {1989},
	pages = {258--261},
}

@article{erell_filterbank-energy_1993-1,
	title = {Filterbank-energy estimation using mixture and {Markov} models for recognition of noisy speech},
	volume = {1},
	issn = {10636676},
	url = {http://ieeexplore.ieee.org/document/221385/},
	doi = {10.1109/89.221385},
	number = {1},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Erell, A. and Weintraub, M.},
	month = jan,
	year = {1993},
	pages = {68--76},
}

@inproceedings{ephraim_minimum_1990,
	address = {Albuquerque, NM, USA},
	title = {A minimum mean square error approach for speech enhancement},
	url = {http://ieeexplore.ieee.org/document/115960/},
	doi = {10.1109/ICASSP.1990.115960},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Ephraim, Y.},
	year = {1990},
	pages = {829--832},
}

@article{bregman_auditory_1994,
	title = {\textit{{Auditory} {Scene} {Analysis}: {The} {Perceptual} {Organization} of {Sound}}},
	volume = {95},
	issn = {0001-4966},
	shorttitle = {{\textless}i{\textgreater}{Auditory} {Scene} {Analysis}},
	url = {http://asa.scitation.org/doi/10.1121/1.408434},
	doi = {10.1121/1.408434},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bregman, Albert S. and McAdams, Stephen},
	month = feb,
	year = {1994},
	note = {https://mitpress.mit.edu/9780262521956/auditory-scene-analysis/},
	pages = {1177--1178},
}

@article{brandstein_practical_1995,
	title = {A practical time-delay estimator for localizing speech sources with a microphone array},
	volume = {9},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230885700095},
	doi = {10.1006/csla.1995.0009},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Brandstein, Michael S. and Adcock, John E. and Silverman, Harvey F.},
	month = apr,
	year = {1995},
	pages = {153--169},
}

@inproceedings{berthommier_source_1995,
	title = {Source separation by a functional model of amplitude demodulation},
	url = {https://www.isca-speech.org/archive/eurospeech_1995/berthommier95_eurospeech.html},
	doi = {10.21437/Eurospeech.1995-37},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {4th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1995)},
	publisher = {ISCA},
	author = {Berthommier, Frédéric and Meyer, Georg F.},
	month = sep,
	year = {1995},
	pages = {135--138},
}

@article{langner_periodicity_1988,
	title = {Periodicity coding in the inferior colliculus of the cat. {I}. {Neuronal} mechanisms},
	volume = {60},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.1988.60.6.1799},
	doi = {10.1152/jn.1988.60.6.1799},
	abstract = {1. Temporal properties of single- and multiple-unit responses were investigated in the inferior colliculus (IC) of the barbiturate-anesthetized cat. Approximately 95\% of recording sites were located in the central nucleus of the inferior colliculus (ICC). Responses to contralateral stimulation with tone bursts and amplitude-modulated tones (100\% sinusoidal modulation) were recorded. Five response parameters were determined for neurons at each location: 1) characteristic frequency (CF); 2) onset latency of responses to CF-tones 60 dB above threshold; 3) Q10 dB (CF divided by bandwidth of tuning curve 10 dB above threshold); 4) best modulation frequency for firing rate (rBMF or BMF; amplitude modulation frequency that elicited the highest firing rate); and 5) best modulation frequency for synchronization (sBMF; amplitude modulation frequency that elicited the highest degree of phase-locking to the modulation frequency). 2. Response characteristics for single units and multiple units corresponded closely. A BMF was obtained at almost all recording sites. For units with a similar CF, a range of BMFs was observed. The upper limit of BMF increased approximately proportional to CF/4 up to BMFs as high as 1 kHz. The lower limit of encountered BMFs for a given CF also increased slightly with CF. BMF ranges for single-unit and multiple-unit responses were similar. Twenty-three percent of the responses revealed rBMFs between 10 and 30 Hz, 51\% between 30 and 100 Hz, 18\% between 100 and 300 Hz, and 8\% between 300 and 1000 Hz. 3. For single units with modulation transfer functions of bandpass characteristics, BMFs determined for firing rate and synchronization were similar (r2 = 0.95). 4. Onset latencies for responses to CF tones 60 dB above threshold varied between 4 and 120 ms. Ninety percent of the onset latencies were between 5 and 18 ms. A range of onset latencies was recorded for different neurons with any given CF. The onset response latency of a given unit or unit cluster was significantly correlated with the period of the BMF and the period of the CF (P less than 0.05). 5."Intrinsic oscillations" of short duration, i.e., regularly timed discharges of units in response to stimuli without a corresponding temporal structure, were frequently observed in the ICC. Oscillation intervals were commonly found to be integer multiples of 0.4 ms. Changes of stimulus frequency or intensity had only minor influences on these intrinsic oscillations.(ABSTRACT TRUNCATED AT 400 WORDS)},
	language = {en},
	number = {6},
	urldate = {2023-02-18},
	journal = {Journal of Neurophysiology},
	author = {Langner, G. and Schreiner, C. E.},
	month = dec,
	year = {1988},
	pages = {1799--1822},
}

@book{cooke_modelling_1993,
	address = {Cambridge [England] ; New York},
	series = {Distinguished dissertations in computer science},
	title = {Modelling auditory processing and organisation},
	isbn = {978-0-521-45094-2},
	publisher = {Cambridge University Press},
	author = {Cooke, Martin},
	year = {1993},
	keywords = {Auditory perception, Computer simulation, Speech analysis use of Computers},
}

@article{noauthor_computational_1994,
	title = {Computational auditory scene analysis},
	volume = {8},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230884710163},
	doi = {10.1006/csla.1994.1016},
	abstract = {Although the ability of human listeners to perceptually segregate concurrent sounds is well documented in the literature, there have been few attempts…},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	month = oct,
	year = {1994},
	note = {Publisher: Academic Press},
	pages = {297--336},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\JCDPJ7ZH\\S0885230884710163.html:text/html},
}

@article{brown_computational_1994,
	title = {Computational auditory scene analysis},
	volume = {8},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230884710163},
	doi = {10.1006/csla.1994.1016},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Brown, Guy J. and Cooke, Martin},
	month = oct,
	year = {1994},
	note = {https://doi.org/10.1006/csla.1994.1016},
	pages = {297--336},
}

@article{makhoul_state_1995,
	title = {State of the art in continuous speech recognition.},
	volume = {92},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.92.22.9956},
	doi = {10.1073/pnas.92.22.9956},
	abstract = {In the past decade, tremendous advances in the state of the art of automatic speech recognition by machine have taken place. A reduction in the word error rate by more than a factor of 5 and an increase in recognition speeds by several orders of magnitude (brought about by a combination of faster recognition search algorithms and more powerful computers), have combined to make high-accuracy, speaker-independent, continuous speech recognition for large vocabularies possible in real time, on off-the-shelf workstations, without the aid of special hardware. These advances promise to make speech recognition technology readily available to the general public. This paper focuses on the speech recognition advances made through better speech modeling techniques, chiefly through more accurate mathematical modeling of speech sounds.},
	language = {en},
	number = {22},
	urldate = {2023-02-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Makhoul, J and Schwartz, R},
	month = oct,
	year = {1995},
	pages = {9956--9963},
}

@article{gong_speech_1995,
	title = {Speech recognition in noisy environments: {A} survey},
	volume = {16},
	issn = {01676393},
	shorttitle = {Speech recognition in noisy environments},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016763939400059J},
	doi = {10.1016/0167-6393(94)00059-J},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Gong, Yifan},
	month = apr,
	year = {1995},
	pages = {261--291},
}

@book{junqua_robustness_1996,
	address = {Boston},
	series = {The {Kluwer} international series in engineering and computer science ; {VLSI}, computer architecture and digital signal processing},
	title = {Robustness in automatic speech recognition: fundamentals and applications},
	isbn = {978-0-7923-9646-8},
	shorttitle = {Robustness in automatic speech recognition},
	number = {SECS 341},
	publisher = {Kluwer Academic Publishers},
	author = {Junqua, Jean-Claude and Haton, Jean-Paul},
	year = {1996},
	keywords = {Automatic speech recognition, Signal processing},
}

@article{carrier_automated_2017,
	title = {Automated {Speech} {Recognition} in language learning: {Potential} models, benefits and impact},
	volume = {1},
	issn = {25202073, 2521442X},
	shorttitle = {Automated {Speech} {Recognition} in language learning},
	url = {http://rudn.tlcjournal.org/issues/1(1)-03.html},
	doi = {10.29366/2017tlc.1.1.3},
	number = {1},
	urldate = {2023-02-18},
	journal = {Training Language and Culture},
	author = {Carrier, Michael},
	month = feb,
	year = {2017},
	pages = {46--61},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TAQ53DBZ\\Carrier - 2017 - Automated Speech Recognition in language learning.pdf:application/pdf},
}

@misc{noauthor_14_nodate,
	title = {(14) {Automatic} speech recognition for second language learning: how and why it actually works {\textbar} {Helmer} {Strik} - {Academia}.edu},
	url = {https://www.academia.edu/19192008/Automatic_speech_recognition_for_second_language_learning_how_and_why_it_actually_works},
	urldate = {2023-02-18},
}

@inproceedings{neri_automatic_2003,
	title = {Automatic {Speech} {Recognition} for second language learning: {How} and why it actually works},
	shorttitle = {Automatic {Speech} {Recognition} for second language learning},
	url = {https://www.semanticscholar.org/paper/Automatic-Speech-Recognition-for-second-language-it-Neri-Cucchiarini/82a4f4b67cc509d96e8b04c68f319d454022673c},
	abstract = {In this paper, we examine various studies and reviews on the usability of Automatic Speech Recognition (ASR) technology as a tool t o train pronunciation in the second language (L2). We show that part of the criticism that has been addressed to this technology is not warranted, being rather the result of limited familiarity with ASR technology and with broader Computer Assisted Language Learning (CALL) courseware design matters. In our analysis we also consider actual problems of state-of-the-art ASR technology, with a view to indicating h ow ASR can be employed to develop courseware that is both pedagogically sound and reliable.},
	urldate = {2023-02-18},
	author = {Neri, A. and Cucchiarini, C. and Strik, H.},
	year = {2003},
	annote = {[TLDR] This paper examines various studies and reviews of Automatic Speech Recognition technology as a tool to train pronunciation in the second language (L2) and considers actual problems of state-of-the-art ASR technology, with a view to indicating ASR can be employed to develop courseware that is both pedagogically sound and reliable.},
}

@inproceedings{srinivasan_harmonicity_2004,
	address = {Montreal, Que., Canada},
	title = {Harmonicity and dynamics-based features for audio},
	volume = {4},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1326828/},
	doi = {10.1109/ICASSP.2004.1326828},
	urldate = {2022-11-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Srinivasan, H. and Kankanhalli, M.},
	year = {2004},
	pages = {iv--321--iv--324},
}

@inproceedings{srinivasan_harmonicity_2003,
	address = {Hong Kong, China},
	title = {Harmonicity and dynamics based audio separation},
	volume = {5},
	isbn = {978-0-7803-7663-2},
	url = {http://ieeexplore.ieee.org/document/1200052/},
	doi = {10.1109/ICASSP.2003.1200052},
	urldate = {2022-11-20},
	booktitle = {2003 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}, 2003. {Proceedings}. ({ICASSP} '03).},
	publisher = {IEEE},
	author = {Srinivasan, S.H. and Kankanhalli, M.},
	year = {2003},
	pages = {V--640--3},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AR44HZB7\\Srinivasan and Kankanhalli - 2003 - Harmonicity and dynamics based audio separation.pdf:application/pdf},
 }



@inproceedings{farooq_improving_2019,
	title = {Improving {Large} {Vocabulary} {Urdu} {Speech} {Recognition} {System} {Using} {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html},
	doi = {10.21437/Interspeech.2019-2629},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Rauf, Sahar and Hussain, Sarmad},
	month = sep,
	year = {2019},
	pages = {2978--2982},



}

@misc{alphacephei_alpha_nodate,
	title = {Alpha {Cephei}},
	url = {https://github.com/alphacep},
	abstract = {Alpha Cephei has 37 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2022-08-16},
	journal = {GitHub},
	author = {Alphacephei},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\C9UHNTWM\\alphacep.html:text/html},
}

@misc{alphacep_vosk_2022,
	title = {Vosk {Speech} {Recognition} {Toolkit}},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-api},
	abstract = {Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C\# and Node},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-09-03T17:48:42Z},
	keywords = {android, asr, deep-learning, deep-neural-networks, deepspeech, google-speech-to-text, ios, kaldi, offline, privacy, python, raspberry-pi, speaker-identification, speaker-verification, speech-recognition, speech-to-text, speech-to-text-android, stt, voice-recognition, vosk},
}

@misc{alphacep_alphacepvosk-server_2022,
	title = {alphacep/vosk-server},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-server},
	abstract = {WebSocket, gRPC and WebRTC speech recognition server based on Vosk and Kaldi libraries},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-05-07T17:24:55Z},
	keywords = {asr, kaldi, python, speech-recognition, vosk, grpc, saas, webrtc, websocket},
}

@misc{alphacep_vosk_nodate,
	title = {{VOSK} {Offline} {Speech} {Recognition} {API}},
	url = {https://alphacephei.com/vosk/},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\H4HQA2XC\\vosk.html:text/html},
}

@misc{alphacep_vosk_nodate-1,
	title = {{VOSK} {Models}},
	url = {https://alphacephei.com/vosk/models},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\J75LJ82E\\models.html:text/html},
}

@misc{daniel_povey_kaldi_nodate,
	title = {Kaldi: {Kaldi}},
	url = {https://kaldi-asr.org/doc/},
	urldate = {2022-08-16},
	author = {Daniel Povey},
	file = {Kaldi\: Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\QYDHVT6P\\doc.html:text/html},
}

@misc{noauthor_openslrorg_nodate,
	title = {openslr.org},
	url = {https://www.openslr.org/resources.php},
	urldate = {2022-08-16},
	file = {openslr.org:C\:\\Users\\DELL\\Zotero\\storage\\EJWY9PRN\\resources.html:text/html},
}

@misc{cmu_cmu_nodate,
	title = {{CMU} {Lexicon} {Tool}},
	url = {http://www.speech.cs.cmu.edu/tools/lextool.html},
	urldate = {2022-08-16},
	author = {CMU},
	file = {CMU Lexicon Tool:C\:\\Users\\DELL\\Zotero\\storage\\BTMXKZQL\\lextool.html:text/html},
}

@misc{coqui-ai_coqui-aistt_2022,
	title = {coqui-ai/{STT}},
	copyright = {MPL-2.0},
	url = {https://github.com/coqui-ai/STT},
	abstract = {��STT - The deep learning toolkit for Speech-to-Text. Training and deploying STT models has never been so easy.},
	urldate = {2022-08-16},
	publisher = {coqui},
	author = {coqui-ai},
	month = aug,
	year = {2022},
	note = {original-date: 2021-03-04T04:54:42Z},
	keywords = {asr, deep-learning, speech-recognition, speech-to-text, stt, voice-recognition, automatic-speech-recognition, speech-recognition-api, speech-recognizer, tensorflow},
}

@misc{noauthor_speechbrain_nodate,
	title = {{SpeechBrain}: {A} {PyTorch} {Speech} {Toolkit}},
	url = {https://speechbrain.github.io/},
	urldate = {2022-08-16},
}

@misc{piero_molino_ludwig_nodate,
	title = {Ludwig - code-free deep learning toolbox},
	url = {http://ludwig.ai},
	abstract = {Ludwig is a toolbox for training and testing deep learning models without writing code},
	language = {en},
	urldate = {2022-08-16},
	author = {Piero Molino},
}

@misc{noauthor_espnet_2022,
	title = {{ESPnet}: end-to-end speech processing toolkit},
	copyright = {Apache-2.0},
	shorttitle = {{ESPnet}},
	url = {https://github.com/espnet/espnet},
	abstract = {End-to-End Speech Processing Toolkit},
	urldate = {2022-08-16},
	publisher = {ESPnet},
	month = aug,
	year = {2022},
	note = {original-date: 2017-12-13T00:45:11Z},
	keywords = {deep-learning, kaldi, speech-recognition, chainer, end-to-end, machine-translation, pytorch, speech-enhancement, speech-separation, speech-synthesis, speech-translation, voice-conversion},
}

@misc{noauthor_espnet_nodate,
	title = {{ESPnet}: end-to-end speech processing toolkit — {ESPnet} 202207 documentation},
	url = {https://espnet.github.io/espnet/},
	urldate = {2022-08-16},
}

@article{georgescu_performance_2021,
	title = {Performance vs. hardware requirements in state-of-the-art automatic speech recognition},
	volume = {2021},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00217-4},
	doi = {10.1186/s13636-021-00217-4},
	abstract = {Abstract
            The last decade brought significant advances in automatic speech recognition (ASR) thanks to the evolution of deep learning methods. ASR systems evolved from pipeline-based systems, that modeled hand-crafted speech features with probabilistic frameworks and generated phone posteriors, to end-to-end (E2E) systems, that translate the raw waveform directly into words using one deep neural network (DNN). The transcription accuracy greatly increased, leading to ASR technology being integrated into many commercial applications. However, few of the existing ASR technologies are suitable for integration in embedded applications, due to their hard constrains related to computing power and memory usage. This overview paper serves as a guided tour through the recent literature on speech recognition and compares the most popular ASR implementations. The comparison emphasizes the trade-off between ASR performance and hardware requirements, to further serve decision makers in choosing the system which fits best their embedded application. To the best of our knowledge, this is the first study to provide this kind of trade-off analysis for state-of-the-art ASR systems.},
	language = {en},
	number = {1},
	urldate = {2022-08-16},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Georgescu, Alexandru-Lucian and Pappalardo, Alessandro and Cucu, Horia and Blott, Michaela},
	month = dec,
	year = {2021},
	pages = {28},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TRA6KAJY\\Georgescu et al. - 2021 - Performance vs. hardware requirements in state-of-.pdf:application/pdf},
}

@misc{cplc_cplc_nodate,
	title = {{CPLC} – {Citizens}-{Police} {Liaison} {Committee}},
	url = {http://www.cplc.org.pk/},
	urldate = {2022-08-16},
	author = {CPLC},
}

@misc{british_broadcast_bbc_nodate,
	title = {{BBC} - {Languages} - {Urdu} - {A} {Guide} to {Urdu} - 10 facts about the {Urdu} language},
	url = {https://www.bbc.co.uk/languages/other/urdu/guide/facts.shtml},
	abstract = {Discover surprising and revealing facts about Urdu, including Urdu words used in the English language and Urdu jokes and quotes.},
	language = {en},
	urldate = {2022-08-16},
	author = {British Broadcast},
	note = {Last Modified: 2008-01-30T12:35:00Z},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4CXS8LEU\\facts.html:text/html},
}

@misc{ethnologue_urdu_nodate,
	title = {Urdu {Language} - {Ethnologue} {Report}},
	url = {https://www.ethnologue.com/language/urdu},
	abstract = {A language profile for Urdu. Get a detailed look at the language, from population to dialects and usage.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Ethnologue},
	author = {Ethnologue},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\WGWY3RRQ\\urd.html:text/html},
}

@article{ali_automatic_2015,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-16},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@phdthesis{sehar_gul_detecting_2020,
	address = {Karachi},
	title = {{DETECTING} {MALICIOUS} {ACTIVITIES} {OVER} {TELEPHONE} {NETWORK} {FOR} {URDU} {SPEAKER}},
	abstract = {Telephone is one of most important invention in the fields of communication it is because on
this invention that we are able to connect with our friends and families without hassle of travelling
and going to their places,but some people are also using it for negative purpose therefore to secure
this medium of communication is one of the most important issue of today as many malicious
activities are taking place on this channel.Humanely it is not possible to tap each and every phone
call so that one could find malicious activities that are being done.In order to find such malicious
activities we need an automatic system that can automatically detect malicious voice activities,
for that we have decided to develop an automatic speech recognition system that will detect malicious
sentences in Urdu Language from the telephonic conversation which will then be processed
further.},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Sehar Gul},
	month = jun,
	year = {2020},
}

@incollection{hutchison_speaker_2010,
	address = {Berlin, Heidelberg},
	title = {Speaker {Independent} {Urdu} {Speech} {Recognition} {Using} {HMM}},
	volume = {6177},
	isbn = {978-3-642-13880-5},
	url = {http://link.springer.com/10.1007/978-3-642-13881-2_14},
	urldate = {2022-08-16},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ashraf, Javed and Iqbal, Naveed and Khattak, Naveed Sarfraz and Zaidi, Ather Mohsin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Hopfe, Christina J. and Rezgui, Yacine and Métais, Elisabeth and Preece, Alun and Li, Haijiang},
	year = {2010},
	doi = {10.1007/978-3-642-13881-2_14},
	note = {Series Title: Lecture Notes in Computer Science
ISBN2: 978-3-642-13881-2},
	pages = {140--148},
}

@inproceedings{sarfraz_large_2010,
	address = {Islamabad, Pakistan},
	title = {Large vocabulary continuous speech recognition for {Urdu}},
	isbn = {978-1-4503-0342-2},
	url = {http://portal.acm.org/citation.cfm?doid=1943628.1943629},
	doi = {10.1145/1943628.1943629},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Frontiers} of {Information} {Technology} - {FIT} '10},
	publisher = {ACM Press},
	author = {Sarfraz, Huda and Parveen, Rahila and Hussain, Sarmad and Bokhari, Riffat and Raza, Agha Ali and Ullah, Inam and Sarfraz, Zahid and Pervez, Sophia and Mustafa, Asad and Javed, Iqra},
	year = {2010},
	pages = {1--5},
}

@inproceedings{qasim_urdu_2016,
	address = {Bali, Indonesia},
	title = {Urdu speech recognition system for district names of {Pakistan}: {Development}, challenges and solutions},
	isbn = {978-1-5090-3516-8},
	shorttitle = {Urdu speech recognition system for district names of {Pakistan}},
	url = {http://ieeexplore.ieee.org/document/7918979/},
	doi = {10.1109/ICSDA.2016.7918979},
	urldate = {2022-08-16},
	booktitle = {2016 {Conference} of {The} {Oriental} {Chapter} of {International} {Committee} for {Coordination} and {Standardization} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Qasim, Muhammad and Nawaz, Sohaib and Hussain, Sarmad and Habib, Tania},
	month = oct,
	year = {2016},
	pages = {28--32},
}

@article{aguiar_de_lima_survey_2020,
	title = {A survey on automatic speech recognition systems for {Portuguese} language and its variations},
	volume = {62},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230819302992},
	doi = {10.1016/j.csl.2019.101055},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Aguiar de Lima, Thales and Da Costa-Abreu, Márjory},
	month = jul,
	year = {2020},
	pages = {101055},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\5YG7YNP8\\Aguiar de Lima and Da Costa-Abreu - 2020 - A survey on automatic speech recognition systems f.pdf:application/pdf},
}

@inproceedings{dash_automatic_2018,
	title = {Automatic {Speech} {Recognition} with {Articulatory} {Information} and a {Unified} {Dictionary} for {Hindi}, {Marathi}, {Bengali} and {Oriya}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/dash18_interspeech.html},
	doi = {10.21437/Interspeech.2018-2122},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Dash, Debadatta and Kim, Myungjong and Teplansky, Kristin and Wang, Jun},
	month = sep,
	year = {2018},
	pages = {1046--1050},
}

@inproceedings{patil_automatic_2016,
	address = {Pune, India},
	title = {Automatic {Speech} {Recognition} of isolated words in {Hindi} language using {MFCC}},
	isbn = {978-1-5090-1338-8},
	url = {http://ieeexplore.ieee.org/document/7915008/},
	doi = {10.1109/CAST.2016.7915008},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Computing}, {Analytics} and {Security} {Trends} ({CAST})},
	publisher = {IEEE},
	author = {Patil, U. G. and Shirbahadurkar, S. D. and Paithane, A. N.},
	month = dec,
	year = {2016},
	pages = {433--438},
}

@article{a_n_mishra_robust_2011,
	title = {Robust {Features} for {Connected} {Hindi} {Digits} {Recognition}},
	volume = {4},
	number = {2},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {A. N. Mishra and Mahesh Chandra and Astik Biswas and S. N. Sharan},
	month = jun,
	year = {2011},
}

@inproceedings{aggarwal_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	abstract = {The goal of automatic speech recognition (ASR) system is to accurately and efficiently convert a speech signal into a text message independent of the device, speaker or the environment. In general the speech signal is captured and pre-processed at front-end for feature extraction and evaluated at back-end using the Gaussian mixture hidden Markov model. In this statistical approach since the evaluation of Gaussian likelihoods dominate the total computational load, the appropriate selection of Gaussian mixtures is very important depending upon the amount of training data. As the small databases are available to train the Indian languages ASR system, the higher range of Gaussian mixtures (i.e. 64 and above), normally used for European languages, cannot be applied for them. This paper reviews the statistical framework and presents an iterative procedure to select an optimum number of Gaussian mixtures that exhibits maximum accuracy in the context of Hindi speech recognition system.},
	booktitle = {International {Journal} of {Signal} {Processing}, {Image} {Processing} and {Pattern} {Recognition}},
	author = {Aggarwal, R. K. and Dave, M.},
	year = {2011},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\FVSZRDG9\\Aggarwal and Dave - 2011 - Using Gaussian Mixtures for Hindi Speech Recogniti.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\NYC9U7U3\\summary.html:text/html},
}

@inproceedings{r_k_aggarwal_and_m_dave_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	author = {R. K. Aggarwal {and} M. Dave},
	year = {2011},
}

@inproceedings{b_venkataraman_sopc-based_2006,
	title = {{SOPC}-based speech-to-text conversion},
	booktitle = {Embedded {Processor} {Design} {Contest} {Outstanding} {Designs}},
	author = {B. Venkataraman},
	year = {2006},
}

@incollection{tanveer_continuous_2019,
	address = {Singapore},
	title = {Continuous {Hindi} {Speech} {Recognition} {Using} {Kaldi} {ASR} {Based} on {Deep} {Neural} {Network}},
	volume = {748},
	isbn = {9789811309229},
	url = {http://link.springer.com/10.1007/978-981-13-0923-6_26},
	urldate = {2022-08-16},
	booktitle = {Machine {Intelligence} and {Signal} {Analysis}},
	publisher = {Springer Singapore},
	author = {Upadhyaya, Prashant and Mittal, Sanjeev Kumar and Farooq, Omar and Varshney, Yash Vardhan and Abidi, Musiur Raza},
	editor = {Tanveer, M. and Pachori, Ram Bilas},
	year = {2019},
	doi = {10.1007/978-981-13-0923-6_26},
	note = {Series Title: Advances in Intelligent Systems and Computing
ISBN2: 9789811309236},
	pages = {303--311},
}

@inproceedings{karel_vesel_sequence-discriminative_2013,
	title = {Sequence-discriminative training of deep neural networks},
	abstract = {Luk Burget},
	author = {Karel Vesel and Arnab Ghoshal and Daniel Povey},
	year = {2013},
}

@inproceedings{k_v_s_parsad_and_s_m_virk_computational_2012,
	title = {Computational evidence that {Hindi} and {Urdu} share a grammar but not the lexicon},
	booktitle = {3rd {Workshop} on {South} and {Southeast} {Asian} {NLP}},
	author = {K. V. S. Parsad {and} S. M. Virk},
	year = {2012},
}

@article{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1402.1128},
	doi = {10.48550/ARXIV.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2022-08-16},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{abdel-hamid_exploring_2013,
	title = {Exploring {Convolutional} {Neural} {Network} {Structures} and {Optimization} {Techniques} for {Speech} {Recognition}},
	url = {https://www.microsoft.com/en-us/research/publication/exploring-convolutional-neural-network-structures-and-optimization-techniques-for-speech-recognition/},
	abstract = {Recently, convolutional neural networks (CNNs) have been shown to outperform the standard fully connected deep neural networks within the hybrid deep neural network / hidden Markov model (DNN/HMM) framework on the phone recognition task. In this paper, we extend the earlier basic form of the CNN and explore it in multiple ways. We first investigate several CNN architectures, including full and limited weight sharing, convolution along frequency and time axes, and stacking of several convolution layers. We then develop a novel weighted softmax pooling layer so that the size in the pooling layer can be automatically learned. Further, we evaluate the effect of CNN pretraining, which is achieved by using a convolutional version of the RBM. We show that all CNN architectures we have investigated outperform the earlier basic form of the DNN on both the phone recognition and large vocabulary speech recognition tasks. The architecture with limited weight sharing provides additional gains over the full weight sharing architecture. The softmax pooling layer performs as well as the best CNN with the manually tuned fixed-pooling size, and has a potential for further improvement. Finally, we show that CNN pretraining produces significantly better results on a large vocabulary speech recognition task.},
	booktitle = {Interspeech 2013},
	publisher = {ISCA},
	author = {Abdel-Hamid, Ossama and Deng, Li and Yu, Dong},
	month = aug,
	year = {2013},
	note = {Edition: Interspeech 2013},
}

@misc{noauthor_gram_nodate,
	title = {{GRAM} {VAANI} {ASR} {Challenge} 2022},
	url = {https://sites.google.com/view/gramvaaniasrchallenge/home},
	abstract = {A challenge on Automatic Speech Recognition for Hindi is being organized as part of INTERSPEECH 2022 by sharing the spontaneous telephone speech recordings collected by a social technology enterprise Gram Vaani. The regional variations of Hindi together with spontaneity of speech, natural},
	language = {en},
	urldate = {2022-08-16},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P4IHK3L5\\home.html:text/html},
}

@misc{noauthor_gramvaaniorg_nodate,
	title = {gramvaani.org},
	url = {https://gramvaani.org/},
	urldate = {2022-08-16},
}

@inproceedings{latif_cross_2018,
	address = {Islamabad, Pakistan},
	title = {Cross {Lingual} {Speech} {Emotion} {Recognition}: {Urdu} vs. {Western} {Languages}},
	isbn = {978-1-5386-9355-1},
	shorttitle = {Cross {Lingual} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/8616972/},
	doi = {10.1109/FIT.2018.00023},
	urldate = {2022-08-16},
	booktitle = {2018 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	publisher = {IEEE},
	author = {Latif, Siddique and Qayyum, Adnan and Usman, Muhammad and Qadir, Junaid},
	month = dec,
	year = {2018},
	pages = {88--93},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\NLRUGBJI\\Latif et al. - 2018 - Cross Lingual Speech Emotion Recognition Urdu vs..pdf:application/pdf},
}

@article{besacier_automatic_2014,
	title = {Automatic speech recognition for under-resourced languages: {A} survey},
	volume = {56},
	issn = {01676393},
	shorttitle = {Automatic speech recognition for under-resourced languages},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988},
	doi = {10.1016/j.specom.2013.07.008},
	language = {en},
	urldate = {2022-08-16},
	journal = {Speech Communication},
	author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
	month = jan,
	year = {2014},
	pages = {85--100},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YRF6UNN8\\Besacier et al. - 2014 - Automatic speech recognition for under-resourced l.pdf:application/pdf},
}

@article{lakshmi_sri_kaldi_2020,
	title = {Kaldi recipe in {Hindi} for word level recognition and phoneme level transcription},
	volume = {171},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050920312606},
	doi = {10.1016/j.procs.2020.04.268},
	language = {en},
	urldate = {2022-08-16},
	journal = {Procedia Computer Science},
	author = {Lakshmi Sri, Karra Venkata and Srinivasan, Mayuka and Nair, Radhika Rajeev and Priya, K. Jeeva and Gupta, Deepa},
	year = {2020},
	pages = {2476--2485},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5J9VQKIM\\Lakshmi Sri et al. - 2020 - Kaldi recipe in Hindi for word level recognition a.pdf:application/pdf},
}

@misc{qureshi_urdu_2021,
	title = {Urdu {Speech} {Recognition}},
	copyright = {MIT},
	url = {https://github.com/ZoraizQ/urdu-speech-recognition},
	abstract = {Urdu Speech Recognition using Kaldi ASR, by training Triphone Acoustic GMMs using the PRUS dataset.},
	urldate = {2022-08-16},
	author = {Qureshi, Zoraiz},
	month = oct,
	year = {2021},
	note = {original-date: 2021-04-21T10:11:17Z},
	keywords = {speech-recognition, kaldi-asr, multi-speaker, prus, urdu},
}

@inproceedings{asadullah_automatic_2016,
	address = {Portsmouth},
	title = {Automatic {Urdu} {Speech} {Recognition} using {Hidden} {Markov} {Model}},
	isbn = {978-1-5090-3755-1},
	url = {https://ieeexplore.ieee.org/document/7571287/},
	doi = {10.1109/ICIVC.2016.7571287},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Image}, {Vision} and {Computing} ({ICIVC})},
	publisher = {IEEE},
	author = {{Asadullah} and Shaukat, Arslan and Ali, Hazrat and Akram, Usman},
	month = aug,
	year = {2016},
	pages = {135--139},
}

@misc{chodroff_corpus_2018,
	title = {Corpus {Phonetics} {Tutorial}},
	url = {http://arxiv.org/abs/1811.05553},
	abstract = {Corpus phonetics has become an increasingly popular method of research in linguistic analysis. With advances in speech technology and computational power, large scale processing of speech data has become a viable technique. This tutorial introduces the speech scientist and engineer to various automatic speech processing tools. These include acoustic model creation and forced alignment using the Kaldi Automatic Speech Recognition Toolkit (Povey et al., 2011), forced alignment using FAVE-align (Rosenfelder et al., 2014), the Montreal Forced Aligner (McAuliffe et al., 2017), and the Penn Phonetics Lab Forced Aligner (Yuan \& Liberman, 2008), as well as stop consonant burst alignment using AutoVOT (Keshet et al., 2014). The tutorial provides a general overview of each program, step-by-step instructions for running the program, as well as several tips and tricks.},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Chodroff, Eleanor},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05553 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3GAPGJTY\\Chodroff - 2018 - Corpus Phonetics Tutorial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5EDZSUNT\\1811.html:text/html},
}

@article{alharbi_automatic_2021,
	title = {Automatic {Speech} {Recognition}: {Systematic} {Literature} {Review}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Automatic {Speech} {Recognition}},
	doi = {10.1109/ACCESS.2021.3112535},
	abstract = {A huge amount of research has been done in the field of speech signal processing in recent years. In particular, there has been increasing interest in the automatic speech recognition (ASR) technology field. ASR began with simple systems that responded to a limited number of sounds and has evolved into sophisticated systems that respond fluently to natural language. This systematic review of automatic speech recognition is provided to help other researchers with the most significant topics published in the last six years. This research will also help in identifying recent major ASR challenges in real-world environments. In addition, it discusses current research gaps in ASR. This review covers articles available in five research databases that were completed according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) protocol. The search strategy yielded 82 conferences and articles related to the study’s scope for the period 2015–2020. The results presented in this review shed light on research trends in the area of ASR and also suggest new research directions.},
	journal = {IEEE Access},
	author = {Alharbi, Sadeen and Alrazgan, Muna and Alrashed, Alanoud and Alnomasi, Turkiayh and Almojel, Raghad and Alharbi, Rimah and Alharbi, Saja and Alturki, Sahar and Alshehri, Fatimah and Almojil, Maha},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {ASR challenges, ASR systematic review, automatic speech recognition, Automatic speech recognition, Databases, Licenses, Nails, Quality assessment, Software, Speech recognition, Systematics},
	pages = {131858--131876},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\TLVJTS37\\9536732.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\7LCUHWDI\\Alharbi et al. - 2021 - Automatic Speech Recognition Systematic Literatur.pdf:application/pdf},
}

@article{alsayadi_arabic_2021,
	title = {Arabic speech recognition using end-to-end deep learning},
	volume = {15},
	issn = {1751-9683},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12057},
	doi = {10.1049/sil2.12057},
	abstract = {Arabic automatic speech recognition (ASR) methods with diacritics have the ability to be integrated with other systems better than Arabic ASR methods without diacritics. In this work, the application of state-of-the-art end-to-end deep learning approaches is investigated to build a robust diacritised Arabic ASR. These approaches are based on the Mel-Frequency Cepstral Coefficients and the log Mel-Scale Filter Bank energies as acoustic features. To the best of our knowledge, end-to-end deep learning approach has not been used in the task of diacritised Arabic automatic speech recognition. To fill this gap, this work presents a new CTC-based ASR, CNN-LSTM, and an attention-based end-to-end approach for improving diacritisedArabic ASR. In addition, a word-based language model is employed to achieve better results. The end-to-end approaches applied in this work are based on state-of-the-art frameworks, namely ESPnet and Espresso. Training and testing of these frameworks are performed based on the Standard Arabic Single Speaker Corpus (SASSC), which contains 7 h of modern standard Arabic speech. Experimental results show that the CNN-LSTM with an attention framework outperforms conventional ASR and the Joint CTC-attention ASR framework in the task of Arabic speech recognition. The CNN-LSTM with an attention framework could achieve a word error rate better than conventional ASR and the Joint CTC-attention ASR by 5.24\% and 2.62\%, respectively.},
	language = {en},
	number = {8},
	urldate = {2022-08-16},
	journal = {IET Signal Processing},
	author = {Alsayadi, Hamzah A. and Abdelhamid, Abdelaziz A. and Hegazy, Islam and Fayed, Zaki T.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/sil2.12057},
	pages = {521--534},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8MD9EIN6\\Alsayadi et al. - 2021 - Arabic speech recognition using end-to-end deep le.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8NYWWX8A\\sil2.html:text/html},
}

@inproceedings{raza_rapid_2018,
	title = {Rapid {Collection} of {Spontaneous} {Speech} {Corpora} {Using} {Telephonic} {Community} {Forums}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/raza18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1139},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Raza, Agha Ali and Athar, Awais and Randhawa, Shan and Tariq, Zain and Saleem, Muhammad Bilal and Bin Zia, Haris and Saif, Umar and Rosenfeld, Roni},
	month = sep,
	year = {2018},
	pages = {1021--1025},
}

@inproceedings{naeem_subspace_2020,
	title = {Subspace {Gaussian} {Mixture} {Model} for {Continuous} {Urdu} {Speech} {Recognition} using {Kaldi}},
	doi = {10.1109/ICOSST51357.2020.9333026},
	abstract = {Automatic Speech Recognition Systems (ASR) have significantly improved in recent years, where deep learning is playing an important role in the development of end to end ASR's. ASR is the task of converting spoken language into computer readable text. ASRs are becoming ever more prevalent way to interact with technology, thereby significantly closing the gap in terms of how humans interact with computers, making it more natural. Urdu is an under resourced language, for which training such a system requires a huge amount of data that is not readily available. In this paper we present improvements to the architecture of a statistical automatic speech recognition system for which the components involved in a statistical ASR have been explored in great detail. We also present the results on various statistical models that are trained for Urdu language. We choose the Kaldi toolkit for training the Urdu ASR using approximately 100 hours of transcribed data. The refined Subspace Gaussian Model gives a word error rate of 9\% on the test set.},
	booktitle = {2020 14th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Naeem, Saad and Iqbal, Majid and Saqib, Muhammad and Saad, Muhammad and Raza, Muhammad Soban and Ali, Zaid and Akhtar, Naveed and Beg, Mirza Omer and Shahzad, Waseem and Arshad, Muhhamad Umair},
	month = dec,
	year = {2020},
	keywords = {Adaptation models, Analytical models, Automatic Speech Recognition, Computational modeling, Context modeling, Hidden Markov models, Hidden Markov Models, Mel-frequency Cepstrum, Probabilistic logic, Subspace Gaussian Mixture Models, Training},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\3KACECHI\\9333026.html:text/html},
}

@inproceedings{khan_multi-genre_2021,
	address = {Singapore, Singapore},
	title = {A {Multi}-{Genre} {Urdu} {Broadcast} {Speech} {Recognition} {System}},
	isbn = {978-1-66540-870-7},
	url = {https://ieeexplore.ieee.org/document/9660552/},
	doi = {10.1109/O-COCOSDA202152914.2021.9660552},
	urldate = {2022-08-16},
	booktitle = {2021 24th {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Khan, Erbaz and Rauf, Sahar and Adeeba, Farah and Hussain, Sarmad},
	month = nov,
	year = {2021},
	pages = {25--30},
}

@incollection{somogyi_automatic_2021,
	address = {Cham},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-3-030-60031-0 978-3-030-60032-7},
	url = {http://link.springer.com/10.1007/978-3-030-60032-7_5},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Application} of {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Somogyi, Zoltán},
	collaborator = {Somogyi, Zoltán},
	year = {2021},
	doi = {10.1007/978-3-030-60032-7_5},
	pages = {145--171},
}

@incollection{chapelle_automatic_2020,
	edition = {1},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4051-9473-0 978-1-4051-9843-1},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781405198431.wbeal0066.pub2},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Encyclopedia} of {Applied} {Linguistics}},
	publisher = {Wiley},
	author = {Levis, John and Suvorov, Ruslan},
	editor = {Chapelle, Carol A.},
	month = dec,
	year = {2020},
	doi = {10.1002/9781405198431.wbeal0066.pub2},
	pages = {1--8},
}

@incollection{gold_brief_2011,
	address = {Hoboken, NJ, USA},
	title = {Brief {History} of {Automatic} {Speech} {Recognition}},
	isbn = {978-1-118-14288-2 978-0-470-19536-9},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118142882.ch4},
	urldate = {2022-08-16},
	booktitle = {Speech and {Audio} {Signal} {Processing}},
	publisher = {John Wiley \& Sons, Inc.},
	collaborator = {Gold, Ben and Morgan, Nelson and Ellis, Dan},
	month = oct,
	year = {2011},
	doi = {10.1002/9781118142882.ch4},
	pages = {40--58},
}

@misc{kincaid_brief_2018,
	title = {A {Brief} {History} of {ASR}: {Automatic} {Speech} {Recognition}},
	shorttitle = {A {Brief} {History} of {ASR}},
	url = {https://medium.com/descript/a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5},
	abstract = {This is the first post in a series on Automatic Speech Recognition, the foundational technology that makes Descript possible. We’ll be…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FHD2LASC\\a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5.html:text/html},
}

@article{suma_swamy_evolution_2013,
	title = {Evolution of {Speech} {Recognition} – {A} {Brief} {History} of {Technology} {Development}},
	volume = {60},
	journal = {Elixir Adv. Engg. Info.},
	author = {Suma Swamy and Ramakrishnan, Kollengode},
	month = jul,
	year = {2013},
}

@article{smit_advances_2021,
	title = {Advances in subword-based {HMM}-{DNN} speech recognition across languages},
	volume = {66},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300917},
	doi = {10.1016/j.csl.2020.101158},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Smit, Peter and Virpioja, Sami and Kurimo, Mikko},
	month = mar,
	year = {2021},
	pages = {101158},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC6HIS7B\\Smit et al. - 2021 - Advances in subword-based HMM-DNN speech recogniti.pdf:application/pdf},
}

@misc{kincaid_state_2018,
	title = {The {State} of {Automatic} {Speech} {Recognition}: {Q}\&{A} with {Kaldi}’s {Dan} {Povey}},
	shorttitle = {The {State} of {Automatic} {Speech} {Recognition}},
	url = {https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85},
	abstract = {This article continues our series on Automatic Speech Recognition, including our recent piece on the History of ASR.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AI8J5D9U\\the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85.html:text/html},
}

@misc{blog_machine_2022,
	title = {Machine {Learning} {Models} {Are} {Only} as {Good} as the {Data} {They} {Are} {Trained} {On}},
	url = {https://deepchecks.com/machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on/},
	abstract = {Learn about the importance of data validation in machine learning, look into the various tools for different sets of data validation techniques \& procedures.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Deepchecks},
	author = {Blog, Deepchecks Community},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5T5L9UMP\\machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on.html:text/html},
}

@misc{noauthor_machine_2018,
	title = {“{A} machine learning model is only as good as the data it is fed”},
	url = {https://devm.io/machine-learning/apache-spark-machine-learning-interview-143122},
	abstract = {Apache Spark 2.3 was released earlier this year; it marked a major milestone for Structured Streaming but there are a lot of other interesting features that deserve your attention. We talked with Reynold Xin, co-founder and Chief Architect at Databricks about the Databricks Runtime and other enhancements introduced in Apache Spark 2.3.},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {devmio - expand your knowledge},
	month = jun,
	year = {2018},
	note = {Section: Artikel},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HS46J6C4\\apache-spark-machine-learning-interview-143122.html:text/html},
}

@misc{brownlee_why_2020,
	title = {Why {Data} {Preparation} {Is} {So} {Important} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/data-preparation-is-important/},
	abstract = {On a predictive modeling project, machine learning algorithms learn a mapping from input variables to a target variable. The most common form of predictive modeling project involves so-called structured data or tabular data. This is data as it looks in a spreadsheet or a matrix, with rows of examples and columns of features for each […]},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HGMCR7CK\\data-preparation-is-important.html:text/html},
}

@book{ilyas_data_2019,
	address = {New York, NY},
	title = {Data {Cleaning}},
	isbn = {978-1-4503-7153-7},
	language = {English},
	publisher = {ACM Books},
	author = {Ilyas, Ihab F. and Chu, Xu},
	month = jun,
	year = {2019},
}

@misc{khan_introduction_2021,
	title = {An {Introduction} to {Classification} {Using} {Mislabeled} {Data}},
	url = {https://towardsdatascience.com/an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5},
	abstract = {The performance of any classifier, or for that matter any machine learning task, depends crucially on the quality of the available data…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Medium},
	author = {Khan, Shihab Shahriar},
	month = mar,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SY93L99I\\an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5.html:text/html},
}

@article{cao_joint_2019,
	title = {Joint {Prostate} {Cancer} {Detection} and {Gleason} {Score} {Prediction} in mp-{MRI} via {FocalNet}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8653866/},
	doi = {10.1109/TMI.2019.2901928},
	number = {11},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Cao, Ruiming and Mohammadian Bajgiran, Amirhossein and Afshari Mirak, Sohrab and Shakeri, Sepideh and Zhong, Xinran and Enzmann, Dieter and Raman, Steven and Sung, Kyunghyun},
	month = nov,
	year = {2019},
	pages = {2496--2506},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WK7STA45\\Cao et al. - 2019 - Joint Prostate Cancer Detection and Gleason Score .pdf:application/pdf},
}

@article{fan_impact_2021,
	title = {The {Impact} of {Mislabeled} {Changes} by {SZZ} on {Just}-in-{Time} {Defect} {Prediction}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2019.2929761},
	abstract = {Just-in-Time (JIT) defect prediction-a technique which aims to predict bugs at change level-has been paid more attention. JIT defect prediction leverages the SZZ approach to identify bug-introducing changes. Recently, researchers found that the performance of SZZ (including its variants) is impacted by a large amount of noise. SZZ may considerably mislabel changes that are used to train a JIT defect prediction model, and thus impact the prediction accuracy. In this paper, we investigate the impact of the mislabeled changes by different SZZ variants on the performance and interpretation of JIT defect prediction models. We analyze four SZZ variants (i.e., B-SZZ, AG-SZZ, MA-SZZ, and RA-SZZ) that are proposed by prior studies. We build the prediction models using the labeled data by these four SZZ variants. Among the four SZZ variants, RA-SZZ is least likely to generate mislabeled changes, and we construct the testing set by using RA-SZZ. All of the four prediction models are then evaluated on the same testing set. We choose the prediction model built on the labeled data by RA-SZZ as the baseline model, and we compare the performance and metric importance of the models trained using the labeled data by the other three SZZ variants with the baseline model. Through a large-scale empirical study on a total of 126,526 changes from ten Apache open source projects, we find that in terms of various performance measures (AUC, F1-score, G-mean and Recall@20\%), the mislabeled changes by B-SZZ and MA-SZZ are not likely to cause a considerable performance reduction, while the mislabeled changes by AG-SZZ cause a statistically significant performance reduction with an average difference of 1-5 percent. When considering developers' inspection effort (measured by LOC) in practice, the changes mislabeled B-SZZ and AG-SZZ lead to 9-10 and 1-15 percent more wasted inspection effort, respectively. And the mislabeled changes by B-SZZ lead to significantly more wasted effort. The mislabeled changes by MA-SZZ do not cause considerably more wasted effort. We also find that the top-most important metric for identifying bug-introducing changes (i.e., number of files modified in a change) is robust to the mislabeling noise generated by SZZ. But the second- and third-most important metrics are more likely to be impacted by the mislabeling noise, unless random forest is used as the underlying classifier.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fan, Yuanrui and Xia, Xin and da Costa, Daniel Alencar and Lo, David and Hassan, Ahmed E. and Li, Shanping},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Analytical models, Computer bugs, Data models, Inspection, Just-in-time defect prediction, Measurement, mining software repositories, noisy data, Predictive models, SZZ, Testing},
	pages = {1559--1586},
}

@inproceedings{van_rooyen_learning_2015,
	title = {Learning with {Symmetric} {Label} {Noise}: {The} {Importance} of {Being} {Unhinged}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {van Rooyen, Brendan and Menon, Aditya and Williamson, Robert C},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{natarajan_learning_2013,
	title = {Learning with {Noisy} {Labels}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@article{frenay_classification_2014,
	title = {Classification in the {Presence} of {Label} {Noise}: {A} {Survey}},
	volume = {25},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Classification in the {Presence} of {Label} {Noise}},
	url = {http://ieeexplore.ieee.org/document/6685834/},
	doi = {10.1109/TNNLS.2013.2292894},
	number = {5},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Frenay, Benoit and Verleysen, Michel},
	month = may,
	year = {2014},
	pages = {845--869},
}

@misc{zhang_uberi_speechrecognition_nodate,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-08-16},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\9RSTEWBT\\SpeechRecognition.html:text/html},
}

@inproceedings{panayotov_librispeech_2015,
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	shorttitle = {Librispeech},
	doi = {10.1109/ICASSP.2015.7178964},
	abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {Bioinformatics, Blogs, Corpus, Electronic publishing, Genomics, Information services, LibriVox, Resource description framework, Speech Recognition},
	pages = {5206--5210},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\2AK7VZ3R\\7178964.html:text/html},
}

@article{backstrom_introduction_2022,
	title = {Introduction to {Speech} {Processing}: 2nd {Edition}},
	copyright = {Open Access},
	shorttitle = {Introduction to {Speech} {Processing}},
	url = {https://zenodo.org/record/6821775},
	doi = {10.5281/ZENODO.6821775},
	abstract = {This release is primarily about migrating all content to jupyter-books and git. The published version is now hosted at https://speechprocessingbook.aalto.fi. In addition to github, the release has long-term storage location at Zenodo, which also assigns a DOI to the release. We have some entirely new sections, such as Forensic speaker recognition. There are also plenty of small improvements everywhere.},
	language = {en},
	urldate = {2022-08-16},
	author = {Bäckström, Tom and Räsänen, Okko and Zewoudie, Abraham and Zarazaga, Pablo Pérez and Koivusalo, Liisa and Das, Sneha and Mellado, Esteban Gómez and Mariem Bouafif Mansali and Ramos, Daniel},
	month = jul,
	year = {2022},
	note = {Publisher: Zenodo
Version Number: v2},
	keywords = {speech processing},
}

@book{yu_automatic_2015,
	address = {London},
	series = {Signals and {Communication} {Technology}},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4471-5778-6},
	url = {http://link.springer.com/10.1007/978-1-4471-5779-3},
	urldate = {2022-08-16},
	publisher = {Springer London},
	author = {Yu, Dong and Deng, Li},
	year = {2015},
	note = {ISBN2: 978-1-4471-5779-3},
}

@article{park_review_2021,
	title = {A {Review} of {Speaker} {Diarization}: {Recent} {Advances} with {Deep} {Learning}},
	volume = {72},
	copyright = {arXiv.org perpetual, non-exclusive license},
	issn = {101317},
	shorttitle = {A {Review} of {Speaker} {Diarization}},
	url = {https://arxiv.org/abs/2101.09624},
	doi = {10.48550/ARXIV.2101.09624},
	abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
	urldate = {2022-08-17},
	journal = {Computer Speech \& Language},
	author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
This article is a preprint version of the article published in Computer Speech \& Language, Volume 72, March 2022, 101317},
}

@misc{juang_automatic_nodate,
	title = {Automatic {Speech} {Recognition} – {A} {Brief} {History} of the {Technology} {Development} {Abstract}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis [1, 2], the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. H. and Rabiner, Lawrence R.},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\3YU9F3ID\\Juang and Rabiner - Automatic Speech Recognition – A Brief History of .pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\I34GUPHY\\summary.html:text/html},
}

@incollection{maglogiannis__2020,
	address = {Cham},
	title = {Α {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4 978-3-030-49161-1},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\K3RKXDZZ\\Filippidou and Moussiades - 2020 - Α Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@inproceedings{morris_wer_2004,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{anusuya_speech_2010,
	title = {Speech {Recognition} by {Machine}, {A} {Review}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1001.2267},
	doi = {10.48550/ARXIV.1001.2267},
	abstract = {This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.},
	urldate = {2022-08-17},
	author = {Anusuya, M. A. and Katti, S. K.},
	year = {2010},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
25 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS December 2009, ISSN 1947 5500, http://sites.google.com/site/ijcsis/},
}

@article{upton_speech_1984,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages — this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-17},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@article{davis_automatic_1952,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@inproceedings{maja_popovic_hjerson_2011,
	title = {Hjerson: {An} {Open} {Source} {Tool} for {Automatic} {Error} {Classification} of {Machine} {Translation} {Output}},
	author = {Maja Popovic},
	year = {2011},
}

@article{errattahi_automatic_2018,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-17},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YWBD74CJ\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@inproceedings{povey_daniel_and_ghoshal_arnab_and_boulianne_gilles_and_burget_lukas_and_glembek_ondrej_and_goel_nagendra_and_hannemann_mirko_and_motlicek_petr_and_qian_yanmin_and_schwarz_petr__and_others_kaldi_2011,
	title = {The {Kaldi} speech recognition toolkit},
	booktitle = {Workshop on automatic speech recognition and understanding},
	author = {Povey, Daniel {and} Ghoshal, Arnab {and} Boulianne, Gilles {and} Burget, Lukas {and} Glembek, Ondrej {and} Goel, Nagendra {and} Hannemann, Mirko {and} Motlicek, Petr {and} Qian, Yanmin {and} Schwarz, Petr  {and} others},
	year = {2011},
}

@misc{sutherland_short_2017,
	title = {A short history of speech recognition},
	url = {https://medium.com/@sutherlandjamie/a-short-history-of-speech-recognition-9b8e78ad086a},
	abstract = {There has been more progress in speech recognition technology in the last 3 years than in the first 30 years. Computing power and…},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Sutherland, Jamie},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\LAELJJKS\\a-short-history-of-speech-recognition-9b8e78ad086a.html:text/html},
}

@inproceedings{xiong_microsoft_2017,
	address = {New Orleans, LA},
	title = {The microsoft 2016 conversational speech recognition system},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953159/},
	doi = {10.1109/ICASSP.2017.7953159},
	urldate = {2022-08-17},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Xiong, W. and Droppo, J. and Huang, X. and Seide, F. and Seltzer, M. and Stolcke, A. and Yu, D. and Zweig, G.},
	month = mar,
	year = {2017},
	pages = {5255--5259},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\ZVTCMYVF\\Xiong et al. - 2017 - The microsoft 2016 conversational speech recogniti.pdf:application/pdf},
}

@misc{qi_benchmarking_2021,
	title = {Benchmarking {Commercial} {Intent} {Detection} {Services} with {Practice}-{Driven} {Evaluations}},
	url = {http://arxiv.org/abs/2012.03929},
	abstract = {Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users' text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the intent detection models can see a different distribution of test data when being deployed in the real world, leading to poor accuracy. Finally, a practical intent detection model must be computationally efficient in both training and single query inference so that it can be used continuously and re-trained frequently. We benchmark intent detection methods on a variety of datasets. Our results show that Watson Assistant's intent detection model outperforms other commercial solutions and is comparable to large pretrained language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ.},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Qi, Haode and Pan, Lin and Sood, Atin and Shah, Abhishek and Kunc, Ladislav and Yu, Mo and Potdar, Saloni},
	month = jun,
	year = {2021},
	note = {arXiv:2012.03929 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at NAACL2021 Industry Track},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\XMUN9XIG\\Qi et al. - 2021 - Benchmarking Commercial Intent Detection Services .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5JX39X9L\\2012.html:text/html},
}

@inproceedings{arora_hint3_2020,
	address = {Online},
	title = {{HINT3}: {Raising} the bar for {Intent} {Detection} in the {Wild}},
	shorttitle = {{HINT3}},
	url = {https://www.aclweb.org/anthology/2020.insights-1.16},
	doi = {10.18653/v1/2020.insights-1.16},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Proceedings of the {First} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Arora, Gaurav and Jain, Chirag and Chaturvedi, Manas and Modi, Krupal},
	year = {2020},
	pages = {100--105},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\SYABT9YP\\Arora et al. - 2020 - HINT3 Raising the bar for Intent Detection in the.pdf:application/pdf},
}

@misc{patel_data-centric_2021,
	title = {Data-{Centric} {Approach} vs {Model}-{Centric} {Approach} in {Machine} {Learning}},
	url = {https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning},
	abstract = {Code and data are the foundations of the AI system. Both of these components play an important role in the development of a robust model but which one should you focus on more? In this article, we’ll go through the data-centric vs model-centric approaches, and see which one is better, we would also talk about […]},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {neptune.ai},
	author = {Patel, Harshil},
	month = dec,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ZT7NNEUA\\data-centric-vs-model-centric-machine-learning.html:text/html},
}

@misc{radecic_data-centric_2022,
	title = {Data-centric vs. {Model}-centric {AI}? {The} {Answer} is {Clear}},
	shorttitle = {Data-centric vs. {Model}-centric {AI}?},
	url = {https://towardsdatascience.com/data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67},
	abstract = {There’s something wrong with the current approach to AI. But there’s a solution.},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Radečić, Dario},
	month = mar,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\X4DU4GPV\\data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67.html:text/html},
}

@misc{noauthor_chat_nodate,
	title = {A {Chat} with {Andrew} on {MLOps}: {From} {Model}-centric to {Data}-centric {AI} - {YouTube}},
	url = {https://www.youtube.com/watch?v=06-AZXmwHjo&t=69s},
	urldate = {2022-08-17},
}

@misc{noauthor_data-centric_nodate,
	title = {Data-centric {Machine} {Learning}: {Making} customized {ML} solutions production-ready},
	shorttitle = {Data-centric {Machine} {Learning}},
	url = {https://dida.do/blog/data-centric-machine-learning},
	abstract = {In this article, we will see why many ML Projects do not make it into production, introduce the concepts of model- and data-centric ML, and give examples how we at dida improve projects by applying data-centric techniques.},
	language = {en},
	urldate = {2022-08-17},
	journal = {dida Machine Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\F2569RZH\\data-centric-machine-learning.html:text/html},
}

@misc{noauthor_significance_nodate,
	title = {The {Significance} of {Data}-centric {AI}},
	url = {https://www.kdnuggets.com/the-significance-of-data-centric-ai.html/},
	abstract = {How a systematic way of maintaining data quality can do wonders to your model performance.},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {KDnuggets},
	note = {Section: KDnuggets Originals},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SGUDCHX6\\significance-data-centric-ai.html:text/html},
}

@misc{deeplearningai_data-centric_2021,
	title = {Data-centric {AI}: {Real} {World} {Approaches}},
	shorttitle = {Data-centric {AI}},
	url = {https://www.youtube.com/watch?v=Yqj7Kyjznh4},
	urldate = {2022-08-17},
	author = {{DeepLearningAI}},
	month = aug,
	year = {2021},
}

@article{creutz_morph-based_2007,
	title = {Morph-based speech recognition and modeling of out-of-vocabulary words across languages},
	volume = {5},
	issn = {1550-4875, 1550-4883},
	url = {https://dl.acm.org/doi/10.1145/1322391.1322394},
	doi = {10.1145/1322391.1322394},
	abstract = {We explore the use of morph-based language models in large-vocabulary continuous-speech recognition systems across four so-called morphologically rich languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. The morphs are subword units discovered in an unsupervised, data-driven way using the
              Morfessor
              algorithm. By estimating
              n
              -gram language models over sequences of morphs instead of words, the quality of the language model is improved through better vocabulary coverage and reduced data sparsity. Standard word models suffer from high out-of-vocabulary (OOV) rates, whereas the morph models can recognize previously unseen word forms by concatenating morphs. It is shown that the morph models do perform fairly well on OOVs without compromising the recognition accuracy on in-vocabulary words. The Arabic experiment constitutes the only exception since here the standard word model outperforms the morph model. Differences in the datasets and the amount of data are discussed as a plausible explanation.},
	language = {en},
	number = {1},
	urldate = {2022-08-18},
	journal = {ACM Transactions on Speech and Language Processing},
	author = {Creutz, Mathias and Hirsimäki, Teemu and Kurimo, Mikko and Puurula, Antti and Pylkkönen, Janne and Siivola, Vesa and Varjokallio, Matti and Arisoy, Ebru and Saraçlar, Murat and Stolcke, Andreas},
	month = dec,
	year = {2007},
	pages = {1--29},
}

@misc{noauthor_mfcc_nodate,
	title = {{MFCC} vs {FBANK} for chain models ?},
	url = {https://groups.google.com/g/kaldi-help/c/_7hB74HKhC4},
	urldate = {2022-08-18},
	file = {MFCC vs FBANK for chain models ?:C\:\\Users\\DELL\\Zotero\\storage\\5FKKZNEQ\\_7hB74HKhC4.html:text/html},
}

@book{reithaug_orchestrating_2002,
	title = {Orchestrating {Success} in {Reading}},
	isbn = {978-0-9694974-4-8},
	url = {https://books.google.com.pk/books?id=\_YGZMQAACAAJ},
	publisher = {Stirling Head Enterprises},
	author = {Reithaug, D.},
	year = {2002},
}

@article{zia_pronouncur_2018,
	title = {{PronouncUR}: {An} {Urdu} {Pronunciation} {Lexicon} {Generator}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{PronouncUR}},
	url = {https://arxiv.org/abs/1801.00409},
	doi = {10.48550/ARXIV.1801.00409},
	abstract = {State-of-the-art speech recognition systems rely heavily on three basic components: an acoustic model, a pronunciation lexicon and a language model. To build these components, a researcher needs linguistic as well as technical expertise, which is a barrier in low-resource domains. Techniques to construct these three components without having expert domain knowledge are in great demand. Urdu, despite having millions of speakers all over the world, is a low-resource language in terms of standard publically available linguistic resources. In this paper, we present a grapheme-to-phoneme conversion tool for Urdu that generates a pronunciation lexicon in a form suitable for use with speech recognition systems from a list of Urdu words. The tool predicts the pronunciation of words using a LSTM-based model trained on a handcrafted expert lexicon of around 39,000 words and shows an accuracy of 64\% upon internal evaluation. For external evaluation on a speech recognition task, we obtain a word error rate comparable to one achieved using a fully handcrafted expert lexicon.},
	urldate = {2022-08-18},
	author = {Zia, Haris Bin and Raza, Agha Ali and Athar, Awais},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
5 pages, LREC 2018},
}

@book{jurafsky_speech_2009,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-187321-6},
	shorttitle = {Speech and language processing},
	publisher = {Pearson Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
	note = {OCLC: 213375806},
	keywords = {Automatic speech recognition, Computational linguistics},
}

@article{likhomanenko_rethinking_2020,
	title = {Rethinking {Evaluation} in {ASR}: {Are} {Our} {Models} {Robust} {Enough}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Rethinking {Evaluation} in {ASR}},
	url = {https://arxiv.org/abs/2010.11745},
	doi = {10.48550/ARXIV.2010.11745},
	abstract = {Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets - in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. We show that, in general, reverberative and additive noise augmentation improves generalization performance across domains. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world noisy data. Finally, we show that training a single acoustic model on the most widely-used datasets - combined - reaches competitive performance on both research and real-world benchmarks.},
	urldate = {2022-08-19},
	author = {Likhomanenko, Tatiana and Xu, Qiantong and Pratap, Vineel and Tomasello, Paden and Kahn, Jacob and Avidov, Gilad and Collobert, Ronan and Synnaeve, Gabriel},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD), 68T07, 68T10, I.2.6; I.5.4},
}

@incollection{hutchison_illustrated_2013,
	address = {Berlin, Heidelberg},
	title = {An {Illustrated} {Methodology} for {Evaluating} {ASR} {Systems}},
	volume = {7836},
	isbn = {978-3-642-37424-1},
	url = {http://link.springer.com/10.1007/978-3-642-37425-8_3},
	urldate = {2022-08-19},
	booktitle = {Adaptive {Multimedia} {Retrieval}. {Large}-{Scale} {Multimedia} {Retrieval} and {Evaluation}},
	publisher = {Springer Berlin Heidelberg},
	author = {González, María and Moreno, Julián and Martínez, José Luis and Martínez, Paloma},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Detyniecki, Marcin and García-Serrano, Ana and Nürnberger, Andreas and Stober, Sebastian},
	year = {2013},
	doi = {10.1007/978-3-642-37425-8_3},
	note = {Series Title: Lecture Notes in Computer Science
ISBN2: 978-3-642-37425-8},
	pages = {33--42},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\XVRNLJ7F\\González et al. - 2013 - An Illustrated Methodology for Evaluating ASR Syst.pdf:application/pdf},
}

@phdthesis{zhang_strategies_2019,
	type = {Thesis},
	title = {Strategies for {Handling} {Out}-of-{Vocabulary} {Words} in {Automatic} {Speech} {Recognition}},
	url = {https://jscholarship.library.jhu.edu/handle/1774.2/62275},
	abstract = {Nowadays, most ASR (automatic speech recognition) systems deployed in  industry are closed-vocabulary systems, meaning we have a limited vocabulary of words the system can recognize, and where pronunciations are provided to the system. Words out of this vocabulary are called out-of-vocabulary (OOV) words, for which either pronunciations or both spellings and pronunciations are not known to the system. The basic motivations of developing strategies to handle OOV words are: First, in the training phase, missing or wrong pronunciations of words in training data results in poor acoustic models. Second, in the test phase, words out of the vocabulary cannot be recognized at all, and mis-recognition of OOV words may affect recognition performance of its in-vocabulary neighbors as well. Therefore, this dissertation is dedicated to exploring strategies of handling OOV words in closed-vocabulary ASR.

First, we investigate dealing with OOV words in ASR training data, by introducing an acoustic-data driven pronunciation learning framework using a likelihood-reduction based criterion for selecting pronunciation candidates from multiple sources, i.e. standard grapheme-to-phoneme algorithms (G2P) and phonetic decoding, in a greedy fashion. This framework effectively expands a small hand-crafted pronunciation lexicon to cover OOV words, for which the learned pronunciations have higher quality than approaches using G2P alone or using other baseline pruning criteria. Furthermore, applying the proposed framework to generate alternative pronunciations for in-vocabulary (IV) words improves both recognition performance on relevant words and overall acoustic model performance.

Second, we investigate dealing with OOV words in ASR test data, i.e. OOV detection and recovery. We first conduct a comparative study of a hybrid lexical model (HLM) approach for OOV detection, and several baseline approaches, with the conclusion that the HLM approach outperforms others in both OOV detection and first pass OOV recovery performance. Next, we introduce a grammar-decoding framework for efficient second pass OOV recovery, showing that with properly designed schemes of estimating OOV unigram probabilities, the framework significantly improves OOV recovery and overall decoding performance compared to first pass decoding.

Finally we propose an open-vocabulary word-level recurrent neural network language model (RNNLM) re-scoring framework, making it possible to re-score lattices containing recovered OOVs using a single word-level RNNLM, that was ignorant of OOVs when it was trained. Above all, the whole OOV recovery pipeline shows the potential of a highly efficient open-vocabulary word-level ASR decoding framework, tightly integrated into a standard WFST decoding pipeline.},
	language = {en\_US},
	urldate = {2022-08-19},
	school = {Johns Hopkins University},
	author = {Zhang, Xiaohui},
	month = oct,
	year = {2019},
	note = {Accepted: 2020-02-06T04:08:11Z},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\6E6BV6R6\\Zhang - 2019 - Strategies for Handling Out-of-Vocabulary Words in.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\QLFGJMPH\\62275.html:text/html},
}

@article{zhang_hello_2017,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@article{morgan_continuous_1995,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {1558-0792},
	doi = {10.1109/79.382443},
	abstract = {The authors focus on a tutorial description of the hybrid HMM/ANN method. The approach has been applied to large vocabulary continuous speech recognition, and variants are in use by many researchers, The method provides a mechanism for incorporating a range of sources of evidence without strong assumptions about their joint statistics, and may have applicability to much more complex systems that can incorporate deep acoustic and linguistic context. The method is inherently discriminant and conservative of parameters. Despite these potential advantages, the hybrid method has focused on implementing fairly simple systems, which do surprisingly well on large continuous speech recognition tasks, Researchers are only beginning to explore the use of more complex structures with this paradigm. In particular, they are just beginning to look at the connectionist inference of language models (including phonology) from data, which may be required in order to take advantage of locally discriminant probabilities rather than simply translating to likelihoods. Finally, the authors' current intuition is that more advanced versions of the hybrid method can greatly benefit from a perceptual perspective.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Speech recognition, Hidden Markov models, Statistics, Vocabulary},
	pages = {24--42},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\SIWUUI5M\\382443.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\99KFFZGH\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{nautsch_gdpr_2019,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\V8ZYWF7E\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@misc{noauthor_spsc_nodate,
	title = {{SPSC} {\textbar} {ISCA} {SIG}-{SPSC}},
	url = {https://www.spsc-sig.org/},
	urldate = {2022-08-19},
}

@misc{noauthor_mozilla_nodate,
	title = {Mozilla {Common} {Voice}},
	url = {https://commonvoice.mozilla.org/},
	language = {en},
	urldate = {2022-08-19},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2ZVA6IM8\\datasets.html:text/html},
}

@article{schertz_acoustic_2020,
	title = {Acoustic cues in production and perception of the four-way stop laryngeal contrast in {Hindi} and {Urdu}},
	volume = {81},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009544702030070X},
	doi = {10.1016/j.wocn.2020.100979},
	language = {en},
	urldate = {2022-08-19},
	journal = {Journal of Phonetics},
	author = {Schertz, Jessamyn and Khan, Sarah},
	month = jul,
	year = {2020},
	pages = {100979},
}

@incollection{dua_urdu_2006,
	title = {Urdu},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022446},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02244-6},
	pages = {269--275},
}

@incollection{dua_hindustani_2006,
	title = {Hindustani},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022203},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02220-3},
	pages = {309--312},
}

@incollection{annamalai_india_2006,
	title = {India: {Language} {Situation}},
	isbn = {978-0-08-044854-1},
	shorttitle = {India},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542046113},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Annamalai, E.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/04611-3},
	pages = {610--613},
}

@misc{noauthor_morfessor_nodate,
	title = {Morfessor 2.0 documentation — {Morfessor} 2.0.4 documentation},
	url = {https://morfessor.readthedocs.io/en/latest/},
	urldate = {2022-08-19},
}

@misc{noauthor_morpho_nodate,
	title = {Morpho project},
	url = {http://morpho.aalto.fi/projects/morpho/},
	urldate = {2022-08-19},
	file = {Morpho project:C\:\\Users\\DELL\\Zotero\\storage\\435PUDQA\\morpho.html:text/html},
}

@inproceedings{farooq_enhancing_2020,
	title = {Enhancing {Large} {Vocabulary} {Continuous} {Speech} {Recognition} {System} for {Urdu}-{English} {Conversational} {Code}-{Switched} {Speech}},
	doi = {10.1109/O-COCOSDA50338.2020.9295036},
	abstract = {This paper presents first step towards Large Vocabulary Continuous Speech Recognition (LVCSR) system for Urdu-English code-switched conversational speech. Urdu is the national language and lingua franca of Pakistan, with 100 million speakers worldwide. English, on the other hand, is official language of Pakistan and commonly mixed with Urdu in daily communication. Urdu, being under-resourced language, have no substantial Urdu-English code-switched corpus in hand to develop speech recognition system. In this research, readily available spontaneous Urdu speech corpus (25 hours) is revised to use it for enhancement of read speech Urdu LVCSR to recognize code-switched speech. This data set is split into 20 hours of train and 5 hours of test set. 10 hours of Urdu BroadCast (BC) data are collected and annotated in a semi-supervised way to enhance the system further. For acoustic modeling, state-of-the-art DNN-HMM modeling technique is used without any prior GMM-HMM training and alignments. Various techniques to improve language model using monolingual data are investigated. The overall percent Word Error Rate (WER) is reduced from 40.71\% to 26.95\% on test set.},
	booktitle = {2020 23rd {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Hussain, Sarmad and Rauf, Sahar and Khalid, Maryam},
	month = nov,
	year = {2020},
	note = {ISSN: 2472-7695},
	keywords = {Speech recognition, Data models, Vocabulary, Acoustics, Speech coding, Speech enhancement, Switches, under-resourced language, Urdu speech recognition, Urdu-English code-switching},
	pages = {155--159},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\88SVPLUM\\9295036.html:text/html},
}

@inproceedings{watanabe_-line_2009,
	address = {Taipei, Taiwan},
	title = {On-line adaptation and {Bayesian} detection of environmental changes based on a macroscopic time evolution system},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4960598/},
	doi = {10.1109/ICASSP.2009.4960598},
	urldate = {2022-08-19},
	booktitle = {2009 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Watanabe, Shinji and Nakamura, Atsushi},
	month = apr,
	year = {2009},
	pages = {4373--4376},
}

@article{davis_automatic_1952-1,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@article{s_review_2016,
	title = {A {Review} on {Automatic} {Speech} {Recognition} {Architecture} and {Approaches}},
	volume = {9},
	issn = {20054254, 20054254},
	url = {http://article.nadiapub.com/IJSIP/vol9_no4/34.pdf},
	doi = {10.14257/ijsip.2016.9.4.34},
	number = {4},
	urldate = {2022-08-19},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {S, Karpagavalli and E, Chandra},
	month = apr,
	year = {2016},
	pages = {393--404},
}

@incollection{maglogiannis__2020-1,
	address = {Cham},
	title = {Α {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology
ISBN2: 978-3-030-49161-1},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\79KCL3R7\\Filippidou and Moussiades - 2020 - Α Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@misc{amodei_deep_2015,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	shorttitle = {Deep {Speech} 2},
	url = {http://arxiv.org/abs/1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	month = dec,
	year = {2015},
	note = {arXiv:1512.02595 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\T5Y77XAY\\Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5HBNU4V7\\1512.html:text/html},
}

@article{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech}},
	url = {https://arxiv.org/abs/1412.5567},
	doi = {10.48550/ARXIV.1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	urldate = {2022-08-19},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{xiong_microsoft_2018,
	title = {The {Microsoft} 2017 {Conversational} {Speech} {Recognition} {System}},
	url = {https://www.microsoft.com/en-us/research/publication/conference-paper-microsoft-2017-conversational-speech-recognition-system/},
	abstract = {We describe the latest version of Microsoft's conversational speech recognition system for the Switchboard and CallHome domains. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby acoustic model posteriors are first combined at the senone/frame level,followed by a word-level voting via confusion networks. We also added another language model rescoring step following the confusion network combination. The resulting system yields a 5.1\% word error rate on the NIST 2000 Switchboard test set, and 9.8\% on the CallHome subset.},
	booktitle = {Proc. {IEEE} {ICASSP}},
	publisher = {IEEE},
	author = {Xiong, Wayne and Wu, Lingfeng and Droppo, Jasha and Huang, Xuedong and Stolcke, Andreas},
	month = apr,
	year = {2018},
	pages = {5934--5938},
}

@article{juang_automatic_2005,
	title = {Automatic {Speech} {Recognition} - {A} {Brief} {History} of the {Technology} {Development}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (1, 2), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. and Rabiner, Lawrence},
	month = jan,
	year = {2005},
}

@incollection{juang_speech_2006,
	address = {Oxford},
	title = {Speech {Recognition}, {Automatic}: {History}},
	isbn = {978-0-08-044854-1},
	shorttitle = {Speech {Recognition}, {Automatic}},
	url = {https://www.sciencedirect.com/science/article/pii/B0080448542009068},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (Dudley, 1939; Dudley et al., 1939), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface. Examples are automatic call processing in the telephone network and query-based information systems that provide updated travel information, stock price quotations, weather reports, etc. In this article we review some major highlights in the research and development of automatic speech recognition during the last few decades to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Juang, B. -H. and Rabiner, L. R.},
	editor = {Brown, Keith},
	month = jan,
	year = {2006},
	doi = {10.1016/B0-08-044854-2/00906-8},
	keywords = {Speech recognition, acoustic modeling, automatic transcription, dialog systems, finite state network, hidden Markov models, keyword spotting, language modeling, neural networks, office automation, pattern recognition, spectral analysis, speech understanding, statistical modeling, time normalization},
	pages = {806--819},
	file = {ScienceDirect Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VWPXBBVS\\B0080448542009068.html:text/html},
}

@book{brown_encyclopedia_2006,
	address = {Amsterdam},
	edition = {2nd ed},
	title = {Encyclopedia of language \& linguistics},
	isbn = {978-0-08-044854-1},
	language = {eng},
	publisher = {Elsevier},
	author = {Brown, E. K. and Anderson, Anne},
	year = {2006},
}

@article{upton_speech_1984-1,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages — this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-19},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@inproceedings{morris_wer_2004-1,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{zhang_hello_2017-1,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@inproceedings{wu_deep_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Deep neural networks employing {Multi}-{Task} {Learning} and stacked bottleneck features for speech synthesis},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178814/},
	doi = {10.1109/ICASSP.2015.7178814},
	urldate = {2022-08-19},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Zhizheng and Valentini-Botinhao, Cassia and Watts, Oliver and King, Simon},
	month = apr,
	year = {2015},
	pages = {4460--4464},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\GFDVBE7Z\\Wu et al. - 2015 - Deep neural networks employing Multi-Task Learning.pdf:application/pdf},
}

@article{pohjalainen_feature_2015,
	title = {Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits},
	volume = {29},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230813001113},
	doi = {10.1016/j.csl.2013.11.004},
	language = {en},
	number = {1},
	urldate = {2022-08-19},
	journal = {Computer Speech \& Language},
	author = {Pohjalainen, Jouni and Räsänen, Okko and Kadioglu, Serdar},
	month = jan,
	year = {2015},
	pages = {145--171},
}

@inproceedings{eyben_opensmile_2010,
	address = {Firenze, Italy},
	title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
	isbn = {978-1-60558-933-6},
	shorttitle = {Opensmile},
	url = {http://dl.acm.org/citation.cfm?doid=1873951.1874246},
	doi = {10.1145/1873951.1874246},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the international conference on {Multimedia} - {MM} '10},
	publisher = {ACM Press},
	author = {Eyben, Florian and Wöllmer, Martin and Schuller, Björn},
	year = {2010},
	pages = {1459},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4F8ACZKL\\Eyben et al. - 2010 - Opensmile the munich versatile and fast open-sour.pdf:application/pdf},
}

@article{chung_unsupervised_2019,
	title = {An {Unsupervised} {Autoregressive} {Model} for {Speech} {Representation} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1904.03240},
	doi = {10.48550/ARXIV.1904.03240},
	abstract = {This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.},
	urldate = {2022-08-19},
	author = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Accepted to Interspeech 2019. Code available at: https://github.com/iamyuanchung/Autoregressive-Predictive-Coding},
}

@inproceedings{boser_training_1992,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {A training algorithm for optimal margin classifiers},
	isbn = {978-0-89791-497-0},
	url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
	doi = {10.1145/130385.130401},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the fifth annual workshop on {Computational} learning theory  - {COLT} '92},
	publisher = {ACM Press},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	year = {1992},
	pages = {144--152},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\29LCGTE4\\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf:application/pdf},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {wav2vec 2.0},
	url = {https://arxiv.org/abs/2006.11477},
	doi = {10.48550/ARXIV.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2022-08-19},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@article{soldz_big_1999,
	title = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}: {A} 45-{Year} {Longitudinal} {Study}},
	volume = {33},
	issn = {00926566},
	shorttitle = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656699922432},
	doi = {10.1006/jrpe.1999.2243},
	language = {en},
	number = {2},
	urldate = {2022-08-19},
	journal = {Journal of Research in Personality},
	author = {Soldz, Stephen and Vaillant, George E.},
	month = jun,
	year = {1999},
	pages = {208--232},
}

@inproceedings{zhou_security_2010,
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	abstract = {Cloud Computing is becoming a well-known buzzword nowadays. Many companies, such as Amazon, Google, Microsoft and so on, accelerate their paces in developing Cloud Computing systems and enhancing their services to provide for a larger amount of users. However, security and privacy issues present a strong barrier for users to adapt into Cloud Computing systems. In this paper, we investigate several Cloud Computing system providers about their concerns on security and privacy issues. We find those concerns are not adequate and more should be added in terms of five aspects (i.e., availability, confidentiality, data integrity, control, audit) for security. Moreover, released acts on privacy are out of date to protect users' private information in the new environment (i.e., Cloud Computing system environment) since they are no longer applicable to the new relationship between users and providers, which contains three parties (i.e., Cloud service user, Cloud service provider/Cloud user, Cloud provider). Multi located data storage and services (i.e., applications) in the Cloud make privacy issues even worse. Hence, adapting released acts for new scenarios in the Cloud, it will result in more users to step into Cloud. We claim that the prosperity in Cloud Computing literature is to be coming after those security and privacy issues having be resolved.},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = dec,
	year = {2010},
	note = {Journal Abbreviation: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
Publication Title: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
DOI:},
	pages = {112},
}

@inproceedings{zhou_security_2010-1,
	address = {Beijing, China},
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	isbn = {978-1-4244-8125-5},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	url = {http://ieeexplore.ieee.org/document/5663489/},
	doi = {10.1109/SKG.2010.19},
	urldate = {2022-08-19},
	booktitle = {2010 {Sixth} {International} {Conference} on {Semantics}, {Knowledge} and {Grids}},
	publisher = {IEEE},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = nov,
	year = {2010},
	pages = {105--112},
}

@inproceedings{karimov_cloud_2021,
	title = {Cloud {Computing} {Security} {Challenges} and {Solutions}},
	doi = {10.1109/ICISCT52966.2021.9670220},
	abstract = {in this paper focuses on development of cloud computing, privacy security issues have become increasingly prominent, which is of concern to industry and academia. We review the research progress on privacy security issues from the perspective of several privacy security protection technologies in cloud computing. First, we introduce some privacy security risks of cloud computing and propose a comprehensive privacy security protection framework.},
	booktitle = {2021 {International} {Conference} on {Information} {Science} and {Communications} {Technologies} ({ICISCT})},
	author = {Karimov, Abdukodir and Olimov, Iskandar and Berdiyev, Khusniddin and Tojiakbarova, Umida and Tursunov, Otabek},
	month = nov,
	year = {2021},
	keywords = {Computational modeling, Access control, Attribute-based encryption, Cloud computing, Communications technology, Industries, Information science, Privacy, Privacy security},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\6B73HGPD\\9670220.html:text/html},
}

@incollection{takabi_introduction_2019,
	title = {Introduction to the {Cloud} and {Fundamental} {Security} and {Privacy} {Issues} of the {Cloud}},
	isbn = {978-1-119-05340-8},
	url = {https://ieeexplore.ieee.org/document/9821151},
	abstract = {Cloud Computing is the most important solution to extend Information Technology's (IT) capabilities. However, Cloud is still vulnerable to a variety of threats and attacks that affects the growth of cloud computing in recent years. Therefore, the security concerns should be considered to improve the assurance of required security for the cloud customers. The cloud security consists of different aspects such as: infrastructure, information, and identity. In this chapter, we provide an introduction to the Cloud and its fundamental security and privacy issues, investigate security issues in different cloud services delivery models and introduce Cloud security standards.},
	urldate = {2022-08-19},
	booktitle = {Security, {Privacy}, and {Digital} {Forensics} in the {Cloud}},
	publisher = {Wiley},
	author = {Takabi, Hassan and GhasemiGol, Mohammad},
	year = {2019},
	doi = {10.1002/9781119053385.ch1},
	note = {Conference Name: Security, Privacy, and Digital Forensics in the Cloud},
	keywords = {Computational modeling, Cloud computing, Privacy, Operating systems, Organizations, Security, Software as a service},
	pages = {1--22},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\NNG85SBV\\9821151.html:text/html},
}

@inproceedings{kumar_systematic_2020,
	title = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}: {Data} {Integrity}, {Confidentiality} and {Availability}},
	shorttitle = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}},
	doi = {10.1109/GUCON48875.2020.9231255},
	abstract = {The cloud computing plays the prominent role in many organizations and researchers were focus on securing the cloud computing. The privacy preserving is the major challenge that grows exponentially with increases in user. In this paper, the depth survey is conducted on the recent methodologies of the cloud storage security related with the cloud computing. The overview of the cloud computing and security issues is analyzed in this paper. The key security requirements such as data integrity, availability and confidentiality. Security issues in the recent methodologies of cloud security is analyzed. The challenges in the cloud security is analyzed and possible future scope of the method is discussed. The paper involves in analyzing the state-of-art method to investigate the advantages and limitations.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Computing}, {Power} and {Communication} {Technologies} ({GUCON})},
	author = {Kumar, Rajeev and Bhatia, M P S},
	month = oct,
	year = {2020},
	keywords = {Systematics, Privacy, Organizations, Cloud Computing, Cloud computing security, Cloud Storage Security, Conferences, Confidentiality, Data integrity, Data Integrity, Data transfer, Privacy Preserving},
	pages = {334--337},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\MT8C247U\\9231255.html:text/html},
}

@inproceedings{godfrey_switchboard_1992,
	address = {San Francisco, CA, USA},
	title = {{SWITCHBOARD}: telephone speech corpus for research and development},
	isbn = {978-0-7803-0532-8},
	shorttitle = {{SWITCHBOARD}},
	url = {http://ieeexplore.ieee.org/document/225858/},
	doi = {10.1109/ICASSP.1992.225858},
	urldate = {2022-08-20},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
	year = {1992},
	pages = {517--520 vol.1},
}

@article{zhang_hello_2017-2,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-20},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@inproceedings{lin_self-attentive_2020,
	title = {Self-{Attentive} {Similarity} {Measurement} {Strategies} in {Speaker} {Diarization}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1908},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Lin, Qingjian and Hou, Yu and Li, Ming},
	month = oct,
	year = {2020},
	pages = {284--288},
}

@article{schuller_interspeech_2013,
	title = {The interspeech 2013 computational paralinguistics challenge: {Social} signals, conflict, emotion, autism},
	shorttitle = {The interspeech 2013 computational paralinguistics challenge},
	journal = {Proceedings of Interspeech},
	author = {Schuller, Björn and Steidl, S. and Batliner, Anton and Vinciarelli, Alessandro and Scherer, K. and Ringeval, Fabien and Chetouani, Mohamed and Weninger, F. and Eyben, Florian and Marchi, Erik and Mortillaro, Marcello and Salamin, H. and Polychroniou, Anna and Valente, F. and Kim, S.},
	month = jan,
	year = {2013},
	pages = {148--152},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IJ9QFIG3\\Schuller et al. - 2013 - The interspeech 2013 computational paralinguistics.pdf:application/pdf},
}

@inproceedings{misra_spectral_2004,
	address = {Montreal, Que., Canada},
	title = {Spectral entropy based feature for robust {ASR}},
	volume = {1},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1325955/},
	doi = {10.1109/ICASSP.2004.1325955},
	urldate = {2022-08-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Misra, H. and Ikbal, S. and Bourlard, H. and Hermansky, H.},
	year = {2004},
	pages = {I--193--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\YLKP828K\\Misra et al. - 2004 - Spectral entropy based feature for robust ASR.pdf:application/pdf},
}

@article{hunt_spectral_2000,
	title = {Spectral {Signal} {Processing} for {ASR}},
	abstract = {The paper begins by discussing the difficulties in obtaining repeatable results in speech recognition. Theoretical arguments are presented for and against copying human auditory properties in automatic speech recognition. The "standard" acoustic analysis for automatic speech recognition, consisting of melscale cepstrum coefficients and their temporal derivatives, is described. Some variations and extensions of the standard analysis --- PLP, cepstrum correlation methods, LDA, and variants on log power --- are then discussed. These techniques pass the test of having been found useful at multiple sites, especially with noisy speech. The extent to which auditory properties can account for the advantage found for particular techniques is considered. It is concluded that the advantages do not in fact stem from auditory properties, and that there is so far little or no evidence that the study of the human auditory system has contributed to advances in automatic speech recognition. Contributio...},
	author = {Hunt, Melvyn},
	month = aug,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\Z9XRFNJG\\Hunt - 2000 - Spectral Signal Processing for ASR.pdf:application/pdf},
}

@article{biswas_speaker_2021,
	title = {Speaker recognition: an enhanced approach to identify singer voice using neural network},
	volume = {24},
	issn = {1381-2416, 1572-8110},
	shorttitle = {Speaker recognition},
	url = {http://link.springer.com/10.1007/s10772-020-09698-8},
	doi = {10.1007/s10772-020-09698-8},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {International Journal of Speech Technology},
	author = {Biswas, Sharmila and Solanki, Sandeep Singh},
	month = mar,
	year = {2021},
	pages = {9--21},
}

@incollection{penn_computational_2012,
	title = {Computational {Linguistics}},
	isbn = {978-0-444-51747-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444517470500056},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Philosophy of {Linguistics}},
	publisher = {Elsevier},
	author = {Penn, Gerald},
	year = {2012},
	doi = {10.1016/B978-0-444-51747-0.50005-6},
	pages = {143--173},
}

@inproceedings{brill_improved_2000,
	address = {Hong Kong},
	title = {An improved error model for noisy channel spelling correction},
	url = {http://portal.acm.org/citation.cfm?doid=1075218.1075255},
	doi = {10.3115/1075218.1075255},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Proceedings of the 38th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '00},
	publisher = {Association for Computational Linguistics},
	author = {Brill, Eric and Moore, Robert C.},
	year = {2000},
	pages = {286--293},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YP8NG3XW\\Brill and Moore - 2000 - An improved error model for noisy channel spelling.pdf:application/pdf},
}

@misc{noauthor_type_2017,
	title = {Type less, talk more},
	url = {https://blog.google/products/search/type-less-talk-more/},
	abstract = {We’re bringing voice typing (aka talking to your phone instead of typing) to 30 new languages and locales around the world, covering more than a billion people.},
	language = {en-us},
	urldate = {2022-08-20},
	journal = {Google},
	month = aug,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L9T2I37L\\type-less-talk-more.html:text/html},
}

@article{noauthor_amazon_2019,
	title = {Amazon {Workers} {Are} {Listening} to {What} {You} {Tell} {Alexa}},
	url = {https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio},
	abstract = {A global team reviews audio clips in an effort to help the voice-activated assistant respond to commands.},
	language = {en},
	urldate = {2022-08-20},
	journal = {Bloomberg.com},
	month = apr,
	year = {2019},
	keywords = {Software, Privacy, ALPHABET INC-CL A, AMAZON.COM INC, APPLE INC, Boston, business, Costa Rica, India, markets, Policy, Romania, technology},
}

@misc{noauthor_amazon_nodate,
	title = {Amazon {Sends} 1,700 {Alexa} {Voice} {Recordings} to a {Random} {Person}},
	url = {https://threatpost.com/amazon-1700-alexa-voice-recordings/140201/},
	abstract = {The intimate recordings paint a detailed picture of a man's life.},
	language = {en},
	urldate = {2022-08-20},
}

@article{wolfson_amazons_2018,
	chapter = {Technology},
	title = {Amazon's {Alexa} recorded private conversation and sent it to random contact},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2018/may/24/amazon-alexa-recorded-conversation},
	abstract = {The company, which has insisted its Echo devices aren’t always recording, has confirmed the audio was sent},
	language = {en-GB},
	urldate = {2022-08-20},
	journal = {The Guardian},
	author = {Wolfson, Sam},
	month = may,
	year = {2018},
	keywords = {Privacy, Amazon, Amazon Alexa, Internet, Surveillance, Technology, US news},
}

@article{stupp_fraudsters_2019,
	chapter = {WSJ Pro},
	title = {Fraudsters {Used} {AI} to {Mimic} {CEO}’s {Voice} in {Unusual} {Cybercrime} {Case}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402},
	abstract = {Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of funds in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.},
	language = {en-US},
	urldate = {2022-08-20},
	journal = {Wall Street Journal},
	author = {Stupp, Catherine},
	month = aug,
	year = {2019},
	keywords = {artificial intelligence, Artificial Intelligence/Machine Learning, Bobby Filar, business in europe, Business in Europe, business in the u.k., Business in the U.K., c\&e executive news filter, C\&E Executive News Filter, c\&e industry news filter, C\&E Industry News Filter, computer science, Computer Science, content types, Content Types, corporate, corporate crime, Corporate Crime/Legal Action, Corporate/Industrial News, crime, Crime/Legal Action, cybercrime, Cybercrime/Hacking, Euler Hermes Group, factiva filters, Factiva Filters, financial services, Financial Services, fraud, Fraud, general news, hacking, humanities, industrial news, insurance, Insurance, Irakli Beridze, legal action, machine learning, management, Management, non-life insurance, Non-life Insurance, Philipp Amann, political, Political/General News, PRO, Rüdiger Kirsch, sciences, Sciences/Humanities, senior level management, Senior Level Management, trade credit insurance, Trade Credit Insurance, WSJ-PRO-CYBER, WSJ-PRO-WSJ.com},
}

@book{petronio_boundaries_2002,
	address = {Albany},
	series = {{SUNY} series in communication studies},
	title = {Boundaries of privacy: dialectics of disclosure},
	isbn = {978-0-7914-5515-9},
	shorttitle = {Boundaries of privacy},
	publisher = {State University of New York Press},
	author = {Petronio, Sandra Sporbert},
	year = {2002},
	note = {ISBN2: 978-0-7914-5516-6},
	keywords = {Privacy, Interpersonal communication, Secrecy, Self-disclosure},
}

@article{xie_how_2009,
	title = {How to repair customer trust after negative publicity: {The} roles of competence, integrity, benevolence, and forgiveness},
	volume = {26},
	issn = {07426046, 15206793},
	shorttitle = {How to repair customer trust after negative publicity},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mar.20289},
	doi = {10.1002/mar.20289},
	language = {en},
	number = {7},
	urldate = {2022-08-20},
	journal = {Psychology and Marketing},
	author = {Xie, Yi and Peng, Siqing},
	month = jul,
	year = {2009},
	pages = {572--589},
}

@article{shi_edge_2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	url = {http://ieeexplore.ieee.org/document/7488250/},
	doi = {10.1109/JIOT.2016.2579198},
	number = {5},
	urldate = {2022-08-20},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	month = oct,
	year = {2016},
	pages = {637--646},
}

@misc{noauthor_mydata_2015,
	type = {Muut julkaisut},
	title = {{MyData} – {A} {Nordic} {Model} for human-centered personal data management and processing},
	copyright = {This publication is copyrighted. You may download, display and print it for Your own personal use. Commercial use is prohibited.},
	url = {https://julkaisut.valtioneuvosto.fi/handle/10024/78439},
	abstract = {This white paper presents a framework, principles, and a model for a human-centric approach to the managing and processing of personal information. The approach – defined as MyData – is based on the right of individuals to access the data collected about them. The core idea is that individuals should be in control of their own data. The MyData approach aims at strengthening digital human rights while opening new opportunities for businesses to develop innovative personal data based services built on mutual trust.},
	language = {en},
	urldate = {2022-08-20},
	year = {2015},
	note = {Accepted: 2016-11-11T10:03:41Z
ISBN: 9789522434555
Publisher: liikenne- ja viestintäministeriö},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\UBFJDIUP\\2015 - MyData – A Nordic Model for human-centered persona.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KEV9QSG2\\78439.html:text/html},
}

@inproceedings{nautsch_gdpr_2019-1,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} towards a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {http://arxiv.org/abs/1907.03458},
	doi = {10.21437/Interspeech.2019-2647},
	abstract = {Privacy preservation and the protection of speech data is in high demand, not least as a result of recent regulation, e.g. the General Data Protection Regulation (GDPR) in the EU. While there has been a period with which to prepare for its implementation, its implications for speech data is poorly understood. This assertion applies to both the legal and technology communities, and is hardly surprising since there is no universal definition of 'privacy', let alone a clear understanding of when or how the GDPR applies to the capture, storage and processing of speech data. In aiming to initiate the discussion that is needed to establish a level of harmonisation that is thus far lacking, this contribution presents some reflections of both legal and technology communities on the implications of the GDPR as regards speech data. The article outlines the need for taxonomies at the intersection of speech technology and data privacy - a discussion that is still very much in its infancy - and describes the ways to safeguards and priorities for future research. In being agnostic to any specific application, the treatment should be of interest to the speech communication community at large.},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2019},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	note = {arXiv:1907.03458 [cs, eess]},
	keywords = {Computer Science - Computers and Society, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {3695--3699},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\W7WB9QLH\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\3UUJ3JJX\\1907.html:text/html},
}

@book{european_data_protection_supervisor_edps_2019,
	address = {LU},
	series = {{EDPS} {TechDispatch}},
	title = {{EDPS} {TechDispatch}},
	url = {https://data.europa.eu/doi/10.2804/004275},
	language = {eng},
	urldate = {2022-08-20},
	publisher = {Publications Office},
	author = {{European Data Protection Supervisor}},
	year = {2019},
}

@article{konig_automatic_2015,
	title = {Automatic speech analysis for the assessment of patients with predementia and {Alzheimer}'s disease},
	volume = {1},
	issn = {2352-8729, 2352-8729},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/j.dadm.2014.11.012},
	doi = {10.1016/j.dadm.2014.11.012},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	author = {König, Alexandra and Satt, Aharon and Sorin, Alexander and Hoory, Ron and Toledo‐Ronen, Orith and Derreumaux, Alexandre and Manera, Valeria and Verhey, Frans and Aalten, Pauline and Robert, Phillipe H. and David, Renaud},
	month = mar,
	year = {2015},
	pages = {112--124},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WZVVDNX2\\König et al. - 2015 - Automatic speech analysis for the assessment of pa.pdf:application/pdf},
}

@article{konig_automatic_2015-1,
	title = {Automatic speech analysis for the assessment of patients with predementia and {Alzheimer}'s disease},
	volume = {1},
	issn = {2352-8729, 2352-8729},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/j.dadm.2014.11.012},
	doi = {10.1016/j.dadm.2014.11.012},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	author = {König, Alexandra and Satt, Aharon and Sorin, Alexander and Hoory, Ron and Toledo‐Ronen, Orith and Derreumaux, Alexandre and Manera, Valeria and Verhey, Frans and Aalten, Pauline and Robert, Phillipe H. and David, Renaud},
	month = mar,
	year = {2015},
	pages = {112--124},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\PGX9R2SR\\König et al. - 2015 - Automatic speech analysis for the assessment of pa.pdf:application/pdf},
}

@incollection{gutwirth_seven_2013,
	address = {Dordrecht},
	title = {Seven {Types} of {Privacy}},
	isbn = {978-94-007-5184-2 978-94-007-5170-5},
	url = {http://link.springer.com/10.1007/978-94-007-5170-5_1},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {European {Data} {Protection}: {Coming} of {Age}},
	publisher = {Springer Netherlands},
	author = {Finn, Rachel L. and Wright, David and Friedewald, Michael},
	editor = {Gutwirth, Serge and Leenes, Ronald and de Hert, Paul and Poullet, Yves},
	year = {2013},
	doi = {10.1007/978-94-007-5170-5_1},
	pages = {3--32},
}

@article{chen_no_2003,
	title = {[{No} title found]},
	volume = {4},
	issn = {1385951X},
	url = {http://link.springer.com/10.1023/A:1022962631249},
	doi = {10.1023/A:1022962631249},
	number = {2/3},
	urldate = {2022-08-20},
	journal = {Information Technology and Management},
	author = {Chen, Sandy C. and Dhillon, Gurpreet S.},
	year = {2003},
	pages = {303--318},
}

@misc{reuter_guide_2015,
	title = {A {Guide} to {Fully} {Homomorphic} {Encryption}},
	url = {https://eprint.iacr.org/2015/1192},
	abstract = {Fully homomorphic encryption (FHE) has been dubbed the holy grail of cryptography, an elusive goal which could solve the IT world's problems of security and trust. Research in the area exploded after 2009 when Craig Gentry showed that FHE can be realised in principle. Since that time considerable progress has been made in finding more practical and more efficient solutions. Whilst research quickly developed, terminology and concepts became diverse and confusing so that today it can be difficult to understand what the achievements of different works actually are. The purpose of this paper is to address three fundamental questions: What is FHE? What can FHE be used for? What is the state of FHE today? As well as surveying the field, we clarify different terminology in use and prove connections between different FHE notions.Updated the acknowledgements.},
	urldate = {2022-08-20},
	author = {Reuter, Colin Boyd, Christopher Carr, Kristian Gjøsteen, Angela Jäschke, Christian A., Frederik Armknecht and Strand, Martin},
	year = {2015},
	note = {Report Number: 1192},
	keywords = {Fully Homomorphic Encryption},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PE6U2WJV\\Reuter and Strand - 2015 - A Guide to Fully Homomorphic Encryption.pdf:application/pdf},
}

@book{jessica_gasiorek_message_2018,
	title = {Message {Processing}: {The} {Science} of {Creating} {Understanding}},
	copyright = {Creative Commons Attribution 4.0 International License},
	url = {http://pressbooks-dev.oer.hawaii.edu/messageprocessing/},
	publisher = {UH Mānoa Outreach College},
	author = {Jessica Gasiorek and R. Kelly Aune},
	year = {2018},
	note = {https://pressbooks.oer.hawaii.edu/messageprocessing/\#:{\textasciitilde}:text=Book\%20Title\%3A\%20Message\%20Processing\%3A\%20The\%20Science\%20of\%20Creating\%20Understanding\&text=Book\%20Description\%3A\%20The\%20text\%20provides,on\%20how\%20people\%20create\%20understanding.},
}

@misc{noauthor_kunstliche_nodate,
	title = {Künstliche {Intelligenz} kommt in {Unternehmen} allmählich voran {\textbar} {Bitkom} e.{V}.},
	url = {https://www.bitkom.org/Presse/Presseinformation/Kuenstliche-Intelligenz-kommt-in-Unternehmen-allmaehlich-voran},
	abstract = {Zwei Drittel halten KI für die wichtigste Zukunftstechnologie Bislang nutzen 8 Prozent KI-Anwendungen, jedes vierte Unternehmen will investieren Bitkom-Präsident Berg: „KI braucht noch mehr Schwung“},
	language = {de},
	urldate = {2022-08-21},
}

@misc{noauthor_scaling_nodate,
	title = {Scaling {AI}: {From} {Experimental} to {Exponential}},
	shorttitle = {Scaling {AI}},
	url = {https://www.accenture.com/us-en/insights/artificial-intelligence/ai-investments},
	abstract = {Most businesses deploy pilot \#AI programs, but they struggle when it comes to scaling it. A new Accenture report explains the 3 critical factors for scaling AI.},
	language = {en},
	urldate = {2022-08-21},
}

@article{ardila_end--end_2019,
	title = {End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-019-0447-x},
	doi = {10.1038/s41591-019-0447-x},
	language = {en},
	number = {6},
	urldate = {2022-08-21},
	journal = {Nature Medicine},
	author = {Ardila, Diego and Kiraly, Atilla P. and Bharadwaj, Sujeeth and Choi, Bokyung and Reicher, Joshua J. and Peng, Lily and Tse, Daniel and Etemadi, Mozziyar and Ye, Wenxing and Corrado, Greg and Naidich, David P. and Shetty, Shravya},
	month = jun,
	year = {2019},
	pages = {954--961},
}

@article{fukushima_neocognitron_1988,
	title = {Neocognitron: {A} hierarchical neural network capable of visual pattern recognition},
	volume = {1},
	issn = {08936080},
	shorttitle = {Neocognitron},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
	doi = {10.1016/0893-6080(88)90014-7},
	language = {en},
	number = {2},
	urldate = {2022-08-21},
	journal = {Neural Networks},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {1988},
	pages = {119--130},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2022-08-21},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@inproceedings{ghahremani_acoustic_2016,
	title = {Acoustic {Modelling} from the {Signal} {Domain} {Using} {CNNs}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/ghahremani16_interspeech.html},
	doi = {10.21437/Interspeech.2016-1495},
	language = {en},
	urldate = {2022-08-21},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Ghahremani, Pegah and Manohar, Vimal and Povey, Daniel and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {3434--3438},
}

@misc{hou_audio-visual_2018,
	title = {Audio-{Visual} {Speech} {Enhancement} {Using} {Multimodal} {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.10893},
	abstract = {Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus only on addressing audio information. In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. We also propose a multi-task learning framework for reconstructing audio and visual signals at the output layer. Precisely speaking, the proposed AVDCNN model is structured as an audio-visual encoder-decoder network, in which audio and visual data are first processed using individual CNNs, and then fused into a joint network to generate enhanced speech (the primary task) and reconstructed images (the secondary task) at the output layer. The model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five instrumental criteria. Results show that the AVDCNN model yields a notably superior performance compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process. In addition, the AVDCNN model also outperforms an existing audio-visual SE model, confirming its capability of effectively combining audio and visual information in SE.},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Hou, Jen-Cheng and Wang, Syu-Siang and Lai, Ying-Hui and Tsao, Yu and Chang, Hsiu-Wen and Wang, Hsin-Min},
	month = jan,
	year = {2018},
	note = {arXiv:1703.10893 [cs, stat]},
	keywords = {Computer Science - Multimedia, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: To appear in IEEE Transactions on Emerging Topics in Computational Intelligence. Some audio samples can be reached in this link: https://sites.google.com/view/avse2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\RZW9TSJU\\Hou et al. - 2018 - Audio-Visual Speech Enhancement Using Multimodal D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\W8FSCKT6\\1703.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2022-08-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{errattahi_automatic_2018-1,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-21},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\HJ993EDS\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@article{morgan_continuous_1995-1,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {10535888},
	url = {http://ieeexplore.ieee.org/document/382443/},
	doi = {10.1109/79.382443},
	number = {3},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	pages = {24--42},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\8AZVI7RK\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{li_hybrid_2013,
	address = {Geneva, Switzerland},
	title = {Hybrid {Deep} {Neural} {Network}--{Hidden} {Markov} {Model} ({DNN}-{HMM}) {Based} {Speech} {Emotion} {Recognition}},
	isbn = {978-0-7695-5048-0},
	url = {http://ieeexplore.ieee.org/document/6681449/},
	doi = {10.1109/ACII.2013.58},
	urldate = {2022-08-21},
	booktitle = {2013 {Humaine} {Association} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction}},
	publisher = {IEEE},
	author = {Li, Longfei and Zhao, Yong and Jiang, Dongmei and Zhang, Yanning and Wang, Fengna and Gonzalez, Isabel and Valentin, Enescu and Sahli, Hichem},
	month = sep,
	year = {2013},
	pages = {312--317},
}

@article{ochiai_speaker_2016,
	title = {Speaker {Adaptive} {Training} {Localizing} {Speaker} {Modules} in {DNN} for {Hybrid} {DNN}-{HMM} {Speech} {Recognizers}},
	volume = {E99.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E99.D/10/E99.D_2016SLP0010/_article},
	doi = {10.1587/transinf.2016SLP0010},
	language = {en},
	number = {10},
	urldate = {2022-08-21},
	journal = {IEICE Transactions on Information and Systems},
	author = {Ochiai, Tsubasa and Matsuda, Shigeki and Watanabe, Hideyuki and Lu, Xugang and Hori, Chiori and Kawai, Hisashi and Katagiri, Shigeru},
	year = {2016},
	pages = {2431--2443},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\8EY8WZF7\\Ochiai et al. - 2016 - Speaker Adaptive Training Localizing Speaker Modul.pdf:application/pdf},
}

@book{muller_fundamentals_2021,
	address = {Cham, Switzerland},
	edition = {Second edition},
	title = {Fundamentals of music processing: using {Python} and {Jupyter} notebooks},
	isbn = {978-3-030-69807-2},
	shorttitle = {Fundamentals of music processing},
	language = {eng},
	publisher = {Springer},
	author = {Müller, Meinard},
	year = {2021},
}

@inproceedings{seide_feature_2011,
	address = {Waikoloa, HI, USA},
	title = {Feature engineering in {Context}-{Dependent} {Deep} {Neural} {Networks} for conversational speech transcription},
	isbn = {978-1-4673-0367-5},
	url = {http://ieeexplore.ieee.org/document/6163899/},
	doi = {10.1109/ASRU.2011.6163899},
	urldate = {2022-08-21},
	booktitle = {2011 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} \& {Understanding}},
	publisher = {IEEE},
	author = {Seide, Frank and Li, Gang and Chen, Xie and Yu, Dong},
	month = dec,
	year = {2011},
	note = {ISBN2: 978-1-4673-0365-1
ISBN3: 978-1-4673-0366-8},
	pages = {24--29},
}

@article{dua_developing_2022,
	title = {Developing a {Speech} {Recognition} {System} for {Recognizing} {Tonal} {Speech} {Signals} {Using} a {Convolutional} {Neural} {Network}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/12/6223},
	doi = {10.3390/app12126223},
	abstract = {Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15\% accuracy rate and a 10.56\% WER for continuous and extensive vocabulary sentences of speech signals with different tones.},
	language = {en},
	number = {12},
	urldate = {2022-08-21},
	journal = {Applied Sciences},
	author = {Dua, Sakshi and Kumar, Sethuraman Sambath and Albagory, Yasser and Ramalingam, Rajakumar and Dumka, Ankur and Singh, Rajesh and Rashid, Mamoon and Gehlot, Anita and Alshamrani, Sultan S. and AlGhamdi, Ahmed Saeed},
	month = jun,
	year = {2022},
	pages = {6223},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\97ILRS3G\\Dua et al. - 2022 - Developing a Speech Recognition System for Recogni.pdf:application/pdf},
}

@misc{garofolo_john_s_timit_1993,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST).{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}ul{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.phn" rel="nofollow"{\textgreater}phonemes{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.txt" rel="nofollow"{\textgreater}transcripts{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wav" rel="nofollow"{\textgreater}audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wrd" rel="nofollow"{\textgreater}word list{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}/ul{\textgreater}{\textless}/br{\textgreater}
Portions © 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	year = {1993},
	doi = {10.35111/17GK-BN40},
	note = {Artwork Size: 715776 KB
Pages: 715776 KB
Type: dataset},
}

@article{bell_adaptation_2020,
	title = {Adaptation algorithms for neural network-based speech recognition: {An} overview},
	volume = {2},
	shorttitle = {Adaptation algorithms for neural network-based speech recognition},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {33--66},
}

@article{bell_adaptation_2021,
	title = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}: {An} {Overview}},
	volume = {2},
	issn = {2644-1322},
	shorttitle = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9296327/},
	doi = {10.1109/OJSP.2020.3045349},
	urldate = {2022-08-21},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
	year = {2021},
	pages = {33--66},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\X557RF2Q\\Bell et al. - 2021 - Adaptation Algorithms for Neural Network-Based Spe.pdf:application/pdf},
}

@misc{garofolo_john_s_csr-i_2007,
	title = {{CSR}-{I} ({WSJ0}) {Complete}},
	url = {https://catalog.ldc.upenn.edu/LDC93S6A},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}LDC93S6A - Complete CSR-I corpus {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6B" rel="nofollow"{\textgreater}LDC93S6B{\textless}/a{\textgreater} - CSR-I Sennheiser speech {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6C" rel="nofollow"{\textgreater}LDC93S6C{\textless}/a{\textgreater} - CSR-I other speech{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}During 1991, the DARPA Spoken Language Program initiated efforts to build a new corpus to support research on large-vocabulary Continuous Speech Recognition (CSR) systems.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The first two CSR Corpora consist primarily of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news text and are thus often known as WSJ0 and WSJ1. (Later sections of the CSR set of corpora, however, will consist of read texts from other sources of North American business news and eventually from other news domains).{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The texts to be read were selected to fall within either a 5,000-word or a 20,000-word subset of the WSJ text corpus. (See the documentation for details). Some spontaneous dictation is included in addition to the read speech. The dictation portion was collected using journalists who dictated hypothetical news articles.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}Two microphones are used throughout: a close-talking Sennheiser HMD414 and a secondary microphone, which may vary. The corpora are thus offered in three configurations: the speech from the Sennheiser, the speech from the other microphone and the speech from both; all three sets include all transcriptions, tests, documentation, etc.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}In general, transcriptions of the speech, test data from ARPA evaluations, scores achieved by various speech recognition systems and software used in scoring are included on separate discs from the waveform data.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}Please listen to this {\textless}a href="desc/addenda/LDC93S6A.wav"{\textgreater}audio sample{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}/br{\textgreater}
Portions © 1987-1989 Dow Jones \& Company, Inc., © 1992, 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Graff, David and Paul, Doug and Pallett, David},
	month = may,
	year = {2007},
	doi = {10.35111/EWKM-CG47},
	note = {Artwork Size: 9542041 KB
Pages: 9542041 KB
Type: dataset},
}

@article{li_deng_mnist_2012,
	title = {The {MNIST} {Database} of {Handwritten} {Digit} {Images} for {Machine} {Learning} {Research} [{Best} of the {Web}]},
	volume = {29},
	issn = {1053-5888},
	url = {http://ieeexplore.ieee.org/document/6296535/},
	doi = {10.1109/MSP.2012.2211477},
	number = {6},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {{Li Deng}},
	month = nov,
	year = {2012},
	pages = {141--142},
}

@inproceedings{deng_imagenet_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	urldate = {2022-08-21},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
}

@misc{noauthor_center_nodate,
	title = {Center for {Language} {Engineering}},
	url = {https://cle.org.pk/},
	urldate = {2022-08-22},
	file = {Center for Language Engineering:C\:\\Users\\DELL\\Zotero\\storage\\LV3V3DMG\\cle.org.pk.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub}: {Where} the world builds software},
	shorttitle = {{GitHub}},
	url = {https://github.com/},
	abstract = {GitHub is where over 83 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositories, review code like a pro, track bugs and feat...},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {citation-265913669.bib:C\:\\Users\\DELL\\Zotero\\storage\\YSPPY2BA\\citation-265913669.bib:application/x-bibtex;citation.bib:C\:\\Users\\DELL\\Zotero\\storage\\6H4SBI5D\\citation.bib:application/x-bibtex},
}

@misc{noauthor_kaggle_nodate,
	title = {Kaggle: {Your} {Machine} {Learning} and {Data} {Science} {Community}},
	shorttitle = {Kaggle},
	url = {https://www.kaggle.com/},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	language = {en},
	urldate = {2022-08-22},
}

@article{verma_i-vectors_2015,
	title = {i-{Vectors} in speech processing applications: a survey},
	volume = {18},
	issn = {1381-2416, 1572-8110},
	shorttitle = {i-{Vectors} in speech processing applications},
	url = {http://link.springer.com/10.1007/s10772-015-9295-3},
	doi = {10.1007/s10772-015-9295-3},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {International Journal of Speech Technology},
	author = {Verma, Pulkit and Das, Pradip K.},
	month = dec,
	year = {2015},
	pages = {529--546},
}

@article{verma_i-vectors_2015-1,
	title = {i-{Vectors} in speech processing applications: a survey},
	volume = {18},
	issn = {1381-2416, 1572-8110},
	shorttitle = {i-{Vectors} in speech processing applications},
	url = {http://link.springer.com/10.1007/s10772-015-9295-3},
	doi = {10.1007/s10772-015-9295-3},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {International Journal of Speech Technology},
	author = {Verma, Pulkit and Das, Pradip K.},
	month = dec,
	year = {2015},
	pages = {529--546},
}

@article{joshi_modified_2016,
	title = {Modified {Mean} and {Variance} {Normalization}: {Transforming} to {Utterance}-{Specific} {Estimates}},
	volume = {35},
	issn = {0278-081X, 1531-5878},
	shorttitle = {Modified {Mean} and {Variance} {Normalization}},
	url = {http://link.springer.com/10.1007/s00034-015-0129-y},
	doi = {10.1007/s00034-015-0129-y},
	language = {en},
	number = {5},
	urldate = {2022-08-22},
	journal = {Circuits, Systems, and Signal Processing},
	author = {Joshi, Vikas and Prasad, N. Vishnu and Umesh, S.},
	month = may,
	year = {2016},
	pages = {1593--1609},
}

@book{institute_of_electrical_and_electronics_engineers_2013_2013,
	address = {Piscataway, NJ},
	title = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013): {Olomouc}, {Czech} {Republic}, 8 - 12 {December} 2013},
	isbn = {978-1-4799-2756-2 978-1-4799-2757-9},
	shorttitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013)},
	language = {eng},
	publisher = {IEEE},
	editor = {Institute of Electrical {and} Electronics Engineers},
	year = {2013},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\K3IX7I3E\\Institute of Electrical and Electronics Engineers - 2013 - 2013 IEEE Workshop on Automatic Speech Recognition.pdf:application/pdf},
}

@misc{noauthor_urdu_nodate,
	title = {Urdu {Speech} {Dataset}},
	url = {https://www.kaggle.com/datasets/hazrat/urdu-speech-dataset},
	abstract = {2,500 Urdu audio samples},
	language = {en},
	urldate = {2022-08-22},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\G6KKGYW6\\urdu-speech-dataset.html:text/html},
}

@misc{noauthor_gramvaani_hindi_asrkaldiasr_nodate,
	title = {gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	url = {https://github.com/anish9208/gramvaani_hindi_asr},
	abstract = {This repo contains the baseline model recipes and pre-trained model for GramVanni hindi ASR challenge  - gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\MXCR2LTK\\asr.html:text/html},
}

@inproceedings{peinl_open_2020,
	address = {Deggendorf, Germany},
	title = {Open {Source} {Speech} {Recognition} on {Edge} {Devices}},
	isbn = {978-1-72816-759-6 978-1-72816-760-2},
	url = {https://ieeexplore.ieee.org/document/9208978/},
	doi = {10.1109/ACIT49673.2020.9208978},
	urldate = {2022-08-22},
	booktitle = {2020 10th {International} {Conference} on {Advanced} {Computer} {Information} {Technologies} ({ACIT})},
	publisher = {IEEE},
	author = {Peinl, Rene and Rizk, Basem and Szabad, Robert},
	month = sep,
	year = {2020},
	pages = {441--445},
}

@inproceedings{christian_gaida_comparing_2014,
	title = {Comparing {Open}-{Source} {Speech} {Recognition} {Toolkits}},
	author = {Christian Gaida and P. Lange and Rico Petrick and Patrick Proba and Ahmed Malatawy and David Suendermann-Oeft},
	year = {2014},
}

@misc{noauthor_tdnn_nodate,
	title = {{TDNN} --{\textgreater} {CNN}},
	url = {https://groups.google.com/g/kaldi-help/c/jsg1Oo4bNGQ/m/uwvFw5PtBwAJ},
	urldate = {2022-08-22},
	file = {TDNN --> CNN:C\:\\Users\\DELL\\Zotero\\storage\\X6FFGDN4\\uwvFw5PtBwAJ.html:text/html},
}

@inproceedings{kreyssig_improved_2018,
	address = {Calgary, AB},
	title = {Improved {Tdnns} {Using} {Deep} {Kernels} and {Frequency} {Dependent} {Grid}-{RNNS}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462523/},
	doi = {10.1109/ICASSP.2018.8462523},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kreyssig, F. L. and Zhang, C. and Woodland, P. C.},
	month = apr,
	year = {2018},
	pages = {4864--4868},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\MDGVZMYB\\Kreyssig et al. - 2018 - Improved Tdnns Using Deep Kernels and Frequency De.pdf:application/pdf},
}

@inproceedings{biswas_semi-supervised_2019,
	title = {Semi-{Supervised} {Acoustic} {Model} {Training} for {Five}-{Lingual} {Code}-{Switched} {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html},
	doi = {10.21437/Interspeech.2019-1325},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Biswas, Astik and Yılmaz, Emre and Wet, Febe de and Westhuizen, Ewald van der and Niesler, Thomas},
	month = sep,
	year = {2019},
	pages = {3745--3749},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5DDQYLKT\\Biswas et al. - 2019 - Semi-Supervised Acoustic Model Training for Five-L.pdf:application/pdf},
}

@inproceedings{zorila_investigation_2019,
	address = {SG, Singapore},
	title = {An {Investigation} into the {Effectiveness} of {Enhancement} in {ASR} {Training} and {Test} for {Chime}-5 {Dinner} {Party} {Transcription}},
	isbn = {978-1-72810-306-8},
	url = {https://ieeexplore.ieee.org/document/9003785/},
	doi = {10.1109/ASRU46091.2019.9003785},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Zorila, Catalin and Boeddeker, Christoph and Doddipatla, Rama and Haeb-Umbach, Reinhold},
	month = dec,
	year = {2019},
	pages = {47--53},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\IE9HPYPR\\Zorila et al. - 2019 - An Investigation into the Effectiveness of Enhance.pdf:application/pdf},
}

@article{abdel-hamid_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Speech} {Recognition}},
	volume = {22},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/6857341/},
	doi = {10.1109/TASLP.2014.2339736},
	number = {10},
	urldate = {2022-08-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
	month = oct,
	year = {2014},
	pages = {1533--1545},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\MQX94C3D\\Abdel-Hamid et al. - 2014 - Convolutional Neural Networks for Speech Recogniti.pdf:application/pdf},
}

@article{eeckt_continual_2021,
	title = {Continual {Learning} for {Monolingual} {End}-to-{End} {Automatic} {Speech} {Recognition}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2112.09427},
	doi = {10.48550/ARXIV.2112.09427},
	abstract = {Adapting Automatic Speech Recognition (ASR) models to new domains results in a deterioration of performance on the original domain(s), a phenomenon called Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to new accents, dialects, topics, etc. without suffering from CF, making them unable to be continually enhanced without storing all past data. Fortunately, Continual Learning (CL) methods, which aim to enable continual adaptation while overcoming CF, can be used. In this paper, we implement an extensive number of CL methods for End-to-End ASR and test and compare their ability to extend a monolingual Hybrid CTC-Transformer model across four new tasks. We find that the best performing CL method closes the gap between the fine-tuned model (lower bound) and the model trained jointly on all tasks (upper bound) by more than 40\%, while requiring access to only 0.6\% of the original data.},
	urldate = {2022-08-22},
	author = {Eeckt, Steven Vander and Van hamme, Hugo},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering},
	annote = {Other
Accepted at EUSIPCO 2022. 5 pages, 1 figure},
}

@article{ali_automatic_2015-1,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-22},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@inproceedings{shaik_riyaz_automatic_2019,
	title = {Automatic {Speaker} {Recognition} {System} in {Urdu} using {MFCC} {\textbackslash}\& {HMM}},
	author = {Shaik Riyaz and Bathula Lakshmi Bhavani and S. Venkatrama Phani Kumar},
	year = {2019},
}

@misc{noauthor_federated_nodate,
	title = {Federated {Learning}: {Collaborative} {Machine} {Learning} without {Centralized} {Training} {Data}},
	shorttitle = {Federated {Learning}},
	url = {http://ai.googleblog.com/2017/04/federated-learning-collaborative.html},
	abstract = {Posted by Brendan McMahan and Daniel Ramage, Research Scientists Standard machine learning approaches require centralizing the training data...},
	language = {en},
	urldate = {2022-08-22},
	journal = {Google AI Blog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\CPB7S2RU\\federated-learning-collaborative.html:text/html},
}

@inproceedings{andreas_stolcke_srilm_2002,
	title = {{SRILM} -- {An} xtensible language modeling toolkit},
	url = {https://www.sri.com/platform/srilm/},
	author = {Andreas Stolcke},
	year = {2002},
}

@inproceedings{ali_complete_2014,
	address = {South Lake Tahoe, NV, USA},
	title = {A complete {KALDI} recipe for building {Arabic} speech recognition systems},
	isbn = {978-1-4799-7129-9},
	url = {http://ieeexplore.ieee.org/document/7078629/},
	doi = {10.1109/SLT.2014.7078629},
	urldate = {2022-08-23},
	booktitle = {2014 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Ali, Ahmed and Zhang, Yifan and Cardinal, Patrick and Dahak, Najim and Vogel, Stephan and Glass, James},
	month = dec,
	year = {2014},
	pages = {525--529},
}

@article{amodei_deep_2015-1,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech} 2},
	url = {https://arxiv.org/abs/1512.02595},
	doi = {10.48550/ARXIV.1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-23},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{alhanai_development_2016,
	address = {San Diego, CA},
	title = {Development of the {MIT} {ASR} system for the 2016 {Arabic} {Multi}-genre {Broadcast} {Challenge}},
	isbn = {978-1-5090-4903-5},
	url = {http://ieeexplore.ieee.org/document/7846280/},
	doi = {10.1109/SLT.2016.7846280},
	urldate = {2022-08-23},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {AlHanai, Tuka and Hsu, Wei-Ning and Glass, James},
	month = dec,
	year = {2016},
	pages = {299--304},
}

@phdthesis{meyerjosh_multi-task_2019,
	title = {Multi-{Task} and {Transfer} {Learning} in {Low}-{Resource} {Speech} {Recognition}},
	copyright = {Pro Quest LLC},
	url = {https://www.proquest.com/openview/0d416c7a0cb00a3f6069c467ad545db5/1?pq-origsite=gscholar&cbl=18750&diss=y},
	abstract = {This thesis investigates methods for Acoustic Modeling in Automatic Speech Recognition, assuming limited access to training data in the target domain. The Acoustic
Models of interest are Deep Neural Network Acoustic Models (in both the Hybrid
and End-to-End approaches), and the target domains in question are either different
languages or different speakers. Inductive bias is transfered from a source domain
during training, via Multi-Task Learning or Transfer Learning.
With regards to Multi-Task Learning, Chapter (5) presents experiments which
explicitly incorporate linguistic knowledge (i.e. phonetics and phonology) into an
auxiliary task during neural Acoustic Model training. In Chapter (6), I investigate
Multi-Task methods which do not rely on expert knowledge (linguistic or otherwise),
by re-using existing parts of the Hybrid training pipeline. In Chapter (7), new tasks
are discovered using unsupervised learning. In Chapter (8), using the “copy-paste”
Transfer Learning approach, I demonstrate that with an appropriate early-stopping
criteria, cross-lingual transfer is possible to both large and small target datasets.
The methods and intuitions which rely on linguistic knowledge are of interest to
the Speech Recognition practitioner working in low-resource domains. These same
sections may be of interest to the theoretical linguist, as a study of the relative import
of phonetic categories in classification. To the Machine Learning practitioner, I hope
to offer approaches which can be easily ported over to other classification tasks. To
the Machine Learning researcher, I hope to inspire new ideas on addressing the small
data problem.},
	language = {en},
	urldate = {2022-08-23},
	school = {University of Arizona},
	author = {Meyer,Josh},
	year = {2019},
	note = {http://jrmeyer.github.io/misc/MEYER\_dissertation\_2019.pdf
Published by Pro Quest LLC},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\IWJEWKS8\\1.html:text/html},
}

@book{international_speech_communication_association_speech_2016,
	address = {Red Hook, NY},
	title = {Speech beyond speech towards a better understanding of the most important biosignal: 16th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({INTERSPEECH} 2015): {Dresden}, {Germany}, 6-10 {September} 2015. {Volume} 5},
	volume = {5},
	isbn = {978-1-5108-1790-6},
	shorttitle = {Speech beyond speech towards a better understanding of the most important biosignal},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {International Speech Communication Association},
	year = {2016},
	note = {Meeting Name: International Speech Communication Association},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\QHEKNZCX\\International Speech Communication Association - 2016 - Speech beyond speech towards a better understandin.pdf:application/pdf},
}

@inproceedings{shaik_improvements_2015,
	title = {Improvements in {RWTH} {LVCSR} evaluation systems for {Polish}, {Portuguese}, {English}, urdu, and {Arabic}},
	url = {https://www.isca-speech.org/archive/interspeech_2015/shaik15_interspeech.html},
	doi = {10.21437/Interspeech.2015-635},
	language = {en},
	urldate = {2022-08-23},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Shaik, M. Ali Basha and Tüske, Zoltán and Tahir, M. Ali and Nußbaum-Thom, Markus and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2015},
	pages = {3154--3158},
}

@article{kumar_large-vocabulary_2004,
	title = {A large-vocabulary continuous speech recognition system for {Hindi}},
	volume = {48},
	issn = {0018-8646, 0018-8646},
	url = {http://ieeexplore.ieee.org/document/5388836/},
	doi = {10.1147/rd.485.0703},
	number = {5.6},
	urldate = {2022-08-23},
	journal = {IBM Journal of Research and Development},
	author = {Kumar, M. and Rajput, N. and Verma, A.},
	month = sep,
	year = {2004},
	pages = {703--715},
}

@article{ming_speech_2017,
	title = {Speech {Enhancement} {Based} on {Full}-{Sentence} {Correlation} and {Clean} {Speech} {Recognition}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7814226/},
	doi = {10.1109/TASLP.2017.2651406},
	number = {3},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Ming, Ji and Crookes, Danny},
	month = mar,
	year = {2017},
	pages = {531--543},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\FKM8TYUH\\Ming and Crookes - 2017 - Speech Enhancement Based on Full-Sentence Correlat.pdf:application/pdf},
}

@article{ganapathy_multivariate_2017,
	title = {Multivariate {Autoregressive} {Spectrogram} {Modeling} for {Noisy} {Speech} {Recognition}},
	volume = {24},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7973047/},
	doi = {10.1109/LSP.2017.2724561},
	number = {9},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Ganapathy, Sriram},
	month = sep,
	year = {2017},
	pages = {1373--1377},
}

@article{lee_dnn-based_2016,
	title = {{DNN}-{Based} {Feature} {Enhancement} {Using} {DOA}-{Constrained} {ICA} for {Robust} {Speech} {Recognition}},
	volume = {23},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7497454/},
	doi = {10.1109/LSP.2016.2583658},
	number = {8},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Lee, Ho-Yong and Cho, Ji-Won and Kim, Minook and Park, Hyung-Min},
	month = aug,
	year = {2016},
	pages = {1091--1095},
}

@article{lee_threshold-based_2018,
	title = {Threshold-{Based} {Noise} {Detection} and {Reduction} for {Automatic} {Speech} {Recognition} {System} in {Human}-{Robot} {Interactions}},
	volume = {18},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/18/7/2068},
	doi = {10.3390/s18072068},
	language = {en},
	number = {7},
	urldate = {2022-08-23},
	journal = {Sensors},
	author = {Lee, Sheng-Chieh and Wang, Jhing-Fa and Chen, Miao-Hia},
	month = jun,
	year = {2018},
	pages = {2068},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TB36Q4AT\\Lee et al. - 2018 - Threshold-Based Noise Detection and Reduction for .pdf:application/pdf},
}

@article{gosztolya_domain_2016,
	title = {Domain {Adaptation} of {Deep} {Neural} {Networks} for {Automatic} {Speech} {Recognition} via {Wireless} {Sensors}},
	volume = {67},
	issn = {1339-309X},
	url = {https://www.sciendo.com/article/10.1515/jee-2016-0017},
	doi = {10.1515/jee-2016-0017},
	abstract = {Abstract
            Wireless sensors are recent, portable, low-powered devices, designed to record and transmit observations of their environment such as speech. To allow portability they are designed to have a small size and weight; this, however, along with their low power consumption, usually means that they have only quite basic recording equipment (e.g. microphone) installed. Recent speech technology applications typically require several dozen hours of audio recordings (nowadays even hundreds of hours is common), which is usually not available as recorded material by such sensors. Since systems trained with studio-level utterances tend to perform suboptimally for such recordings, a sensible idea is to adapt models which were trained on existing, larger, noise-free corpora. In this study, we experimented with adapting Deep Neural Network-based acoustic models trained on noise-free speech data to perform speech recognition on utterances recorded by wireless sensors. In the end, we were able to achieve a 5\% gain in terms of relative error reduction compared to training only on the sensor-recorded, restricted utterance subset.},
	language = {en},
	number = {2},
	urldate = {2022-08-23},
	journal = {Journal of Electrical Engineering},
	author = {Gosztolya, Gábor and Grósz, Tamás},
	month = apr,
	year = {2016},
	pages = {124--130},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CSYDKLJL\\Gosztolya and Grósz - 2016 - Domain Adaptation of Deep Neural Networks for Auto.pdf:application/pdf},
}

@article{chen_progressive_2018,
	title = {Progressive {Joint} {Modeling} in {Unsupervised} {Single}-{Channel} {Overlapped} {Speech} {Recognition}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/8080252/},
	doi = {10.1109/TASLP.2017.2765834},
	number = {1},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Chen, Zhehuai and Droppo, Jasha and Li, Jinyu and Xiong, Wayne},
	month = jan,
	year = {2018},
	pages = {184--196},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U75U9AZK\\Chen et al. - 2018 - Progressive Joint Modeling in Unsupervised Single-.pdf:application/pdf},
}

@misc{dautume_episodic_2019,
	title = {Episodic {Memory} in {Lifelong} {Language} {Learning}},
	url = {http://arxiv.org/abs/1906.01076},
	abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\textasciitilde}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {d'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
	month = nov,
	year = {2019},
	note = {arXiv:1906.01076 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Proceedings of NeurIPS 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\F68HIZ99\\d'Autume et al. - 2019 - Episodic Memory in Lifelong Language Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\U9A7WITC\\1906.html:text/html},
}

@misc{chang_towards_2021,
	title = {Towards {Lifelong} {Learning} of {End}-to-end {ASR}},
	url = {http://arxiv.org/abs/2104.01616},
	abstract = {Automatic speech recognition (ASR) technologies today are primarily optimized for given datasets; thus, any changes in the application environment (e.g., acoustic conditions or topic domains) may inevitably degrade the performance. We can collect new data describing the new environment and fine-tune the system, but this naturally leads to higher error rates for the earlier datasets, referred to as catastrophic forgetting. The concept of lifelong learning (LLL) aiming to enable a machine to sequentially learn new tasks from new datasets describing the changing real world without forgetting the previously learned knowledge is thus brought to attention. This paper reports, to our knowledge, the first effort to extensively consider and analyze the use of various approaches of LLL in end-to-end (E2E) ASR, including proposing novel methods in saving data for past domains to mitigate the catastrophic forgetting problem. An overall relative reduction of 28.7\% in WER was achieved compared to the fine-tuning baseline when sequentially learning on three very different benchmark corpora. This can be the first step toward the highly desired ASR technologies capable of synchronizing with the continuously changing real world.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Chang, Heng-Jui and Lee, Hung-yi and Lee, Lin-shan},
	month = jul,
	year = {2021},
	note = {arXiv:2104.01616 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Interspeech 2021. We acknowledge the support of Salesforce Research Deep Learning Grant},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\NZP72WVC\\Chang et al. - 2021 - Towards Lifelong Learning of End-to-end ASR.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\PTMWQGE5\\2104.html:text/html},
}

@misc{yang_online_2022,
	title = {Online {Continual} {Learning} of {End}-to-{End} {Speech} {Recognition} {Models}},
	url = {http://arxiv.org/abs/2207.05071},
	abstract = {Continual Learning, also known as Lifelong Learning, aims to continually learn from new data as it becomes available. While prior research on continual learning in automatic speech recognition has focused on the adaptation of models across multiple different speech recognition tasks, in this paper we propose an experimental setting for {\textbackslash}textit\{online continual learning\} for automatic speech recognition of a single task. Specifically focusing on the case where additional training data for the same task becomes available incrementally over time, we demonstrate the effectiveness of performing incremental model updates to end-to-end speech recognition models with an online Gradient Episodic Memory (GEM) method. Moreover, we show that with online continual learning and a selective sampling strategy, we can maintain an accuracy that is similar to retraining a model from scratch while requiring significantly lower computation costs. We have also verified our method with self-supervised learning (SSL) features.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Yang, Muqiao and Lane, Ian and Watanabe, Shinji},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05071 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at InterSpeech 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D5PLRPCB\\Yang et al. - 2022 - Online Continual Learning of End-to-End Speech Rec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Q2KHNKDV\\2207.html:text/html},
}

@article{kumar_leveraging_2020,
	title = {Leveraging {Linguistic} {Context} in {Dyadic} {Interactions} to {Improve} {Automatic} {Speech} {Recognition} for {Children}},
	volume = {63},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300346},
	doi = {10.1016/j.csl.2020.101101},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Lyon, Thomas D. and Narayanan, Shrikanth},
	month = sep,
	year = {2020},
	pages = {101101},
}

@article{pironkov_hybrid-task_2020,
	title = {Hybrid-task learning for robust automatic speech recognition},
	volume = {64},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S088523082030036X},
	doi = {10.1016/j.csl.2020.101103},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Pironkov, Gueorgui and Wood, Sean UN and Dupont, Stéphane},
	month = nov,
	year = {2020},
	pages = {101103},
}

@article{wang_wavenet_2020,
	title = {{WaveNet} {With} {Cross}-{Attention} for {Audiovisual} {Speech} {Recognition}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9197622/},
	doi = {10.1109/ACCESS.2020.3024218},
	urldate = {2022-08-23},
	journal = {IEEE Access},
	author = {Wang, Hui and Gao, Fei and Zhao, Yue and Wu, Licheng},
	year = {2020},
	pages = {169160--169168},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\FRECEY55\\Wang et al. - 2020 - WaveNet With Cross-Attention for Audiovisual Speec.pdf:application/pdf},
}

@inproceedings{sarfraz_huda_speech_2016,
	address = {Khatmandu, Nepal},
	title = {Speech {Corpus} {Development} for a {Speaker} {Independent} {Spontaneous} {Urdu} {Speech} {Recognition} {System}},
	booktitle = {Proceedings of the {O}-{COCOSDA}},
	author = {Sarfraz, Huda and Hussain, Sarmad and Bokhari, Riffat and Agha, Ali and Agha Ali Raza and Inamullah and Sarfaraz, Zahid and Parvez, Sophia and Mustafa, Asad and Javed, Iqra and Parveen, Raheela},
	month = mar,
	year = {2016},
}

@misc{mozilla_deep_nodate,
	title = {Deep {Speech} {Documentation}},
	url = {https://deepspeech.readthedocs.io/en/r0.9/?badge=latest},
	urldate = {2022-08-23},
	author = {Mozilla},
}

@inproceedings{wang_application_2015,
	address = {Beijing},
	title = {The {Application} of {Data} {Mining} {Technology} for the {Judgment} of {Poisoning} {Cases}},
	isbn = {978-1-4673-7211-4},
	url = {https://ieeexplore.ieee.org/document/7518465/},
	doi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.284},
	urldate = {2022-08-24},
	booktitle = {2015 {IEEE} 12th {Intl} {Conf} on {Ubiquitous} {Intelligence} and {Computing} and 2015 {IEEE} 12th {Intl} {Conf} on {Autonomic} and {Trusted} {Computing} and 2015 {IEEE} 15th {Intl} {Conf} on {Scalable} {Computing} and {Communications} and {Its} {Associated} {Workshops} ({UIC}-{ATC}-{ScalCom})},
	publisher = {IEEE},
	author = {Wang, Jiong and Zhang, Yunfeng and Wang, Fanglin and Gao, Bin},
	month = aug,
	year = {2015},
	pages = {1567--1571},
}

@article{zhao_data_2022,
	title = {Data {Poisoning} {Attacks} and {Defenses} in {Dynamic} {Crowdsourcing} with {Online} {Data} {Quality} {Learning}},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/9640529/},
	doi = {10.1109/TMC.2021.3133365},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Zhao, Yuxi and Gong, Xiaowen and Lin, Fuhong and Chen, Xu},
	year = {2022},
	pages = {1--1},
}

@inproceedings{hu_data_2020,
	title = {Data {Poisoning} on {Deep} {Learning} {Models}},
	doi = {10.1109/CSCI51800.2020.00111},
	abstract = {Deep learning is a form of artificial intelligence (AI) that has seen rapid development and deployment in computer software as a means to implementing AI functionality with greater efficiency and ease as compared to other alternative AI solutions, with usage seen in systems varying from search and recommendation engines to autonomous vehicles. With the demand for deep learning algorithms that can perform increasingly complex tasks in a shorter time frame growing at an exponential pace, the developments in the efficiency and productivity of algorithms has far outpaced that of the security of such algorithms, drawing concerns over the many unaddressed vulnerabilities that may be exploited to compromise the integrity of these software. This study investigated the ability of poisoning attacks, a form of attack targeting the vulnerability of deep learning training data, to compromise the integrity of a deep learning model's classificational functionality. Experimentation involved the processing of training data sets with varying deep learning models and the incremental introduction of poisoned data sets to view the efficacy of a poisoning attack under multiple circumstances and correlate such with aspects of the model's design conditions. Analysis of results showed evidence of a decrease of classificational ability correlating with an increase of poison percentage in the training data sets, but the scale of which the decrease occurred varied with the specified parameters in the model design. Based on this, it was concluded that poisoning can provide varying levels of damage to deep learning classificational ability depending on the parameters utilized in the model design, and methods to countermeasure such were proposed, such as increasing epoch count, implementing mechanisms bolstering model fit, and integrating input level filtration systems.},
	booktitle = {2020 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Hu, Charles and Hu, Yen-Hung Frank},
	month = dec,
	year = {2020},
	keywords = {Software, Computational modeling, Data models, artificial intelligence, machine learning, data poisoning, deep learning, Deep learning, Software algorithms, Toxicology, Training data},
	pages = {628--632},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\EU5RHS82\\9457922.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\2MNURGVH\\Hu and Hu - 2020 - Data Poisoning on Deep Learning Models.pdf:application/pdf},
}

@inproceedings{uprety_mitigating_2021,
	title = {Mitigating {Poisoning} {Attack} in {Federated} {Learning}},
	doi = {10.1109/SSCI50451.2021.9659839},
	abstract = {Adversarial machine learning (AML) has emerged as one of the significant research areas in machine learning (ML) because models we train lack robustness and trustworthiness. Federated learning (FL) trains models over distributed devices and model parameters are shared instead of actual data in a privacy-preserving manner. Unfortunately, FL is also vulnerable to attacks including parameter/data poisoning attacks. In this paper, we first analyze the impact of the data poisoning attack on this training method with a label-flipping attack. We propose a poisoning attack mitigation technique based on the reputation of nodes' involved in the training process. The reputation score for each client is calculated using the beta probability distribution method. This is the first work to show the removal of malicious nodes with the poisoned dataset from the training environment based on the calculated reputation score. The improvement in model performance after filtering malicious nodes is validated using the benchmark MNIST dataset. At the same time, our work contributes to preventing denial of service attacks by considering a blockchain-based server network. Our results hold for two different attack settings with different proportions of poisoned data samples.},
	booktitle = {2021 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Uprety, Aashma and Rawat, Danda B.},
	month = dec,
	year = {2021},
	keywords = {Computational modeling, Training, Data models, Collaborative work, Data poisoning attack, Data privacy, Distance learning, Filtering, reputation model, secure federated learning},
	pages = {01--07},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PM3PPZV8\\Uprety and Rawat - 2021 - Mitigating Poisoning Attack in Federated Learning.pdf:application/pdf},
}

@inproceedings{seetharaman_influence_2022,
	title = {Influence {Based} {Defense} {Against} {Data} {Poisoning} {Attacks} in {Online} {Learning}},
	doi = {10.1109/COMSNETS53615.2022.9668557},
	abstract = {Data poisoning is a type of adversarial attack on training data where an attacker manipulates a fraction of data to degrade the performance of machine learning model. There are several known defensive mechanisms for handling offline attacks, however defensive measures for online learning, where data points arrive sequentially, have not garnered similar interest. In this work, we propose a defense mechanism to minimize the degradation caused by the poisoned training data on a learner's model in an online setup. Our proposed method utilizes an influence function which is a classic technique in robust statistics. Further, we supplement it with the existing data sanitization methods for filtering out some of the poisoned data points. We study the effectiveness of our defense mechanism on multiple datasets and across multiple attack strategies against an online learner.},
	booktitle = {2022 14th {International} {Conference} on {COMmunication} {Systems} \& {NETworkS} ({COMSNETS})},
	author = {Seetharaman, Sanjay and Malaviya, Shubham and Vasu, Rosni and Shukla, Manish and Lodha, Sachin},
	month = jan,
	year = {2022},
	note = {ISSN: 2155-2509},
	keywords = {Data models, Data integrity, Training data, Filtering, Adversarial Machine Learning, Data Poisoning, Degradation, Influence Function, Linear programming, Machine learning, Online Learning},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\QI6QPWYV\\Seetharaman et al. - 2022 - Influence Based Defense Against Data Poisoning Att.pdf:application/pdf},
}

@article{zhao_garbage_2021,
	title = {Garbage {In}, {Garbage} {Out}: {Poisoning} {Attacks} {Disguised} {With} {Plausible} {Mobility} in {Data} {Aggregation}},
	volume = {8},
	issn = {2327-4697, 2334-329X},
	shorttitle = {Garbage {In}, {Garbage} {Out}},
	url = {https://ieeexplore.ieee.org/document/9511094/},
	doi = {10.1109/TNSE.2021.3103919},
	number = {3},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Zhao, Ping and Jiang, Hongbo and Li, Jie and Xiao, Zhu and Liu, Daibo and Ren, Ju and Guo, Deke},
	month = jul,
	year = {2021},
	pages = {2679--2693},
}


@article{da_silva_quality_2008,
	title = {Quality assessment of interactive voice applications},
	volume = {52},
	issn = {13891286},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128608000042},
	doi = {10.1016/j.comnet.2008.01.002},
	language = {en},
	number = {6},
	urldate = {2023-02-22},
	journal = {Computer Networks},
	author = {da Silva, Ana Paula Couto and Varela, Martín and de Souza e Silva, Edmundo and Leão, Rosa M.M. and Rubino, Gerardo},
	month = apr,
	year = {2008},
	pages = {1179--1192},
}

@inproceedings{franci_influence-driven_2022,
	title = {Influence-{Driven} {Data} {Poisoning} in {Graph}-{Based} {Semi}-{Supervised} {Classifiers}},
	abstract = {Graph-based Semi-Supervised Learning (GSSL) is a practical solution to learn from a limited amount of labelled data together with a vast amount of unlabelled data. However, due to their reliance on the known labels to infer the unknown labels, these algorithms are sensitive to data quality. It is therefore essential to study the potential threats related to the labelled data, more specifically, label poisoning. In this paper, we propose a novel data poisoning method which efficiently approximates the result of label inference to identify the inputs which, if poisoned, would produce the highest number of incorrectly inferred labels. We extensively evaluate our approach on three classification problems under 24 different experimental settings each. Compared to the state of the art, our influence-driven attack produces an average increase of error rate 50\% higher, while being faster by multiple orders of magnitude. Moreover, our method can inform engineers of inputs that deserve investigation (relabelling them) before training the learning model. We show that relabelling one-third of the poisoned inputs (selected based on their influence) reduces the poisoning effect by 50\%. ACM Reference Format: Adriano Franci, Maxime Cordy, Martin Gubri, Mike Papadakis, and Yves Le Traon. 2022. Influence-Driven Data Poisoning in Graph-Based Semi-Supervised Classifiers. In 1st Conference on AI Engineering - Software Engineering for AI (CAIN’22), May 16–24, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3522664.3528606},
	booktitle = {2022 {IEEE}/{ACM} 1st {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Franci, Adriano and Cordy, Maxime and Gubri, Martin and Papadakis, Mike and Traon, Yves Le},
	month = may,
	year = {2022},
	keywords = {Training, Measurement, Data integrity, data poisoning, Machine learning, Approximation algorithms, Error analysis, Inference algorithms, semi-supervised learning, Semisupervised learning},
	pages = {77--87},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\J9JGMGKF\\Franci et al. - 2022 - Influence-Driven Data Poisoning in Graph-Based Sem.pdf:application/pdf},
}

@article{zhang_poisongan_2021,
	title = {{PoisonGAN}: {Generative} {Poisoning} {Attacks} {Against} {Federated} {Learning} in {Edge} {Computing} {Systems}},
	volume = {8},
	issn = {2327-4662, 2372-2541},
	shorttitle = {{PoisonGAN}},
	url = {https://ieeexplore.ieee.org/document/9194010/},
	doi = {10.1109/JIOT.2020.3023126},
	number = {5},
	urldate = {2022-08-24},
	journal = {IEEE Internet of Things Journal},
	author = {Zhang, Jiale and Chen, Bing and Cheng, Xiang and Binh, Huynh Thi Thanh and Yu, Shui},
	month = mar,
	year = {2021},
	pages = {3310--3322},
}

@inproceedings{doku_mitigating_2021,
	address = {Las Vegas, NV, USA},
	title = {Mitigating {Data} {Poisoning} {Attacks} {On} a {Federated} {Learning}-{Edge} {Computing} {Network}},
	isbn = {978-1-72819-794-4},
	url = {https://ieeexplore.ieee.org/document/9369581/},
	doi = {10.1109/CCNC49032.2021.9369581},
	urldate = {2022-08-24},
	booktitle = {2021 {IEEE} 18th {Annual} {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	publisher = {IEEE},
	author = {Doku, Ronald and Rawat, Danda B.},
	month = jan,
	year = {2021},
	pages = {1--6},
}

@article{wen_great_2021,
	title = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}: {Efficient} {Poisoning} {Attacks} and {Defenses} for {Linear} {Regression} {Models}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}},
	url = {https://ieeexplore.ieee.org/document/9448089/},
	doi = {10.1109/TIFS.2021.3087332},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Wen, Jialin and Zhao, Benjamin Zi Hao and Xue, Minhui and Oprea, Alina and Qian, Haifeng},
	year = {2021},
	pages = {3709--3723},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\PWEN89AG\\Wen et al. - 2021 - With Great Dispersion Comes Greater Resilience Ef.pdf:application/pdf},
}

@article{chen_-pois_2021,
	title = {De-{Pois}: {An} {Attack}-{Agnostic} {Defense} against {Data} {Poisoning} {Attacks}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {De-{Pois}},
	url = {https://ieeexplore.ieee.org/document/9431105/},
	doi = {10.1109/TIFS.2021.3080522},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Chen, Jian and Zhang, Xuxin and Zhang, Rui and Wang, Chen and Liu, Ling},
	year = {2021},
	pages = {3412--3425},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\AK6SW628\\Chen et al. - 2021 - De-Pois An Attack-Agnostic Defense against Data P.pdf:application/pdf},
}

@inproceedings{kwon_selective_2019,
	address = {Sardinia, Italy},
	title = {Selective {Poisoning} {Attack} on {Deep} {Neural} {Network} to {Induce} {Fine}-{Grained} {Recognition} {Error}},
	isbn = {978-1-72811-488-0},
	url = {https://ieeexplore.ieee.org/document/8791700/},
	doi = {10.1109/AIKE.2019.00033},
	urldate = {2022-08-24},
	booktitle = {2019 {IEEE} {Second} {International} {Conference} on {Artificial} {Intelligence} and {Knowledge} {Engineering} ({AIKE})},
	publisher = {IEEE},
	author = {Kwon, Hyun and Yoon, Hyunsoo and Park, Ki-Woong},
	month = jun,
	year = {2019},
	pages = {136--139},
}

@inproceedings{kontopoulos_countering_2018,
	address = {Athens},
	title = {Countering {Real}-{Time} {Stream} {Poisoning}: {An} {Architecture} for {Detecting} {Vessel} {Spoofing} in {Streams} of {AIS} {Data}},
	isbn = {978-1-5386-7518-2},
	shorttitle = {Countering {Real}-{Time} {Stream} {Poisoning}},
	url = {https://ieeexplore.ieee.org/document/8512006/},
	doi = {10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00139},
	urldate = {2022-08-24},
	booktitle = {2018 {IEEE} 16th {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, 16th {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, 4th {Intl} {Conf} on {Big} {Data} {Intelligence} and {Computing} and {Cyber} {Science} and {Technology} {Congress}({DASC}/{PiCom}/{DataCom}/{CyberSciTech})},
	publisher = {IEEE},
	author = {Kontopoulos, Ioannis and Spiliopoulos, Giannis and Zissis, Dimitrios and Chatzikokolakis, Konstantinos and Artikis, Alexander},
	month = aug,
	year = {2018},
	pages = {981--986},
}

@misc{noauthor_adoption_nodate,
	title = {Adoption of {AI} advances, but foundational barriers remain {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/featured-insights/artificial-intelligence/ai-adoption-advances-but-foundational-barriers-remain},
	urldate = {2022-08-24},
	file = {Adoption of AI advances, but foundational barriers remain | McKinsey:C\:\\Users\\DELL\\Zotero\\storage\\7BXXNTS6\\ai-adoption-advances-but-foundational-barriers-remain.html:text/html},
}

@misc{noauthor_exclusive_nodate,
	title = {Exclusive: {What} is data poisoning and why should we be concerned? - {International} {Security} {Journal} ({ISJ})},
	shorttitle = {Exclusive},
	url = {https://internationalsecurityjournal.com/what-is-data-poisoning/},
	abstract = {Machine learning could be one of the most disruptive technologies the world has seen in decades. Virtually every industry can benefit from these artificial},
	language = {en-GB},
	urldate = {2022-08-24},
	note = {Section: AI \& Deep Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VYNPR2KK\\what-is-data-poisoning.html:text/html},
}

@misc{vincent_twitter_2016,
	title = {Twitter taught {Microsoft}’s friendly {AI} chatbot to be a racist asshole in less than a day},
	url = {https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist},
	abstract = {It took less than 24 hours for Twitter to corrupt an innocent AI chatbot. Yesterday, Microsoft unveiled Tay — a Twitter bot that the company described as an experiment in "conversational...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Vincent, James},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5ZCPBDM9\\tay-microsoft-chatbot-racist.html:text/html},
}

@misc{kastrenakes_microsoft_2016,
	title = {Microsoft made a chatbot that tweets like a teen},
	url = {https://www.theverge.com/2016/3/23/11290200/tay-ai-chatbot-released-microsoft},
	abstract = {Microsoft is trying to create AI that can pass for a teen. Its research team launched a chatbot this morning called Tay, which is meant to test and improve Microsoft's understanding of...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Kastrenakes, Jacob},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SJ6QZ3R3\\tay-ai-chatbot-released-microsoft.html:text/html},
}

@article{steinhardt_certified_2017,
	title = {Certified {Defenses} for {Data} {Poisoning} {Attacks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03691},
	doi = {10.48550/ARXIV.1706.03691},
	abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.},
	urldate = {2022-08-24},
	author = {Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
	annote = {Other
Appeared at NIPS 2017},
}

@article{newman_ai_nodate,
	title = {{AI} {Can} {Help} {Cybersecurity}—{If} {It} {Can} {Fight} {Through} the {Hype}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/ai-machine-learning-cybersecurity/},
	abstract = {There are a ton of claims around AI and cybersecurity that don't quite add up. Here's what's really going on.},
	language = {en-US},
	urldate = {2022-08-24},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {artificial intelligence, machine learning, ai, cybersecurity},
}

@misc{ilmoi_evasion_2019,
	title = {Evasion attacks on {Machine} {Learning} (or “{Adversarial} {Examples}”)},
	url = {https://towardsdatascience.com/evasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1},
	abstract = {Your ML model is easier to fool than you think},
	language = {en},
	urldate = {2022-08-24},
	journal = {Medium},
	author = {ilmoi},
	month = jul,
	year = {2019},
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	copyright = {Creative Commons Attribution 3.0 Unported},
	url = {https://arxiv.org/abs/1312.6199},
	doi = {10.48550/ARXIV.1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2022-08-24},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV)},
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1412.6572},
	doi = {10.48550/ARXIV.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2022-08-24},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{bursztein_attacks_nodate,
	title = {Attacks against machine learning — an overview},
	url = {https://elie.net/blog/ai/attacks-against-machine-learning-an-overview/},
	abstract = {This blog post surveys the attacks techniques that target AI (Artificial Intelligence) systems and how to protect against them.},
	language = {en},
	urldate = {2022-08-24},
	journal = {Elie Bursztein's site},
	author = {Bursztein, Elie},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P5FGBPU5\\attacks-against-machine-learning-an-overview.html:text/html},
}

@article{goh_comprehensive_2015,
	title = {Comprehensive {Literature} {Review} on {Machine} {Learning} {Structures} for {Web} {Spam} {Classification}},
	volume = {70},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915032330},
	doi = {10.1016/j.procs.2015.10.069},
	language = {en},
	urldate = {2022-08-24},
	journal = {Procedia Computer Science},
	author = {Goh, Kwang Leng and Singh, Ashutosh Kumar},
	year = {2015},
	pages = {434--441},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ZZY6ZDLK\\Goh and Singh - 2015 - Comprehensive Literature Review on Machine Learnin.pdf:application/pdf},
}

@article{jo_measuring_2017,
	title = {Measuring the tendency of {CNNs} to {Learn} {Surface} {Statistical} {Regularities}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.11561},
	doi = {10.48550/ARXIV.1711.11561},
	abstract = {Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset. We are concerned with the following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities. For the SVHN and CIFAR-10 datasets, we present two Fourier filtered variants: a low frequency variant and a randomly filtered variant. Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects. Our main finding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap across the various test sets. Moreover, we observe that significantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap. Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts.},
	urldate = {2022-08-24},
	author = {Jo, Jason and Bengio, Yoshua},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	annote = {Other
Submitted},
}

@inproceedings{florian_tramer_stealing_2016,
	address = {Austin, Texas, USA},
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	isbn = {978-1-931971-32-4},
	url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer},
	urldate = {2022-08-24},
	publisher = {USENIX Association},
	author = {Florian Tramer and Fan Zhang and Ari Juels and Michael K. Reiter and Thomas Ristenpart},
	month = aug,
	year = {2016},
	pages = {601--618},
	file = {Stealing Machine Learning Models via Prediction APIs | USENIX:C\:\\Users\\DELL\\Zotero\\storage\\KYVFLJL7\\tramer.html:text/html},
}

@misc{papernot_semi-supervised_2017,
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	url = {http://arxiv.org/abs/1610.05755},
	abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
	month = mar,
	year = {2017},
	note = {arXiv:1610.05755 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted to ICLR 17 as an oral},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\B2VJHRWF\\Papernot et al. - 2017 - Semi-supervised Knowledge Transfer for Deep Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\JK48PTLF\\1610.html:text/html},
}

@misc{papernot_scalable_2018,
	title = {Scalable {Private} {Learning} with {PATE}},
	url = {http://arxiv.org/abs/1802.08908},
	abstract = {The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a "student" model the knowledge of an ensemble of "teacher" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy (\${\textbackslash}varepsilon\$ {\textless} 1.0).},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Úlfar},
	month = feb,
	year = {2018},
	note = {arXiv:1802.08908 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Published as a conference paper at ICLR 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Y9TDCBWP\\Papernot et al. - 2018 - Scalable Private Learning with PATE.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L8FAK7HV\\1802.html:text/html},
}

@article{wang_poisoning_2022,
	title = {Poisoning attacks and countermeasures in intelligent networks: {Status} quo and prospects},
	volume = {8},
	issn = {23528648},
	shorttitle = {Poisoning attacks and countermeasures in intelligent networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S235286482100050X},
	doi = {10.1016/j.dcan.2021.07.009},
	language = {en},
	number = {2},
	urldate = {2022-08-24},
	journal = {Digital Communications and Networks},
	author = {Wang, Chen and Chen, Jian and Yang, Yang and Ma, Xiaoqiang and Liu, Jiangchuan},
	month = apr,
	year = {2022},
	pages = {225--234},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5924FJWC\\Wang et al. - 2022 - Poisoning attacks and countermeasures in intellige.pdf:application/pdf},
}

@misc{noauthor_google_nodate,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/emergency-response/},
	urldate = {2022-08-24},
	file = {Google - Site Reliability Engineering:C\:\\Users\\DELL\\Zotero\\storage\\27RWN7KZ\\emergency-response.html:text/html},
}

@misc{noauthor_google_nodate-1,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/managing-incidents/},
	urldate = {2022-08-24},
}

@misc{gaudesi_channelaugment_2021,
	title = {{ChannelAugment}: {Improving} generalization of multi-channel {ASR} by training with input channel randomization},
	shorttitle = {{ChannelAugment}},
	url = {http://arxiv.org/abs/2109.11225},
	abstract = {End-to-end (E2E) multi-channel ASR systems show state-of-the-art performance in far-field ASR tasks by joint training of a multi-channel front-end along with the ASR model. The main limitation of such systems is that they are usually trained with data from a fixed array geometry, which can lead to degradation in accuracy when a different array is used in testing. This makes it challenging to deploy these systems in practice, as it is costly to retrain and deploy different models for various array configurations. To address this, we present a simple and effective data augmentation technique, which is based on randomly dropping channels in the multi-channel audio input during training, in order to improve the robustness to various array configurations at test time. We call this technique ChannelAugment, in contrast to SpecAugment (SA) which drops time and/or frequency components of a single channel input audio. We apply ChannelAugment to the Spatial Filtering (SF) and Minimum Variance Distortionless Response (MVDR) neural beamforming approaches. For SF, we observe 10.6\% WER improvement across various array configurations employing different numbers of microphones. For MVDR, we achieve a 74\% reduction in training time without causing degradation of recognition accuracy.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Gaudesi, Marco and Weninger, Felix and Sharma, Dushyant and Zhan, Puming},
	month = sep,
	year = {2021},
	note = {arXiv:2109.11225 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	annote = {Comment: To appear in ASRU 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D2RMIMH8\\Gaudesi et al. - 2021 - ChannelAugment Improving generalization of multi-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4F2SMWT4\\2109.html:text/html},
}

@article{lin_ml_2021,
	title = {{ML} {Attack} {Models}: {Adversarial} {Attacks} and {Data} {Poisoning} {Attacks}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{ML} {Attack} {Models}},
	url = {https://arxiv.org/abs/2112.02797},
	doi = {10.48550/ARXIV.2112.02797},
	abstract = {Many state-of-the-art ML models have outperformed humans in various tasks such as image classification. With such outstanding performance, ML models are widely used today. However, the existence of adversarial attacks and data poisoning attacks really questions the robustness of ML models. For instance, Engstrom et al. demonstrated that state-of-the-art image classifiers could be easily fooled by a small rotation on an arbitrary image. As ML systems are being increasingly integrated into safety and security-sensitive applications, adversarial attacks and data poisoning attacks pose a considerable threat. This chapter focuses on the two broad and important areas of ML security: adversarial attacks and data poisoning attacks.},
	urldate = {2022-08-24},
	author = {Lin, Jing and Dang, Long and Rahouti, Mohamed and Xiong, Kaiqi},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
}

@misc{noauthor_why_nodate,
	title = {Why does kaldi require mono channel audio for training instead of stereo or surround?},
	url = {https://groups.google.com/g/kaldi-help/c/92-jEzqyNb4},
	urldate = {2022-08-24},
	file = {Why does kaldi require mono channel audio for training instead of stereo or surround?:C\:\\Users\\DELL\\Zotero\\storage\\8P3Y6TTD\\92-jEzqyNb4.html:text/html},
}



@book{international_speech_communication_association_speech_2016-1,
	address = {Red Hook, NY},
	title = {Speech beyond speech towards a better understanding of the most important biosignal: 16th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({INTERSPEECH} 2015): {Dresden}, {Germany}, 6-10 {September} 2015. {Volume} 5},
	isbn = {978-1-5108-1790-6},
	shorttitle = {Speech beyond speech towards a better understanding of the most important biosignal},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {International Speech Communication Association},
	year = {2016},
	note = {Meeting Name: International Speech Communication Association},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\22CXWDL9\\International Speech Communication Association - 2016 - Speech beyond speech towards a better understandin.pdf:application/pdf},
}

@article{kocon_offensive_2021,
	title = {Offensive, aggressive, and hate speech analysis: {From} data-centric to human-centered approach},
	volume = {58},
	issn = {03064573},
	shorttitle = {Offensive, aggressive, and hate speech analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457321001333},
	doi = {10.1016/j.ipm.2021.102643},
	language = {en},
	number = {5},
	urldate = {2022-08-26},
	journal = {Information Processing \& Management},
	author = {Kocoń, Jan and Figas, Alicja and Gruza, Marcin and Puchalska, Daria and Kajdanowicz, Tomasz and Kazienko, Przemysław},
	month = sep,
	year = {2021},
	pages = {102643},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\NSSERT5Y\\Kocoń et al. - 2021 - Offensive, aggressive, and hate speech analysis F.pdf:application/pdf},
}

@inproceedings{ghahremani_investigation_2017,
	address = {Okinawa},
	title = {Investigation of transfer learning for {ASR} using {LF}-{MMI} trained neural networks},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268947/},
	doi = {10.1109/ASRU.2017.8268947},
	urldate = {2022-09-04},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Ghahremani, Pegah and Manohar, Vimal and Hadian, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = dec,
	year = {2017},
	pages = {279--286},
}

@inproceedings{wallington_learning_2021,
	title = {On the {Learning} {Dynamics} of {Semi}-{Supervised} {Training} for {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/wallington21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1777},
	language = {en},
	urldate = {2022-09-04},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Wallington, Electra and Kershenbaum, Benji and Klejch, Ondřej and Bell, Peter},
	month = aug,
	year = {2021},
	pages = {716--720},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\F2TLM7QF\\Wallington et al. - 2021 - On the Learning Dynamics of Semi-Supervised Traini.pdf:application/pdf},
}

@inproceedings{sarkar_novel_2014,
	title = {A novel boosting algorithm for improved i-vector based speaker verification in noisy environments},
	booktitle = {Proceedings of the {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {INTERSPEECH}},
	author = {Sarkar, Sourjya and Rao, K.},
	month = sep,
	year = {2014},
}

@inproceedings{sarkar_study_2012,
	title = {Study of the {Effect} of {I}-vector {Modeling} on {Short} and {Mismatch} {Utterance} {Duration} for {Speaker} {Verification}},
	booktitle = {{INTERSPEECH}},
	author = {Sarkar, Achintya Kumar and Matrouf, Driss and Bousquet, Pierre-Michel and Bonastre, Jean-François},
	year = {2012},
}

@misc{zhang_uberi_speechrecognition_nodate-1,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-09-18},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4MV996BJ\\SpeechRecognition.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What {Is} {Automatic} {Speech} {Recognition}? - {Alexa} {Skills} {Kit} {Official} {Site}},
	shorttitle = {What {Is} {Automatic} {Speech} {Recognition}?},
	url = {https://developer.amazon.com/en-US/alexa/alexa-skills-kit/asr.html},
	abstract = {Automatic speech recognition (ASR) is technology that converts spoken words into text. Explore the topic of ASR and learn about building for voice.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Amazon (Alexa)},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\EA9BYMAB\\asr.html:text/html},
}

@misc{noauthor_speech--text_nodate,
	title = {Speech-to-{Text}: {Automatic} {Speech} {Recognition}},
	shorttitle = {Speech-to-{Text}},
	url = {https://cloud.google.com/speech-to-text},
	abstract = {Accurately convert voice to text in over 125 languages and variants by applying Google’s powerful machine learning models with an easy-to-use API.},
	language = {en},
	urldate = {2022-09-19},
	journal = {Google Cloud},
}

@misc{noauthor_siri_nodate,
	title = {Siri},
	url = {https://www.apple.com/siri/},
	abstract = {Siri is an easy way to make calls, send texts, use apps, and get things done with just your voice. And Siri is the most private intelligent assistant.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Apple},
}

@misc{noauthor_cortana_nodate,
	title = {Cortana - {Your} personal productivity assistant},
	url = {https://www.microsoft.com/en-us/cortana},
	abstract = {Cortana helps you achieve more with less effort. Your personal productivity assistant helps you stay on top of what matters, follow through, and do your best work.},
	language = {en-us},
	urldate = {2022-09-19},
	journal = {Cortana - Your personal productivity assistant},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\APLZ98WM\\cortana.html:text/html},
}

@misc{noauthor_jarvis_nodate,
	title = {Jarvis {\textbar} {NVIDIA} {NGC}},
	url = {https://catalog.ngc.nvidia.com/orgs/nvidia/collections/jarvis},
	abstract = {NVIDIA Jarvis is a framework for production-grade conversational AI inference. The Jarvis Collection on NGC includes all the resources required for getting started with Jarvis.},
	language = {en},
	urldate = {2022-09-19},
	journal = {NVIDIA NGC Catalog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8U4FAMTW\\jarvis.html:text/html},
}

@misc{noauthor_sox_nodate,
	title = {{SoX} - {Sound} {eXchange} {\textbar} {HomePage}},
	url = {http://sox.sourceforge.net/},
	urldate = {2022-09-19},
	file = {SoX - Sound eXchange | HomePage:C\:\\Users\\DELL\\Zotero\\storage\\Y5XJDGHX\\sox.sourceforge.net.html:text/html},
}

@misc{raj_note_nodate,
	title = {A note on {MFCCs} and delta features},
	url = {https://desh2608.github.io/2019-07-26-delta-feats/},
	abstract = {What are MFCCs and how are they computed? Feature extraction is the first step in any automatic speech recognition (ASR) pipeline. The objective is to compute features from speech waveforms which contain relevant information about the linguistic content of the speech, and ignore information about the background noise, emotions, etc....},
	language = {en},
	urldate = {2022-09-19},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\R4WGBL8Q\\2019-07-26-delta-feats.html:text/html},
}

@inproceedings{menne_analysis_2019,
	title = {Analysis of {Deep} {Clustering} as {Preprocessing} for {Automatic} {Speech} {Recognition} of {Sparsely} {Overlapping} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html},
	doi = {10.21437/Interspeech.2019-1728},
	language = {en},
	urldate = {2022-09-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Menne, Tobias and Sklyar, Ilya and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2019},
	pages = {2638--2642},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5X2I92JR\\Menne et al. - 2019 - Analysis of Deep Clustering as Preprocessing for A.pdf:application/pdf},
}

@article{li_tenet_2019,
	title = {{TEnet}: target speaker extraction network with accumulated speaker embedding for automatic speech recognition},
	volume = {55},
	issn = {0013-5194, 1350-911X},
	shorttitle = {{TEnet}},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/el.2019.1228},
	doi = {10.1049/el.2019.1228},
	language = {en},
	number = {14},
	urldate = {2022-09-19},
	journal = {Electronics Letters},
	author = {Li, Wenjie and Zhang, Pengyuan and Yan, Yonghong},
	month = jul,
	year = {2019},
	pages = {816--819},
}

@article{van_wyk_multivaluedness_2021,
	title = {Multivaluedness in {Networks}: {Shannon}’s {Noisy}-{Channel} {Coding} {Theorem}},
	volume = {68},
	issn = {1549-7747, 1558-3791},
	shorttitle = {Multivaluedness in {Networks}},
	url = {https://ieeexplore.ieee.org/document/9410598/},
	doi = {10.1109/TCSII.2021.3074925},
	number = {10},
	urldate = {2022-09-19},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {van Wyk, Michael Antonie and Ping, Li and Chen, Guanrong},
	month = oct,
	year = {2021},
	pages = {3234--3235},
}

@inproceedings{nautsch_gdpr_2019-2,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-09-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GPBKHSGG\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@inproceedings{mohamed_understanding_2012,
	address = {Kyoto, Japan},
	title = {Understanding how {Deep} {Belief} {Networks} perform acoustic modelling},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	url = {http://ieeexplore.ieee.org/document/6288863/},
	doi = {10.1109/ICASSP.2012.6288863},
	urldate = {2022-09-19},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Mohamed, Abdel-rahman and Hinton, Geoffrey and Penn, Gerald},
	month = mar,
	year = {2012},
	pages = {4273--4276},
}

@misc{shrawankar_adverse_2013,
	title = {Adverse {Conditions} and {ASR} {Techniques} for {Robust} {Speech} {User} {Interface}},
	url = {http://arxiv.org/abs/1303.5515},
	abstract = {The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.},
	urldate = {2022-09-19},
	publisher = {arXiv},
	author = {Shrawankar, Urmila and Thakare, V. M.},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	annote = {Comment: 10 pages 2 Tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Y9HZ2V8C\\Shrawankar and Thakare - 2013 - Adverse Conditions and ASR Techniques for Robust S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AWG8ADWP\\1303.html:text/html},
}

@article{benesty_springer_2009,
	title = {Springer {Handbook} of {Speech} {Processing}},
	volume = {126},
	issn = {00014966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/126/4/10.1121/1.3203918},
	doi = {10.1121/1.3203918},
	language = {en},
	number = {4},
	urldate = {2022-09-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Benesty, Jacob and Sondhi, Mohan M. and Huang, Yiteng and Greenberg, Steven},
	year = {2009},
	pages = {2130},
}



@article{hussein_arabic_2022,
	title = {Arabic speech recognition by end-to-end, modular systems and human},
	volume = {71},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230821000760},
	doi = {10.1016/j.csl.2021.101272},
	language = {en},
	urldate = {2022-10-02},
	journal = {Computer Speech \& Language},
	author = {Hussein, Amir and Watanabe, Shinji and Ali, Ahmed},
	month = jan,
	year = {2022},
	pages = {101272},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\WJXMYF47\\Hussein et al. - 2022 - Arabic speech recognition by end-to-end, modular s.pdf:application/pdf},
}

@article{dahl_context-dependent_2012,
	title = {Context-{Dependent} {Pre}-{Trained} {Deep} {Neural} {Networks} for {Large}-{Vocabulary} {Speech} {Recognition}},
	volume = {20},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5740583/},
	doi = {10.1109/TASL.2011.2134090},
	number = {1},
	urldate = {2022-10-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Dahl, G. E. and {Dong Yu} and {Li Deng} and Acero, A.},
	month = jan,
	year = {2012},
	pages = {30--42},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CT4BGLDU\\Dahl et al. - 2012 - Context-Dependent Pre-Trained Deep Neural Networks.pdf:application/pdf},
}

@article{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1303.5778},
	doi = {10.48550/ARXIV.1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates {\textbackslash}emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	urldate = {2022-10-02},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},
	annote = {Other
To appear in ICASSP 2013},
}

@article{hifny_unified_2015,
	title = {Unified {Acoustic} {Modeling} using {Deep} {Conditional} {Random} {Fields}},
	issn = {20547390},
	url = {http://scholarpublishing.org/index.php/TMLAI/article/view/1124},
	doi = {10.14738/tmlai.32.1124},
	urldate = {2022-10-02},
	journal = {Transactions on Machine Learning and Artificial Intelligence},
	author = {Hifny, Yasser},
	month = apr,
	year = {2015},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\V8U7GBTW\\Hifny - 2015 - Unified Acoustic Modeling using Deep Conditional R.pdf:application/pdf},
}

@inproceedings{povey_purely_2016,
	title = {Purely {Sequence}-{Trained} {Neural} {Networks} for {ASR} {Based} on {Lattice}-{Free} {MMI}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/povey16_interspeech.html},
	doi = {10.21437/Interspeech.2016-595},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Povey, Daniel and Peddinti, Vijayaditya and Galvez, Daniel and Ghahremani, Pegah and Manohar, Vimal and Na, Xingyu and Wang, Yiming and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {2751--2755},
}

@inproceedings{vijayaditya_time_2015,
	title = {A time delay neural network architecture for efficient modeling of long temporal contexts},
	url = {https://www.semanticscholar.org/paper/A-time-delay-neural-network-architecture-for-of-Peddinti-Povey/3a79ac688f2558b2d9693e434f010e041eba0fae},
	author = {Vijayaditya and Peddinti and Sanjeev Khudanpur and Daniel Povey},
	year = {2015},
}

@article{ali_speech_2017,
	title = {Speech {Recognition} {Challenge} in the {Wild}: {Arabic} {MGB}-3},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Speech {Recognition} {Challenge} in the {Wild}},
	url = {https://arxiv.org/abs/1709.07276},
	doi = {10.48550/ARXIV.1709.07276},
	abstract = {This paper describes the Arabic MGB-3 Challenge - Arabic Speech Recognition in the Wild. Unlike last year's Arabic MGB-2 Challenge, for which the recognition task was based on more than 1,200 hours broadcast TV news recordings from Aljazeera Arabic TV programs, MGB-3 emphasises dialectal Arabic using a multi-genre collection of Egyptian YouTube videos. Seven genres were used for the data collection: comedy, cooking, family/kids, fashion, drama, sports, and science (TEDx). A total of 16 hours of videos, split evenly across the different genres, were divided into adaptation, development and evaluation data sets. The Arabic MGB-Challenge comprised two tasks: A) Speech transcription, evaluated on the MGB-3 test set, along with the 10 hour MGB-2 test set to report progress on the MGB-2 evaluation; B) Arabic dialect identification, introduced this year in order to distinguish between four major Arabic dialects - Egyptian, Levantine, North African, Gulf, as well as Modern Standard Arabic. Two hours of audio per dialect were released for development and a further two hours were used for evaluation. For dialect identification, both lexical features and i-vector bottleneck features were shared with participants in addition to the raw audio recordings. Overall, thirteen teams submitted ten systems to the challenge. We outline the approaches adopted in each system, and summarise the evaluation results.},
	urldate = {2022-10-02},
	author = {Ali, Ahmed and Vogel, Stephan and Renals, Steve},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{khurana_qcri_2016,
	address = {San Diego, CA},
	title = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition: {MGB}-2 challenge},
	isbn = {978-1-5090-4903-5},
	shorttitle = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition},
	url = {http://ieeexplore.ieee.org/document/7846279/},
	doi = {10.1109/SLT.2016.7846279},
	urldate = {2022-10-02},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Khurana, Sameer and Ali, Ahmed},
	month = dec,
	year = {2016},
	pages = {292--298},
}

@inproceedings{smit_aalto_2017,
	address = {Okinawa},
	title = {Aalto system for the 2017 {Arabic} multi-genre broadcast challenge},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268955/},
	doi = {10.1109/ASRU.2017.8268955},
	urldate = {2022-10-02},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Smit, Peter and Gangireddy, Siva Reddy and Enarvi, Seppo and Virpioja, Sami and Kurimo, Mikko},
	month = dec,
	year = {2017},
	pages = {338--345},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\YUJC9CV7\\Smit et al. - 2017 - Aalto system for the 2017 Arabic multi-genre broad.pdf:application/pdf},
}

@inproceedings{snyder_speaker_2019,
	address = {Brighton, United Kingdom},
	title = {Speaker {Recognition} for {Multi}-speaker {Conversations} {Using} {X}-vectors},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683760/},
	doi = {10.1109/ICASSP.2019.8683760},
	urldate = {2022-10-02},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and McCree, Alan and Povey, Daniel and Khudanpur, Sanjeev},
	month = may,
	year = {2019},
	pages = {5796--5800},
}

@article{dutta_performance_2021,
	title = {Performance analysis of {ASR} system in hybrid {DNN}-{HMM} framework using a {PWL} euclidean activation function},
	volume = {15},
	issn = {2095-2228, 2095-2236},
	url = {https://link.springer.com/10.1007/s11704-020-9419-z},
	doi = {10.1007/s11704-020-9419-z},
	language = {en},
	number = {4},
	urldate = {2022-10-02},
	journal = {Frontiers of Computer Science},
	author = {Dutta, Anirban and Ashishkumar, Gudmalwar and Rao, Ch V. Rama},
	month = aug,
	year = {2021},
	pages = {154705},
}

@inproceedings{georgescu_kaldi-based_2019,
	address = {Timisoara, Romania},
	title = {Kaldi-based {DNN} {Architectures} for {Speech} {Recognition} in {Romanian}},
	isbn = {978-1-72810-984-8},
	url = {https://ieeexplore.ieee.org/document/8906555/},
	doi = {10.1109/SPED.2019.8906555},
	urldate = {2022-10-04},
	booktitle = {2019 {International} {Conference} on {Speech} {Technology} and {Human}-{Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Georgescu, Alexandru-Lucian and Cucu, Horia and Burileanu, Corneliu},
	month = oct,
	year = {2019},
	pages = {1--6},
}

@incollection{ekstein_cnn-tdnn-based_2021,
	address = {Cham},
	title = {{CNN}-{TDNN}-{Based} {Architecture} for {Speech} {Recognition} {Using} {Grapheme} {Models} in {Bilingual} {Czech}-{Slovak} {Task}},
	volume = {12848},
	isbn = {978-3-030-83526-2 978-3-030-83527-9},
	url = {https://link.springer.com/10.1007/978-3-030-83527-9_45},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {Text, {Speech}, and {Dialogue}},
	publisher = {Springer International Publishing},
	author = {Psutka, Josef V. and Švec, Jan and Pražák, Aleš},
	editor = {Ekštein, Kamil and Pártl, František and Konopík, Miloslav},
	year = {2021},
	doi = {10.1007/978-3-030-83527-9_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {523--533},
}

@misc{noauthor_lattice_nodate,
	title = {On lattice free {MMI} and {Chain} models in {Kaldi}},
	url = {https://desh2608.github.io/2019-05-21-chain/},
	urldate = {2022-10-04},
	file = {On lattice free MMI and Chain models in Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\WVI67A8B\\2019-05-21-chain.html:text/html},
}

@misc{raj_experiments_nodate,
	title = {Experiments with {Subword} {Modeling}},
	url = {https://desh2608.github.io/2018-11-22-subword-segmentation/},
	abstract = {Think about tasks such as machine translation (MT), automatic speech recognition (ASR), or handwriting recognition (HWR). While these appear very distinct, on abstraction they share the same pipeline wherein given an input signal, we are required to predict some text. The difference only lies in the form of the input...},
	language = {en},
	urldate = {2022-10-04},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ID8BRFHZ\\2018-11-22-subword-segmentation.html:text/html},
}

@inproceedings{tian_consistent_2022,
	address = {Singapore, Singapore},
	title = {Consistent {Training} and {Decoding} for {End}-to-{End} {Speech} {Recognition} {Using} {Lattice}-{Free} {MMI}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9746579/},
	doi = {10.1109/ICASSP43922.2022.9746579},
	urldate = {2022-10-05},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Tian, Jinchuan and Yu, Jianwei and Weng, Chao and Zhang, Shi-Xiong and Su, Dan and Yu, Dong and Zou, Yuexian},
	month = may,
	year = {2022},
	pages = {7782--7786},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\UFAHYT8M\\Tian et al. - 2022 - Consistent Training and Decoding for End-to-End Sp.pdf:application/pdf},
}

@misc{fayek_speech_2016,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}’s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2022-10-06},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
}

@misc{wiesner_lattice_2020,
	title = {Lattice {Free} {Maximum} {Mutual} {Information} ({LF}-{MMI})},
	url = {https://m-wiesner.github.io/LF-MMI/},
	abstract = {Everything about LF-MMI},
	language = {en},
	urldate = {2022-10-08},
	journal = {Matthew Wiesner},
	author = {Wiesner, Matthew and MatthewWiesner},
	month = jan,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ELAZNE4P\\LF-MMI.html:text/html},
}

@article{liu_time_2019,
	title = {Time {Delay} {Recurrent} {Neural} {Network} for {Speech} {Recognition}},
	volume = {1229},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1229/1/012078},
	doi = {10.1088/1742-6596/1229/1/012078},
	abstract = {Abstract
            In Automatic Speech Recognition(ASR), Time Delay Neural Network (TDNN) has been proven to be an efficient network structure for its strong ability in context modeling. In addition, as a feed-forward neural architecture, it is faster to train TDNN, compared with recurrent neural networks such as Long Short-Term Memory (LSTM). However, different from recurrent neural networks, the context in TDNN is carefully designed and is limited. Although stacking Long Short-Term Memory (LSTM) together with TDNN in order to extend the context information have been proven to be useful, it is too complex and is hard to train. In this paper, we focus on directly extending the context modeling capability of TDNNs by adding recurrent connections. Several new network architectures were investigated. The results on the Switchboard show that the best model significantly outperforms the base line TDNN system and is comparable with TDNN-LSTM architecture. In addition, the training process is much simpler than that of TDNN-LSTM.},
	number = {1},
	urldate = {2022-10-12},
	journal = {Journal of Physics: Conference Series},
	author = {Liu, Boji and Zhang, Weibin and Xu, Xiangming and Chen, Dongpeng},
	month = may,
	year = {2019},
	pages = {012078},
}

@phdthesis{ritter_neural_2019,
	title = {Neural {Architecture} {Search} for {Finding} the {Best} {Time} {Delay} {Neural} {Network} {Acoustic} {Model} for {Speech} {Recognition}},
	abstract = {Time Delay Neural Network (TDNN) is a popular type of acoustic model used for speech recognition applications. Their popularity is mainly due to their faster training and decoding times, with word error rates (WER) comparable to a long-short term memory (LSTM) based acoustic model. The popularity of TDNNs picked up in 2015 when a more efficient setup was proposed, which is known as a TDNN with sub-sampling scheme. However, the 2015 proposal with sub-sampling scheme does not offer any details on the sub-sampling used. Neural Architecture Search (NAS) is a new research field that has garnered a lot of attention with successful results in computer vision research. Despite this, NAS has received little attention in speech recognition, where design architectures for acoustic models is crucial. TDNNs are provided in the Kaldi speech recognition toolkit to be used for research or deployment purposes. From the literature, we have observed that it is common for a TDNN to be used in Kaldi as is, with no additional tuning of its hyperparameters. For this reason, this project aims to investigate if the Kaldi baseline TDNN is actually the best configuration to be used for a speech recognition application. To do so, we have made use of a recently proposed algorithm which integrates reinforcement learning in its training process. Specifically, we have used Neural Architecture Search (NAS) to target and improve automatically the sub-sampling scheme of the Kaldi TDNN. For reproducibility we have based all our results on the standard Wall Street Journal database. We performed experiments by setting a RNN based TDNN architecture generator, with the added constraint that the TDNNs evaluated have the same number of frames as the one provided with Kaldi. Our results show that NAS is able to sample a better TDNN architecture than the one provided by Kaldi in less than 40,000 iterations, achieving a 0.19 WER reduction. i},
	author = {Ritter, Fabian},
	month = aug,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8Y4PMA9Y\\Ritter - 2019 - Neural Architecture Search for Finding the Best Ti.pdf:application/pdf},
}

@misc{fayek_speech_2016-1,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}’s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2022-10-13},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\GNNWTTYV\\speech-processing-for-machine-learning.html:text/html},
}

@misc{shrawankar_adverse_2013-1,
	title = {Adverse {Conditions} and {ASR} {Techniques} for {Robust} {Speech} {User} {Interface}},
	url = {http://arxiv.org/abs/1303.5515},
	abstract = {The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Shrawankar, Urmila and Thakare, V. M.},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	annote = {Comment: 10 pages 2 Tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Q3SPV23K\\Shrawankar and Thakare - 2013 - Adverse Conditions and ASR Techniques for Robust S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\82368FRG\\1303.html:text/html},
}

@article{vipperla_ageing_2010,
	title = {Ageing {Voices}: {The} {Effect} of {Changes} in {Voice} {Parameters} on {ASR} {Performance}},
	volume = {2010},
	issn = {1687-4714, 1687-4722},
	shorttitle = {Ageing {Voices}},
	url = {http://asmp.eurasipjournals.com/content/2010/1/525783},
	doi = {10.1155/2010/525783},
	language = {en},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	year = {2010},
	pages = {1--10},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\G6TNUYYM\\Vipperla et al. - 2010 - Ageing Voices The Effect of Changes in Voice Para.pdf:application/pdf},
}

@techreport{markus_forsberg_why_2003,
	title = {Why is {Speech} {Recognition} {Difficult}?},
	url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.3677},
	institution = {Chalmers University of Technology},
	author = {Markus Forsberg},
	month = feb,
	year = {2003},
}

@article{schuller_recognition_2009,
	title = {Recognition of {Noisy} {Speech}: {A} {Comparative} {Survey} of {Robust} {Model} {Architecture} and {Feature} {Enhancement}},
	volume = {2009},
	issn = {1687-4722},
	shorttitle = {Recognition of {Noisy} {Speech}},
	url = {http://asmp.eurasipjournals.com/content/2009/1/942617},
	doi = {10.1155/2009/942617},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Schuller, Björn and Wöllmer, Martin and Moosmayr, Tobias and Rigoll, Gerhard},
	year = {2009},
	pages = {942617},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9UQKPJZ2\\Schuller et al. - 2009 - Recognition of Noisy Speech A Comparative Survey .pdf:application/pdf},
}

@article{hoge_basic_2007,
	title = {Basic parameters in speech processing. {The} need for evaluation},
	volume = {32},
	copyright = {Copyright on any open access article in the Archives of Acoustics published by Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society is retained by the author(s).   Authors grant Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society a license to publish the article and identify itself as the original publisher.   Authors also grant any user the right to use the article freely as long as its integrity is maintained and its original authors, citation details and publisher are identified.           The Creative Commons Attribution License-ShareAlike 4.0 formalizes these and other terms and conditions of publishing articles.    Exceptions to copyright policy    For the articles which were previously published, before year 2019, policies that are different from the above. In all such, access to these articles is free from fees or any other access restrictions.   Permissions for the use of the texts published in that journal may be sought directly from the Editorial Office of Archives of Acoustics},
	issn = {2300-262X},
	url = {https://acoustics.ippt.pan.pl/index.php/aa/article/view/766},
	abstract = {As basic parameters in speech processing we regard pitch, duration, intensity, voice quality, signal to noise ratio, voice activity detection and strength of Lombard effect. Taking in account also adverse conditions the performance of many published algorithms to extract those parameters from the speech signal automatically is not known. A framework based on competitive evaluation is proposed to push algorithmic research and to make progress comparable.},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {Archives of Acoustics},
	author = {Höge, Harald},
	year = {2007},
	note = {Number: 1},
	pages = {67},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\9Z4BXXSK\\Höge - 2007 - Basic parameters in speech processing. The need fo.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FSTAYK66\\766.html:text/html},
}

@article{venkatagiri_speech_2002,
	title = {Speech {Recognition} {Technology} {Applications} in {Communication} {Disorders}},
	volume = {11},
	issn = {1058-0360, 1558-9110},
	url = {http://pubs.asha.org/doi/10.1044/1058-0360%282002/037%29},
	doi = {10.1044/1058-0360(2002/037)},
	abstract = {Speech recognition (SR) is the process whereby a microprocessor-based system, typically a computer with sound processing hardware and speech recognition software, responds in predictable ways to spoken commands and/or converts speech into text. This tutorial describes the types and the general uses of SR and provides an explanation of the technology behind it. The emerging applications of SR technology for dictation, articulation training, language and literacy development, environmental control, and communication augmentation are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-10-13},
	journal = {American Journal of Speech-Language Pathology},
	author = {Venkatagiri, H. S.},
	month = nov,
	year = {2002},
	pages = {323--332},
}

@techreport{noauthor_alternative_nodate,
	title = {Alternative {Control} {Technologies}: {Human} {Factors} {Issues}},
	shorttitle = {Alternative {Control} {Technologies}},
	url = {https://apps.dtic.mil/sti/citations/ADA355911},
	abstract = {With the increasing intelligence of computer systems, it is becoming more desirable to have an operator communicate with machines rather than simply operate them. In combat aircraft, this need to communicate is made quite crucial due to high temporal pressure and workload during critical phases of the flight ingress, engagement, deployment of self-defense. The HOTAS concept, with manual controls fitted on the stick and throttle, has been widely used in modern fighters such as F16, F18, EFA and Rafale. This concept allows pilots to input real time commands to the aircraft system. However, it increases the complexity of the pilot task due to inflation of real time controls, with some controls being multifunction. It is therefore desirable, in the framework of ecological interfaces, to introduce alternative input channels in order to reduce the complexity of manual control in the HOTAS concept and allow more direct and natural access to the aircraft systems. Control and display technologies are the critical enablers for these advanced interfaces. There are a variety of novel alternative control technologies that when integrated usefully with critical mission tasks can make natural use of the innate potential of human sensory and motor systems. Careful design and integration of candidate control technologies will result in human-machine interfaces which are natural, easier to learn, easier to use, and less prone to error. Significant progress is being made on using signals from the brain, muscles, voice, lip, head position, eye position and gestures for the control of computers and other devices. Judicious application of alternative control technologies has the potential to increase the bandwidth of operator-system interaction, improve the effectiveness of military systems, and realize cost savings. Alternative controls can reduce workload and improve efficiency within the cockpit, directly supporting the warfighter.},
	language = {en},
	urldate = {2022-10-13},
	note = {Section: Technical Reports},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\96Z76MGM\\ADA355911.html:text/html},
}

@book{vasilescu_cross-lingual_2011,
	title = {Cross-{Lingual} {Study} of {ASR} {Errors}: {On} the {Role} of the {Context} in {Human} {Perception} of {Near}-{Homophones}.},
	shorttitle = {Cross-{Lingual} {Study} of {ASR} {Errors}},
	author = {Vasilescu, Ioana and Yahia, Dahbia and Snoeren, Natalie and Adda-Decker, Martine and Lamel, Lori},
	month = aug,
	year = {2011},
	note = {Pages: 1952},
}

@inproceedings{povey_semi-orthogonal_2018,
	title = {Semi-{Orthogonal} {Low}-{Rank} {Matrix} {Factorization} for {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/povey18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1417},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Povey, Daniel and Cheng, Gaofeng and Wang, Yiming and Li, Ke and Xu, Hainan and Yarmohammadi, Mahsa and Khudanpur, Sanjeev},
	month = sep,
	year = {2018},
	pages = {3743--3747},
}

@inproceedings{yeh_taiwanese_2020,
	address = {Taipei, Taiwan},
	title = {Taiwanese {Speech} {Recognition} {Based} on {Hybrid} {Deep} {Neural} {Network} {Architecture}},
	url = {https://aclanthology.org/2020.rocling-1.11},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 32nd {Conference} on {Computational} {Linguistics} and {Speech} {Processing} ({ROCLING} 2020)},
	publisher = {The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)},
	author = {Yeh, Yu-Fu and Su, Bo-Hao and Ou, Yang-Yen and Wang, Jhing-Fa and Tsai, An-Chao},
	month = sep,
	year = {2020},
	pages = {102--113},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\MTSZB5PS\\Yeh et al. - 2020 - Taiwanese Speech Recognition Based on Hybrid Deep .pdf:application/pdf},
}

@article{s_review_2016-1,
	title = {A {Review} on {Automatic} {Speech} {Recognition} {Architecture} and {Approaches}},
	volume = {9},
	issn = {20054254, 20054254},
	url = {http://article.nadiapub.com/IJSIP/vol9_no4/34.pdf},
	doi = {10.14257/ijsip.2016.9.4.34},
	number = {4},
	urldate = {2022-10-18},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {S, Karpagavalli and E, Chandra},
	month = apr,
	year = {2016},
	pages = {393--404},
}

@misc{hannun_speech_nodate,
	title = {Speech {Recognition} {Is} {Not} {Solved}},
	url = {https://awni.github.io/speech-recognition/},
	abstract = {Ever since Deep Learning hit the scene in speech recognition, word error rates
have fallen dramatically. But despite articles you may have read, we still
don’t have human-level speech recognition. Speech recognizers have many failure
modes. Acknowledging these and taking steps towards solving them is critical to
progress. It’s the only way to go from ASR
which works for some people, most of the time to ASR which works for all
people, all of the time.},
	urldate = {2022-10-23},
	author = {Hannun, Awni},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HK2ZJCFX\\speech-recognition.html:text/html},
}

@misc{rev_speech_2021,
	title = {Speech {Recognition} {Challenges} and {How} to {Solve} {Them}},
	url = {https://www.rev.com/blog/speech-to-text-technology/speech-recognition-challenges-and-how-to-solve-them},
	abstract = {Cutting edge tech is always a challenge. That’s one of the reasons we love it. The breakthrough discovery, the moment when we figure out how to solve the},
	language = {en-US},
	urldate = {2022-10-27},
	journal = {Rev},
	author = {Rev},
	month = sep,
	year = {2021},
}

@phdthesis{ander_gonzalez_docasal_noisy_2018,
	title = {Noisy speech recognition using {Kaldi} and neural architectures},
	url = {http://hdl.handle.net/10810/27865},
	abstract = {Noisy Speech Recognition using Kaldi and Neural Architectures ABSTRACT The goal of an Automatic Speech Recognition (ASR) system is to transform a set of acoustic features into a sequence of words. It mainly consists of various parts: the feature extraction part which extracts information from a speech signal; the acoustic model, in charge of the conversion from speech to phonemes; and the language model that transforms the detected phonemes into the most probable sequence of words. Throughout their history, these systems were built with statistical methods, mainly Hidden Markov Models (HMM) and Gaussian Mixture Models (GMM). However, in recent years the use of neural architectures such as Deep, Convolutional and Recurrent Neural Networks (DNN, CNN and RNN), have improved the achieved results significantly. Moreover, freely available tools made ASR research develop quickly. Kaldi is one of the most known and widely used ASR systems. It includes a set of neural network packages —nnet1, nnet2 and nnet3— which can be used for implementing the acoustic model. These are fast, accurate and able to handle huge databases since they distribute the load on clusters of machines. However, Kaldi’s slow development cycle implies that new neural architectures may be introduced many years after their publications. Therefore, in this work we substitute the neural acoustic model of Kaldi by our own implementations written in TensorFlow. TensorFlow has the largest community of users and the best support among the available deep learning libraries. By substituting the Acoustic Model of Kaldi with different architectures and testing their performance on the well-known database Aurora-4, we managed to reduce Word Error Rate (WER) by 3.17 \% (baseline 15.14 \%) when using a CNN architecture. Also, focusing on just the clean subset of the Test part of the database, a further improvement has been achieved once implementing a CNN + RNN structure, from a 4.54 \% WER with the CNNs alone to a 4.13 \% with this architecture. This work is therefore believed to improve the results on obtained by one of the widely used ASR tools simply by implementing more advanced deep learning techniques, which could be executed by more powerful and dedicated external programs. For future work, a further analysis on more complex convolutional networks could lead to a better performance in this particular database and, in general, in noisy environments. Finally, further improvement of convolutional and recurrent architectures is suggested in clean and noise-free conditions, since they have been shown to obtain the best results in this specific circumstances.},
	school = {University of Crete},
	author = {Ander González Docasal},
	month = feb,
	year = {2018},
}

@inproceedings{mirzaei_errors_2015,
	title = {Errors in automatic speech recognition versus difficulties in second language listening},
	isbn = {978-1-908416-29-2},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2015.000367},
	doi = {10.14705/rpnet.2015.000367},
	urldate = {2022-10-27},
	booktitle = {Critical {CALL} – {Proceedings} of the 2015 {EUROCALL} {Conference}, {Padova}, {Italy}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Meshgi, Kourosh and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2015},
	pages = {410--415},
}

@inproceedings{noauthor_notitle_nodate,
	url = {http://nl.ijs.si/janes/wp-content/uploads/2014/09/baron2003.pdf},
}

@book{farghaly_handbook_2003,
	address = {Stanford, Calif},
	series = {{CSLI} lecture notes},
	title = {Handbook for language engineers},
	isbn = {978-1-57586-395-5 978-1-57586-396-2},
	url = {https://web.stanford.edu/group/cslipublications/cslipublications/site/1575863960.shtml},
	number = {no. 164},
	publisher = {CSLI Publications},
	author = {Farghaly, Ali Ahmed Sabry},
	year = {2003},
	keywords = {Computational linguistics, Applied linguistics},
}

@techreport{kardome_how_2022,
	title = {How {Kardome}'s {Advanced} {Speech} {Recognition} {Technology} {Improves} {Voice} {Enabled} {Devices}},
	url = {https://uploads-ssl.webflow.com/5f6ccdcd2e8b3e529677788d/6306b0cf32d2742c5a0e8db7_kardome-wake-word-detection-and-speech-recognition-study-for-consumer-voice-devices_6306ab45.pdf},
	urldate = {2022-10-27},
	author = {Kardome},
	year = {2022},
}

@misc{noauthor_solving_nodate,
	title = {Solving {Automatic} {Speech} {Recognition} {Deployment} {Challenges} {\textbar} {NVIDIA} {Technical} {Blog}},
	url = {https://developer.nvidia.com/blog/solving-automatic-speech-recognition-deployment-challenges/},
	urldate = {2022-10-29},
	file = {Solving Automatic Speech Recognition Deployment Challenges | NVIDIA Technical Blog:C\:\\Users\\DELL\\Zotero\\storage\\4ZD2CTIL\\solving-automatic-speech-recognition-deployment-challenges.html:text/html},
}

@misc{noauthor_global_nodate,
	title = {Global voice recognition market 2026},
	url = {https://www.statista.com/statistics/1133875/global-voice-recognition-market-size/},
	abstract = {The global voice recognition market size was forecast to grow from 10.7 billion U.S.},
	language = {en},
	urldate = {2022-10-29},
	journal = {Statista},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2MV2NQEG\\global-voice-recognition-market-size.html:text/html},
}

@misc{noauthor_speech_nodate,
	title = {Speech {Recognition} {Software} {Market} 2022 by {Size}, {Share}, international business analysis, {Key} firms {Profile} and {Forecast} to 2029 {\textbar} 104 {Pages} {Report}},
	url = {https://www.marketwatch.com/press-release/speech-recognition-software-market-2022-by-size-share-international-business-analysis-key-firms-profile-and-forecast-to-2029-104-pages-report-2022-09-29},
	abstract = {Sep 29, 2022 (The Expresswire) --
[Premium 104 Pages Report] Speech Recognition Software market trend that pertains to the world market for ICT Industry. The...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KKRL736U\\speech-recognition-software-market-2022-by-size-share-international-business-analysis-key-firms.html:text/html},
}

@misc{noauthor_speech_nodate-1,
	title = {Speech and {Voice} {Recognition} {Market} {Size} {And} {Opportunities} for {New} {Players}, {Forecast} from 2022 {To} 2029 with {Top} {Countries} {Data} {\textbar} 127 {Pages} {Report}},
	url = {https://www.marketwatch.com/press-release/speech-and-voice-recognition-market-size-and-opportunities-for-new-players-forecast-from-2022-to-2029-with-top-countries-data-127-pages-report-2022-10-13},
	abstract = {Oct 13, 2022 (The Expresswire) --
According to this latest study, In 2022 the growth of Speech and Voice Recognition Market is projected to reach...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\CPRRIETA\\speech-and-voice-recognition-market-size-and-opportunities-for-new-players-forecast-from-2022-t.html:text/html},
}

@misc{noauthor_speech_nodate-2,
	title = {Speech and {Voice} {Recognition} {Technology} {Market} {Size} 2022, {Demand}, {Share}, {Global} {Trend}, {Business} {Growth}, {Top} {Key} {Players} {Update}, {Business} {Statistics} and {Research} {Methodology} by {Forecast} to 2028},
	url = {https://www.marketwatch.com/press-release/speech-and-voice-recognition-technology-market-size-2022-demand-share-global-trend-business-growth-top-key-players-update-business-statistics-and-research-methodology-by-forecast-to-2028-2022-10-14},
	abstract = {Oct 14, 2022 (The Expresswire) --
"Final Report will add the analysis of the impact of Pre and Post COVID-19 on this Speech and Voice Recognition Technology...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
}

@misc{noauthor_speech_nodate-3,
	title = {Speech {Recognition} {Software} {Market} {Size}, {Share} and {Forecast} till 2028},
	url = {https://www.marketwatch.com/press-release/speech-recognition-software-market-size-share-and-forecast-till-2028-2022-09-16},
	abstract = {Sep 16, 2022 (Reportmines via Comtex) --
Pre and Post Covid is covered and Report Customization is available. It provides a detailed market research report...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\GF4FZHDT\\speech-recognition-software-market-size-share-and-forecast-till-2028-2022-09-16.html:text/html},
}

@misc{noauthor_pwc_2018,
	title = {{PwC}: {Lack} of trust in {AI} assistants like {Alexa} could hinder adoption},
	shorttitle = {{PwC}},
	url = {https://venturebeat.com/ai/pwc-lack-of-trust-in-ai-assistants-like-alexa-could-hinder-adoption/},
	abstract = {Register now for your free virtual pass to the Low-Code/No-Code Summit this November 9. Hear from executives from Service Now, Credit Karma, Stitch Fix, Appian, and more. Learn more. Tech leaders like Microsoft CEO Satya Nadella argue that voice and conversational AI represent a new age of computing, but a survey and report released today by […]},
	language = {en-US},
	urldate = {2022-10-29},
	journal = {VentureBeat},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KIY5FN7G\\pwc-lack-of-trust-in-ai-assistants-like-alexa-could-hinder-adoption.html:text/html},
}

@misc{noauthor_rise_2022,
	title = {The {Rise} and {Stall} of the {U}.{S}. {Smart} {Speaker} {Market} - {New} {Report}},
	url = {https://voicebot.ai/2022/03/02/the-rise-and-stall-of-the-u-s-smart-speaker-market-new-report/},
	abstract = {Smart speakers powered by Alexa, Google Assistant, and Siri represented a white-hot consumer device market in the 2016-2019 period. Surging..},
	language = {en-US},
	urldate = {2022-10-29},
	journal = {Voicebot.ai},
	month = mar,
	year = {2022},
	note = {Section: Ai},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\LNTBT9F4\\the-rise-and-stall-of-the-u-s-smart-speaker-market-new-report.html:text/html},
}

@inproceedings{k_e_mats_blomberg_automatisk_1997,
	title = {Automatisk igenk¨anning av tal},
	author = {K. E. Mats Blomberg},
	year = {1997},
}

@phdthesis{lowerre_harpy_1976,
	title = {The {Harpy} speech recognition system},
	url = {https://ui.adsabs.harvard.edu/abs/1976PhDT........81L},
	abstract = {The Harpy connected speech recognition system is the result of an attempt to understand the relative importance of various design choices of two earlier speech recognition systems developed at Carnegie-Mellon University: The Hearsay-1 system and the Dragon system. Knowledge is represented in the Hearsay-1 system as procedures and in the Dragon system as a Markov network with a-priori transition probabilities between states. Systematic performance analysis of various design choices of these two systems resulted in the HARPY system, in which knowledge is represented as a finite state transition network but without the a-priori transition probabilities. Harpy searches only a few 'best' syntactic (and acoustic) paths in parallel to determine the optimal path, and uses segmentation to effectively reduce the utterance length, thereby reducing the number of state probability updates that must be done. Several new heuristics have been added to the HARPY system to improve its performance and speed: detection of common sub-nets and collapsing them to reduce overall network size and complexity, eliminating the need for doing an acoustic match for all phonemic types at every time sample, and semi-automatic techniques for learning the lexical representations (that are needed for a steady-state system of this type) and the phonemic templates from training data, thus automatically accounting for the commonly occurring intra-word coarticulation and juncture phenomena. Inter-word phenomena are handled by the use of juncture rules which are applied at network generation time, thereby eliminating the need for repetitive and time consuming application of phonological rules during the recognition phase.},
	urldate = {2022-10-30},
	author = {Lowerre, B. T.},
	month = apr,
	year = {1976},
	note = {Publication Title: Ph.D. Thesis
ADS Bibcode: 1976PhDT........81L},
	keywords = {Speech Recognition, Communications and Radar, Dictionaries, Grammars, Heuristic Methods, Knowledge, Performance Tests},
}

@misc{noauthor_ibm_2003,
	type = {{TS200}},
	title = {{IBM} {Archives}: {IBM} {Shoebox}},
	copyright = {© Copyright IBM Corp. 2011},
	shorttitle = {{IBM} {Archives}},
	url = {//www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html},
	abstract = {IBM Archives: Exhibits: IBM special products (vol. 1): IBM Shoebox},
	language = {en-US},
	urldate = {2022-10-30},
	month = jan,
	year = {2003},
}

@misc{noauthor_audrey_2021,
	title = {Audrey, {Alexa}, {Hal}, and {More}},
	url = {https://computerhistory.org/blog/audrey-alexa-hal-and-more/},
	abstract = {Star Trek got speech recognition right. How did this science fiction fantasy of only a few decades ago come true? What's the history of speech recognition and where is it headed?},
	language = {en},
	urldate = {2022-10-30},
	journal = {CHM},
	month = jun,
	year = {2021},
	note = {Section: Curatorial Insights},
}

@article{levinson_continuously_1986,
	title = {Continuously variable duration hidden {Markov} models for automatic speech recognition},
	volume = {1},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230886800092},
	doi = {10.1016/S0885-2308(86)80009-2},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {Computer Speech \& Language},
	author = {Levinson, S.E.},
	month = mar,
	year = {1986},
	pages = {29--45},
}

@article{lee_speech_1990,
	title = {Speech recognition using hidden {Markov} models: {A} {CMU} perspective},
	volume = {9},
	issn = {01676393},
	shorttitle = {Speech recognition using hidden {Markov} models},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167639390900255},
	doi = {10.1016/0167-6393(90)90025-5},
	language = {en},
	number = {5-6},
	urldate = {2022-10-30},
	journal = {Speech Communication},
	author = {Lee, Kai-Fu and Hon, Hsiao-Wuen and Hwang, Mei-Yuh and Huang, Xuedong},
	month = dec,
	year = {1990},
	pages = {497--508},
}

@article{baker_dragon_1975,
	title = {The {DRAGON} system--{An} overview},
	volume = {23},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1162650/},
	doi = {10.1109/TASSP.1975.1162650},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Baker, J.},
	month = feb,
	year = {1975},
	pages = {24--29},
}

@misc{kikel_history_2022,
	title = {History of {Voice} {Recognition} {Technology}},
	url = {https://www.totalvoicetech.com/a-brief-history-of-voice-recognition-technology/},
	abstract = {Learn about the history of voice recognition technology. It has come a long way since the 1950's, and will continue to evolve.},
	language = {en-US},
	urldate = {2022-10-30},
	journal = {Total Voice Technologies},
	author = {Kikel, Chris},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Y55S4SD6\\a-brief-history-of-voice-recognition-technology.html:text/html},
}

@article{shneiderman_limits_2000,
	title = {The limits of speech recognition},
	volume = {43},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/348941.348990},
	doi = {10.1145/348941.348990},
	language = {en},
	number = {9},
	urldate = {2022-10-30},
	journal = {Communications of the ACM},
	author = {Shneiderman, Ben},
	month = sep,
	year = {2000},
	pages = {63--65},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC9Z8S9X\\Shneiderman - 2000 - The limits of speech recognition.pdf:application/pdf},
}

@article{humes_speech-recognition_1990,
	title = {Speech-{Recognition} {Difficulties} of the {Hearing}-{Impaired} {Elderly}: {The} {Contributions} of {Audibility}},
	volume = {33},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Speech-{Recognition} {Difficulties} of the {Hearing}-{Impaired} {Elderly}},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3304.726},
	doi = {10.1044/jshr.3304.726},
	abstract = {The role that sensorineural hearing loss plays in the speech-recognition difficulties of the hearing-impaired elderly is examined. One approach to this issue was to make between-group comparisons of performance for three groups of subjects: (a) young normal-hearing adults; (b) elderly hearing-impaired adults; and (c) young normal-hearing adults with simulated sensorineural hearing loss equivalent to that of the elderly subjects produced by a spectrally shaped masking noise. Another approach to this issue employed correlational analyses to examine the relation between audibility and speech recognition within the group of elderly hearing-impaired subjects. An additional approach was pursued in which an acoustical index incorporating adjustments for threshold elevation was used to examine the role audibility played in the speech-recognition performance of the hearing-impaired elderly. A wide range of listening conditions was sampled in this experiment. The conclusion was that the primary determiner of speech-recognition performance in the elderly hearing-impaired subjects was their threshold elevation.},
	language = {en},
	number = {4},
	urldate = {2022-10-30},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Humes, Larry E. and Roberts, Lisa},
	month = dec,
	year = {1990},
	pages = {726--735},
}

@techreport{meyer_bernd_whats_2013,
	title = {What's the difference? {Comparing} humans and machines on the {Aurora} 2 speech recognition task},
	url = {https://www.researchgate.net/publication/290246535_What's_the_difference_Comparing_humans_and_machines_on_the_Aurora_2_speech_recognition_task/references},
	abstract = {The comparison of human speech recognition (HSR) and machine performance allows to learn from the differences between HSR and automatic speech recognition (ASR) and serves as motivation for using auditory-inspired strategies in ASR. The recognition of noisy digit strings from the Aurora 2 framework is one of the most widely used tasks in the ASR community. This paper establishes a baseline with a close-to-optimal classifier, i.e., our auditory system by comparing results from 10 normal-hearing listeners to the Aurora 2 reference system using identical speech material. The baseline ASR system reaches the human performance level only when the signal-to-noise ratio is increased by 10 or 21 dB depending on the training condition. The recognition of 1-digit recordings was found to be considerably better for HSR, indicating that onset detection is an important feature neglected in standard ASR systems. Results of recent studies are considered in the light of these findings to measure how far we have come on the way to human speech recognition performance.},
	institution = {Medical Physics, Carl-von-Ossietzky Universit¨at Oldenburg, Germany},
	author = {Meyer, Bernd},
	month = jan,
	year = {2013},
	pages = {2634--2638},
}

@article{lippmann_speech_1997,
	title = {Speech recognition by machines and humans},
	volume = {22},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639397000216},
	doi = {10.1016/S0167-6393(97)00021-6},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {Speech Communication},
	author = {Lippmann, Richard P.},
	month = jul,
	year = {1997},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\D25IW8PV\\Lippmann - 1997 - Speech recognition by machines and humans.pdf:application/pdf},
}

@article{meyer_effect_2011,
	title = {Effect of speech-intrinsic variations on human and automatic recognition of spoken phonemes},
	volume = {129},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3514525},
	doi = {10.1121/1.3514525},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Meyer, Bernd T. and Brand, Thomas and Kollmeier, Birger},
	month = jan,
	year = {2011},
	pages = {388--403},
}

@inproceedings{vasilescu_ioana_cross-lingual_2012,
	address = {Istanbul, Turkey},
	title = {Cross-lingual studies of {ASR} errors: paradigms for perceptual evaluations},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/300_Paper.pdf},
	abstract = {It is well-known that human listeners significantly outperform machines when it comes to transcribing speech. This paper presents a progress report of the joint research in the automatic vs human speech transcription and of the perceptual experiments developed at LIMSI that aims to increase our understanding of automatic speech recognition errors. Two paradigms are described here in which human listeners are asked to transcribe speech segments containing words that are frequently misrecognized by the system. In particular, we sought to gain information about the impact of increased context to help humans disambiguate problematic lexical items, typically homophone or near-homophone words. The long-term aim of this research is to improve the modeling of ambiguous contexts so as to reduce automatic transcription errors."},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} (\{{LREC}\}'12)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Vasilescu, Ioana and Adda-Decker, Martine and Lamel, Lori},
	month = may,
	year = {2012},
	pages = {3511--3518},
}


@book{gold_speech_2011,
	address = {Hoboken, N.J},
	edition = {2nd ed},
	title = {Speech and audio signal processing: processing and perception of speech and music},
	isbn = {978-0-470-19536-9},
	shorttitle = {Speech and audio signal processing},
	publisher = {Wiley},
	author = {Gold, Bernard and Morgan, Nelson and Ellis, Dan},
	year = {2011},
	keywords = {Digital techniques, Electronic music, Signal processing, Speech processing systems},
}

@inproceedings{xu_semantic_2010,
	address = {Berkeley, CA, USA},
	title = {Semantic understanding by combining extended {CFG} parser with {HMM} model},
	isbn = {978-1-4244-7904-7},
	url = {http://ieeexplore.ieee.org/document/5700824/},
	doi = {10.1109/SLT.2010.5700824},
	urldate = {2022-10-30},
	booktitle = {2010 {IEEE} {Spoken} {Language} {Technology} {Workshop}},
	publisher = {IEEE},
	author = {Xu, Yushi and Seneff, Stephanie and Li, Alice and Polifroni, Joe},
	month = dec,
	year = {2010},
	pages = {67--72},
}

@inproceedings{ye-yi_wang_is_2003,
	address = {St Thomas, VI, USA},
	title = {Is word error rate a good indicator for spoken language understanding accuracy},
	isbn = {978-0-7803-7980-0},
	url = {http://ieeexplore.ieee.org/document/1318504/},
	doi = {10.1109/ASRU.2003.1318504},
	urldate = {2022-10-30},
	booktitle = {2003 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({IEEE} {Cat}. {No}.{03EX721})},
	publisher = {IEEE},
	author = {{Ye-Yi Wang} and Acero, A. and Chelba, C.},
	year = {2003},
	pages = {577--582},
}

@inproceedings{riccardi_stochastic_1998,
	title = {Stochastic language models for speech recognition and understanding},
	url = {https://www.isca-speech.org/archive/icslp_1998/riccardi98b_icslp.html},
	doi = {10.21437/ICSLP.1998-502},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {5th {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1998)},
	publisher = {ISCA},
	author = {Riccardi, Giuseppe and Gorin, Allen L.},
	month = nov,
	year = {1998},
	pages = {paper 0111--0},
}

@inproceedings{esteve_conceptual_2003,
	title = {Conceptual decoding for spoken dialog systems},
	url = {https://www.isca-speech.org/archive/eurospeech_2003/esteve03_eurospeech.html},
	doi = {10.21437/Eurospeech.2003-260},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {8th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 2003)},
	publisher = {ISCA},
	author = {Esteve, Yannick and Raymond, Christian and Bechet, Frédéric and Mori, Renato De},
	month = sep,
	year = {2003},
	pages = {617--620},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\KSIVHEB4\\Esteve et al. - 2003 - Conceptual decoding for spoken dialog systems.pdf:application/pdf},
}

@phdthesis{muhammad_danyal_khan_automatic_2022,
	address = {Karachi, Pakistan},
	title = {Automatic {Speech} {Recognition} to {Detect} {Malicious}/{Mal}-intended {Speech}},
	url = {https://www.researchgate.net/publication/365349074_Automatic_Speech_Recognition_to_Detect_MaliciousMal-intended_Speech},
	abstract = {Speech Recognition is a growing field since last five decades and there have been many advancements which has led to its applications like Speech to Text. This has allowed a possibility of Transcription of audio files to text and much of work is available on this in English, Arabic and Cantonese Languages.

However, Urdu is a low-resource language in field of ASR although it is the world's \$11{\textasciicircum}\{th\}\$ most widely spoken language, with 232 Million speakers worldwide. There are no applicable models available which can be readily deployed for Speech To Text in a noisy scenario which is why Urdu Community is devoid of all the benefits of ASR.

Apart from noise problems in normal telephonic or call-center conversations in Urdu, people tend to spontaneously use words from other language since Pakistan is a multi-cultural society, which presents a code-switching problem.

Hence, we proposed an implementation of Automatic Speech Recognition/ Speech to Text System in a noisy/ call center environment with less labelled training data available using Hybrid HMM-DNN in a Resource constraint environment in terms of time, budget, computation power, HR etc.

We were able to access to call center audio files, thanks to CPLC {\textbackslash}cite\{cplc\_cplc\_nodate\} (a semi-government Law Enforcement Agency), some of which was labelled manually. We further integrated various open source data-sets to include more variety in data-set. The data comprised of mix of noisy and clean audio as well as single utterances and long sentences (1-20 second audios). It was split into 6.5 hours and 3.5 hours of train and test data-set respectively.

The Language Model was developed from the training data-set and for acoustic modelling we used HMM (Monophone and Triphone) based on which we trained a Neural Network based model using Chain CNN-TDNN, achieving up to 5.2{\textbackslash}\% WER with noisy and clean data-set as well as on single word to spontaneous speech data as well.


    {\textbackslash}textbf\{{\textbackslash}{\textbackslash} Keywords:\} {\textbackslash}textit\{{\textbackslash}{\textbackslash} Speech Recognition, ASR, Call Center, audio transcription, Urdu language, Code-switched Urdu ASR, Speech to Text, AI, Cyber Security, ASR for Resource Constrained Environment, ASR for Noisy Environment, ASR for Low Resource Languages, Under Resourced Languages\}},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Muhammad Danyal Khan},
	month = oct,
	year = {2022},
	doi = {10.13140/RG.2.2.34319.05281},
}

@inproceedings{hercigonja_comparative_2017,
	title = {Comparative {Analysis} of {Cryptographic} {Algorithms}},
	author = {Hercigonja, Zoran},
	year = {2017},
}

@article{abood_survey_2018,
	title = {A {Survey} on {Cryptography} {Algorithms}},
	volume = {8},
	doi = {10.29322/IJSRP.8.7.2018.p7978},
	journal = {International Journal of Scientific and Research Publications},
	author = {Abood, Omar and Guirguis, Shawkat},
	month = jul,
	year = {2018},
	pages = {495--516},
}

@article{coretti_constructive_2013,
	title = {A {Constructive} {Perspective} on {Key} {Encapsulation}},
	volume = {8260},
	doi = {10.1007/978-3-642-42001-6_16},
	author = {Coretti, Sandro and Maurer, Ueli and Tackmann, Björn},
	month = jan,
	year = {2013},
	note = {ISBN: 978-3-642-42000-9},
}

@inproceedings{shor_algorithms_1994,
	title = {Algorithms for quantum computation: discrete logarithms and factoring},
	doi = {10.1109/SFCS.1994.365700},
	booktitle = {Proceedings 35th {Annual} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Shor, P.W.},
	year = {1994},
	pages = {124--134},
}

@inproceedings{grover_fast_1996,
	title = {A fast quantum mechanical algorithm for database search},
	booktitle = {{STOC} '96},
	author = {Grover, Lov K.},
	year = {1996},
}

@article{harrow_quantum_2009,
	title = {Quantum {Algorithm} for {Linear} {Systems} of {Equations}},
	volume = {103},
	doi = {10.1103/PhysRevLett.103.150502},
	journal = {Physical review letters},
	author = {Harrow, Aram and Hassidim, Avinatan and Lloyd, Seth},
	month = oct,
	year = {2009},
	pages = {150502},
}

@article{micciancio_hardness_2001,
	title = {The hardness of the closest vector problem with preprocessing},
	volume = {47},
	journal = {IEEE Transactions on Information Theory},
	author = {Micciancio, Daniele},
	year = {2001},
	pages = {2001},
}

@inproceedings{ajtai_generating_1996,
	title = {Generating {Hard} {Instances} of {Lattice} {Problems} ({Extended} {Abstract})},
	booktitle = {In {Proceedings} of the {Twenty}-{Eighth} {Annual} {ACM} {Symposium} on the {Theory} of {Computing}},
	publisher = {ACM},
	author = {Ajtai, M.},
	year = {1996},
	pages = {99--108},
}

@article{regev_learning_2010,
	title = {The {Learning} with {Errors} {Problem}},
	author = {Regev, Oded},
	month = jan,
	year = {2010},
}

@misc{lyubashevsky_o_2010,
	title = {O.: {On} ideal lattices and learning with errors over rings},
	author = {Lyubashevsky, Vadim and Peikert, Chris and Regev, Oded},
	year = {2010},
}

@misc{langlois_worst-case_2012,
	title = {Worst-{Case} to {Average}-{Case} {Reductions} for {Module} {Lattices}},
	url = {https://eprint.iacr.org/2012/090},
	author = {Langlois, Adeline and Stehle, Damien},
	year = {2012},
	note = {Published: Cryptology ePrint Archive, Paper 2012/090},
	annote = {https://eprint.iacr.org/2012/090},
}

@misc{boudgoust_hardness_2022,
	title = {On the {Hardness} of {Module} {Learning} {With} {Errors} with {Short} {Distributions}},
	url = {https://eprint.iacr.org/2022/472},
	author = {Boudgoust, Katharina and Jeudy, Corentin and Roux-Langlois, Adeline and Wen, Weiqiang},
	year = {2022},
	note = {Published: Cryptology ePrint Archive, Paper 2022/472},
	annote = {https://eprint.iacr.org/2022/472},
}

@misc{bos_crystals_2017,
	title = {{CRYSTALS} – {Kyber}: a {CCA}-secure module-lattice-based {KEM}},
	url = {https://eprint.iacr.org/2017/634},
	author = {Bos, Joppe and Ducas, Léo and Kiltz, Eike and Lepoint, Tancrède and Lyubashevsky, Vadim and Schanck, John M. and Schwabe, Peter and Seiler, Gregor and Stehlé, Damien},
	year = {2017},
	doi = {10.1109/EuroSP.2018.00032},
	note = {Published: Cryptology ePrint Archive, Paper 2017/634},
	annote = {https://eprint.iacr.org/2017/634},
}

@article{schonhage_schnelle_2005,
	title = {Schnelle {Multiplikation} großer {Zahlen}},
	volume = {7},
	journal = {Computing},
	author = {Schönhage, Arnold and Strassen, Volker},
	year = {2005},
	pages = {281--292},
}

@inproceedings{avanzi_crystals-kyber_2019,
	title = {{CRYSTALS}-{Kyber} {Algorithm} {Specification} {And} {Supporting} {Documentation} ( version 2 . 0 )},
	author = {Avanzi, Roberto Maria and Bos, Joppe W. and Ducas, Léo and Kiltz, Eike and Lepoint, Tancrède and Lyubashevsky, Vadim and Schanck, John M. and Schwabe, Peter and Seiler, Gregor and Stehlé, Damien},
	year = {2019},
}

@article{lyubashevsky_nttru_2019,
	title = {{NTTRU}: {Truly} {Fast} {NTRU} {Using} {NTT}},
	volume = {2019},
	journal = {IACR Cryptol. ePrint Arch.},
	author = {Lyubashevsky, Vadim and Seiler, Gregor},
	year = {2019},
	pages = {40},
}

@article{cooley_algorithm_1965,
	title = {An algorithm for the machine calculation of complex {Fourier} series},
	volume = {19},
	journal = {Mathematics of Computation},
	author = {Cooley, James W. and Tukey, John W.},
	year = {1965},
	pages = {297--301},
}

@article{montgomery_modular_1985,
	title = {Modular multiplication without trial division},
	volume = {44},
	journal = {Mathematics of Computation},
	author = {Montgomery, Peter L.},
	year = {1985},
	pages = {519--521},
}

@inproceedings{barrett_implementing_1986,
	title = {Implementing the {Rivest} {Shamir} and {Adleman} {Public} {Key} {Encryption} {Algorithm} on a {Standard} {Digital} {Signal} {Processor}},
	booktitle = {{CRYPTO}},
	author = {Barrett, Paul},
	year = {1986},
}

@inproceedings{gentleman_fast_1966,
	title = {Fast {Fourier} {Transforms}: for fun and profit},
	booktitle = {{AFIPS} '66 ({Fall})},
	author = {Gentleman, W. Morven and Sande, Gordon},
	year = {1966},
}

@phdthesis{kannwischer_polynomial_2022,
	type = {{PhD} {Thesis}},
	title = {Polynomial {Multiplication} for {Post}-{Quantum} {Cryptography}},
	url = {https://kannwischer.eu/thesis/},
	school = {Radboud University, Max Planck Institute for Security and Privacy, and Academia Sinica},
	author = {Kannwischer, Matthias J.},
	year = {2022},
}

@misc{roy_compact_2013,
	title = {Compact {Ring}-{LWE} based {Cryptoprocessor}},
	url = {https://eprint.iacr.org/2013/866},
	author = {Roy, Sujoy Sinha and Vercauteren, Frederik and Mentens, Nele and Chen, Donald Donglong and Verbauwhede, Ingrid},
	year = {2013},
	note = {Published: Cryptology ePrint Archive, Paper 2013/866},
	annote = {https://eprint.iacr.org/2013/866},
}

@misc{poppelmann_high-performance_2015,
	title = {High-{Performance} {Ideal} {Lattice}-{Based} {Cryptography} on 8-bit {ATxmega} {Microcontrollers}},
	url = {https://eprint.iacr.org/2015/382},
	author = {Pöppelmann, Thomas and Oder, Tobias and Güneysu, Tim},
	year = {2015},
	note = {Published: Cryptology ePrint Archive, Paper 2015/382},
	annote = {https://eprint.iacr.org/2015/382},
}

@inproceedings{chu_inside_2019,
	title = {Inside the {FFT} {Black} {Box}: {Serial} and {Parallel} {Fast} {Fourier} {Transform} {Algorithms}},
	author = {Chu, Eleanor and George, Alan},
	year = {2019},
}

@misc{klemsa_fast_2021,
	title = {Fast and {Error}-{Free} {Negacyclic} {Integer} {Convolution} using {Extended} {Fourier} {Transform}},
	url = {https://eprint.iacr.org/2021/480},
	author = {Klemsa, Jakub},
	year = {2021},
	note = {Published: Cryptology ePrint Archive, Paper 2021/480},
	annote = {https://eprint.iacr.org/2021/480},
}

@book{noauthor_arm_2001,
	edition = {updated},
	title = {{ARM} {Developer} {Suite} ® {Version} 1.2 {Assembler} {Guide}},
	shorttitle = {www.arm.com},
	url = {https://developer.arm.com/documentation/dui0068/latest/},
	language = {English},
	month = nov,
	year = {2001},
}

@misc{sprenkels_kyberdilithium_2020,
	title = {The {Kyber}/{Dilithium} {NTT}},
	url = {https://dsprenkels.com/ntt.html},
	author = {Sprenkels, Daan},
	month = sep,
	year = {2020},
	note = {Publication Title: Blog Post
Type: Blog Post},
}

@misc{seiler_faster_2018,
	title = {Faster {AVX2} optimized {NTT} multiplication for {Ring}-{LWE} lattice cryptography},
	url = {https://eprint.iacr.org/2018/039},
	author = {Seiler, Gregor},
	year = {2018},
	note = {Published: Cryptology ePrint Archive, Paper 2018/039},
	annote = {https://eprint.iacr.org/2018/039},
}

@misc{noauthor_round_nodate,
	title = {Round 2 {Submissions} - {Post}-{Quantum} {Cryptography} {\textbar} {CSRC} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/Projects/post-quantum-cryptography/post-quantum-cryptography-standardization/round-2-submissions},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_round_nodate-1,
	title = {Round 1 {Submissions} - {Post}-{Quantum} {Cryptography} {\textbar} {CSRC} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/Projects/post-quantum-cryptography/post-quantum-cryptography-standardization/Round-1-Submissions},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_round_nodate-2,
	title = {Round 3 {Submissions} - {Post}-{Quantum} {Cryptography} {\textbar} {CSRC} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/Projects/post-quantum-cryptography/post-quantum-cryptography-standardization/round-3-submissions},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_announcing_nodate,
	title = {Announcing {PQC} {Candidates} to be {Standardized}, {Plus} {Fourth} {Round} {Candidates} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/News/2022/pqc-candidates-to-be-standardized-and-round-4},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_ibm_nodate,
	title = {{IBM} {Unveils} {New} {Roadmap} to {Practical} {Quantum} {Computing} {Era}; {Plans} to {Deliver} 4,000+ {Qubit} {System} — newsroom.ibm.com},
	url = {https://newsroom.ibm.com/2022-05-10-IBM-Unveils-New-Roadmap-to-Practical-Quantum-Computing-Era-Plans-to-Deliver-4,000-Qubit-System},
	annote = {[Accessed 24-Sep-2022]},
}

@inproceedings{padamvathi_quantum_2016,
	title = {Quantum {Cryptography} and {Quantum} {Key} {Distribution} {Protocols}: {A} {Survey}},
	doi = {10.1109/IACC.2016.109},
	booktitle = {2016 {IEEE} 6th {International} {Conference} on {Advanced} {Computing} ({IACC})},
	author = {Padamvathi, V. and Vardhan, B. Vishnu and Krishna, A.V.N.},
	year = {2016},
	pages = {556--562},
}

@book{noauthor_submission_2016,
	title = {Submission {Requirements} and {Evaluation} {Criteria} for the {Post}-{Quantum} {Cryptography} {Standardization} {Process}},
	url = {https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf},
	publisher = {National Institute of Standards and Technology},
	year = {2016},
}

@inproceedings{cheng_efficient_2005,
	title = {Efficient {FPGA} implementation of {FFT} based multipliers},
	doi = {10.1109/CCECE.2005.1557215},
	booktitle = {Canadian {Conference} on {Electrical} and {Computer} {Engineering}, 2005.},
	author = {Cheng, Lo Sing and Miri, A. and Yeap, Tet Hin},
	year = {2005},
	pages = {1300--1303},
}

@inproceedings{emeliyanenko_efficient_2009,
	address = {Berlin, Heidelberg},
	title = {Efficient {Multiplication} of {Polynomials} on {Graphics} {Hardware}},
	isbn = {978-3-642-03644-6},
	abstract = {We present the algorithm to multiply univariate polynomials with integer coefficients efficiently using the Number Theoretic transform (NTT) on Graphics Processing Units (GPU). The same approach can be used to multiply large integers encoded as polynomials. Our algorithm exploits fused multiply-add capabilities of the graphics hardware. NTT multiplications are executed in parallel for a set of distinct primes followed by reconstruction using the Chinese Remainder theorem (CRT) on the GPU. Our benchmarking experiences show the NTT multiplication performance up to 77 GMul/s. We compared our approach with CPU-based implementations of polynomial and large integer multiplication provided by NTL and GMP libraries.},
	booktitle = {Advanced {Parallel} {Processing} {Technologies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Emeliyanenko, Pavel},
	editor = {Dou, Yong and Gruber, Ralf and Joller, Josef M.},
	year = {2009},
	pages = {134--149},
}

@inproceedings{renteria-mejia_hardware_2014,
	title = {Hardware {Design} of an n-coefficient {NTT}-{Based} {Polynomial} {Multiplier}},
	doi = {10.1109/SPL.2014.7002209},
	booktitle = {2014 {IX} {Southern} {Conference} on {Programmable} {Logic} ({SPL})},
	author = {Rentería-Mejía, C. P. and Velasco-Medina, J.},
	year = {2014},
	pages = {1--5},
}

@inproceedings{ghosh_speed_2012,
	title = {A {Speed} {Area} {Optimized} {Embedded} {Co}-processor for {McEliece} {Cryptosystem}},
	doi = {10.1109/ASAP.2012.16},
	booktitle = {2012 {IEEE} 23rd {International} {Conference} on {Application}-{Specific} {Systems}, {Architectures} and {Processors}},
	author = {Ghosh, Santosh and Delvaux, Jeroen and Uhsadel, Leif and Verbauwhede, Ingrid},
	year = {2012},
	pages = {102--108},
}

@inproceedings{nguyen_high-level_2019,
	title = {A {High}-{Level} {Synthesis} {Approach} to the {Software}/{Hardware} {Codesign} of {NTT}-{Based} {Post}-{Quantum} {Cryptography} {Algorithms}},
	doi = {10.1109/ICFPT47387.2019.00070},
	booktitle = {2019 {International} {Conference} on {Field}-{Programmable} {Technology} ({ICFPT})},
	author = {Nguyen, Duc Tri and Dang, Viet B. and Gaj, Kris},
	year = {2019},
	pages = {371--374},
}

@inproceedings{farahmand_softwarehardware_2019,
	title = {Software/{Hardware} {Codesign} of the {Post} {Quantum} {Cryptography} {Algorithm} {NTRUEncrypt} {Using} {High}-{Level} {Synthesis} and {Register}-{Transfer} {Level} {Design} {Methodologies}},
	doi = {10.1109/FPL.2019.00042},
	booktitle = {2019 29th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Farahmand, Farnoud and Nguyen, Duc Tri and Dang, Viet B. and Ferozpuri, Ahmed and Gaj, Kris},
	year = {2019},
	pages = {225--231},
}

@inproceedings{alkim_newhope_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NewHope} on {ARM} {Cortex}-{M}},
	volume = {10076},
	booktitle = {Security, {Privacy}, and {Advanced} {Cryptography} {Engineering}},
	publisher = {Springer-Verlag Berlin Heidelberg},
	author = {Alkim, Erdem and Jakubeit, Philipp and Schwabe, Peter},
	editor = {Carlet, Claude and Hasan, Anwar and Saraswat, Vishal},
	year = {2016},
	pages = {332--349},
	annote = {Document ID: c7a82d41d39c535fd09ca1b032ebca1b, http://cryptojedi.org/papers/\#newhopearm},
}


@book{kim_comparison_2019,
	title = {A {Comparison} of {Online} {Automatic} {Speech} {Recognition} {Systems} and the {Nonverbal} {Responses} to {Unintelligible} {Speech}},
	abstract = {Automatic Speech Recognition (ASR) systems have proliferated over the recent years to the point that free platforms such as YouTube now provide speech recognition services. Given the wide selection of ASR systems, we contribute to the field of automatic speech recognition by comparing the relative performance of two sets of manual transcriptions and five sets of automatic transcriptions (Google Cloud, IBM Watson, Microsoft Azure, Trint, and YouTube) to help researchers to select accurate transcription services. In addition, we identify nonverbal behaviors that are associated with unintelligible speech, as indicated by high word error rates. We show that manual transcriptions remain superior to current automatic transcriptions. Amongst the automatic transcription services, YouTube offers the most accurate transcription service. For non-verbal behavioral involvement, we provide evidence that the variability of smile intensities from the listener is high (low) when the speaker is clear (unintelligible). These findings are derived from videoconferencing interactions between student doctors and simulated patients; therefore, we contribute towards both the ASR literature and the healthcare communication skills teaching community.},
	author = {Kim, Joshua and Liu, Chunfeng and Calvo, Rafael and McCabe, Kathryn and Taylor, Silas and Schuller, Björn and Wu, Kaihang},
	month = apr,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\83SMHLHA\\Kim et al. - 2019 - A Comparison of Online Automatic Speech Recognitio.pdf:application/pdf},
}

@misc{bisheh-niasar_high-speed_2021,
	title = {High-{Speed} {NTT}-based {Polynomial} {Multiplication} {Accelerator} for {CRYSTALS}-{Kyber} {Post}-{Quantum} {Cryptography}},
	url = {https://eprint.iacr.org/2021/563},
	author = {Bisheh-Niasar, Mojtaba and Azarderakhsh, Reza and Mozaffari-Kermani, Mehran},
	year = {2021},
	note = {Published: Cryptology ePrint Archive, Paper 2021/563},
	annote = {https://eprint.iacr.org/2021/563},
}

@inproceedings{du_towards_2016,
	title = {Towards efficient polynomial multiplication for lattice-based cryptography},
	doi = {10.1109/ISCAS.2016.7527456},
	booktitle = {2016 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Du, Chaohui and Bai, Guoqiang},
	year = {2016},
	pages = {1178--1181},
}

@article{chung_ntt_2021,
	title = {{NTT} {Multiplication} for {NTT}-unfriendly {Rings}: {New} {Speed} {Records} for {Saber} and {NTRU} on {Cortex}-{M4} and {AVX2}},
	volume = {2021},
	url = {https://tches.iacr.org/index.php/TCHES/article/view/8791},
	doi = {10.46586/tches.v2021.i2.159-188},
	number = {2},
	journal = {IACR Transactions on Cryptographic Hardware and Embedded Systems},
	author = {Chung, Chi-Ming Marvin and Hwang, Vincent and Kannwischer, Matthias J. and Seiler, Gregor and Shih, Cheng-Jhih and Yang, Bo-Yin},
	month = feb,
	year = {2021},
	pages = {159--188},
}

@inproceedings{poppelmann_towards_2012,
	address = {Berlin, Heidelberg},
	title = {Towards {Efficient} {Arithmetic} for {Lattice}-{Based} {Cryptography} on {Reconfigurable} {Hardware}},
	isbn = {978-3-642-33481-8},
	abstract = {In recent years lattice-based cryptography has emerged as quantum secure and theoretically elegant alternative to classical cryptographic schemes (like ECC or RSA). In addition to that, lattices are a versatile tool and play an important role in the development of efficient fully or somewhat homomorphic encryption (SHE/FHE) schemes. In practice, ideal lattices defined in the polynomial ring ℤp[x]/〈xnþinspace+þinspace1〉 allow the reduction of the generally very large key sizes of lattice constructions. Another advantage of ideal lattices is that polynomial multiplication is a basic operation that has, in theory, only quasi-linear time complexity of \$\{{\textbackslash}backslashmathcal O\}(n {\textbackslash}backslashlog\{n\})\$in ℤp[x]/〈xnþinspace+þinspace1〉. However, few is known about the practical performance of the FFT in this specific application domain and whether it is really an alternative. In this work we make a first step towards efficient FFT-based arithmetic for lattice-based cryptography and show that the FFT can be implemented efficiently on reconfigurable hardware. We give instantiations of recently proposed parameter sets for homomorphic and public-key encryption. In a generic setting we are able to multiply polynomials with up to 4096 coefficients and a 17-bit prime in less than 0.5 milliseconds. For a parameter set of a SHE scheme (n=1024,p=1061093377) our implementation performs 9063 polynomial multiplications per second on a mid-range Spartan-6.},
	booktitle = {Progress in {Cryptology} – {LATINCRYPT} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Pöppelmann, Thomas and Güneysu, Tim},
	editor = {Hevia, Alejandro and Neven, Gregory},
	year = {2012},
	pages = {139--158},
}

@inproceedings{el-kady_high-level_2021,
	title = {High-{Level} {Synthesis} design approach for {Number}-{Theoretic} {Transform} {Implementations}},
	doi = {10.1109/VLSI-SoC53125.2021.9607003},
	booktitle = {2021 {IFIP}/{IEEE} 29th {International} {Conference} on {Very} {Large} {Scale} {Integration} ({VLSI}-{SoC})},
	author = {El-Kady, Alexander and Fournaris, Apostolos P. and Tsakoulis, Thanasis and Haleplidis, Evangelos and Paliouras, Vassilis},
	year = {2021},
	pages = {1--6},
}

@article{nannipieri_risc-v_2021,
	title = {A {RISC}-{V} {Post} {Quantum} {Cryptography} {Instruction} {Set} {Extension} for {Number} {Theoretic} {Transform} to {Speed}-{Up} {CRYSTALS} {Algorithms}},
	volume = {9},
	doi = {10.1109/ACCESS.2021.3126208},
	journal = {IEEE Access},
	author = {Nannipieri, Pietro and Di Matteo, Stefano and Zulberti, Luca and Albicocchi, Francesco and Saponara, Sergio and Fanucci, Luca},
	year = {2021},
	pages = {150798--150808},
}

@inproceedings{karabulut_rantt_2020,
	title = {{RANTT}: {A} {RISC}-{V} {Architecture} {Extension} for the {Number} {Theoretic} {Transform}},
	doi = {10.1109/FPL50879.2020.00016},
	booktitle = {2020 30th {International} {Conference} on {Field}-{Programmable} {Logic} and {Applications} ({FPL})},
	author = {Karabulut, Emre and Aysu, Aydin},
	year = {2020},
	pages = {26--32},
}

@misc{kannwischer_pqm4_nodate,
	title = {{PQM4}: {Post}-quantum crypto library for the {ARM} {Cortex}-{M4}},
	author = {Kannwischer, Matthias J. and Petri, Richard and Rijneveld, Joost and Schwabe, Peter and Stoffelen, Ko},
	annote = {https://github.com/mupq/pqm4},
}

@misc{alkim_newhope_2016-1,
	title = {{NewHope} on {ARM} {Cortex}-{M}},
	url = {https://eprint.iacr.org/2016/758},
	author = {Alkim, Erdem and Jakubeit, Philipp and Schwabe, Peter},
	year = {2016},
	note = {Published: Cryptology ePrint Archive, Paper 2016/758},
	annote = {https://eprint.iacr.org/2016/758},
}

@misc{botros_memory-efficient_2019,
	title = {Memory-{Efficient} {High}-{Speed} {Implementation} of {Kyber} on {Cortex}-{M4}},
	url = {https://eprint.iacr.org/2019/489},
	author = {Botros, Leon and Kannwischer, Matthias J. and Schwabe, Peter},
	year = {2019},
	note = {Published: Cryptology ePrint Archive, Paper 2019/489},
	annote = {https://eprint.iacr.org/2019/489},
}

@misc{kannwischer_pqm4_nodate-1,
	title = {{PQM4}: {Post}-quantum crypto library for the {ARM} {Cortex}-{M4}},
	author = {Kannwischer, Matthias J. and Petri, Richard and Rijneveld, Joost and Schwabe, Peter and Stoffelen, Ko},
	annote = {https://github.com/mupq/pqm4},
}

@misc{kannwischer_pqclean_nodate,
	title = {{PQClean}: {Clean}, portable, tested implementations of post-quantum cryptography},
	abstract = {Clean, portable, tested implementations of post-quantum cryptography - GitHub - PQClean/PQClean: Clean, portable tested implementations of post-quantum cryptography},
	author = {Kannwischer, Matthias J. and Rijneveld, Joost and Schwabe, Peter and Stebila, Douglas and Wiggers, Thom},
	annote = {https://github.com/PQClean/PQClean},
}

@article{venkatagiri_speech_2002-1,
	title = {Speech {Recognition} {Technology} {Applications} in {Communication} {Disorders}},
	volume = {11},
	issn = {1058-0360, 1558-9110},
	url = {http://pubs.asha.org/doi/10.1044/1058-0360%282002/037%29},
	doi = {10.1044/1058-0360(2002/037)},
	abstract = {Speech recognition (SR) is the process whereby a microprocessor-based system, typically a computer with sound processing hardware and speech recognition software, responds in predictable ways to spoken commands and/or converts speech into text. This tutorial describes the types and the general uses of SR and provides an explanation of the technology behind it. The emerging applications of SR technology for dictation, articulation training, language and literacy development, environmental control, and communication augmentation are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {American Journal of Speech-Language Pathology},
	author = {Venkatagiri, H. S.},
	month = nov,
	year = {2002},
	pages = {323--332},
}

@article{wang_overview_2019,
	title = {Overview of end-to-end speech recognition},
	volume = {1187},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1187/5/052068},
	doi = {10.1088/1742-6596/1187/5/052068},
	number = {5},
	urldate = {2022-11-05},
	journal = {Journal of Physics: Conference Series},
	author = {Wang, Song and Li, Guanyu},
	month = apr,
	year = {2019},
	pages = {052068},
}

@techreport{timothy_r_anderson_applications_1998,
	address = {Bre’tigny, France},
	title = {{APPLICATIONS} {OF} {SPEECH}-{BASED} {CONTROL}},
	author = {Timothy R. Anderson},
	month = oct,
	year = {1998},
	note = {Ohio, USA, 14-15 October 1998
Published in RTO EN-3},
}

@article{haton_problems_1994,
	title = {Problems and solutions for noisy speech recognition},
	volume = {04},
	issn = {1155-4339},
	url = {http://www.edpsciences.org/10.1051/jp4:1994592},
	doi = {10.1051/jp4:1994592},
	number = {C5},
	urldate = {2022-11-05},
	journal = {Le Journal de Physique IV},
	author = {Haton, J.-P.},
	month = may,
	year = {1994},
	pages = {C5--439--C5--448},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5UM3AEJV\\Haton - 1994 - Problems and solutions for noisy speech recognitio.pdf:application/pdf},
}

@inproceedings{garcia_lecumberri_non-native_2008,
	title = {The non-native consonant challenge for european languages},
	url = {https://www.isca-speech.org/archive/interspeech_2008/garcialecumberri08_interspeech.html},
	doi = {10.21437/Interspeech.2008-490},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Interspeech 2008},
	publisher = {ISCA},
	author = {Garcia Lecumberri, M. Luisa and Cooke, Martin and Cutugno, Francesco and Giurgiu, Mircea and Meyer, Bernd T. and Scharenborg, Odette and Dommelen, Wim van and Volin, Jan},
	month = sep,
	year = {2008},
	pages = {1781--1784},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\Z7KDFLJI\\Garcia Lecumberri et al. - 2008 - The non-native consonant challenge for european la.pdf:application/pdf},
}

@article{boothroyd_statistical_1968,
	title = {Statistical {Theory} of the {Speech} {Discrimination} {Score}},
	volume = {43},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1910787},
	doi = {10.1121/1.1910787},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Boothroyd, A.},
	month = feb,
	year = {1968},
	pages = {362--367},
}

@techreport{l_lamel_quaero_2010,
	address = {Limsi Orsay},
	title = {Quaero program - ctc project - progress report on task 5.1: {Speech} to text},
	url = {https://www.linkedin.com/company/quaero/},
	urldate = {2022-11-05},
	author = {L. Lamel},
	year = {2010},
	note = {http://www.quaero.org/developpement-scientifique-onglet-synthese/
Cd.ctc.5.6., Quaero Program, .},
}

@article{lippmann_speech_1997-1,
	title = {Speech recognition by machines and humans},
	volume = {22},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639397000216},
	doi = {10.1016/S0167-6393(97)00021-6},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Lippmann, Richard P.},
	month = jul,
	year = {1997},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4VM3MB94\\Lippmann - 1997 - Speech recognition by machines and humans.pdf:application/pdf},
}

@inproceedings{lawrence_r_rabiner_and_sorin_dusan_can_2005,
	title = {{CAN} {AUTOMATIC} {SPEECH} {RECOGNITION} {LEARN} {MORE} {FROM} {HUMAN} {SPEECH} {PERCEPTION}},
	author = {Lawrence R. Rabiner {and} Sorin Dusan},
	year = {2005},
}

@article{benzeghiba_automatic_2007,
	title = {Automatic speech recognition and speech variability: {A} review},
	volume = {49},
	issn = {01676393},
	shorttitle = {Automatic speech recognition and speech variability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000404},
	doi = {10.1016/j.specom.2007.02.006},
	language = {en},
	number = {10-11},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Benzeghiba, M. and De Mori, R. and Deroo, O. and Dupont, S. and Erbes, T. and Jouvet, D. and Fissore, L. and Laface, P. and Mertins, A. and Ris, C. and Rose, R. and Tyagi, V. and Wellekens, C.},
	month = oct,
	year = {2007},
	pages = {763--786},
}

@inproceedings{rk_moore_comparison_2003,
	address = {Gen`eve, Suisse},
	title = {A comparison of the data requirements of automatic speech recognition systems and human listeners},
	booktitle = {In {Proceedings} of {Eurospeech}},
	author = {R.K. Moore},
	year = {2003},
	pages = {2582--2584},
}

@inproceedings{rk_moore_and_a_cutler_constraints_2001,
	title = {Constraints on theories of humans vs. machine recognition of speech.},
	booktitle = {{SPRAAC} {Workshop} on {HSR} as {Pattern} {Classification}},
	author = {R.K. Moore {and} A. Cutler},
	year = {2001},
}

@inproceedings{n_deshmuk_rj_duncan_a_ganapathraju_and_j_picone_response_nodate,
	address = {Philadelphia, USA},
	title = {Response time as metric for comparison of speech recognition by humans and machines},
	booktitle = {{ICSLP}},
	author = {N. Deshmuk, R.J. Duncan, A. Ganapathraju, {and} J. Picone},
}

@inproceedings{lcw_pols_flexible_1997,
	title = {Flexible human speech recognition},
	booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}},
	author = {L.C.W. Pols},
	year = {1997},
}

@inproceedings{o_scharenborg_d_norris_l_ten_bosch_and_jm_mc-queen_how_2005,
	title = {How should a speech recognizer work? {Cognitive} {Science}, 29(2005):867–918.},
	author = {O. Scharenborg, D. Norris, L. ten Bosch, {and} J.M. Mc-Queen},
	year = {2005},
}

@article{scharenborg_how_2005,
	title = {How {Should} a {Speech} {Recognizer} {Work}?},
	volume = {29},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1207/s15516709cog0000_37},
	doi = {10.1207/s15516709cog0000_37},
	language = {en},
	number = {6},
	urldate = {2022-11-05},
	journal = {Cognitive Science},
	author = {Scharenborg, Odette and Norris, Dennis and ten Bosch, Louis and McQueen, James M.},
	month = nov,
	year = {2005},
	pages = {867--918},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\8MAVAN8F\\Scharenborg et al. - 2005 - How Should a Speech Recognizer Work.pdf:application/pdf},
}

@article{scharenborg_reaching_2007,
	title = {Reaching over the gap: {A} review of efforts to link human and automatic speech recognition research},
	volume = {49},
	issn = {01676393},
	shorttitle = {Reaching over the gap},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000106},
	doi = {10.1016/j.specom.2007.01.009},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Scharenborg, Odette},
	month = may,
	year = {2007},
	pages = {336--347},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\A8IJ4SXY\\Scharenborg - 2007 - Reaching over the gap A review of efforts to link.pdf:application/pdf},
}

@inproceedings{furui_robust_2003,
	title = {Robust methods in automatic speech recognition and understanding},
	url = {https://www.isca-speech.org/archive/eurospeech_2003/furui03_eurospeech.html},
	doi = {10.21437/Eurospeech.2003-575},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {8th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 2003)},
	publisher = {ISCA},
	author = {Furui, Sadaoki},
	month = sep,
	year = {2003},
	pages = {1993--1998},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\BJUNT7HU\\Furui - 2003 - Robust methods in automatic speech recognition and.pdf:application/pdf},
}

@inproceedings{takahiro_shinozaki_and_sadaoki_furui_assessment_2003,
	title = {An assessment of automatic recognition techniques for spontaneous speech in comparison with human performance},
	url = {https://www.isca-speech.org/archive/sspr_2003/shinozaki03_sspr.html},
	booktitle = {Proc. {ISCA}/{IEEE} {Workshop} on {Spontaneous} {Speech} {Processing} and {Recognition}},
	author = {Takahiro Shinozaki {and} Sadaoki Furui},
	year = {2003},
	pages = {paper MAP15},
}

@inproceedings{a_cutler_and_t_robinson_1992_1992,
	address = {Banff, Canada.},
	title = {1992. {Response} time as metric for comparison of speech recognition by humans and machines},
	booktitle = {In {Proc}. of {ICSLP}},
	author = {A. Cutler {and} T. Robinson},
	year = {1992},
	pages = {189--192},
}

@inproceedings{da_van_leeuwen_l-g_van_den_berg_and_hjmsteeneken_human_1995,
	address = {Madrid, Spain},
	title = {Human benchmarks for speaker independent large vocabulary recognition performance},
	booktitle = {In {Proc}. of {Eurospeech}},
	author = {D.A. Van Leeuwen, L.-G. Van den Berg, {and} H.J.M.Steeneken},
	year = {1995},
	pages = {1461--1464},
}

@inproceedings{b_meyer_and_twesker_human-machine_2006,
	address = {Toulouse, France},
	title = {A human-machine comparison in speech recognition based on a logatome corpus},
	booktitle = {In {Proc}. of {Workshop} on {Speech} {Recognition} and {Intrinsic} {Variation}},
	author = {B. Meyer {and} T.Wesker},
	year = {2006},
}

@article{sroka_and_ld_braida_human_2005,
	title = {Human and machine consonant recognition.},
	journal = {Speech Communication, 45(2005)},
	author = {Sroka {and} L.D. Braida},
	year = {2005},
	pages = {401--423},
}

@inproceedings{w_shen_j_olive_and_d_jones_two_2008,
	address = {Brisbane, Australia},
	title = {Two protocols comparing human and machine phonetic discrimination performance in conversational speech},
	booktitle = {In {Proc}. of {Interspeech}},
	author = {W. Shen, J. Olive, {and} D. Jones},
	year = {2008},
}

@article{rost_michael_introducing_1994,
	title = {Introducing listening},
	author = {Rost, Michael},
	month = jan,
	year = {1994},
}

@book{rost_michael_teaching_2016,
	title = {Teaching and researching listening: {Third} edition},
	author = {Rost, Michael},
	month = jan,
	year = {2016},
}

@article{webb_vocabulary_2009,
	title = {Vocabulary {Demands} of {Television} {Programs}},
	volume = {59},
	issn = {00238333, 14679922},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9922.2009.00509.x},
	doi = {10.1111/j.1467-9922.2009.00509.x},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Language Learning},
	author = {Webb, Stuart and Rodgers, Michael P. H.},
	month = jun,
	year = {2009},
	pages = {335--366},
}

@article{webb_lexical_2009,
	title = {The {Lexical} {Coverage} of {Movies}},
	volume = {30},
	issn = {0142-6001, 1477-450X},
	url = {https://academic.oup.com/applij/article-lookup/doi/10.1093/applin/amp010},
	doi = {10.1093/applin/amp010},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Applied Linguistics},
	author = {Webb, S. and Rodgers, M. P. H.},
	month = sep,
	year = {2009},
	pages = {407--427},
}

@article{webb_stuart_using_2010,
	title = {Using glossaries to increase the lexical coverage of television programs},
	volume = {22},
	journal = {Reading in a Foreign Language},
	author = {Webb, Stuart},
	month = jan,
	year = {2010},
}

@article{goldwater_which_2010,
	title = {Which words are hard to recognize? {Prosodic}, lexical, and disfluency factors that increase speech recognition error rates},
	volume = {52},
	issn = {01676393},
	shorttitle = {Which words are hard to recognize?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639309001599},
	doi = {10.1016/j.specom.2009.10.001},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Goldwater, Sharon and Jurafsky, Dan and Manning, Christopher D.},
	month = mar,
	year = {2010},
	pages = {181--200},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\W4MWM6JM\\Goldwater et al. - 2010 - Which words are hard to recognize Prosodic, lexic.pdf:application/pdf},
}

@article{field_bricks_2008,
	title = {Bricks or {Mortar}: {Which} {Parts} of the {Input} {Does} a {Second} {Language} {Listener} {Rely} on?},
	volume = {42},
	issn = {0039-8322, 1545-7249},
	shorttitle = {Bricks or {Mortar}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.1545-7249.2008.tb00139.x},
	doi = {10.1002/j.1545-7249.2008.tb00139.x},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {TESOL Quarterly},
	author = {Field, John},
	month = sep,
	year = {2008},
	pages = {411--432},
}

@article{webb_learning_2011,
	title = {Learning {Collocations}: {Do} the {Number} of {Collocates}, {Position} of the {Node} {Word}, and {Synonymy} {Affect} {Learning}?},
	volume = {32},
	issn = {1477-450X, 0142-6001},
	shorttitle = {Learning {Collocations}},
	url = {https://academic.oup.com/applij/article-lookup/doi/10.1093/applin/amq051},
	doi = {10.1093/applin/amq051},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Applied Linguistics},
	author = {Webb, Stuart and Kagimoto, Eve},
	month = jul,
	year = {2011},
	pages = {259--276},
}

@article{webb_pre-learning_2010,
	title = {Pre-learning low-frequency vocabulary in second language television programmes},
	volume = {14},
	issn = {1362-1688, 1477-0954},
	url = {http://journals.sagepub.com/doi/10.1177/1362168810375371},
	doi = {10.1177/1362168810375371},
	abstract = {This study investigated the potential of pre-learning frequently occurring low-frequency vocabulary as a means to increase comprehension of television and incidental vocabulary learning through watching television. Eight television programmes, each representing different television genres, were analysed using the RANGE program to determine the 10 most frequent low-frequency word-families in each programme and the coverage that they represented. The results showed that coverage of the 10 most frequent low-frequency word-families ranged from 0.70\% to 3.91\%, coverage of the most frequent 3,000 to 3,999 word-families ranged from 0.22\% to 2.58\%, and coverage of the 4,000 to 4,999 word-families ranged from 0.35\% to 1.96\%. This result shows the relative value of pre-learning vocabulary in television programmes and provides a strong argument for pre-learning vocabulary. The findings also suggested that if learners knew the most frequent 3,000 word-families and pre-learned low-frequency vocabulary, comprehension and incidental vocabulary learning may increase.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Language Teaching Research},
	author = {Webb, Stuart},
	month = oct,
	year = {2010},
	pages = {501--515},
}

@article{webb_corpus_2010,
	title = {A corpus driven study of the potential for vocabulary learning through watching movies},
	volume = {15},
	issn = {1384-6655, 1569-9811},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.4.03web},
	doi = {10.1075/ijcl.15.4.03web},
	abstract = {In this corpus driven study, the scripts of 143 movies consisting of 1,267,236 running words were analyzed using the RANGE program (Heatley et al. 2002) to determine the number of encounters with low frequency words. Low frequency words were operationalized as items from Nation’s (2004) 4th to 14th 1,000-word BNC lists. The results showed that in a single movie, few words were encountered 10 or more times indicating that only a small number of words may be learned through watching one movie. However, as the number of movies analyzed increased, the number of words encountered 10 or more times increased. Twenty-three percent of the word families from Nation’s (2004) 4th 1,000-word list were encountered 10 or more times in a set of 70 movies. This indicates that if learners watch movies regularly over a long period of time, there is the potential for significant incidental learning to occur},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {International Journal of Corpus Linguistics},
	author = {Webb, Stuart},
	month = nov,
	year = {2010},
	pages = {497--519},
}

@article{webb_selecting_2011,
	title = {Selecting {Television} {Programs} for {Language} {Learning}: {Investigating} {Television} {Programs} from the {Same} {Genre}},
	volume = {11},
	shorttitle = {Selecting {Television} {Programs} for {Language} {Learning}},
	doi = {10.6018/ijes.11.1.137131},
	abstract = {The scripts of 288 television episodes were analysed to determine the extent to which vocabulary reoccurs in television programs from the same subgenres and unrelated television programs from different genres. Episodes from two programs from each of the following three subgenres of the American drama genre: medical, spy/action, and criminal forensic investigation were compared with different sets of random episodes. The results showed that although there were an equivalent number of running words in each set of episodes, the episodes from programs within the same subgenre contained fewer word families than random programs. The findings also showed that low frequency word families (4000-14,000 levels) reoccur more often in programs within the same subgenre. Together the results indicate that watching programs within the same subgenre may be an effective approach to language learning with television because it reduces the lexical demands of viewing and increases the potential for vocabulary learning. RESUMEN Los guiones de 288 episodios televisivos se analizaron para determinar el alcance de la recursividad del vocabulario en programas de televisión del mismo subgénero y en programas no relacionados de géneros diferentes. Se compararon episodios de tres subgéneros del drama americano: médico, de espías/acción y de investigación forense, con varios grupos de episodios elegidos al azar. Los resultados muestran que, aunque el número de palabras en cada grupo de episodios era equivalente, los episodios del mismo subgénero contienen menos familias de palabras que aquellos elegidos al azar. Los hallazgos mostraron que las familias de baja frecuencia (niveles de 4.000-14.000) se repiten con más frecuencia en los programas del mismo subgénero. En conjunto, los resultados indican que el visionado de programas del mismo subgénero puede ser un método efectivo para aprender el lenguaje por medio de la televisión porque reduce la demanda léxica de la proyección y aumenta el potencial de aprendizaje de vocabulario. PALABRAS CLAVE: Comprensión, Lingüística de corpus, género, aprendizaje incidental, televisión, cobertura del vocabulario, frecuencia léxica.},
	journal = {International Journal of English Studies (IJES)},
	author = {Webb, Stuart},
	month = jun,
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IBB7BUSU\\Webb - 2011 - Selecting Television Programs for Language Learnin.pdf:application/pdf},
}

@book{holmes_introduction_2013,
	edition = {0},
	title = {An {Introduction} to {Sociolinguistics}},
	isbn = {978-1-292-00506-5},
	url = {https://www.taylorfrancis.com/books/9781317860723},
	language = {en},
	urldate = {2022-11-05},
	publisher = {Routledge},
	author = {Holmes, Janet},
	month = oct,
	year = {2013},
	doi = {10.4324/9781315833057},
}

@inproceedings{mirzaei_errors_2015-1,
	title = {Errors in automatic speech recognition versus difficulties in second language listening},
	isbn = {978-1-908416-29-2},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2015.000367},
	doi = {10.14705/rpnet.2015.000367},
	urldate = {2022-11-05},
	booktitle = {Critical {CALL} – {Proceedings} of the 2015 {EUROCALL} {Conference}, {Padova}, {Italy}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Meshgi, Kourosh and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2015},
	pages = {410--415},
}

@inproceedings{mirzaei_partial_2014,
	title = {Partial and synchronized captioning: {A} new tool for second language listening development},
	isbn = {978-1-908416-20-9},
	shorttitle = {Partial and synchronized captioning},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2014.000223},
	doi = {10.14705/rpnet.2014.000223},
	urldate = {2022-11-05},
	booktitle = {{CALL} {Design}: {Principles} and {Practice} - {Proceedings} of the 2014 {EUROCALL} {Conference}, {Groningen}, {The} {Netherlands}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2014},
	pages = {230--236},
}

@inproceedings{shimogori_automatically_2010,
	address = {Copenhagen, Denmark},
	title = {Automatically generated captions: will they help non-native speakers communicate in english?},
	isbn = {978-1-4503-0108-4},
	shorttitle = {Automatically generated captions},
	url = {http://portal.acm.org/citation.cfm?doid=1841853.1841865},
	doi = {10.1145/1841853.1841865},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 3rd international conference on {Intercultural} collaboration - {ICIC} '10},
	publisher = {ACM Press},
	author = {Shimogori, Nobuhiro and Ikeda, Tomoo and Tsuboi, Sougo},
	year = {2010},
	pages = {79},
}

@inproceedings{shinozaki_error_2001,
	address = {Madonna di Campiglio, Italy},
	title = {Error analysis using decision trees in spontaneous presentation speech recognition},
	isbn = {978-0-7803-7343-3},
	url = {http://ieeexplore.ieee.org/document/1034621/},
	doi = {10.1109/ASRU.2001.1034621},
	urldate = {2022-11-05},
	booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}, 2001. {ASRU} '01.},
	publisher = {IEEE},
	author = {Shinozaki, T. and Furui, S.},
	year = {2001},
	pages = {198--201},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AYTJH6KH\\Shinozaki and Furui - 2001 - Error analysis using decision trees in spontaneous.pdf:application/pdf},
}

@article{thomson_computer_2011,
	title = {Computer {Assisted} {Pronunciation} {Training}: {Targeting} {Second} {Language} {Vowel} {Perception} {Improves} {Pronunciation}},
	volume = {28},
	issn = {07427778},
	shorttitle = {Computer {Assisted} {Pronunciation} {Training}},
	url = {http://www.equinoxpub.com/journals/index.php/CALICO/article/view/22985},
	doi = {10.11139/cj.28.3.744-765},
	number = {3},
	urldate = {2022-11-05},
	journal = {CALICO Journal},
	author = {Thomson, Ron I.},
	month = may,
	year = {2011},
	pages = {744--765},
}

@article{yuan_pauses_2021,
	title = {Pauses for {Detection} of {Alzheimer}’s {Disease}},
	volume = {2},
	issn = {2624-9898},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2020.624488/full},
	doi = {10.3389/fcomp.2020.624488},
	abstract = {Pauses, disfluencies and language problems in Alzheimer’s disease can be naturally modeled by fine-tuning Transformer-based pre-trained language models such as BERT and ERNIE. Using this method with pause-encoded transcripts, we achieved 89.6\% accuracy on the test set of the ADReSS (
              A
              lzheimer’s
              D
              ementia
              Re
              cognition through
              S
              pontaneous
              S
              peech) Challenge. The best accuracy was obtained with ERNIE, plus an encoding of pauses. Robustness is a challenge for large models and small training sets. Ensemble over many runs of BERT/ERNIE fine-tuning reduced variance and improved accuracy. We found that
              um
              was used much less frequently in Alzheimer’s speech, compared to
              uh
              . We discussed this interesting finding from linguistic and cognitive perspectives.},
	urldate = {2022-11-05},
	journal = {Frontiers in Computer Science},
	author = {Yuan, Jiahong and Cai, Xingyu and Bian, Yuchen and Ye, Zheng and Church, Kenneth},
	month = jan,
	year = {2021},
	pages = {624488},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\67RYRPW8\\Yuan et al. - 2021 - Pauses for Detection of Alzheimer’s Disease.pdf:application/pdf},
}

@article{bucks_analysis_2000,
	title = {Analysis of spontaneous, conversational speech in dementia of {Alzheimer} type: {Evaluation} of an objective technique for analysing lexical performance},
	volume = {14},
	issn = {0268-7038, 1464-5041},
	shorttitle = {Analysis of spontaneous, conversational speech in dementia of {Alzheimer} type},
	url = {http://www.tandfonline.com/doi/abs/10.1080/026870300401603},
	doi = {10.1080/026870300401603},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Aphasiology},
	author = {Bucks, R. S. and Singh, S. and Cuerden, J. M. and Wilcock, G. K.},
	month = jan,
	year = {2000},
	pages = {71--91},
}

@article{singh_evaluation_2001,
	title = {Evaluation of an objective technique for analysing temporal variables in {DAT} spontaneous speech},
	volume = {15},
	issn = {0268-7038, 1464-5041},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02687040143000041},
	doi = {10.1080/02687040143000041},
	language = {en},
	number = {6},
	urldate = {2022-11-05},
	journal = {Aphasiology},
	author = {Singh, Sameer and Bucks, Romola S. and Cuerden, Joanne M.},
	month = jun,
	year = {2001},
	pages = {571--583},
}

@article{pulido_alzheimers_2020,
	title = {Alzheimer's disease and automatic speech analysis: {A} review},
	volume = {150},
	issn = {09574174},
	shorttitle = {Alzheimer's disease and automatic speech analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420300397},
	doi = {10.1016/j.eswa.2020.113213},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Pulido, María Luisa Barragán and Hernández, Jesús Bernardino Alonso and Ballester, Miguel Ángel Ferrer and González, Carlos Manuel Travieso and Mekyska, Jiří and Smékal, Zdeněk},
	month = jul,
	year = {2020},
	pages = {113213},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4YMA25IC\\Pulido et al. - 2020 - Alzheimer's disease and automatic speech analysis.pdf:application/pdf},
}

@inproceedings{li_comparative_2021,
	address = {Toronto, ON, Canada},
	title = {A {Comparative} {Study} of {Acoustic} and {Linguistic} {Features} {Classification} for {Alzheimer}'s {Disease} {Detection}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414147/},
	doi = {10.1109/ICASSP39728.2021.9414147},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	pages = {6423--6427},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\54WJ2NZX\\Li et al. - 2021 - A Comparative Study of Acoustic and Linguistic Fea.pdf:application/pdf},
}

@inproceedings{yuan_pause-encoded_2021,
	address = {Toronto, ON, Canada},
	title = {Pause-{Encoded} {Language} {Models} for {Recognition} of {Alzheimer}’s {Disease} and {Emotion}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413548/},
	doi = {10.1109/ICASSP39728.2021.9413548},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Yuan, Jiahong and Cai, Xingyu and Church, Kenneth},
	month = jun,
	year = {2021},
	pages = {7293--7297},
}

@inproceedings{koo_exploiting_2020,
	title = {Exploiting {Multi}-{Modal} {Features} from {Pre}-{Trained} {Networks} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/koo20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3153},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Koo, Junghyun and Lee, Jie Hwan and Pyo, Jaewoo and Jo, Yujin and Lee, Kyogu},
	month = oct,
	year = {2020},
	pages = {2217--2221},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\P9SR4M7P\\Koo et al. - 2020 - Exploiting Multi-Modal Features from Pre-Trained N.pdf:application/pdf},
}

@article{szatloczki_speaking_2015,
	title = {Speaking in {Alzheimer}’s {Disease}, is {That} an {Early} {Sign}? {Importance} of {Changes} in {Language} {Abilities} in {Alzheimer}’s {Disease}},
	volume = {7},
	issn = {1663-4365},
	shorttitle = {Speaking in {Alzheimer}’s {Disease}, is {That} an {Early} {Sign}?},
	url = {http://journal.frontiersin.org/Article/10.3389/fnagi.2015.00195/abstract},
	doi = {10.3389/fnagi.2015.00195},
	urldate = {2022-11-05},
	journal = {Frontiers in Aging Neuroscience},
	author = {Szatloczki, Greta and Hoffmann, Ildiko and Vincze, Veronika and Kalman, Janos and Pakaski, Magdolna},
	month = oct,
	year = {2015},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ZFBPTM9I\\Szatloczki et al. - 2015 - Speaking in Alzheimer’s Disease, is That an Early .pdf:application/pdf},
}

@article{meilan_speech_2014,
	title = {Speech in {Alzheimer}'s {Disease}: {Can} {Temporal} and {Acoustic} {Parameters} {Discriminate} {Dementia}?},
	volume = {37},
	issn = {1420-8008, 1421-9824},
	shorttitle = {Speech in {Alzheimer}'s {Disease}},
	url = {https://www.karger.com/Article/FullText/356726},
	doi = {10.1159/000356726},
	abstract = {\textbf{\textit{Aims:}} The study explores how speech measures may be linked to language profiles in participants with Alzheimer's disease (AD) and how these profiles could distinguish AD from changes associated with normal aging. \textbf{\textit{Methods:}} We analysed simple sentences spoken by older adults with and without AD. Spectrographic analysis of temporal and acoustic characteristics was carried out using the Praat software. \textbf{\textit{Results:}} We found that measures of speech, such as variations in the percentage of voice breaks, number of periods of voice, number of voice breaks, shimmer (amplitude perturbation quotient), and noise-to-harmonics ratio, characterise people with AD with an accuracy of 84.8\%. \textbf{\textit{Discussion:}} These measures offer a sensitive method of assessing spontaneous speech output in AD, and they discriminate well between people with AD and healthy older adults. This method of evaluation is a promising tool for AD diagnosis and prognosis, and it could be used as a dependent measure in clinical trials.},
	language = {en},
	number = {5-6},
	urldate = {2022-11-05},
	journal = {Dementia and Geriatric Cognitive Disorders},
	author = {Meilán, Juan José G. and Martínez-Sánchez, Francisco and Carro, Juan and López, Dolores E. and Millian-Morell, Lymarie and Arana, José M.},
	year = {2014},
	pages = {327--334},
}

@inproceedings{meghanani_exploration_2021,
	address = {Shenzhen, China},
	title = {An {Exploration} of {Log}-{Mel} {Spectrogram} and {MFCC} {Features} for {Alzheimer}’s {Dementia} {Recognition} from {Spontaneous} {Speech}},
	isbn = {978-1-72817-066-4},
	url = {https://ieeexplore.ieee.org/document/9383491/},
	doi = {10.1109/SLT48900.2021.9383491},
	urldate = {2022-11-05},
	booktitle = {2021 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Meghanani, Amit and C. S., Anoop and Ramakrishnan, A. G.},
	month = jan,
	year = {2021},
	pages = {670--677},
}

@inproceedings{raghavendra_pappagari_using_2020,
	title = {Using state of the art speaker recognition and natural language processing technologies to detect alzheimer’s disease and assess its severity},
	author = {Raghavendra Pappagari and Laureano Moro-Velazquez and Jaejin Cho},
	year = {2020},
	pages = {2177--2181},
}


@article{buchsbaum_algorithmic_1997,
	title = {Algorithmic aspects in speech recognition: an introduction},
	volume = {2},
	issn = {1084-6654, 1084-6654},
	shorttitle = {Algorithmic aspects in speech recognition},
	url = {https://dl.acm.org/doi/10.1145/264216.264219},
	doi = {10.1145/264216.264219},
	abstract = {Speech recognition is an area with a considerable literature, but there is little discussion of the topic within the computer science algorithms literature. Many computer scientists, however, are interested in the computational problems of speech recognition. This paper presents the field of speech recognition and describes some of its major open problems from an algorithmic viewpoint. Our goal is to stimulate the interest of algorithm designers and experimenters to investigate the algorithmic problems of effective automatic speech recognition.},
	language = {en},
	urldate = {2023-03-04},
	journal = {ACM Journal of Experimental Algorithmics},
	author = {Buchsbaum, Adam L. and Giancarlo, Raffaele},
	month = jan,
	year = {1997},
	pages = {1},
}



@inproceedings{pompili_anna_inesc-id_2020,
	title = {The {INESC}-{ID} {Multi}-{Modal} {System} for the {ADReSS} 2020 {Challenge}},
	author = {Pompili, Anna and Rolland, Thomas and Abad, Alberto},
	month = may,
	year = {2020},
	note = {arXiv preprint arXiv:2005.14646, 2020},
}


@inproceedings{cummins_comparison_2020,
	title = {A {Comparison} of {Acoustic} and {Linguistics} {Methodologies} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/cummins20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2635},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Cummins, Nicholas and Pan, Yilin and Ren, Zhao and Fritsch, Julian and Nallanthighal, Venkata Srikanth and Christensen, Heidi and Blackburn, Daniel and Schuller, Björn W. and Magimai-Doss, Mathew and Strik, Helmer and Härmä, Aki},
	month = oct,
	year = {2020},
	pages = {2182--2186},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\B32UPSNS\\Cummins et al. - 2020 - A Comparison of Acoustic and Linguistics Methodolo.pdf:application/pdf},
}

@inproceedings{eyben_openear_2009,
	address = {Amsterdam},
	title = {{OpenEAR} — {Introducing} the munich open-source emotion and affect recognition toolkit},
	isbn = {978-1-4244-4800-5},
	url = {http://ieeexplore.ieee.org/document/5349350/},
	doi = {10.1109/ACII.2009.5349350},
	urldate = {2022-11-05},
	booktitle = {2009 3rd {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} and {Workshops}},
	publisher = {IEEE},
	author = {Eyben, Florian and Wollmer, Martin and Schuller, Bjorn},
	month = sep,
	year = {2009},
	pages = {1--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\MU67XKJM\\Eyben et al. - 2009 - OpenEAR — Introducing the munich open-source emoti.pdf:application/pdf},
}

@inproceedings{rohanian_multi-modal_2020,
	title = {Multi-{Modal} {Fusion} with {Gating} {Using} {Audio}, {Lexical} and {Disfluency} {Features} for {Alzheimer}’s {Dementia} {Recognition} from {Spontaneous} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/rohanian20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2721},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Rohanian, Morteza and Hough, Julian and Purver, Matthew},
	month = oct,
	year = {2020},
	pages = {2187--2191},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\956T2T7L\\Rohanian et al. - 2020 - Multi-Modal Fusion with Gating Using Audio, Lexica.pdf:application/pdf},
}

@inproceedings{balagopalan_impact_2020,
	address = {Online},
	title = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}: {All} {Errors} are {Equal}, but {Deletions} are {More} {Equal} than {Others}},
	shorttitle = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}},
	url = {https://www.aclweb.org/anthology/2020.wnut-1.21},
	doi = {10.18653/v1/2020.wnut-1.21},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Noisy} {User}-generated {Text} ({W}-{NUT} 2020)},
	publisher = {Association for Computational Linguistics},
	author = {Balagopalan, Aparna and Shkaruta, Ksenia and Novikova, Jekaterina},
	year = {2020},
	pages = {159--164},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\459E52FL\\Balagopalan et al. - 2020 - Impact of ASR on Alzheimer’s Disease Detection Al.pdf:application/pdf},
}

@inproceedings{liu_detecting_2021,
	address = {Toronto, ON, Canada},
	title = {Detecting {Alzheimer}’s {Disease} from {Speech} {Using} {Neural} {Networks} with {Bottleneck} {Features} and {Data} {Augmentation}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413566/},
	doi = {10.1109/ICASSP39728.2021.9413566},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Liu, Zhaoci and Guo, Zhiqiang and Ling, Zhenhua and Li, Yunxia},
	month = jun,
	year = {2021},
	pages = {7323--7327},
}

@inproceedings{toth_automatic_2015,
	title = {Automatic detection of mild cognitive impairment from spontaneous speech using {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2015/toth15_interspeech.html},
	doi = {10.21437/Interspeech.2015-568},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Tóth, László and Gosztolya, Gábor and Vincze, Veronika and Hoffmann, Ildikó and Szatlóczki, Gréta and Biró, Edit and Zsura, Fruzsina and Pákáski, Magdolna and Kálmán, János},
	month = sep,
	year = {2015},
	pages = {2694--2698},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\3FGE8JIU\\Tóth et al. - 2015 - Automatic detection of mild cognitive impairment f.pdf:application/pdf},
}

@misc{tits_asr-based_2018,
	title = {{ASR}-based {Features} for {Emotion} {Recognition}: {A} {Transfer} {Learning} {Approach}},
	shorttitle = {{ASR}-based {Features} for {Emotion} {Recognition}},
	url = {http://arxiv.org/abs/1805.09197},
	abstract = {During the last decade, the applications of signal processing have drastically improved with deep learning. However areas of affecting computing such as emotional speech synthesis or emotion recognition from spoken language remains challenging. In this paper, we investigate the use of a neural Automatic Speech Recognition (ASR) as a feature extractor for emotion recognition. We show that these features outperform the eGeMAPS feature set to predict the valence and arousal emotional dimensions, which means that the audio-to-text mapping learning by the ASR system contain information related to the emotional dimensions in spontaneous speech. We also examine the relationship between first layers (closer to speech) and last layers (closer to text) of the ASR and valence/arousal.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Tits, Noé and Haddad, Kevin El and Dutoit, Thierry},
	month = jun,
	year = {2018},
	note = {arXiv:1805.09197 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to be published in the First Workshop on Computational Modeling of Human Multimodal Language - ACL 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\RIXSMVYF\\Tits et al. - 2018 - ASR-based Features for Emotion Recognition A Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\DDGNTFFZ\\1805.html:text/html},
}

@misc{zhang_unified_2021,
	title = {Unified {Streaming} and {Non}-streaming {Two}-pass {End}-to-end {Model} for {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2012.05481},
	abstract = {In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60\% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42\% CER with 640ms latency in a streaming ASR system.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Zhang, Binbin and Wu, Di and Yao, Zhuoyuan and Wang, Xiong and Yu, Fan and Yang, Chao and Guo, Liyong and Hu, Yaguang and Xie, Lei and Lei, Xin},
	month = dec,
	year = {2021},
	note = {arXiv:2012.05481 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3YGQUSLL\\Zhang et al. - 2021 - Unified Streaming and Non-streaming Two-pass End-t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Y9XSYRY3\\2012.html:text/html},
}

@article{sun_ernie_2020,
	title = {{ERNIE} 2.0: {A} {Continual} {Pre}-{Training} {Framework} for {Language} {Understanding}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{ERNIE} 2.0},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6428},
	doi = {10.1609/aaai.v34i05.6428},
	abstract = {Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
	number = {05},
	urldate = {2022-11-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
	month = apr,
	year = {2020},
	pages = {8968--8975},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WVPE3NJM\\Sun et al. - 2020 - ERNIE 2.0 A Continual Pre-Training Framework for .pdf:application/pdf},
}

@inproceedings{schuller_interspeech_2010,
	title = {The {INTERSPEECH} 2010 paralinguistic challenge},
	url = {https://www.isca-speech.org/archive/interspeech_2010/schuller10b_interspeech.html},
	doi = {10.21437/Interspeech.2010-739},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2010},
	publisher = {ISCA},
	author = {Schuller, Björn and Steidl, Stefan and Batliner, Anton and Burkhardt, Felix and Devillers, Laurence and Müller, Christian and Narayanan, Shrikanth S.},
	month = sep,
	year = {2010},
	pages = {2794--2797},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3QAQXZVB\\Schuller et al. - 2010 - The INTERSPEECH 2010 paralinguistic challenge.pdf:application/pdf},
}

@misc{noauthor_ad2021_2022,
	title = {{AD2021}: {Alzheimer}'s {Disease} {Recognition} {Evaluation} 2021},
	copyright = {Apache-2.0},
	shorttitle = {{AD2021}},
	url = {https://github.com/THUsatlab/AD2021},
	abstract = {Alzheimer's Disease Recognition Evaluation 2021},
	urldate = {2022-11-05},
	publisher = {THUsatlab},
	month = aug,
	year = {2022},
	note = {original-date: 2021-07-08T04:10:11Z},
}

@inproceedings{gui_end--end_2022,
	address = {Singapore, Singapore},
	title = {End-to-{End} {ASR}-{Enhanced} {Neural} {Network} for {Alzheimer}’s {Disease} {Diagnosis}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9747856/},
	doi = {10.1109/ICASSP43922.2022.9747856},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Gui, Jiancheng and Li, Yikai and Chen, Kai and Siebert, Joanna and Chen, Qingcai},
	month = may,
	year = {2022},
	pages = {8562--8566},
}

@inproceedings{garnerin_investigating_2021,
	address = {Online},
	title = {Investigating the {Impact} of {Gender} {Representation} in {ASR} {Training} {Data}: a {Case} {Study} on {Librispeech}},
	shorttitle = {Investigating the {Impact} of {Gender} {Representation} in {ASR} {Training} {Data}},
	url = {https://aclanthology.org/2021.gebnlp-1.10},
	doi = {10.18653/v1/2021.gebnlp-1.10},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 3rd {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	year = {2021},
	pages = {86--92},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9ZB3LE4W\\Garnerin et al. - 2021 - Investigating the Impact of Gender Representation .pdf:application/pdf},
}

@inproceedings{abushariah_mohammad_and_sawalha_majdi_effects_2013,
	address = {Lancaster, UK},
	title = {The effects of speakers' gender, age, and region on overall performance of {Arabic} automatic speech recognition systems using the phonetically rich and balanced {Modern} {Standard} {Arabic} speech corpus},
	url = {http://eprints.whiterose.ac.uk/81859/},
	booktitle = {Proceedings of the 2nd {Workshop} of {Arabic} {Corpus} {Linguistics}},
	author = {Abushariah, Mohammad {and} Sawalha, Majdi},
	month = jul,
	year = {2013},
	note = {University of Leeds},
}

@inproceedings{shah_predictive_2020,
	address = {Online},
	title = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}: {A} {Conceptual} {Framework} and {Overview}},
	shorttitle = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.468},
	doi = {10.18653/v1/2020.acl-main.468},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shah, Deven Santosh and Schwartz, H. Andrew and Hovy, Dirk},
	year = {2020},
	pages = {5248--5264},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\C44TW2PL\\Shah et al. - 2020 - Predictive Biases in Natural Language Processing M.pdf:application/pdf},
}

@inproceedings{hovy_social_2016,
	address = {Berlin, Germany},
	title = {The {Social} {Impact} of {Natural} {Language} {Processing}},
	url = {http://aclweb.org/anthology/P16-2096},
	doi = {10.18653/v1/P16-2096},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Spruit, Shannon L.},
	year = {2016},
	pages = {591--598},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\Z3PQLK4Y\\Hovy and Spruit - 2016 - The Social Impact of Natural Language Processing.pdf:application/pdf},
}

@article{garg_word_2018,
	title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1720347115},
	doi = {10.1073/pnas.1720347115},
	abstract = {Significance
            Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science.
          ,
            Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.},
	language = {en},
	number = {16},
	urldate = {2022-11-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	month = apr,
	year = {2018},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\RQYPG4XA\\Garg et al. - 2018 - Word embeddings quantify 100 years of gender and e.pdf:application/pdf},
}

@book{kutuzov_diachronic_2018,
	title = {Diachronic word embeddings and semantic shifts: a survey},
	shorttitle = {Diachronic word embeddings and semantic shifts},
	abstract = {Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.},
	author = {Kutuzov, Andrei and Øvrelid, Lilja and Szymanski, Terrence and Velldal, Erik},
	month = jun,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5PQKNLMN\\Kutuzov et al. - 2018 - Diachronic word embeddings and semantic shifts a .pdf:application/pdf},
}

@inproceedings{sap_risk_2019,
	address = {Florence, Italy},
	title = {The {Risk} of {Racial} {Bias} in {Hate} {Speech} {Detection}},
	url = {https://www.aclweb.org/anthology/P19-1163},
	doi = {10.18653/v1/P19-1163},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
	year = {2019},
	pages = {1668--1678},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\85P2NR4F\\Sap et al. - 2019 - The Risk of Racial Bias in Hate Speech Detection.pdf:application/pdf},
}

@article{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	month = jul,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5YSE3CSF\\Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf},
}

@inproceedings{zhao_gender_2019,
	address = {Minneapolis, Minnesota},
	title = {Gender {Bias} in {Contextualized} {Word} {Embeddings}},
	url = {http://aclweb.org/anthology/N19-1064},
	doi = {10.18653/v1/N19-1064},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
	year = {2019},
	pages = {629--634},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\JX738BCA\\Zhao et al. - 2019 - Gender Bias in Contextualized Word Embeddings.pdf:application/pdf},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	abstract = {Machines learn what people know implicitly

              AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan
              et al.
              now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.


              Science
              , this issue p.
              183
              ; see also p.
              133

          ,
            Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.
          ,
            Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	language = {en},
	number = {6334},
	urldate = {2022-11-05},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	pages = {183--186},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\ER9DI443\\Caliskan et al. - 2017 - Semantics derived automatically from language corp.pdf:application/pdf},
}

@inproceedings{adda-decker_speech_2005,
	title = {Do speech recognizers prefer female speakers?},
	url = {https://www.isca-speech.org/archive/interspeech_2005/addadecker05_interspeech.html},
	doi = {10.21437/Interspeech.2005-699},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2005},
	publisher = {ISCA},
	author = {Adda-Decker, Martine and Lamel, Lori},
	month = sep,
	year = {2005},
	pages = {2205--2208},
}

@misc{feng_quantifying_2021,
	title = {Quantifying {Bias} in {Automatic} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2103.15122},
	abstract = {Automatic speech recognition (ASR) systems promise to deliver objective interpretation of human speech. Practice and recent evidence suggests that the state-of-the-art (SotA) ASRs struggle with the large variation in speech due to e.g., gender, age, speech impairment, race, and accents. Many factors can cause the bias of an ASR system. Our overarching goal is to uncover bias in ASR systems to work towards proactive bias mitigation in ASR. This paper is a first step towards this goal and systematically quantifies the bias of a Dutch SotA ASR system against gender, age, regional accents and non-native accents. Word error rates are compared, and an in-depth phoneme-level error analysis is conducted to understand where bias is occurring. We primarily focus on bias due to articulation differences in the dataset. Based on our findings, we suggest bias mitigation strategies for ASR development.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Feng, Siyuan and Kudina, Olya and Halpern, Bence Mark and Scharenborg, Odette},
	month = apr,
	year = {2021},
	note = {arXiv:2103.15122 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	annote = {Comment: Submitted to INTERSPEECH (IS) 2021. This preprint version differs slightly from the version submitted to IS 2021: Figure 1 is not included in IS 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\GJK7YX95\\Feng et al. - 2021 - Quantifying Bias in Automatic Speech Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\XRIQKM99\\2103.html:text/html},
}

@inproceedings{tatman_gender_2017,
	address = {Valencia, Spain},
	title = {Gender and {Dialect} {Bias} in {YouTube}'s {Automatic} {Captions}},
	url = {http://aclweb.org/anthology/W17-1606},
	doi = {10.18653/v1/W17-1606},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tatman, Rachael},
	year = {2017},
	pages = {53--59},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\QHGSA22L\\Tatman - 2017 - Gender and Dialect Bias in YouTube's Automatic Cap.pdf:application/pdf},
}

@inproceedings{tatman_effects_2017,
	title = {Effects of {Talker} {Dialect}, {Gender} \& {Race} on {Accuracy} of {Bing} {Speech} and {YouTube} {Automatic} {Captions}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/tatman17_interspeech.html},
	doi = {10.21437/Interspeech.2017-1746},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Tatman, Rachael and Kasten, Conner},
	month = aug,
	year = {2017},
	pages = {934--938},
}

@techreport{frank_wilcoxon_sk_katti_and_roberta_a_wilcox_critical_1963,
	address = {Pearl River, NY, USA},
	title = {Critical values and probability levels for the {Wilcoxon} rank sum test and the {Wilcoxon} signed rank test},
	institution = {American Cyanamid Company},
	author = {Frank Wilcoxon, SK Katti, {and} Roberta A Wilcox},
	year = {1963},
}

@inproceedings{garnerin_gender_2019,
	address = {Nice, France},
	title = {Gender {Representation} in {French} {Broadcast} {Corpora} and {Its} {Impact} on {ASR} {Performance}},
	isbn = {978-1-4503-6917-6},
	url = {http://dl.acm.org/citation.cfm?doid=3347449.3357480},
	doi = {10.1145/3347449.3357480},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {AI} for {Smart} {TV} {Content} {Production}, {Access} and {Delivery} - {AI4TV} '19},
	publisher = {ACM Press},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	year = {2019},
	pages = {3--9},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GWSDNHSG\\Garnerin et al. - 2019 - Gender Representation in French Broadcast Corpora .pdf:application/pdf},
}

@inproceedings{garnerin_gender_2020,
	address = {Marseille, France},
	title = {Gender {Representation} in {Open} {Source} {Speech} {Resources}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.813},
	abstract = {With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited/non elicited speech, low/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.},
	language = {English},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	month = may,
	year = {2020},
	pages = {6599--6605},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\HW5HZXVR\\Garnerin et al. - 2020 - Gender Representation in Open Source Speech Resour.pdf:application/pdf},
}

@article{adamek_whose_2018,
	title = {Whose {Artifacts}? {Whose} {Stories}? {Public} {History} and {Representation} of {Women} at the {Canada} {Science} and {Technology} {Museum}},
	issn = {0121-1617, 1900-6152},
	shorttitle = {Whose {Artifacts}?},
	url = {https://revistas.uniandes.edu.co/doi/10.7440/histcrit68.2018.03},
	doi = {10.7440/histcrit68.2018.03},
	language = {pt},
	number = {68},
	urldate = {2022-11-05},
	journal = {Historia Crítica},
	author = {Adamek, Anna and Gann, Emily},
	month = apr,
	year = {2018},
	pages = {47--66},
}

@incollection{goos_classification_2001,
	address = {Berlin, Heidelberg},
	title = {Classification on {Data} with {Biased} {Class} {Distribution}},
	volume = {2167},
	isbn = {978-3-540-42536-6 978-3-540-44795-5},
	url = {http://link.springer.com/10.1007/3-540-44795-4_45},
	urldate = {2022-11-05},
	booktitle = {Machine {Learning}: {ECML} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Vucetic, Slobodan and Obradovic, Zoran},
	editor = {Goos, G. and Hartmanis, J. and van Leeuwen, J. and De Raedt, Luc and Flach, Peter},
	year = {2001},
	doi = {10.1007/3-540-44795-4_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {527--538},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WWEYIB8Z\\Vucetic and Obradovic - 2001 - Classification on Data with Biased Class Distribut.pdf:application/pdf},
}

@article{haibo_he_learning_2009,
	title = {Learning from {Imbalanced} {Data}},
	volume = {21},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5128907/},
	doi = {10.1109/TKDE.2008.239},
	number = {9},
	urldate = {2022-11-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {{Haibo He} and Garcia, E.A.},
	month = sep,
	year = {2009},
	pages = {1263--1284},
}

@inproceedings{solon_barocas_problem_2017,
	address = {Philadelphia, PA, USA},
	title = {The problem with bias: {Allocative} versus representational harms in machine learning},
	author = {Solon Barocas and Kate Crawford and Aaron Shapiro and Hanna Wallach},
	year = {2017},
}

@inproceedings{noauthor_trouble_2017,
	address = {Long Beach, CA, USA},
	title = {The trouble with bias},
	url = {https://www.youtube.com/watch?v=fMym_BKWQzk},
	booktitle = {31st {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	year = {2017},
	note = {Keynote at the 31st Annual Conference on Neural Information Processing Systems},
}

@article{d_a_mahler_r_a_rosiello_and_j_loke_aging_1986,
	title = {The {Aging} {Lung}},
	volume = {2},
	number = {2},
	journal = {Clinics in Geriatric Medicine},
	author = {D. A. Mahler, R. A. Rosiello, {and} J. Loke},
	year = {1986},
	pages = {215--225},
}

@article{karam_anatomic_2013,
	title = {Anatomic and {Physiologic} {Changes} of the {Aging} {Kidney}},
	volume = {29},
	issn = {07490690},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749069013000396},
	doi = {10.1016/j.cger.2013.05.006},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Clinics in Geriatric Medicine},
	author = {Karam, Zeina and Tuazon, Jennifer},
	month = aug,
	year = {2013},
	pages = {555--564},
}

@article{tolep_comparison_1995,
	title = {Comparison of diaphragm strength between healthy adult elderly and young men.},
	volume = {152},
	issn = {1073-449X, 1535-4970},
	url = {https://www.atsjournals.org/doi/10.1164/ajrccm.152.2.7633725},
	doi = {10.1164/ajrccm.152.2.7633725},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {American Journal of Respiratory and Critical Care Medicine},
	author = {Tolep, K and Higgins, N and Muza, S and Criner, G and Kelsen, S G},
	month = aug,
	year = {1995},
	pages = {677--682},
}

@article{linville_vocal_1995,
	title = {Vocal aging:},
	volume = {3},
	issn = {1068-9508},
	shorttitle = {Vocal aging},
	url = {http://journals.lww.com/00020840-199506000-00006},
	doi = {10.1097/00020840-199506000-00006},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Current Opinion in Otolaryngology \& Head and Neck Surgery},
	author = {Linville, Sue Ellen},
	month = jun,
	year = {1995},
	pages = {183--187},
}

@article{ramig_aging_2001,
	title = {The {Aging} {Voice}: {A} {Review}, {Treatment} {Data} and {Familial} and {Genetic} {Perspectives}},
	volume = {53},
	issn = {1021-7762, 1421-9972},
	shorttitle = {The {Aging} {Voice}},
	url = {https://www.karger.com/Article/FullText/52680},
	doi = {10.1159/000052680},
	abstract = {This paper will provide a review of aspects of vocal aging within the context of general body aging and describe two data sets related to the aging voice. Data will be presented which document pre- to posttreatment improvement in select voice characteristics (sound pressure level, subglottal air pressure, thyroarytenoid laryngeal muscle activity and voice quality) following application of an intensive voice treatment program (the LSVT$^{\textrm{®}}$) to 3 individuals with aged voice. Additionally, physiological data (forced expiratory volume, visual accommodation, bone density, taste discrimination, white blood count and resting heart rate) and select perceptual (perceived age) and acoustic measures (reflecting both cycle-to-cycle and longer-term intensity and frequency stability) from 67 subjects will be reviewed from the work of Gray and colleagues to document the differential impact of the global aging process across organ systems including the aging voice.},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {Folia Phoniatrica et Logopaedica},
	author = {Ramig, Lorraine Olson and Gray, Steven and Baker, Kristin and Corbin-Lewis, Kim and Buder, Eugene and Luschei, Erich and Coon, Hillary and Smith, Marshall},
	year = {2001},
	pages = {252--265},
}

@article{paulsen_degenerative_1998,
	title = {Degenerative {Changes} in the {Human} {Cricoarytenoid} {Joint}},
	volume = {124},
	issn = {0886-4470},
	url = {http://archotol.jamanetwork.com/article.aspx?doi=10.1001/archotol.124.8.903},
	doi = {10.1001/archotol.124.8.903},
	language = {en},
	number = {8},
	urldate = {2022-11-05},
	journal = {Archives of Otolaryngology–Head \& Neck Surgery},
	author = {Paulsen, Friedrich P. and Tillmann, Bernhard N.},
	month = aug,
	year = {1998},
	pages = {903},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\VHRPKC7W\\Paulsen and Tillmann - 1998 - Degenerative Changes in the Human Cricoarytenoid J.pdf:application/pdf},
}

@article{rodeno_histochemical_1993,
	title = {Histochemical and {Morphometrical} {Ageing} {Changes} in {Human} {Vocal} {Cord} {Muscles}},
	volume = {113},
	issn = {0001-6489, 1651-2251},
	url = {http://www.tandfonline.com/doi/full/10.3109/00016489309135842},
	doi = {10.3109/00016489309135842},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Acta Oto-Laryngologica},
	author = {Rodeño, M. T. and Sánchez-fernández, J. M. and Rivera-pomar, J. M.},
	month = jan,
	year = {1993},
	pages = {445--449},
}

@article{hirano_ageing_1989,
	title = {Ageing of the {Vibratory} {Tissue} of {Human} {Vocal} {Folds}},
	volume = {107},
	issn = {0001-6489, 1651-2251},
	url = {http://www.tandfonline.com/doi/full/10.3109/00016488909127535},
	doi = {10.3109/00016488909127535},
	language = {en},
	number = {5-6},
	urldate = {2022-11-05},
	journal = {Acta Oto-Laryngologica},
	author = {Hirano, Minoru and Kurita, Shigejiro and Sakaguchi, Shinji},
	month = jan,
	year = {1989},
	pages = {428--433},
}

@article{sato_age-related_1997,
	title = {Age-{Related} {Changes} of {Elastic} {Fibers} in the {Superficial} {Layer} of the {Lamina} {Propria} of {Vocal} {Folds}},
	volume = {106},
	issn = {0003-4894, 1943-572X},
	url = {http://journals.sagepub.com/doi/10.1177/000348949710600109},
	doi = {10.1177/000348949710600109},
	abstract = {An investigation was carried out to determine the morphologic characteristics of elastic fibers in the superficial layer of the lamina propria of aged vocal folds (EFAVFs). Excised human adult vocal folds served as the material for this study. Scanning and transmission electron microscopic observations were made. The results can be summarized as follows. First, the EFAVFs were composed of amorphous substances and microfibrils. The amorphous substances increased in amount and the microfibrils became less numerous. Second, the EFAVFs ran in various directions, were branched, and formed a complicated network. The surface of the fibers was rough, and the fibers appeared to vary in size. Some EFAVFs united to form a sheet with a rough surface. Third, the EFAVFs could not be easily digested by elastase compared with those of younger adults. We conclude that the morphologic and metabolic changes of elastic fibers in the most important vibrating portion (superficial layer of the lamina propria) of the aged vocal folds contribute partially to aging of the voice.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Annals of Otology, Rhinology \& Laryngology},
	author = {Sato, Kiminori and Hirano, Minoru},
	month = jan,
	year = {1997},
	pages = {44--48},
}

@article{rother_morphometrically_2002,
	title = {Morphometrically observable aging changes in the human tongue},
	volume = {184},
	issn = {09409602},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0940960202800115},
	doi = {10.1016/S0940-9602(02)80011-5},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Annals of Anatomy - Anatomischer Anzeiger},
	author = {Rother, Paul and Wohlgemuth, Balthasar and Wolff, Werner and Rebentrost, Ines},
	month = mar,
	year = {2002},
	pages = {159--164},
}

@article{b_weinstein_biology_nodate,
	title = {The biology of aging},
	journal = {Geriatric Audiology},
	author = {B. Weinstein},
	pages = {pp. 15--40},
}

@book{weinstein_geriatric_2013,
	address = {New York},
	edition = {Second edition},
	title = {Geriatric audiology},
	isbn = {978-1-60406-174-1 978-1-60406-775-0},
	publisher = {Thieme},
	author = {Weinstein, Barbara E.},
	year = {2013},
	note = {“The biology of aging,” in Geriatric Audiology,
pp. 15–40, Georg Thieme, Stuttgart, Germany, 2000},
	keywords = {Aged, Aging, Health Services for the Aged, Hearing, Hearing Disorders, physiology},
}

@article{xue_changes_2003,
	title = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}: {A} {Pilot} {Study}},
	volume = {46},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282003/054%29},
	doi = {10.1044/1092-4388(2003/054)},
	abstract = {This investigation used a derivation of acoustic reflection (AR) technology to make cross-sectional measurements of changes due to aging in the oral and pharyngeal lumina of male and female speakers. The purpose of the study was to establish preliminary normative data for such changes and to obtain acoustic measurements of changes due to aging in the formant frequencies of selected spoken vowels and their long-term average spectra (LTAS) analysis. Thirty- eight young men and women and 38 elderly men and women were involved in the study. The oral and pharyngeal lumina of the participants were measured with AR technology, and their formant frequencies were analyzed using the Kay Elemetrics Computerized Speech Lab. The findings have delineated specific and similar patterns of aging changes in human vocal tract configurations in speakers of both genders. Namely, the oral cavity length and volume of elderly speakers increased significantly compared to their young cohorts. The total vocal tract volume of elderly speakers also showed a significant increment, whereas the total vocal tract length of elderly speakers did not differ significantly from their young cohorts. Elderly speakers of both genders also showed similar patterns of acoustic changes of speech production, that is, consistent lowering of formant frequencies (especially F1) across selected vowel productions. Although new research models are still needed to succinctly account for the speech acoustic changes of the elderly, especially for their specific patterns of human vocal tract dimensional changes, this study has innovatively applied the noninvasive and cost-effective AR technology to monitor age-related human oral and pharyngeal lumina changes that have direct consequences for speech production.},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Xue, Steve An and Hao, Grace Jianping},
	month = jun,
	year = {2003},
	pages = {689--701},
}

@article{baba_acoustic_2004,
	title = {Acoustic models of the elderly for large-vocabulary continuous speech recognition},
	volume = {87},
	issn = {8756-663X, 1520-6432},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ecjb.20101},
	doi = {10.1002/ecjb.20101},
	language = {en},
	number = {7},
	urldate = {2022-11-05},
	journal = {Electronics and Communications in Japan (Part II: Electronics)},
	author = {Baba, Akira and Yoshizawa, Shinichi and Yamada, Miichi and Lee, Akinobu and Shikano, Kiyohiro},
	month = jul,
	year = {2004},
	pages = {49--57},
}

@article{xue_changes_2003-1,
	title = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}: {A} {Pilot} {Study}},
	volume = {46},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282003/054%29},
	doi = {10.1044/1092-4388(2003/054)},
	abstract = {This investigation used a derivation of acoustic reflection (AR) technology to make cross-sectional measurements of changes due to aging in the oral and pharyngeal lumina of male and female speakers. The purpose of the study was to establish preliminary normative data for such changes and to obtain acoustic measurements of changes due to aging in the formant frequencies of selected spoken vowels and their long-term average spectra (LTAS) analysis. Thirty- eight young men and women and 38 elderly men and women were involved in the study. The oral and pharyngeal lumina of the participants were measured with AR technology, and their formant frequencies were analyzed using the Kay Elemetrics Computerized Speech Lab. The findings have delineated specific and similar patterns of aging changes in human vocal tract configurations in speakers of both genders. Namely, the oral cavity length and volume of elderly speakers increased significantly compared to their young cohorts. The total vocal tract volume of elderly speakers also showed a significant increment, whereas the total vocal tract length of elderly speakers did not differ significantly from their young cohorts. Elderly speakers of both genders also showed similar patterns of acoustic changes of speech production, that is, consistent lowering of formant frequencies (especially F1) across selected vowel productions. Although new research models are still needed to succinctly account for the speech acoustic changes of the elderly, especially for their specific patterns of human vocal tract dimensional changes, this study has innovatively applied the noninvasive and cost-effective AR technology to monitor age-related human oral and pharyngeal lumina changes that have direct consequences for speech production.},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Xue, Steve An and Hao, Grace Jianping},
	month = jun,
	year = {2003},
	pages = {689--701},
}

@book{vipperla_longitudinal_2008,
	title = {Longitudinal study of {ASR} performance on ageing voices},
	abstract = {This paper presents the results of a longitudinal study of ASR performance on ageing voices. Experiments were conducted on the audio recordings of the proceedings of the Supreme Court Of The United States (SCOTUS). Results show that the Au- tomatic Speech Recognition (ASR) Word Error Rates (WERs) for elderly voices are significantly higher than those of adult voices. The word error rate increases gradually as the age of the elderly speakers increase. Use of maximum likelihood linear regression (MLLR) based speaker adaptation on ageing voices improves the WER though the performance is still considerably lower compared to adult voices. Speaker adaptation however reduces the increase in WER with age during old age. IndexTerms: Ageing Voices, longitudinal study, SCOTUScor- pus, MLLR},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	month = jan,
	year = {2008},
	note = {Pages: 2553},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\YEYS3FPY\\Vipperla et al. - 2008 - Longitudinal study of ASR performance on ageing vo.pdf:application/pdf},
}

@inproceedings{wilpon_study_1996,
	address = {Atlanta, GA, USA},
	title = {A study of speech recognition for children and the elderly},
	volume = {1},
	isbn = {978-0-7803-3192-1},
	url = {http://ieeexplore.ieee.org/document/541104/},
	doi = {10.1109/ICASSP.1996.541104},
	urldate = {2022-11-05},
	booktitle = {1996 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing} {Conference} {Proceedings}},
	publisher = {IEEE},
	author = {Wilpon, J.G. and Jacobsen, C.N.},
	year = {1996},
	pages = {349--352},
}

@book{moller_corpus_2008,
	title = {Corpus {Analysis} of {Spoken} {Smart}-{Home} {Interactions} with {Older} {Users}.},
	abstract = {In this paper, we present the collection and analysis of a spoken dialogue corpus obtained from interactions of older and younger users with a smart-home system. Our aim is to identify the amount and the origin of linguistic differences in the way older and younger users address the system. In addition, we investigate changes in the users' linguistic behaviour after exposure to the system. The results show that the two user groups differ in their speaking style as well as their vocabulary. In contrast to younger users, who adapt their speaking style to the expected limitations of the system, older users tend to use a speaking style that is closer to human-human communication in terms of sentence complexity and politeness. However, older users are far less easy to stereotype than younger users.},
	author = {Möller, Sebastian and Wolters, Maria},
	month = jan,
	year = {2008},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\SHJVDLVP\\Möller and Wolters - 2008 - Corpus Analysis of Spoken Smart-Home Interactions .pdf:application/pdf},
}

@article{wolters_being_2009,
	title = {Being {Old} {Doesn}’t {Mean} {Acting} {Old}: {How} {Older} {Users} {Interact} with {Spoken} {Dialog} {Systems}},
	volume = {2},
	issn = {1936-7228, 1936-7236},
	shorttitle = {Being {Old} {Doesn}’t {Mean} {Acting} {Old}},
	url = {https://dl.acm.org/doi/10.1145/1525840.1525842},
	doi = {10.1145/1525840.1525842},
	abstract = {Most studies on adapting voice interfaces to older users work top-down by comparing the interaction behavior of older and younger users. In contrast, we present a bottom-up approach. A statistical cluster analysis of 447 appointment scheduling dialogs between 50 older and younger users and 9 simulated spoken dialog systems revealed two main user groups, a “social” group and a “factual” group. “Factual” users adapted quickly to the systems and interacted efficiently with them. “Social” users, on the other hand, were more likely to treat the system like a human, and did not adapt their interaction style. While almost all “social” users were older, over a third of all older users belonged in the “factual” group. Cognitive abilities and gender did not predict group membership. We conclude that spoken dialog systems should adapt to users based on observed behavior, not on age.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {ACM Transactions on Accessible Computing},
	author = {Wolters, Maria and Georgila, Kallirroi and Moore, Johanna D. and MacPherson, Sarah E.},
	month = may,
	year = {2009},
	pages = {1--39},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\7MBYCCBB\\Wolters et al. - 2009 - Being Old Doesn’t Mean Acting Old How Older Users.pdf:application/pdf},
}

@incollection{stephanidis_speech_2009,
	address = {Berlin, Heidelberg},
	title = {Speech {Input} from {Older} {Users} in {Smart} {Environments}: {Challenges} and {Perspectives}},
	volume = {5615},
	isbn = {978-3-642-02709-3 978-3-642-02710-9},
	shorttitle = {Speech {Input} from {Older} {Users} in {Smart} {Environments}},
	url = {http://link.springer.com/10.1007/978-3-642-02710-9_14},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Intelligent} and {Ubiquitous} {Interaction} {Environments}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vipperla, Ravichander and Wolters, Maria and Georgila, Kallirroi and Renals, Steve},
	editor = {Stephanidis, Constantine},
	year = {2009},
	doi = {10.1007/978-3-642-02710-9_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {117--126},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\SHIF9M32\\Vipperla et al. - 2009 - Speech Input from Older Users in Smart Environment.pdf:application/pdf},
}

@book{mller_combining_2007,
	title = {Combining short-term cepstral and long-term pitch features for automatic recognition of speaker age.},
	abstract = {The most successful systems in previous comparative studies on speaker age recognition used short-term cepstral features modeled with Gaussian Mixture Models (GMMs) or applied multiple phone recognizers trained with the data of speakers of the respective class. Acoustic analyses, however, indic ate that certain features such as pitch extracted from a longer s pan of speech correlate clearly with the speaker age although the systems based on those features have been inferior to the be- fore mentioned approaches. In this paper, three novel systems combining short-term cepstral features and long-term features for speaker age recognition are compared to each other. A sys- tem combining GMMs using frame-based MFCCs and Support- Vector-Machines using long-term pitch performs best. The re- sults indicate that the combination of the two feature types is a promising approach, which corresponds to findings in relat ed fields like speaker recognition. Index Terms: speaker classifi- cation, age recognition, GMM, SVM},
	author = {Mller, Christian and Burkhardt, Felix},
	month = jan,
	year = {2007},
	note = {Pages: 2280},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PUAFDFIY\\Mller and Burkhardt - 2007 - Combining short-term cepstral and long-term pitch .pdf:application/pdf},
}

@book{wolters_age_2009,
	title = {Age recognition for spoken dialogue systems: do we need it?},
	shorttitle = {Age recognition for spoken dialogue systems},
	abstract = {When deciding whether to adapt relevant aspects of the system to the particular needs of older users, spoken dialogue systems often rely on automatic detection of chronological age. In this paper, we show that vocal age- ing as measured by acoustic features is an unreliable indi- cator of the need for adaptation. Simple lexical features greatly improve the prediction of both relevant aspects of cognition and interactions style. Lexical features also boost age group prediction. We suggest that adaptation should be based on observed behaviour, not on chrono- logical age, unless it is not feasible to build classifiers for relevant adaptation decisions. Index Terms :a ge recognition, pitch, keyword spotting, cognitive ageing},
	author = {Wolters, Maria and Vipperla, Ravichander and Renals, Steve},
	month = jan,
	year = {2009},
	note = {Pages: 1438},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\S7NGAXUA\\Wolters et al. - 2009 - Age recognition for spoken dialogue systems do we.pdf:application/pdf},
}

@article{hillenbrand_acoustic_1996,
	title = {Acoustic correlates of breathy vocal quality: {Dysphonic} voices and continuous speech},
	volume = {39},
	shorttitle = {Acoustic correlates of breathy vocal quality},
	abstract = {In an earlier study, we evaluated the effectiveness of several acoustic measures in predicting breathiness ratings for sustained vowels spoken by nonpathological talkers who were asked to produce nonbreathy, moderately breathy, and very breathy phonation (Hillenbrand, Cleveland, \& Erickson, 1994). The purpose of the present study was to extend these results to speakers with laryngeal pathologies and to conduct tests using connected speech in addition to sustained vowels. Breathiness ratings were obtained from a sustained vowel and a 12-word sentence spoken by 20 pathological and 5 nonpathological talkers. Acoustic measures were made of (a) signal periodicity, (b) first harmonic amplitude, and (c) spectral tilt. For the sustained vowels, a frequency domain measure of periodicity provided the most accurate predictions of perceived breathiness, accounting for 92\% of the variance in breathiness ratings. The relative amplitude of the first harmonic and two measures of spectral tilt correlated moderately with breathiness ratings. For the sentences, both signal periodicity and spectral tilt provided accurate predictions of breathiness ratings, accounting for 70\%-85\% of the variance.},
	journal = {Journal of speech and hearing research},
	author = {Hillenbrand, James and Houde, Robert},
	month = apr,
	year = {1996},
	pages = {311--21},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\445N846Q\\Hillenbrand and Houde - 1996 - Acoustic correlates of breathy vocal quality Dysp.pdf:application/pdf},
}

@article{klich_relationships_1982,
	title = {Relationships of {Vowel} {Characteristics} to {Listener} {Ratings} of {Breathiness}},
	volume = {25},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.2504.574},
	doi = {10.1044/jshr.2504.574},
	abstract = {This experiment was designed to investigate the relationships of listener ratings of breathiness to vowel duration, speaking rate, and the relative energy in three frequency ranges (100–500, 1500–2500, and 3500–4500 Hz) in vowel spectra. The effects of vowel SPL also were considered. Listeners used a seven-point equal-appearing interval scale to rate a sentence spoken by each of 10 young adult females in each of four voice qualities: normal speech, mildly breathy, severely breathy, and whisper. Significant Pearson correlations to the ratings were found only for mean SPL and the relative energy in the 100–500 and 3500–4500 Hz ranges. After the effects of mean SPL were accounted for in partial correlation and multiple regression analyses, all vowel parameters were related significantly to the mean ratings. The partial correlations for vowel duration were as high as those for the three frequency ranges. Vowel duration may be as important as spectral characteristics of vowels when breathiness is judged from samples of connected discourse.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Klich, Richard J.},
	month = dec,
	year = {1982},
	pages = {574--580},
}

@article{hillenbrand_acoustic_1994,
	title = {Acoustic {Correlates} of {Breathy} {Vocal} {Quality}},
	volume = {37},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3704.769},
	doi = {10.1044/jshr.3704.769},
	abstract = {The purpose of this study was to evaluate the effectiveness of several acoustic measures in predicting breathiness ratings. Recordings were made of eight normal men and seven normal women producing normally phonated, moderately breathy, and very breathy sustained vowels. Twenty listeners rated the degree of breathiness using a direct magnitude estimation procedure. Acoustic measures were made of: (a) signal periodicity, (b) first harmonic amplitude, and (c) spectral tilt. Periodicity measures provided the most accurate predictions of perceived breathiness, accounting for approximately 80\% of the variance in breathiness ratings. The relative amplitude of the first harmonic correlated moderately with breathiness ratings, and two measures of spectral tilt correlated weakly with perceived breathiness.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Hillenbrand, James and Cleveland, Ronald A. and Erickson, Robert L.},
	month = aug,
	year = {1994},
	pages = {769--778},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\SCZ928SH\\Hillenbrand et al. - 1994 - Acoustic Correlates of Breathy Vocal Quality.pdf:application/pdf},
}

@article{boersma_accurate_2000,
	title = {Accurate {Short}-{Term} {Analysis} {Of} {The} {Fundamental} {Frequency} {And} {The} {Harmonics}-{To}-{Noise} {Ratio} {Of} {A} {Sampled} {Sound}},
	volume = {17},
	abstract = {We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods. By definition, the best candidate for the acoustic pitch period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for p...},
	journal = {Proceedings of the Institute of Phonetic Sciences},
	author = {Boersma, Paul},
	month = jan,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\YHEXVA6R\\Boersma - 2000 - Accurate Short-Term Analysis Of The Fundamental Fr.pdf:application/pdf},
}

@article{deliyski_effects_2001,
	title = {{EFFECTS} {OF} {AGING} {ON} {SELECTED} {ACOUSTIC} {VOICE} {PARAMETERS}: {PRELIMINARY} {NORMATIVE} {DATA} {AND} {EDUCATIONAL} {IMPLICATIONS}},
	volume = {27},
	issn = {0360-1277, 1521-0472},
	shorttitle = {{EFFECTS} {OF} {AGING} {ON} {SELECTED} {ACOUSTIC} {VOICE} {PARAMETERS}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03601270151075561},
	doi = {10.1080/03601270151075561},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Educational Gerontology},
	author = {Deliyski, Dimitar, Steve An Xue},
	month = mar,
	year = {2001},
	pages = {159--168},
}

@article{vipperla_ageing_2010-1,
	title = {Ageing {Voices}: {The} {Effect} of {Changes} in {Voice} {Parameters} on {ASR} {Performance}},
	volume = {2010},
	issn = {1687-4714, 1687-4722},
	shorttitle = {Ageing {Voices}},
	url = {http://asmp.eurasipjournals.com/content/2010/1/525783},
	doi = {10.1155/2010/525783},
	language = {en},
	urldate = {2022-11-05},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	year = {2010},
	pages = {1--10},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AJZD42W5\\Vipperla et al. - 2010 - Ageing Voices The Effect of Changes in Voice Para.pdf:application/pdf},
}

@misc{noauthor_lombard_2022,
	title = {Lombard effect},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Lombard_effect&oldid=1100217835},
	abstract = {The Lombard effect or Lombard reflex is the involuntary tendency of speakers to increase their vocal effort when speaking in loud noise to enhance the audibility of their voice. This change includes not only loudness but also other acoustic features such as pitch, rate, and duration of syllables. This compensation effect maintains the auditory signal-to-noise ratio of the speaker's spoken words.
The effect links to the needs of effective communication, as there is a reduced effect when words are repeated or lists are read where communication intelligibility is not important. Since the effect is involuntary it is used as a means to detect malingering in those simulating hearing loss. Research on birds and monkeys find that the effect also occurs in the vocalizations of animals.
The effect was discovered in 1909 by Étienne Lombard, a French otolaryngologist.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Wikipedia},
	month = jul,
	year = {2022},
	note = {Page Version ID: 1100217835},
}

@article{lane_lombard_1971,
	title = {The {Lombard} {Sign} and the {Role} of {Hearing} in {Speech}},
	volume = {14},
	issn = {0022-4685},
	url = {http://pubs.asha.org/doi/10.1044/jshr.1404.677},
	doi = {10.1044/jshr.1404.677},
	abstract = {Lombard noted in 1911 that a speaker changes his voice level similarly when the ambient noise level increases, on the one hand, and when the level at which he hears his own voice (his sidetone) decreases, on the other. We can now state the form of these two functions, show that they are related to each other and to the equal-sensation function for imitating speech or noise loudness, and account for their form in terms of the underlying sensory scales and the hypothesis that the speaker tries to maintain a speech-to-noise ratio favorable for communication.
            Perturbations in the timing and spectrum of sidetone also lead the speaker to compensate for the apparent deterioration in his intelligibility. Such compensations reflect direct and indirect audience control of speech, rather than its autoregulation by sidetone. When not harassed by prying experimenters or an unfavorable acoustic environment, the speaker need no more listen to himself while speaking than he need speak to himself while listening.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech and Hearing Research},
	author = {Lane, Harlan and Tranel, Bernard},
	month = dec,
	year = {1971},
	pages = {677--709},
}

@article{lane_lombard_1971-1,
	title = {The {Lombard} {Sign} and the {Role} of {Hearing} in {Speech}},
	volume = {14},
	issn = {0022-4685},
	url = {http://pubs.asha.org/doi/10.1044/jshr.1404.677},
	doi = {10.1044/jshr.1404.677},
	abstract = {Lombard noted in 1911 that a speaker changes his voice level similarly when the ambient noise level increases, on the one hand, and when the level at which he hears his own voice (his sidetone) decreases, on the other. We can now state the form of these two functions, show that they are related to each other and to the equal-sensation function for imitating speech or noise loudness, and account for their form in terms of the underlying sensory scales and the hypothesis that the speaker tries to maintain a speech-to-noise ratio favorable for communication.
            Perturbations in the timing and spectrum of sidetone also lead the speaker to compensate for the apparent deterioration in his intelligibility. Such compensations reflect direct and indirect audience control of speech, rather than its autoregulation by sidetone. When not harassed by prying experimenters or an unfavorable acoustic environment, the speaker need no more listen to himself while speaking than he need speak to himself while listening.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech and Hearing Research},
	author = {Lane, Harlan and Tranel, Bernard},
	month = dec,
	year = {1971},
	pages = {677--709},
}

@article{derryberry_singing_2020,
	title = {Singing in a silent spring: {Birds} respond to a half-century soundscape reversion during the {COVID}-19 shutdown},
	volume = {370},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Singing in a silent spring},
	url = {https://www.science.org/doi/10.1126/science.abd5777},
	doi = {10.1126/science.abd5777},
	abstract = {Songbirds reclaim favored frequencies

              When severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic lockdowns were instituted across entire countries, human activities ceased in an unprecedented way. Derryberry
              et al.
              found that the reduction in traffic sound in the San Francisco Bay Area of California to levels not seen for half a century led to a shift in song frequency in white-crowned sparrows (see the Perspective by Halfwerk). This shift was especially notable because the frequency of human-produced traffic noise occurs within a range that interferes with the highest performance and most effective song. Thus, our “quiet” allowed the birds to quickly fill the most effective song space.


              Science
              , this issue p.
              575
              ; see also p.
              523

          ,
            Reductions in noise pollution during the pandemic shutdown allowed for more effective song production in white-crowned sparrows.
          ,
            Actions taken to control the coronavirus disease 2019 (COVID-19) pandemic have conspicuously reduced motor vehicle traffic, potentially alleviating auditory pressures on animals that rely on sound for survival and reproduction. Here, by comparing soundscapes and songs across the San Francisco Bay Area before and during the recent statewide shutdown, we evaluated whether a common songbird responsively exploited newly emptied acoustic space. We show that noise levels in urban areas were substantially lower during the shutdown, characteristic of traffic in the mid-1950s. We also show that birds responded by producing higher performance songs at lower amplitudes, effectively maximizing communication distance and salience. These findings illustrate that behavioral traits can change rapidly in response to newly favorable conditions, indicating an inherent resilience to long-standing anthropogenic pressures such as noise pollution.},
	language = {en},
	number = {6516},
	urldate = {2022-11-05},
	journal = {Science},
	author = {Derryberry, Elizabeth P. and Phillips, Jennifer N. and Derryberry, Graham E. and Blum, Michael J. and Luther, David},
	month = oct,
	year = {2020},
	pages = {575--579},
}

@article{summers_effects_1988,
	title = {Effects of noise on speech production: {Acoustic} and perceptual analyses},
	volume = {84},
	issn = {0001-4966},
	shorttitle = {Effects of noise on speech production},
	url = {http://asa.scitation.org/doi/10.1121/1.396660},
	doi = {10.1121/1.396660},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Summers, W. Van and Pisoni, David B. and Bernacki, Robert H. and Pedlow, Robert I. and Stokes, Michael A.},
	month = sep,
	year = {1988},
	pages = {917--928},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\HVCCM58H\\Summers et al. - 1988 - Effects of noise on speech production Acoustic an.pdf:application/pdf},
}

@article{summers_effects_1988-1,
	title = {Effects of noise on speech production: {Acoustic} and perceptual analyses},
	volume = {84},
	issn = {0001-4966},
	shorttitle = {Effects of noise on speech production},
	url = {http://asa.scitation.org/doi/10.1121/1.396660},
	doi = {10.1121/1.396660},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Summers, W. Van and Pisoni, David B. and Bernacki, Robert H. and Pedlow, Robert I. and Stokes, Michael A.},
	month = sep,
	year = {1988},
	pages = {917--928},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\P8ZUXZP7\\Summers et al. - 1988 - Effects of noise on speech production Acoustic an.pdf:application/pdf},
}

@article{manabe_control_1998,
	title = {Control of vocal intensity in budgerigars ( \textit{{Melopsittacus} undulatus} ): {Differential} reinforcement of vocal intensity and the {Lombard} effect},
	volume = {103},
	issn = {0001-4966},
	shorttitle = {Control of vocal intensity in budgerigars ( \textit{{Melopsittacus} undulatus} )},
	url = {http://asa.scitation.org/doi/10.1121/1.421227},
	doi = {10.1121/1.421227},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Manabe, Kazuchika and Sadr, Ehsanee I. and Dooling, Robert J.},
	month = feb,
	year = {1998},
	pages = {1190--1198},
}

@article{brumm_causes_2004,
	title = {Causes and consequences of song amplitude adjustment in a territorial bird: a case study in nightingales},
	volume = {76},
	issn = {0001-3765},
	shorttitle = {Causes and consequences of song amplitude adjustment in a territorial bird},
	url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0001-37652004000200017&lng=en&tlng=en},
	doi = {10.1590/S0001-37652004000200017},
	abstract = {Vocal amplitude, one of the crucial factors for the exchange of acoustic signals, has been neglected in studies of animal communication, but recent studies on song variation in Common Nightingales Luscinia megarhynchos have revealed new insights into its importance in the singing behavior of territorial birds. In nightingales song amplitude is not maximized per se, but is individually regulated according to the level of masking background noise. Also, birds adjust their vocal intensity according to social variables, as in male-male interactions. Moreover, during such interactions, males exploited the directionality of their songs to broadcast them in the direction of the intended receivers ensuring the most effective signal transmission. Studies of the development of this typical long-range signaling suggest that sound level is highly interrelated with overall developmental progression and learning, and thus should be viewed as an integral part of song ontogeny. I conclude that song amplitude is a dynamic feature of the avian signal system, which is individually regulated according to the ecological demands of signal transmission and the social context of communication.
          ,
            A amplitude vocal, um dos fatores cruciais para a troca de sinais acústicos, tem sido negligenciada nos estudos da comunicação animal, mas trabalhos recentes sobre a variação do canto do Rouxinol-comum Luscinia megarhynchos evidenciaram sua importância no comportamento de canto das aves territoriais. No rouxinol a amplitude do canto não é aumentada ao máximo per se, mas é regulada individualmente de acordo com o nível de ruído de fundo que mascara o sinal. As aves também ajustam sua intensidade vocal às variáveis sociais, tais como nas interações entre machos. Além disso, durante essas interações, os machos tiram proveito da direcionalidade de seus cantos para emiti-los em direção aos receptores desejados no intuito de garantir a mais eficiente transmissão do sinal. Estudos do desenvolvimento desta sinalização típica de longo alcance sugerem que o nível sonoro seja altamente relacionado com o desenvolvimento geral e a aprendizagem, e deveria portanto ser visto como parte integrante da ontogenia do canto. Concluímos que a amplitude do canto é um parâmetro dinâmico do sistema de sinalização em aves, que é regulado individualmente de acordo com as exigências ecológicas da transmissão do sinal e o contexto social da comunicação.},
	number = {2},
	urldate = {2022-11-05},
	journal = {Anais da Academia Brasileira de Ciências},
	author = {Brumm, Henrik},
	month = jun,
	year = {2004},
	pages = {289--295},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TL5VXJ8V\\Brumm - 2004 - Causes and consequences of song amplitude adjustme.pdf:application/pdf},
}

@article{sinnott_regulation_1975,
	title = {Regulation of voice amplitude by the monkey},
	volume = {58},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.380685},
	doi = {10.1121/1.380685},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Sinnott, Joan M. and Stebbins, William C. and Moody, David B.},
	month = aug,
	year = {1975},
	pages = {412--414},
}

@article{patel_influence_2008,
	title = {The {Influence} of {Linguistic} {Content} on the {Lombard} {Effect}},
	volume = {51},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282008/016%29},
	doi = {10.1044/1092-4388(2008/016)},
	abstract = {Purpose

                The
                Lombard effect
                describes the tendency for speakers to increase pitch, intensity, and duration in the presence of noise. It is unclear whether these modifications are uniformly applied across all words within an utterance or whether information-bearing content words are further enhanced compared with function words. In the present study, the authors investigated the influence of linguistic content on acoustic modifications made to speech in noise.



              Method

                Sixteen speaker–listener pairs engaged in an interactive cooperative game in quiet, 60 dB of multitalker noise, and 90 dB of multitalker noise. Speaker productions were analyzed to examine differences in fundamental frequency (F
                0
                ), intensity, and duration of target words in sentences across noise conditions.



              Results

                Proportional increases in F
                0,
                intensity, and duration were noted for all word types as noise increased from quiet to 60 dB. From quiet to 90 dB, content words that referred to agents, objects, and locations were disproportionately elongated compared with function words. Additionally, agents were further enhanced by increased F
                0
                .



              Conclusions

                At moderate noise levels, most word types appear to be uniformly boosted in F
                0
                , intensity, and duration. As noise increases, linguistic content shapes the extent of the Lombard effect, with F
                0
                and duration serving as primary cues for marking information-bearing word types.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Patel, Rupal and Schell, Kevin W.},
	month = feb,
	year = {2008},
	pages = {209--220},
}

@article{winkworth_speech_1997,
	title = {Speech {Breathing} and the {Lombard} {Effect}},
	volume = {40},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jslhr.4001.159},
	doi = {10.1044/jslhr.4001.159},
	abstract = {Respiratory measurements were made using linearized magnetometers placed antero-posteriorly over the rib cages and abdomens of five healthy young women. Background noise was introduced over headphones simultaneously as "babble" presented binaurally at 55 dB ("moderate noise") and 70 dB ("high noise"). Speech during oral reading and spontaneous monologue was transduced with a microphone positioned near the lips, from which a speaking intensity signal (dBA) was derived. Subjects were instructed to speak during the noise conditions, but no instruction was given to alter speaking intensity. Compared with a "no noise" condition, the speaking intensities of all the subjects increased significantly for both speech tasks in the moderate and high noise conditions, thereby replicating the well-documented Lombard effect. No consistent trend of lung volume change was observed, in contrast to the linear increases in speech intensity as the noise level increased. For the higher speech intensities during the moderate and high noise conditions both initiation and termination lung volumes either increased or decreased. These preliminary findings suggest that when speech intensity is increased following the introduction of noise via headphones rather than by specific instructions to speak more loudly, speakers employ variable lung volume strategies for intensity control.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Winkworth, Alison L. and Davis, Pamela J.},
	month = feb,
	year = {1997},
	pages = {159--169},
}

@article{vatikiotisbateson_auditory_2006,
	title = {Auditory, but perhaps not visual, processing of {Lombard} speech},
	volume = {119},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4786950},
	doi = {10.1121/1.4786950},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Vatikiotis‐Bateson, Eric and Chung, Victor and Lutz, Kevin and Mirante, Nicole and Otten, Jolien and Tan, Johanna},
	month = may,
	year = {2006},
	pages = {3444--3444},
}

@article{pick_inhibiting_1989,
	title = {Inhibiting the {Lombard} effect},
	volume = {85},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.397561},
	doi = {10.1121/1.397561},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Pick, Herbert L. and Siegel, Gerald M. and Fox, Paul W. and Garber, Sharon R. and Kearney, Joseph K.},
	month = feb,
	year = {1989},
	pages = {894--900},
}

@inproceedings{choudhury_comparitive_2015,
	address = {Silchar, India},
	title = {A comparitive study on classifiers to classify languages into {Tonal} and {Non}-{Tonal} {Languages}},
	isbn = {978-1-4673-6707-3 978-1-4673-6708-0},
	url = {http://ieeexplore.ieee.org/document/7377329/},
	doi = {10.1109/ISACC.2015.7377329},
	urldate = {2022-11-05},
	booktitle = {2015 {International} {Symposium} on {Advanced} {Computing} and {Communication} ({ISACC})},
	publisher = {IEEE},
	author = {Choudhury, Biplav and Choudhury, Tameem Salman},
	month = sep,
	year = {2015},
	pages = {132--135},
}

@incollection{ide_bruce_2000,
	address = {Dordrecht},
	title = {Bruce, {Pierrehumbert}, and the {Elements} of {Intonational} {Phonology}},
	volume = {14},
	isbn = {978-90-481-5562-0 978-94-015-9413-4},
	url = {http://link.springer.com/10.1007/978-94-015-9413-4_3},
	urldate = {2022-11-05},
	booktitle = {Prosody: {Theory} and {Experiment}},
	publisher = {Springer Netherlands},
	author = {Ladd, D. Robert},
	editor = {Ide, Nancy and Véronis, Jean and Horne, Merle},
	year = {2000},
	doi = {10.1007/978-94-015-9413-4_3},
	note = {Series Title: Text, Speech and Language Technology},
	pages = {37--50},
}

@article{werker_cross-language_1984,
	title = {Cross-language speech perception: {Evidence} for perceptual reorganization during the first year of life},
	volume = {7},
	issn = {01636383},
	shorttitle = {Cross-language speech perception},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0163638384800223},
	doi = {10.1016/S0163-6383(84)80022-3},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Infant Behavior and Development},
	author = {Werker, Janet F. and Tees, Richard C.},
	month = jan,
	year = {1984},
	pages = {49--63},
}

@article{greenberg_temporal_2003,
	title = {Temporal properties of spontaneous speech—a syllable-centric perspective},
	volume = {31},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447003000603},
	doi = {10.1016/j.wocn.2003.09.005},
	language = {en},
	number = {3-4},
	urldate = {2022-11-05},
	journal = {Journal of Phonetics},
	author = {Greenberg, Steven and Carvey, Hannah and Hitchcock, Leah and Chang, Shuangyu},
	month = jul,
	year = {2003},
	pages = {465--485},
}

@incollection{van_oostendorp_stress-timed_2011,
	address = {Oxford, UK},
	title = {Stress-{Timed} \textit{vs} . {Syllable}-{Timed} {Languages}: {Stress}-timed \textit{vs} . {Syllable}-timed {Languages}},
	isbn = {978-1-4443-3526-2},
	shorttitle = {Stress-{Timed} \textit{vs} . {Syllable}-{Timed} {Languages}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781444335262.wbctp0048},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {The {Blackwell} {Companion} to {Phonology}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Nespor, Marina and Shukla, Mohinish and Mehler, Jacques},
	editor = {van Oostendorp, Marc and Ewen, Colin J. and Hume, Elizabeth and Rice, Keren},
	month = apr,
	year = {2011},
	doi = {10.1002/9781444335262.wbctp0048},
	pages = {1--13},
}

@book{pike_intonation_1963,
	series = {Linguistics},
	title = {The {Intonation} of {American} {English}},
	url = {https://books.google.com.pk/books?id=Jz3iAAAAMAAJ},
	publisher = {University of Michigan Press},
	author = {Pike, K.L.},
	year = {1963},
	lccn = {45004529},
}

@inproceedings{adda-decker_m_reconnaissance_2006,
	title = {De la reconnaissance automatique de la parole `a l’analyse linguistique de corpus oraux},
	author = {Adda-Decker, M},
	year = {2006},
}

@article{avendano_properties_1998,
	title = {On the {Properties} of {Temporal} {Processing} for {Speech} in {Adverse} {Environments}},
	url = {https://www.researchgate.net/publication/2466964_Carlos_Avendano_and_Hynek_Hermansky_On_the_Properties_of_Temporal_Processing_for_Speech_in_Adverse_Environments_On_the_Properties_of_Temporal_Processing_for_Speech_in_Adverse_Environments},
	abstract = {In this paper we report on the results that we have obtained in the application of temporal processing to speech signals. We describe what are the properties that make temporal processing an interesting and useful technique to alleviate the harmful effects that environmental factors have on speech. Though temporal processing has been used in the past, its analysis and properties have not been studied in detail. We summarize some results that we obtained in a detailed analysis, and describe a data-driven design technique to design the processing. We demonstrate a speech enhancementsystem which illustrates some properties, advantages, and short-comings of the technique. 1. Introduction The performance of speech communication systems often degrades under realistic environmental conditions. Adverse environmental factors include additive noise sources, room reverberation, and transmission channel distortions. This work discusses the processing of speech in the temporal-feature or modulati...},
	author = {Avendaño, Carlos and Hermansky, Hynek},
	month = sep,
	year = {1998},
}

@inproceedings{melot_analysis_2015,
	address = {Scottsdale, AZ, USA},
	title = {Analysis of factors affecting system performance in the {ASpIRE} challenge},
	isbn = {978-1-4799-7291-3},
	url = {http://ieeexplore.ieee.org/document/7404838/},
	doi = {10.1109/ASRU.2015.7404838},
	urldate = {2022-11-05},
	booktitle = {2015 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU})},
	publisher = {IEEE},
	author = {Melot, Jennifer and Malyska, Nicolas and Ray, Jessica and Shen, Wade},
	month = dec,
	year = {2015},
	pages = {512--517},
}

@misc{noauthor_literature_nodate,
	title = {Literature {Review} - {Speech} {Recognition} for {Noisy} {Environments}},
	url = {https://users.dcc.uchile.cl/~abassi/WWW/Voz/speech-recog.html},
	urldate = {2022-11-08},
	file = {Literature Review - Speech Recognition for Noisy Environments:C\:\\Users\\DELL\\Zotero\\storage\\66WTUREM\\speech-recog.html:text/html},
}

@article{endres_voice_1971,
	title = {Voice {Spectrograms} as a {Function} of {Age}, {Voice} {Disguise}, and {Voice} {Imitation}},
	volume = {49},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1912589},
	doi = {10.1121/1.1912589},
	language = {en},
	number = {6B},
	urldate = {2022-11-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Endres, W. and Bambach, W. and Flösser, G.},
	month = jun,
	year = {1971},
	pages = {1842--1848},
}

@incollection{muller_study_2007,
	address = {Berlin, Heidelberg},
	title = {A {Study} of {Acoustic} {Correlates} of {Speaker} {Age}},
	volume = {4441},
	isbn = {978-3-540-74121-3 978-3-540-74122-0},
	url = {http://link.springer.com/10.1007/978-3-540-74122-0_1},
	language = {en},
	urldate = {2022-11-10},
	booktitle = {Speaker {Classification} {II}},
	publisher = {Springer Berlin Heidelberg},
	author = {Schötz, Susanne and Müller, Christian},
	editor = {Müller, Christian},
	year = {2007},
	doi = {10.1007/978-3-540-74122-0_1},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {1--9},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\WBR85E2H\\Schötz and Müller - 2007 - A Study of Acoustic Correlates of Speaker Age.pdf:application/pdf},
}

@inproceedings{gillick_statistical_1989,
	address = {Glasgow, UK},
	title = {Some statistical issues in the comparison of speech recognition algorithms},
	url = {http://ieeexplore.ieee.org/document/266481/},
	doi = {10.1109/ICASSP.1989.266481},
	urldate = {2022-11-10},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Gillick, L. and Cox, S.J.},
	year = {1989},
	pages = {532--535},
}

@inproceedings{aleksic_improved_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Improved recognition of contact names in voice commands},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178957/},
	doi = {10.1109/ICASSP.2015.7178957},
	urldate = {2022-11-10},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Aleksic, Petar and Allauzen, Cyril and Elson, David and Kracun, Aleksandar and Casado, Diego Melendo and Moreno, Pedro J.},
	month = apr,
	year = {2015},
	pages = {5172--5175},
}

@article{isaacs_rater_2013,
	title = {Rater {Experience}, {Rating} {Scale} {Length}, and {Judgments} of {L2} {Pronunciation}: {Revisiting} {Research} {Conventions}},
	volume = {10},
	issn = {1543-4303, 1543-4311},
	shorttitle = {Rater {Experience}, {Rating} {Scale} {Length}, and {Judgments} of {L2} {Pronunciation}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/15434303.2013.769545},
	doi = {10.1080/15434303.2013.769545},
	language = {en},
	number = {2},
	urldate = {2022-11-10},
	journal = {Language Assessment Quarterly},
	author = {Isaacs, Talia and Thomson, Ron I.},
	month = apr,
	year = {2013},
	pages = {135--159},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\D4D585T7\\Isaacs and Thomson - 2013 - Rater Experience, Rating Scale Length, and Judgmen.pdf:application/pdf},
}

@article{saito_second_2016,
	title = {Second language speech production: {Investigating} linguistic correlates of comprehensibility and accentedness for learners at different ability levels},
	volume = {37},
	issn = {0142-7164, 1469-1817},
	shorttitle = {Second language speech production},
	url = {https://www.cambridge.org/core/product/identifier/S0142716414000502/type/journal_article},
	doi = {10.1017/S0142716414000502},
	abstract = {ABSTRACT
            The current project aimed to investigate the potentially different linguistic correlates of comprehensibility (i.e., ease of understanding) and accentedness (i.e., linguistic nativelikeness) in adult second language (L2) learners’ extemporaneous speech production. Timed picture descriptions from 120 beginner, intermediate, and advanced Japanese learners of English were analyzed using native speaker global judgments based on learners’ comprehensibility and accentedness, and then submitted to segmental, prosodic, temporal, lexical, and grammatical analyses. Results showed that comprehensibility was related to all linguistic domains, and accentedness was strongly tied with pronunciation (specifically segmentals) rather than lexical and grammatical domains. In particular, linguistic correlates of L2 comprehensibility and accentedness were found to vary by learners’ proficiency levels. In terms of comprehensibility, optimal rate of speech, appropriate and rich vocabulary use, and adequate and varied prosody were important for beginner to intermediate levels, whereas segmental accuracy, good prosody, and correct grammar featured strongly for intermediate to advanced levels. For accentedness, grammatical complexity was a feature of intermediate to high-level performance, whereas segmental and prosodic variables were essential to accentedness across all levels. These findings suggest that syllabi tailored to learners’ proficiency level (beginner, intermediate, or advanced) and learning goal (comprehensibility or nativelike accent) would be advantageous for the teaching of L2 speaking.},
	language = {en},
	number = {2},
	urldate = {2022-11-10},
	journal = {Applied Psycholinguistics},
	author = {Saito, Kazuya and Trofimovich, Pavel and Isaacs, Talia},
	month = mar,
	year = {2016},
	pages = {217--240},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\VPJLHZP5\\Saito et al. - 2016 - Second language speech production Investigating l.pdf:application/pdf},
}

@inproceedings{chuchual_inbound_2010,
	address = {Tokyo, Japan},
	title = {Inbound and outbound calls assignment for an efficient call center},
	isbn = {978-1-4244-6485-2},
	url = {http://ieeexplore.ieee.org/document/5530196/},
	doi = {10.1109/ICSSSM.2010.5530196},
	urldate = {2022-11-13},
	booktitle = {2010 7th {International} {Conference} on {Service} {Systems} and {Service} {Management}},
	publisher = {IEEE},
	author = {Chuchual, Panichapat and Chongpravatisakul, Narissa and Kusolmanomai, Teerasarn and Komolavanij, Somrote},
	month = jun,
	year = {2010},
	pages = {1--4},
}

@inproceedings{liston_simulation_2017,
	address = {Las Vegas, NV},
	title = {Simulation based decision support for contact centers},
	isbn = {978-1-5386-3428-8},
	url = {http://ieeexplore.ieee.org/document/8248217/},
	doi = {10.1109/WSC.2017.8248217},
	urldate = {2022-11-13},
	booktitle = {2017 {Winter} {Simulation} {Conference} ({WSC})},
	publisher = {IEEE},
	author = {Liston, Paul and Byrne, James and Keogh, Orla and Byrne, P.J. and Bourke, Joe and Jones, Karl},
	month = dec,
	year = {2017},
	pages = {4586--4587},
}

@inproceedings{nwobodo-anyadiegwu_evaluating_2018,
	address = {Singapore},
	title = {Evaluating variables that affect job satisfaction of bank customer contact centre agents in {South} {Africa}},
	isbn = {978-1-5386-5747-8 978-1-5386-5748-5},
	url = {https://ieeexplore.ieee.org/document/8387081/},
	doi = {10.1109/IEA.2018.8387081},
	urldate = {2022-11-13},
	booktitle = {2018 5th {International} {Conference} on {Industrial} {Engineering} and {Applications} ({ICIEA})},
	publisher = {IEEE},
	author = {Nwobodo-Anyadiegwu, Eveth and Mbohwa, Charles and Ndlovu, Nokukhanya},
	month = apr,
	year = {2018},
	pages = {116--121},
}

@inproceedings{liu_research_2012,
	address = {Hangzhou, China},
	title = {Research on {Forecasting} {Call} {Center} {Traffic} through {PCA} and {BP} {Artificial} {Neural} {Network}},
	isbn = {978-1-4673-2646-9},
	url = {http://ieeexplore.ieee.org/document/6407017/},
	doi = {10.1109/ISCID.2012.117},
	urldate = {2022-11-13},
	booktitle = {2012 {Fifth} {International} {Symposium} on {Computational} {Intelligence} and {Design}},
	publisher = {IEEE},
	author = {Liu, Tao and Liu, Lieli},
	month = oct,
	year = {2012},
	pages = {444--447},
}

@inproceedings{archawaporn_erlang_2013,
	address = {Nakorn Pathom, Thailand},
	title = {Erlang {C} model for evaluate incoming call uncertainty in automotive call centers},
	isbn = {978-1-4673-5324-3 978-1-4673-5322-9},
	url = {http://ieeexplore.ieee.org/document/6694762/},
	doi = {10.1109/ICSEC.2013.6694762},
	urldate = {2022-11-13},
	booktitle = {2013 {International} {Computer} {Science} and {Engineering} {Conference} ({ICSEC})},
	publisher = {IEEE},
	author = {Archawaporn, Laksamon and Wongseree, Waranyu},
	month = sep,
	year = {2013},
	pages = {109--113},
}

@inproceedings{nguyen_development_2017,
	address = {Seoul},
	title = {Development of a {Vietnamese} speech recognition system for {Viettel} call center},
	isbn = {978-1-5386-3333-5},
	url = {https://ieeexplore.ieee.org/document/8384456/},
	doi = {10.1109/ICSDA.2017.8384456},
	urldate = {2022-11-17},
	booktitle = {2017 20th {Conference} of the {Oriental} {Chapter} of the {International} {Coordinating} {Committee} on {Speech} {Databases} and {Speech} {I}/{O} {Systems} and {Assessment} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Nguyen, Quoc Bao and Do, Van Hai and Dam, Ba Quyen and Le, Minh Hung},
	month = nov,
	year = {2017},
	pages = {1--5},
}

@inproceedings{do_agentclient_2020,
	address = {Kuala Lumpur, Malaysia},
	title = {Agent/{Client} {Speech} {Identification} for {Mixed}-{Channel} {Conversation} in {Customer} {Service} {Call} {Centers}},
	isbn = {978-1-72817-689-5},
	url = {https://ieeexplore.ieee.org/document/9310469/},
	doi = {10.1109/IALP51396.2020.9310469},
	urldate = {2022-11-17},
	booktitle = {2020 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	publisher = {IEEE},
	author = {Do, Van Hai and Mai, Van Tuan},
	month = dec,
	year = {2020},
	pages = {197--200},
}

@inproceedings{min_tang_call-type_2003,
	address = {St Thomas, VI, USA},
	title = {Call-type classification and unsupervised training for the call center domain},
	isbn = {978-0-7803-7980-0},
	url = {http://ieeexplore.ieee.org/document/1318429/},
	doi = {10.1109/ASRU.2003.1318429},
	urldate = {2022-11-17},
	booktitle = {2003 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({IEEE} {Cat}. {No}.{03EX721})},
	publisher = {IEEE},
	author = {{Min Tang} and Pellom, B. and Hacioglu, K.},
	year = {2003},
	pages = {204--208},
}

@inproceedings{galanis_classification_2013,
	address = {Budapest, Hungary},
	title = {Classification of emotional speech units in call centre interactions},
	isbn = {978-1-4799-1546-0 978-1-4799-1543-9},
	url = {http://ieeexplore.ieee.org/document/6719279/},
	doi = {10.1109/CogInfoCom.2013.6719279},
	urldate = {2022-11-17},
	booktitle = {2013 {IEEE} 4th {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
	publisher = {IEEE},
	author = {Galanis, Dimitrios and Karabetsos, Sotiris and Koutsombogera, Maria and Papageorgiou, Harris and Esposito, Anna and Riviello, Maria-Teresa},
	month = dec,
	year = {2013},
	pages = {403--406},
}

@inproceedings{draman_malay_2017,
	address = {Malacca City},
	title = {Malay speech corpus of telecommunication call center preparation for {ASR}},
	isbn = {978-1-5090-4912-7},
	url = {https://ieeexplore.ieee.org/document/8074675/},
	doi = {10.1109/ICoICT.2017.8074675},
	urldate = {2022-11-17},
	booktitle = {2017 5th {International} {Conference} on {Information} and {Communication} {Technology} ({ICoIC7})},
	publisher = {IEEE},
	author = {Draman, M. and Tee, D.C. and Lambak, Z. and Yahya, M.R. and Mohd Yusoff, M.I. and Ibrahim, S.H. and Saidon, S. and Abu Haris, N. and Tan, T.P.},
	month = may,
	year = {2017},
	pages = {1--6},
}

@inproceedings{deschamps-berger_end--end_2021,
	address = {Nara, Japan},
	title = {End-to-{End} {Speech} {Emotion} {Recognition}: {Challenges} of {Real}-{Life} {Emergency} {Call} {Centers} {Data} {Recordings}},
	isbn = {978-1-66540-019-0},
	shorttitle = {End-to-{End} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9597419/},
	doi = {10.1109/ACII52823.2021.9597419},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo and Lamel, Lori and Devillers, Laurence},
	month = sep,
	year = {2021},
	pages = {1--8},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\EQSN7492\\Deschamps-Berger et al. - 2021 - End-to-End Speech Emotion Recognition Challenges .pdf:application/pdf},
}

@inproceedings{zweig_automated_2006,
	address = {Toulouse, France},
	title = {Automated {Quality} {Monitoring} in the {Call} {Center} with {ASR} and {Maximum} {Entropy}},
	volume = {1},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1660089/},
	doi = {10.1109/ICASSP.2006.1660089},
	urldate = {2022-11-17},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Zweig, G. and Siohan, O. and Saon, G. and Ramabhadran, B. and Povey, D. and Mangu, L. and Kingsbury, B.},
	year = {2006},
	pages = {I--589--I--592},
}

@inproceedings{tarjan_n-gram_2019,
	address = {Naples, Italy},
	title = {N-gram {Approximation} of {LSTM} {Recurrent} {Language} {Models} for {Single}-pass {Recognition} of {Hungarian} {Call} {Center} {Conversations}},
	isbn = {978-1-72814-793-2},
	url = {https://ieeexplore.ieee.org/document/9089959/},
	doi = {10.1109/CogInfoCom47531.2019.9089959},
	urldate = {2022-11-17},
	booktitle = {2019 10th {IEEE} {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
	publisher = {IEEE},
	author = {Tarjan, Balazs and Szaszak, Gyorgy and Fegyo, Tibor and Mihajlik, Peter},
	month = oct,
	year = {2019},
	pages = {131--136},
}

@inproceedings{tarjan_improved_2013,
	address = {Cluj-Napoca, Romania},
	title = {Improved recognition of {Hungarian} call center conversations},
	isbn = {978-1-4799-1065-6 978-1-4799-1063-2},
	url = {http://ieeexplore.ieee.org/document/6682652/},
	doi = {10.1109/SpeD.2013.6682652},
	urldate = {2022-11-17},
	booktitle = {2013 7th {Conference} on {Speech} {Technology} and {Human} - {Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Tarjan, Balazs and Sarosi, Gellert and Fegyo, Tibor and Mihajlik, Peter},
	month = oct,
	year = {2013},
	pages = {1--6},
}

@article{plaza_call_2021,
	title = {Call {Transcription} {Methodology} for {Contact} {Center} {Systems}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9508438/},
	doi = {10.1109/ACCESS.2021.3102502},
	urldate = {2022-11-17},
	journal = {IEEE Access},
	author = {Plaza, Miroslaw and Pawlik, Lukasz and Deniziak, Stanislaw},
	year = {2021},
	pages = {110975--110988},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\JM7QLB6V\\Plaza et al. - 2021 - Call Transcription Methodology for Contact Center .pdf:application/pdf},
}

@article{fan_gated_2021,
	title = {Gated {Recurrent} {Fusion} {With} {Joint} {Training} {Framework} for {Robust} {End}-to-{End} {Speech} {Recognition}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9265262/},
	doi = {10.1109/TASLP.2020.3039600},
	urldate = {2022-11-17},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Fan, Cunhang and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Liu, Bin and Wen, Zhengqi},
	year = {2021},
	pages = {198--209},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\36B4AJ37\\Fan et al. - 2021 - Gated Recurrent Fusion With Joint Training Framewo.pdf:application/pdf},
}

@inproceedings{deschamps-berger_end--end_2021-1,
	address = {Nara, Japan},
	title = {End-to-{End} {Speech} {Emotion} {Recognition}: {Challenges} of {Real}-{Life} {Emergency} {Call} {Centers} {Data} {Recordings}},
	isbn = {978-1-66540-019-0},
	shorttitle = {End-to-{End} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9597419/},
	doi = {10.1109/ACII52823.2021.9597419},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo and Lamel, Lori and Devillers, Laurence},
	month = sep,
	year = {2021},
	pages = {1--8},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\DJESTVXN\\Deschamps-Berger et al. - 2021 - End-to-End Speech Emotion Recognition Challenges .pdf:application/pdf},
}

@inproceedings{chunwijitra_improving_2021,
	address = {Chiang Mai, Thailand},
	title = {Improving automatic transcription of call center speech using data simulation},
	isbn = {978-1-66540-382-5},
	url = {https://ieeexplore.ieee.org/document/9454803/},
	doi = {10.1109/ECTI-CON51831.2021.9454803},
	urldate = {2022-11-17},
	booktitle = {2021 18th {International} {Conference} on {Electrical} {Engineering}/{Electronics}, {Computer}, {Telecommunications} and {Information} {Technology} ({ECTI}-{CON})},
	publisher = {IEEE},
	author = {Chunwijitra, Vataya and Kurpukdee, Nattapong},
	month = may,
	year = {2021},
	pages = {842--845},
}

@inproceedings{deschamps-berger_emotion_2021,
	address = {Nara, Japan},
	title = {Emotion {Recognition} {In} {Emergency} {Call} {Centers}: {The} challenge of real-life emotions},
	isbn = {978-1-66540-021-3},
	shorttitle = {Emotion {Recognition} {In} {Emergency} {Call} {Centers}},
	url = {https://ieeexplore.ieee.org/document/9666308/},
	doi = {10.1109/ACIIW52867.2021.9666308},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} {Workshops} and {Demos} ({ACIIW})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo},
	month = sep,
	year = {2021},
	pages = {1--5},
}

@inproceedings{seknedy_speech_2021,
	address = {Cairo, Egypt},
	title = {Speech {Emotion} {Recognition} {System} for {Human} {Interaction} {Applications}},
	isbn = {978-1-66544-076-9},
	url = {https://ieeexplore.ieee.org/document/9694246/},
	doi = {10.1109/ICICIS52592.2021.9694246},
	urldate = {2022-11-17},
	booktitle = {2021 {Tenth} {International} {Conference} on {Intelligent} {Computing} and {Information} {Systems} ({ICICIS})},
	publisher = {IEEE},
	author = {Seknedy, Mai El and Fawzi, Sahar},
	month = dec,
	year = {2021},
	pages = {361--368},
}

@article{bai_fast_2021,
	title = {Fast {End}-to-{End} {Speech} {Recognition} {Via} {Non}-{Autoregressive} {Models} and {Cross}-{Modal} {Knowledge} {Transferring} {From} {BERT}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9437636/},
	doi = {10.1109/TASLP.2021.3082299},
	urldate = {2022-11-17},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Wen, Zhengqi and Zhang, Shuai},
	year = {2021},
	pages = {1897--1911},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U8MZQEJI\\Bai et al. - 2021 - Fast End-to-End Speech Recognition Via Non-Autoreg.pdf:application/pdf},
}

@techreport{tits_sector_effect_1993,
	address = {Helsinki},
	title = {Effect of transmission impairments},
	url = {https://www.itu.int/ITU-T/recommendations/rec.aspx?rec=1718&lang=en},
	language = {en},
	urldate = {2022-11-17},
	institution = {International Telecommunications Union},
	author = {T.I.T.S Sector},
	month = mar,
	year = {1993},
	pages = {11},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VRNTN6WT\\rec.html:text/html},
}

@article{huerta_speech_1970,
	title = {Speech {Recognition} {From} {Gsm} {Codec} {Parameters}},
	volume = {4},
	abstract = {Speech coding affects speech recognition performance, with recognition accuracy deteriorating as the coded bit rate decreases. Virtually all systems that recognize coded speech reconstruct the speech waveform from the coded parameters, and then perform recognition (after possible noise and/or channel compensation) using conventional techniques. In this paper we compare the recognition accuracy of coded speech obtained by reconstructing the speech waveform with the speech recognition accuracy obtained when using cepstral features derived from the coding parameters. We focus our efforts on speech that has been coded using the 13-kbps full-rate GSM codec, a Regular Pulse Excited Long Term Prediction (RPE-LTP) codec. The GSM codec develops separate representations for the linear prediction (LPC) filter and the residual signal components of the coded speech. We measure the effects of quantization and coding on the accuracy with which these parameters are represented, and present two differe...},
	author = {Huerta, Juan and Stern, Richard},
	month = feb,
	year = {1970},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\AF6Y34IV\\Huerta and Stern - 1970 - Speech Recognition From Gsm Codec Parameters.pdf:application/pdf},
}

@inproceedings{besacier_effect_2001,
	address = {Cannes, France},
	title = {The effect of speech and audio compression on speech recognition performance},
	isbn = {978-0-7803-7025-8},
	url = {http://ieeexplore.ieee.org/document/962750/},
	doi = {10.1109/MMSP.2001.962750},
	urldate = {2022-11-17},
	booktitle = {2001 {IEEE} {Fourth} {Workshop} on {Multimedia} {Signal} {Processing} ({Cat}. {No}.{01TH8564})},
	publisher = {IEEE},
	author = {Besacier, L. and Bergamini, C. and Vaufreydaz, D. and Castelli, E.},
	year = {2001},
	pages = {301--306},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\CJL6XFAB\\Besacier et al. - 2001 - The effect of speech and audio compression on spee.pdf:application/pdf},
}

@incollection{sojka_robust_2018,
	address = {Cham},
	title = {Robust {Recognition} of {Conversational} {Telephone} {Speech} via {Multi}-condition {Training} and {Data} {Augmentation}},
	volume = {11107},
	isbn = {978-3-030-00793-5 978-3-030-00794-2},
	url = {http://link.springer.com/10.1007/978-3-030-00794-2_35},
	language = {en},
	urldate = {2022-11-17},
	booktitle = {Text, {Speech}, and {Dialogue}},
	publisher = {Springer International Publishing},
	author = {Málek, Jiří and Ždánský, Jindřich and Červa, Petr},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2018},
	doi = {10.1007/978-3-030-00794-2_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {324--333},
}

@inproceedings{vu_audio_2019,
	address = {Lanzhou, China},
	title = {Audio {Codec} {Simulation} based {Data} {Augmentation} for {Telephony} {Speech} {Recognition}},
	isbn = {978-1-72813-248-8},
	url = {https://ieeexplore.ieee.org/document/9023257/},
	doi = {10.1109/APSIPAASC47483.2019.9023257},
	urldate = {2022-11-17},
	booktitle = {2019 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Vu, Thi-Ly and Zeng, Zhiping and Xu, Haihua and Chng, Eng-Siong},
	month = nov,
	year = {2019},
	pages = {198--203},
}

@inproceedings{zeng_end--end_2019,
	title = {On the {End}-to-{End} {Solution} to {Mandarin}-{English} {Code}-{Switching} {Speech} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/zeng19_interspeech.html},
	doi = {10.21437/Interspeech.2019-1429},
	language = {en},
	urldate = {2022-11-17},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Zeng, Zhiping and Khassanov, Yerbolat and Pham, Van Tung and Xu, Haihua and Chng, Eng Siong and Li, Haizhou},
	month = sep,
	year = {2019},
	pages = {2165--2169},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\NGBDPXY8\\Zeng et al. - 2019 - On the End-to-End Solution to Mandarin-English Cod.pdf:application/pdf},
}

@book{neustein_advances_2010,
	address = {Boston, MA},
	title = {Advances in {Speech} {Recognition}},
	isbn = {978-1-4419-5950-8 978-1-4419-5951-5},
	url = {http://link.springer.com/10.1007/978-1-4419-5951-5},
	language = {en},
	urldate = {2022-11-17},
	publisher = {Springer US},
	editor = {Neustein, Amy},
	year = {2010},
	doi = {10.1007/978-1-4419-5951-5},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3U2RL686\\Neustein - 2010 - Advances in Speech Recognition.pdf:application/pdf},
}

@article{valizada_development_2021,
	title = {Development of {Speech} {Recognition} {Systems} in {Emergency} {Call} {Centers}},
	volume = {13},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/13/4/634},
	doi = {10.3390/sym13040634},
	abstract = {In this paper, various methodologies of acoustic and language models, as well as labeling methods for automatic speech recognition for spoken dialogues in emergency call centers were investigated and comparatively analyzed. Because of the fact that dialogue speech in call centers has specific context and noisy, emotional environments, available speech recognition systems show poor performance. Therefore, in order to accurately recognize dialogue speeches, the main modules of speech recognition systems—language models and acoustic training methodologies—as well as symmetric data labeling approaches have been investigated and analyzed. To find an effective acoustic model for dialogue data, different types of Gaussian Mixture Model/Hidden Markov Model (GMM/HMM) and Deep Neural Network/Hidden Markov Model (DNN/HMM) methodologies were trained and compared. Additionally, effective language models for dialogue systems were defined based on extrinsic and intrinsic methods. Lastly, our suggested data labeling approaches with spelling correction are compared with common labeling methods resulting in outperforming the other methods with a notable percentage. Based on the results of the experiments, we determined that DNN/HMM for an acoustic model, trigram with Kneser–Ney discounting for a language model and using spelling correction before training data for a labeling method are effective configurations for dialogue speech recognition in emergency call centers. It should be noted that this research was conducted with two different types of datasets collected from emergency calls: the Dialogue dataset (27 h), which encapsulates call agents’ speech, and the Summary dataset (53 h), which contains voiced summaries of those dialogues describing emergency cases. Even though the speech taken from the emergency call center is in the Azerbaijani language, which belongs to the Turkic group of languages, our approaches are not tightly connected to specific language features. Hence, it is anticipated that suggested approaches can be applied to the other languages of the same group.},
	language = {en},
	number = {4},
	urldate = {2022-11-17},
	journal = {Symmetry},
	author = {Valizada, Alakbar and Akhundova, Natavan and Rustamov, Samir},
	month = apr,
	year = {2021},
	pages = {634},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\RWD8YXGN\\Valizada et al. - 2021 - Development of Speech Recognition Systems in Emerg.pdf:application/pdf},
}

@article{bernstein_recognizing_2000,
	title = {Recognizing {Call}-{Center} {Speech} {Using} {Models} {Trained} {From} {Other} {Domains}},
	abstract = {In this paper, we introduce a new conversational speech task -- recognizing call-center speech -- using data collected from Dragon's own technical support line. We compare performance of models trained from conversational telephone speech (the Switchboard corpus) and models trained from predominantly read, microphone speech, and report on a series of experiments focusing on adapting the microphone speech models to the telephone channel and conversational task. We also discuss the importance of task-specific language model data. We benchmark our test set by comparing the performance of our 1998 Switchboard Evaluation system to that of our simpler call-center system. 1. INTRODUCTION In this paper we investigate what happens when we take models trained for other tasks/domains and apply them to a new task for which we have no transcribed data: recognition of telephone calls to Dragon Systems' technical support line. The goal of the study was not to produce a highly optimized multi-pass s...},
	author = {Bernstein, Erica and McAllaster, Don and Gillick, Larry and Peskin, Barbara},
	month = nov,
	year = {2000},
}

@article{nollenburg_visual_2022,
	title = {Visual {Business} {Analytics}: {Using} the {Example} of a {Call} {Center}},
	volume = {8},
	issn = {18495664, 18495419},
	shorttitle = {Visual {Business} {Analytics}},
	url = {https://researchleap.com/visual-business-analytics-using-the-example-of-a-call-center/},
	doi = {10.18775/ijmsba.1849-5664-5419.2014.84.1001},
	abstract = {In this article we will examine an approach to the analysis of semi-structured log data using the example of a call center as a subsection of a central corporate service center. In such data all events of a caller passing through the routing are stored. Consequently, it is possible to trace more precisely what the customer experiences during his call. However, this information is only available in semi-structured form. A little-known approach in Anglo-Saxon literature, the Visual Business Analytics (VBA), represents a holistic concept for achieving added value from semi-structured data. In the VBA, data is initially transformed and structured and then prepared for analysis purposes. The goal is to derive recommendations for supporting management decisions in a call center. In the further course, the development of the approaches of information representation is examined first, then the VBA is presented and applied to the log protocols in a call center. Finally, other call center applications of VBA are considered, and an outlook is given on other industries in which the use of VBA offers advantages.},
	number = {4},
	urldate = {2022-11-17},
	journal = {THE INTERNATIONAL JOURNAL OF MANAGEMENT SCIENCE AND BUSINESS ADMINISTRATION},
	author = {Nollenburg, Pascal-Philipp and Dill, Arthur},
	month = may,
	year = {2022},
	pages = {7--16},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\L9CKJ7CH\\Nollenburg and Dill - 2022 - Visual Business Analytics Using the Example of a .pdf:application/pdf},
}

@article{aygen_zetter_systematic_2021,
	title = {A {SYSTEMATIC} {REVIEW} {OF} {THESES} {ON} “{CALL} {CENTER}” {IN} {TURKEY} {BASED} {ON} {CALL} {CENTER} {COMPONENTS}: 2003-2020},
	volume = {6},
	issn = {27177386},
	shorttitle = {A {SYSTEMATIC} {REVIEW} {OF} {THESES} {ON} “{CALL} {CENTER}” {IN} {TURKEY} {BASED} {ON} {CALL} {CENTER} {COMPONENTS}},
	url = {https://www.pearsonjournal.com/DergiTamDetay.aspx?ID=354&Detay=Ozet},
	doi = {10.46872/pj.354},
	abstract = {In this study, it is aimed to systematically examine the theses on the call center. On the subject the thesis archive of the Council of Higher Education was searched using the keyword "Call Center". As a result of the search, 224 studies were reached, however, a total of 206 studies that met the inclusion criteria were reviewed. It was determined that 120 of the scanned articles were written by the Social Sciences Institute, 28 were written by Marmara University, and 180 were postgraduate theses. When the subjects of the examined studies were classi-fied in terms of call center components, it was observed that 114 postgraduate theses were related to the human factor. In addition, it was found that strategy, process and technology-related subjects were among the research topics in the call center components. As a result, other disciplines as well as social sciences should show interest equally in the subject of call center, which should be addressed by many disciplines, in terms of contributing to the development of call centers. It is thought that it is possible to improve the system by focusing on the studies that examine the "strategy" and "technology" factor, as well as the employee and customer-oriented studies investigating the "human" factor from the call center components.},
	number = {15},
	urldate = {2022-11-17},
	journal = {IEDSR Association},
	author = {Aygen Zetter, Selin and Bi̇Li̇Şli̇, Yasemin},
	month = sep,
	year = {2021},
	pages = {246--260},
}

@article{tovar_rethinking_2022,
	title = {Rethinking call centers: {From} stigma to productive experience},
	volume = {16},
	issn = {1750-8657, 1750-8649},
	shorttitle = {Rethinking call centers},
	url = {https://journal.equinoxpub.com/SS/article/view/21181},
	doi = {10.1558/sols.42282},
	abstract = {Call centers have been critiqued in academia and the media for widespread standardization. This paper argues that although this critique of working conditions is well-intended, it has led to unwanted stigmatization of not just call center work but also of call center agents. Much has been published on call centers, but the stigma this work entails and the effect this has on agents on and off the phone has been overlooked. This paper applies Goffman’s notion of stigma to data collected through long-term ethnography and interviews with over seventy call center agents in a London call center. I show how agents experience, manage, and resist stigma. The analysis reveals that agents attempt to hide where they work by adopting different accents and avoiding specific lexis associated with call center language. I conclude by suggesting potential avenues for reducing the stigma of working in a call center, e.g. shifting the dominant discussion in academia beyond debates surrounding standardization.},
	number = {1},
	urldate = {2022-11-17},
	journal = {Sociolinguistic Studies},
	author = {Tovar, Johanna},
	month = apr,
	year = {2022},
}

@book{herzog_callcenter_2017,
	address = {Wiesbaden},
	title = {Callcenter – {Analyse} und {Management}},
	isbn = {978-3-658-18308-0 978-3-658-18309-7},
	url = {http://link.springer.com/10.1007/978-3-658-18309-7},
	language = {de},
	urldate = {2022-11-17},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Herzog, Alexander},
	year = {2017},
	doi = {10.1007/978-3-658-18309-7},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\J6LI87LS\\Herzog - 2017 - Callcenter – Analyse und Management.pdf:application/pdf},
}

@incollection{glas_einzelhandel_2017,
	address = {Berlin, Heidelberg},
	title = {Einzelhandel in {Läden} – {Ein} {Auslaufmodell}?},
	isbn = {978-3-662-53331-4 978-3-662-53332-1},
	url = {http://link.springer.com/10.1007/978-3-662-53332-1_2},
	language = {de},
	urldate = {2022-11-17},
	booktitle = {Handel 4.0},
	publisher = {Springer Berlin Heidelberg},
	author = {Jahn, Manuel},
	editor = {Gläß, Rainer and Leukert, Bernd},
	year = {2017},
	doi = {10.1007/978-3-662-53332-1_2},
	pages = {25--50},
}

@misc{commerzbank_commerzbank_2021,
	title = {Commerzbank beschließt neue {Strategie} bis 2024, {Pressemitteilung}, {Frankfurt} am {Main}: {Group} {Communications} (2021).},
	url = {https://www.commerzbank.de/de/hauptnavigation/presse/pressemitteilungen/archiv1/2021/1__quartal_1/presse_archiv_detail_21_01_93834.htm},
	author = {Commerzbank},
	month = feb,
	year = {2021},
}

@incollection{fasel_digitalisierung_2016,
	address = {Wiesbaden},
	title = {Die {Digitalisierung} als {Herausforderung} für {Unternehmen}: {Status} {Quo}, {Chancen} und {Herausforderungen} im {Umfeld} {BI} \& {Big} {Data}},
	isbn = {978-3-658-11588-3 978-3-658-11589-0},
	shorttitle = {Die {Digitalisierung} als {Herausforderung} für {Unternehmen}},
	url = {http://link.springer.com/10.1007/978-3-658-11589-0_3},
	language = {de},
	urldate = {2022-11-17},
	booktitle = {Big {Data}},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Seufert, Andreas},
	editor = {Fasel, Daniel and Meier, Andreas},
	year = {2016},
	doi = {10.1007/978-3-658-11589-0_3},
	note = {Series Title: Edition HMD},
	pages = {39--57},
}

@book{kopparapu_non-linguistic_2015,
	address = {Cham},
	series = {{SpringerBriefs} in {Electrical} and {Computer} {Engineering}},
	title = {Non-{Linguistic} {Analysis} of {Call} {Center} {Conversations}},
	isbn = {978-3-319-00896-7 978-3-319-00897-4},
	url = {http://link.springer.com/10.1007/978-3-319-00897-4},
	urldate = {2022-11-17},
	publisher = {Springer International Publishing},
	author = {Kopparapu, Sunil Kumar},
	year = {2015},
	doi = {10.1007/978-3-319-00897-4},
}

@article{benzeghiba_automatic_2007-1,
	title = {Automatic speech recognition and speech variability: {A} review},
	volume = {49},
	issn = {01676393},
	shorttitle = {Automatic speech recognition and speech variability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000404},
	doi = {10.1016/j.specom.2007.02.006},
	language = {en},
	number = {10-11},
	urldate = {2022-11-19},
	journal = {Speech Communication},
	author = {Benzeghiba, M. and De Mori, R. and Deroo, O. and Dupont, S. and Erbes, T. and Jouvet, D. and Fissore, L. and Laface, P. and Mertins, A. and Ris, C. and Rose, R. and Tyagi, V. and Wellekens, C.},
	month = oct,
	year = {2007},
	pages = {763--786},
}

@article{chang_preliminary_2022,
	title = {A {Preliminary} {Study} of {Robust} {Speech} {Feature} {Extraction} {Based} on {Maximizing} the {Probability} of {States} in {Deep} {Acoustic} {Models}},
	volume = {5},
	issn = {2571-5577},
	url = {https://www.mdpi.com/2571-5577/5/4/71},
	doi = {10.3390/asi5040071},
	abstract = {This study proposes a novel robust speech feature extraction technique to improve speech recognition performance in noisy environments. This novel method exploits the information provided by the original acoustic model in the automatic speech recognition (ASR) system to learn a deep neural network that converts the original speech features. This deep neural network is trained to maximize the posterior accuracy of the state sequences of acoustic models with respect to the speech feature sequences. Compared with the robustness methods that retrain or adapt acoustic models, the new method has the advantages of less computation load and faster training. In the experiments conducted on the medium-vocabulary TIMIT database and task, the presented method provides lower word error rates than the unprocessed baseline and speech-enhancement-based techniques. These results indicate that the presented method is promising and worth further developing.},
	language = {en},
	number = {4},
	urldate = {2022-11-19},
	journal = {Applied System Innovation},
	author = {Chang, Li-Chia and Hung, Jeih-Weih},
	month = jul,
	year = {2022},
	pages = {71},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\NPV2JZ6E\\Chang and Hung - 2022 - A Preliminary Study of Robust Speech Feature Extra.pdf:application/pdf},
}

@article{sinha_acoustic-phonetic_2015,
	title = {Acoustic-{Phonetic} {Feature} {Based} {Dialect} {Identification} in {Hindi} {Speech}},
	volume = {8},
	issn = {1178-5608},
	url = {https://www.sciendo.com/article/10.21307/ijssis-2017-757},
	doi = {10.21307/ijssis-2017-757},
	abstract = {Abstract
            Every individual has some unique speaking style and this variation influences their speech characteristics. Speakers’ native dialect is one of the major factors influencing their speech characteristics that influence the performance of automatic speech recognition system (ASR). In this paper, we describe a method to identify Hindi dialects and examine the contribution of different acoustic-phonetic features for the purpose. Mel frequency cepstral coefficients (MFCC), Perceptual linear prediction coefficients (PLP) and PLP derived from Mel-scale filter bank (MF- PLP) have been extracted as spectral features from the spoken utterances. They are further used to measure the capability of Auto-associative neural networks (AANN) for capturing non-linear relation specific to information from spectral features. Prosodic features are for capturing long - range features. Based on these features efficiency of AANN is measured to model intrinsic characteristics of speech features due to dialects.},
	language = {en},
	number = {1},
	urldate = {2022-11-19},
	journal = {International Journal on Smart Sensing and Intelligent Systems},
	author = {Sinha, Shweta and Jain, Aruna and Agrawal, S. S.},
	month = jan,
	year = {2015},
	pages = {235--254},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9EHTGM7N\\Sinha et al. - 2015 - Acoustic-Phonetic Feature Based Dialect Identifica.pdf:application/pdf},
}

@inproceedings{koo_exploiting_2020-1,
	title = {Exploiting {Multi}-{Modal} {Features} from {Pre}-{Trained} {Networks} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/koo20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3153},
	language = {en},
	urldate = {2022-11-19},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Koo, Junghyun and Lee, Jie Hwan and Pyo, Jaewoo and Jo, Yujin and Lee, Kyogu},
	month = oct,
	year = {2020},
	pages = {2217--2221},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\9RFH3BIS\\Koo et al. - 2020 - Exploiting Multi-Modal Features from Pre-Trained N.pdf:application/pdf},
}

@inproceedings{balagopalan_impact_2020-1,
	address = {Online},
	title = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}: {All} {Errors} are {Equal}, but {Deletions} are {More} {Equal} than {Others}},
	shorttitle = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}},
	url = {https://www.aclweb.org/anthology/2020.wnut-1.21},
	doi = {10.18653/v1/2020.wnut-1.21},
	language = {en},
	urldate = {2022-11-19},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Noisy} {User}-generated {Text} ({W}-{NUT} 2020)},
	publisher = {Association for Computational Linguistics},
	author = {Balagopalan, Aparna and Shkaruta, Ksenia and Novikova, Jekaterina},
	year = {2020},
	pages = {159--164},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\IB23EMTY\\Balagopalan et al. - 2020 - Impact of ASR on Alzheimer’s Disease Detection Al.pdf:application/pdf},
}

@misc{koo_exploiting_2021,
	title = {Exploiting {Multi}-{Modal} {Features} {From} {Pre}-trained {Networks} for {Alzheimer}'s {Dementia} {Recognition}},
	url = {http://arxiv.org/abs/2009.04070},
	abstract = {Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient's medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer's Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer's Dementia by providing acoustic and textual data. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. Our test results surpass baseline's accuracy by 18.75\%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70\%.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Koo, Junghyun and Lee, Jie Hwan and Pyo, Jaewoo and Jo, Yujin and Lee, Kyogu},
	month = mar,
	year = {2021},
	note = {arXiv:2009.04070 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Machine Learning},
	annote = {Comment: In the Proceedings of INTERSPEECH 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\QY98I8C8\\Koo et al. - 2021 - Exploiting Multi-Modal Features From Pre-trained N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\UYAE7LTN\\2009.html:text/html},
}

@inproceedings{li_comparative_2021-1,
	address = {Toronto, ON, Canada},
	title = {A {Comparative} {Study} of {Acoustic} and {Linguistic} {Features} {Classification} for {Alzheimer}'s {Disease} {Detection}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414147/},
	doi = {10.1109/ICASSP39728.2021.9414147},
	urldate = {2022-11-19},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	pages = {6423--6427},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\XNGARJH9\\Li et al. - 2021 - A Comparative Study of Acoustic and Linguistic Fea.pdf:application/pdf},
}

@misc{pompili_inesc-id_2020,
	title = {The {INESC}-{ID} {Multi}-{Modal} {System} for the {ADReSS} 2020 {Challenge}},
	url = {http://arxiv.org/abs/2005.14646},
	abstract = {This paper describes a multi-modal approach for the automatic detection of Alzheimer's disease proposed in the context of the INESC-ID Human Language Technology Laboratory participation in the ADReSS 2020 challenge. Our classification framework takes advantage of both acoustic and textual feature embeddings, which are extracted independently and later combined. Speech signals are encoded into acoustic features using DNN speaker embeddings extracted from pre-trained models. For textual input, contextual embedding vectors are first extracted using an English Bert model and then used either to directly compute sentence embeddings or to feed a bidirectional LSTM-RNNs with attention. Finally, an SVM classifier with linear kernel is used for the individual evaluation of the three systems. Our best system, based on the combination of linguistic and acoustic information, attained a classification accuracy of 81.25\%. Results have shown the importance of linguistic features in the classification of Alzheimer's Disease, which outperforms the acoustic ones in terms of accuracy. Early stage features fusion did not provide additional improvements, confirming that the discriminant ability conveyed by speech in this case is smooth out by linguistic data.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Pompili, Anna and Rolland, Thomas and Abad, Alberto},
	month = may,
	year = {2020},
	note = {arXiv:2005.14646 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 5 pages, 1 figure. Submitted to INTERSPEECH2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\JFYDTKI9\\Pompili et al. - 2020 - The INESC-ID Multi-Modal System for the ADReSS 202.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\MTUUKIBC\\2005.html:text/html},
}

@inproceedings{cummins_comparison_2020-1,
	title = {A {Comparison} of {Acoustic} and {Linguistics} {Methodologies} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/cummins20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2635},
	language = {en},
	urldate = {2022-11-19},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Cummins, Nicholas and Pan, Yilin and Ren, Zhao and Fritsch, Julian and Nallanthighal, Venkata Srikanth and Christensen, Heidi and Blackburn, Daniel and Schuller, Björn W. and Magimai-Doss, Mathew and Strik, Helmer and Härmä, Aki},
	month = oct,
	year = {2020},
	pages = {2182--2186},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\G885GZTY\\Cummins et al. - 2020 - A Comparison of Acoustic and Linguistics Methodolo.pdf:application/pdf},
}

@inproceedings{yuan_pause-encoded_2021-1,
	address = {Toronto, ON, Canada},
	title = {Pause-{Encoded} {Language} {Models} for {Recognition} of {Alzheimer}’s {Disease} and {Emotion}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413548/},
	doi = {10.1109/ICASSP39728.2021.9413548},
	urldate = {2022-11-19},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Yuan, Jiahong and Cai, Xingyu and Church, Kenneth},
	month = jun,
	year = {2021},
	pages = {7293--7297},
}

@misc{noauthor_ad2021_2022-1,
	title = {{AD2021}: {Alzheimer}'s {Disease} {Recognition} {Evaluation} 2021},
	copyright = {Apache-2.0},
	shorttitle = {{AD2021}},
	url = {https://github.com/THUsatlab/AD2021},
	abstract = {Alzheimer's Disease Recognition Evaluation 2021},
	urldate = {2022-11-20},
	publisher = {THUsatlab},
	month = nov,
	year = {2022},
	note = {original-date: 2021-07-08T04:10:11Z},
}

@article{abushariah_modern_2012,
	title = {Modern standard {Arabic} speech corpus for implementing and evaluating automatic continuous speech recognition systems},
	volume = {349},
	issn = {00160032},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016003211001013},
	doi = {10.1016/j.jfranklin.2011.04.011},
	language = {en},
	number = {7},
	urldate = {2022-11-20},
	journal = {Journal of the Franklin Institute},
	author = {Abushariah, Mohammad Abd-Alrahman Mahmoud and Ainon, Raja Noor and Zainuddin, Roziati and Alqudah, Assal Ali Mustafa and Elshafei Ahmed, Moustafa and Khalifa, Othman Omran},
	month = sep,
	year = {2012},
	pages = {2215--2242},
}

@article{abushariah_arabic_2012,
	title = {Arabic {Speaker}-{Independent} {Continuous} {Automatic} {Speech} {Recognition} {Based} on a {Phonetically} {Rich} and {Balanced} {Speech} {Corpus}},
	volume = {9},
	abstract = {This paper describes and proposes an efficient and effective framework for the design and development of a speaker-independent continuous automatic Arabic speech recognition system based on a phonetically rich and balanced speech corpus. The speech corpus contains a total of 415 sentences recorded by 40 (20 male and 20 female) Arabic native speakers from 11 different Arab countries representing the three major regions (Levant, Gulf, and Africa) in the Arab world. The proposed Arabic speech recognition system is based on the Carnegie Mellon University (CMU) Sphinx tools, and the Cambridge HTK tools were also used at some testing stages. The speech engine uses 3-emitting state Hidden Markov Models (HMM) for tri-phone based acoustic models. Based on experimental analysis of about 7 hours of training speech data, the acoustic model is best using continuous observation's probability model of 16 Gaussian mixture distributions and the state distributions were tied to 500 senones. The language model contains both bi-grams and tri-grams. For similar speakers but different sentences, the system obtained a word recognition accuracy of 92.67\% and 93.88\% and a Word Error Rate (WER) of 11.27\% and 10.07\% with and without diacritical marks respectively. For different speakers with similar sentences, the system obtained a word recognition accuracy of 95.92\% and 96.29\% and a WER of 5.78\% and 5.45\% with and without diacritical marks respectively. Whereas different speakers and different sentences, the system obtained a word recognition accuracy of 89.08\% and 90.23\% and a WER of 15.59\% and 14.44\% with and without diacritical marks respectively.},
	journal = {Int. Arab J. Inf. Technol.},
	author = {Abushariah, Mohammad and Ainon, Raja and Zainuddin, Roziati and Elshafei, Moustafa and Khalifa, Othman},
	month = jan,
	year = {2012},
	pages = {84--93},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\HLDTCDBU\\Abushariah et al. - 2012 - Arabic Speaker-Independent Continuous Automatic Sp.pdf:application/pdf},
}

@inproceedings{abushariah_phonetically_2010,
	address = {Kuala Lumpur, Malaysia},
	title = {Phonetically rich and balanced speech corpus for {Arabic} speaker-independent continuous automatic speech recognition systems},
	isbn = {978-1-4244-7165-2},
	url = {http://ieeexplore.ieee.org/document/5605554/},
	doi = {10.1109/ISSPA.2010.5605554},
	urldate = {2022-11-20},
	booktitle = {10th {International} {Conference} on {Information} {Science}, {Signal} {Processing} and their {Applications} ({ISSPA} 2010)},
	publisher = {IEEE},
	author = {Abushariah, Mohammad A. M. and Ainon, Raja N. and Zainuddin, Roziati and Elshafei, Moustafa and Khalifa, Othman O.},
	month = may,
	year = {2010},
	pages = {65--68},
}

@article{park_study_1992,
	title = {A {Study} on the {Spoken} {KOrean}-{Digit} {Recognition} {Using} the {Neural} {Netwok}},
	volume = {11},
	abstract = {Taking devantage of the property that Korean digit is a mono-syllable word, we proposed a spoken Korean-digit recognition scheme using the multi-layer perceptron. The spoken Korean-digit is divided into three segments (initial sound, medial vowel, and final consonant) based on the voice starting / ending points and a peak point in the middle of vowel sound. The feature vectors such as cepstrum, reflection coefficients, cepstrum and energy are extracted from each segment. It has been shown that cepstrum, as an input vector to the neural network, gives higher recognition rate than reflection coefficients. Regression coefficients of cepstrum did not affect as much as we expected on the recognition rate. That is because, it is believed, we extracted features from the selected stationary segments of the input speech signal. With 150 ceptral coefficients obtained from each spoken digit, we achieved correct recognition rate of 97.8\%.},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Park, Hyun-Hwa and Gahang, Hae and Bae, Keun},
	month = jan,
	year = {1992},
}

@inproceedings{willett_discriminatively_2006,
	address = {Toulouse, France},
	title = {Discriminatively {Trained} {Context}-{Dependent} {Duration}-{Bigram} {Models} for {Korean} {Digit} {Recognition}},
	volume = {1},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1659948/},
	doi = {10.1109/ICASSP.2006.1659948},
	urldate = {2022-11-20},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Willett, D. and Gerl, F. and Brueckner, R.},
	year = {2006},
	pages = {I--25--I--28},
}

@article{pfitzinger_local_2000,
	title = {Local {Speech} {Rate} {Perception} {In} {German} {Speech}},
	abstract = {A model for deriving perceived local speech rate directly out of the speech signal is developed based on perception experiments. Since local speech rate modifies acoustic cues (e.g. transitions and voice-onset time [5]), phones [1, 4, 7, 19], syllables [3, 15], and even words, it is one of the most important prosodic cues. Our local speech rate estimation method is based on a linear combination of the local syllable rate and the local phone rate, since earlier investigations [11, 12] strongly suggest that neither the syllable rate nor the phone rate on its own represent the speech rate sufficiently. In the literature [6] effects of F0 level and F0 movement on speech rate perception have been reported. Therefore we included these cues in our linear combination model. Our results show 1) that the duration of speech stimuli has a strong influence on the perception of speech rate, 2) that the linear combination of local syllable rate and phone rate is wellcorrelated with perceptual local...},
	author = {Pfitzinger, Hartmut},
	month = aug,
	year = {2000},
}

@article{aldholmi_perception_2016,
	title = {Perception of speech rate in speech rate perception},
	volume = {140},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4970623},
	doi = {10.1121/1.4970623},
	language = {en},
	number = {4},
	urldate = {2022-11-20},
	journal = {The Journal of the Acoustical Society of America},
	author = {Aldholmi, Yahya and Park, Hanyong},
	month = oct,
	year = {2016},
	pages = {3333--3333},
}

@book{matejka_phonotactic_2005,
	title = {Phonotactic language identification using high quality phoneme recognition},
	abstract = {Phoneme Recognizers followed by Language Modeling (PRLM) have consistently yielded top performance in language identification (LID) task. Parallel ordering of PRLMs (PPRLM) improves performance even more. Since tokenizer is the most important part of LID system the high quality phoneme rec-ognizer is employed. Two different multilingual databases for training phoneme recognizers are compared and the amount of sufficient training data is studied. Reported results are on data from NIST 2003 LID evaluation. Our four PRLM systems have Equal Error Rate (EER) of 2.4\% on 12 languages task. This re-sult compares favorably to the best known result from this task.},
	author = {Matejka, Pavel and Schwarz, Petr and Cernocký, Jan and Chytil, Pavel},
	month = jan,
	year = {2005},
	note = {Pages: 2240},
}

@inproceedings{liang_wang_multi-lingual_2006,
	address = {Hong Kong, China},
	title = {Multi-lingual {Phoneme} {Recognition} and {Language} {Identification} {Using} {Phonotactic} {Information}},
	isbn = {978-0-7695-2521-1},
	url = {http://ieeexplore.ieee.org/document/1699826/},
	doi = {10.1109/ICPR.2006.823},
	urldate = {2022-11-20},
	booktitle = {18th {International} {Conference} on {Pattern} {Recognition} ({ICPR}'06)},
	publisher = {IEEE},
	author = {{Liang Wang} and Ambikairajah, E. and Choi, E.H.C.},
	year = {2006},
	pages = {245--248},
}

@inproceedings{adell_database_2006,
	address = {Toulouse, France},
	title = {Database {Pruning} for {Unsupervised} {Building} of {Text}-{To}-{Speech} {Voices}},
	volume = {1},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1660164/},
	doi = {10.1109/ICASSP.2006.1660164},
	urldate = {2022-11-20},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Adell, J. and Aguero, P.D. and Bonafonte, A.},
	year = {2006},
	pages = {I--889--I--892},
}

@article{ramig_effects_1983,
	title = {Effects of {Physiological} {Aging} on {Selected} {Acoustic} {Characteristics} of {Voice}},
	volume = {26},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.2601.22},
	doi = {10.1044/jshr.2601.22},
	abstract = {The relationship between age-related changes in body physiology and certain acoustic characteristics of voice was studied in a sample of 48 men representing three chronological age grouping (25–35, 45–55 and 65–75) and two levels of physical condition (good and poor). A fundamental frequency analysis program (SEARP) was used to measure mean fundamental frequency, jitter, shimmer, and phonation range from samples of connected speech and sustained vowel production. Subjects in good physical condition produced maximum duration vowel phonation with significantly less jitter and shimmer and had larger phonation ranges than did subjects of similar chronological ages who were in poor physical condition. These differences were most apparent in the production of the elderly subjects. While chronological aging is undoubtedly a contributor to such changes in the acoustic characteristics of voice, these results suggest that age-related changes in body physiology, or physiological aging, also must be considered.},
	language = {en},
	number = {1},
	urldate = {2022-11-20},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Ramig, Lorraine A. and Ringel, Robert L.},
	month = mar,
	year = {1983},
	pages = {22--30},
}

@book{bruckl_aging_2003,
	title = {Aging female voices: {An} acoustic and perceptive analysis},
	shorttitle = {Aging female voices},
	abstract = {This study examines the changes in adult female voices due to the aging process. Acoustic cues in voices that enable listeners to recognize a speaker's vocal age are specified as well as acoustic cues that straightly indicate the speaker's chronological age. The analysed data are recordings of the voices of 56 female speakers differing in age. The recorded speech samples include sustained vowels, read speech and spontaneous speech. Our methods are acoustic analyses and perception tests. The perception tests are designed to analyse the influence of the vowel onset on the amount of information about aging transferred by sustained vowels. The acoustic evaluation comprises phonic parameters like measurements of stability of voice or of vocal tremor which are supposed to reflect voice qualities that are expected to vary with age. Changes in tempo of articulation are also investigated. We found that increasing amplitude perturbation is an indicator of increasing age even on the basis of spontaneous speech. Reading rate decreases with increasing age, whereas there is no significant change in articulation rate of spontaneous speech in women's voices. Based on sustained vowels of female voices, the frequency tremor intensity index indicates age more accurately than F0 and amplitude perturbations. We also found ev-idence for the relevance of the vowel onset to recognize age more accurately.},
	author = {Brückl, Markus and Sendlmeier, Walter},
	month = aug,
	year = {2003},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\USHRR5AA\\Brückl and Sendlmeier - 2003 - Aging female voices An acoustic and perceptive an.pdf:application/pdf},
}

@book{winkler_aging_2003,
	title = {The aging voice: an acoustic, electroglottographic and perceptive analysis of male and female voices},
	shorttitle = {The aging voice},
	abstract = {This study examines the differences between young and old adult voices. Acoustic cues in voices that enable listeners to recognize a speaker's vocal age are specified as well as acoustic cues that straightly indicate the speaker's chronological age. Electroglottographic data were used to directly examine glottal behaviour in aging voices. We found a strong spectral attenuation of high frequencies in aging male voices assumed to result from rather sinusoidal glottal excitation. Increasing amplitude perturbation is an indicator of increasing age even on the basis of spontaneous speech. Reading rate decreases with increasing age, whereas there is no significant change in articulation rate of spontaneous speech in women's voices. Based on sustained vowels of female voices, the frequency tremor intensity index indicates age more accurately than F0 and amplitude perturbations. We also found evidence for the relevance of the vowel onset to recognize age more accurately.},
	author = {Winkler, Ralf and Brückl, Markus and Sendlmeier, Walter},
	month = jan,
	year = {2003},
	note = {Journal Abbreviation: Proceedings of The IEEE - PIEEE
Publication Title: Proceedings of The IEEE - PIEEE},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\ZH8D4LWQ\\Winkler et al. - 2003 - The aging voice an acoustic, electroglottographic.pdf:application/pdf},
}

@inproceedings{vaseghi_speech_2006,
	address = {Toulouse, France},
	title = {Speech {Bandwidth} {Extension}: {Extrapolations} of {Spectral} {Envelop} and {Harmonicity} {Quality} of {Excitation}},
	volume = {3},
	isbn = {978-1-4244-0469-8},
	shorttitle = {Speech {Bandwidth} {Extension}},
	url = {http://ieeexplore.ieee.org/document/1660786/},
	doi = {10.1109/ICASSP.2006.1660786},
	urldate = {2022-11-20},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Vaseghi, S. and Zavarehei, E. and {Qin Yan}},
	year = {2006},
	pages = {III--844--III--847},
}

@inproceedings{srinivasan_harmonicity_2004,
	address = {Montreal, Que., Canada},
	title = {Harmonicity and dynamics-based features for audio},
	volume = {4},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1326828/},
	doi = {10.1109/ICASSP.2004.1326828},
	urldate = {2022-11-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Srinivasan, H. and Kankanhalli, M.},
	year = {2004},
	pages = {iv--321--iv--324},
}

@inproceedings{srinivasan_harmonicity_2003,
	address = {Hong Kong, China},
	title = {Harmonicity and dynamics based audio separation},
	volume = {5},
	isbn = {978-0-7803-7663-2},
	url = {http://ieeexplore.ieee.org/document/1200052/},
	doi = {10.1109/ICASSP.2003.1200052},
	urldate = {2022-11-20},
	booktitle = {2003 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}, 2003. {Proceedings}. ({ICASSP} '03).},
	publisher = {IEEE},
	author = {Srinivasan, S.H. and Kankanhalli, M.},
	year = {2003},
	pages = {V--640--3},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AR44HZB7\\Srinivasan and Kankanhalli - 2003 - Harmonicity and dynamics based audio separation.pdf:application/pdf},
}

@misc{noauthor_unilever_nodate,
	title = {Unilever {Pakistan} {Homepage}},
	url = {https://www.unilever.pk/undefined},
	abstract = {At Unilever we meet everyday needs for nutrition, hygiene and personal care with brands that help people feel good, look good and get more out of life.},
	language = {en-GB},
	urldate = {2022-11-22},
	journal = {Unilever},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\3YUJBJNC\\www.unilever.pk.html:text/html},
}

@techreport{forrester_human_2017,
	title = {Human vs. {Machines}: {How} to stop your virtual agent from lagging behind},
	url = {https://telecoms.com/wp-content/blogs.dir/1/files/2017/10/Amdocs-TLP-Virtual-Agents.pdf},
	urldate = {2022-02-12},
	institution = {AMDOCS},
	author = {Forrester},
	year = {2017},
	note = {www.forrester.com},
}

@inproceedings{vasilescu_perceptual_2009,
	title = {A perceptual investigation of speech transcription errors involving frequent near-homophones in {French} and american {English}},
	url = {https://www.isca-speech.org/archive/interspeech_2009/vasilescu09_interspeech.html},
	doi = {10.21437/Interspeech.2009-53},
	language = {en},
	urldate = {2022-12-02},
	booktitle = {Interspeech 2009},
	publisher = {ISCA},
	author = {Vasilescu, Ioana and Adda-Decker, Martine and Lamel, Lori and Hallé, Pierre},
	month = sep,
	year = {2009},
	pages = {144--147},
}

@incollection{ide_corpus-based_2000,
	address = {Dordrecht},
	title = {A {Corpus}-{Based} {Approach} to the {Study} of {Speaking} {Style}},
	volume = {14},
	isbn = {978-90-481-5562-0 978-94-015-9413-4},
	url = {http://link.springer.com/10.1007/978-94-015-9413-4_12},
	urldate = {2022-12-02},
	booktitle = {Prosody: {Theory} and {Experiment}},
	publisher = {Springer Netherlands},
	author = {Hirschberg, Julia},
	editor = {Ide, Nancy and Véronis, Jean and Horne, Merle},
	year = {2000},
	doi = {10.1007/978-94-015-9413-4_12},
	note = {Series Title: Text, Speech and Language Technology},
	pages = {335--350},
}

@article{ermakova_corpus-based_2018,
	title = {Corpus-based research of spoken language: {The} state-of-the-art for {Czech} and {English}},
	volume = {79},
	shorttitle = {Corpus-based research of spoken language},
	abstract = {The article aims to review corpus-based research on spoken language, emphasizing issues in description and conceptualization of the grammar of spoken language in relation to the grammar of written language. The review first briefy looks at the development of spoken corpora, from simply transcribed corpora without sound alignment to today's sophisticated multi-modal corpora. The main part of the article deals with issues concerning the metalanguage for the description of spoken language, the choice of its basic descriptive unit, the status of basic linguistic categories such as part-of-speech, and typical lexical and grammatical devices. The existing extensive research on spoken English is reviewed and in line with it, illustrative examples based on Czech spoken corpora are provided. These are further contrasted with examples from written data to enhance the inherent differences between spoken and written language and the need to adjust the metalanguage of the description. © 2018 Czech Language Institute of the Academy of Sciences. All rights reserved.},
	journal = {Slovo a Slovesnost},
	author = {Ermáková, A. and Kopřivova, Marie},
	month = jan,
	year = {2018},
	pages = {217--240},
}

@article{servan_corpus-based_2008,
	title = {Corpus-based spoken language understanding for mixed initiative spoken dialog systems},
	abstract = {Spoken dialogues systems are interfaces between users and services. Simple examples of services for which theses dialogue systems can be used include : banking, booking (hotels, trains, flights), etc. Dialogue systems are composed of a number of modules. The main modules include Automatic Speech Recognition (ASR), Spoken Language Understanding (SLU), Dialogue Management and Speech Generation. In this thesis, we concentrate on the Spoken Language Understanding component of dialogue systems. In the past, it has usual to separate the Spoken Language Understanding process from that of Automatic Speech Recognition. First, the Automatic Speech Recognition process finds the best word hypothesis. Given this hypothesis, we then find the best semantic interpretation. This thesis presents a method for the robust extraction of basic conceptual constituents (or concepts) from an audio message. The conceptual decoding model proposed follows a stochastic paradigm and is directly integrated into the Automatic Speech Recognition process. This approach allows us to keep the probabilistic search space on sequences of words produced by the Automatic Speech Recognition module, and to project it to a probabilistic search space of sequences of concepts. The experiments carried out on the French spoken dialogue corpus MEDIA, available through ELDA, show that the performance reached by our new approach is better than the traditional sequential approach. As a starting point for evaluation, the effect that deterioration of word error rate (WER) has on SLU systems is examined though use of different ASR outputs. The SLU performance appears to decrease lineary as a function of ASR word error rate.We show, however, that the proposed integrated method of searching for both words and concets, gives better results to that of a traditionnanl sequential approach. In order to validate our approach, we conduct experiments on the MEDIA corpus in the same assessment conditions used during the MEDIA campaign. The goal is toproduce error-free semantic interpretations from transcripts. The results show that the performance achieved by our model is as good as the systems involved in the evaluation campaign. Studies made on the MEDIA corpus show the concept error rate is related to the word error rate, the size of the training corpus and a priori knwoledge added to conceptual model languages. Error analyses show the interest of modifying the probabilities of word lattice with triggers, a template cache or by using arbitrary rules requiring passage through a portion of the graph and applying the presence of triggers (words or concepts) based on history. Methods based on machine learning are generally quite demanding in terms of amount of training data required. By changing the size of the training corpus, the minimum and the optimal number of dialogues needed for training conceptual language models can be measured. Research conducted in this thesis aims to determine the size of corpus necessary for training conceptual language models from which the semantic evaluation scores stagnated. A correlation is established between the necessary corpus size for learning and the corpus size necessary to validate the manual annotations. In the case of the MEDIA evaluation campaign, it took roughly the same number of examples, first to validate the semantic annotations and, secondly, to obtain a "quality" corpus-trained stochastic model. The addition of a priori knowledge to our stochastic models reduce significantly the size of the training corpus needed to achieve the same scores as a fully stochastic system (nearly half the size for the same score). It allows us to confirm that the addition of basic intuitive rules (numbers, zip codes, dates) gives very encouraging results. It leeds us to create a hybrid system combining corpus-based and knowledge-based models. The second part of the thesis examines the application of the understanding module to another simple dialogue system task, a callrouting system. A problem with this specific task is a lack of data available for training the requiered language models. We attempt to resolve this issue by supplementing he in-domain data with various other generic corpora already available, and data from the MEDIA campaing. We show the benefits of integrating a call classification task in a SLU process. Unfortunately, we have very little training corpus in the field under consideration. By using our integrated approach to decode concepts, along with an integrated process, we propose a bag of words and concepts approach. This approach used by a classifier achieved encouraging call classification rates on the test corpus, while the WER was relativelyhigh. The methods developed are shown to improve the call routing system process robustness.},
	author = {Servan, Christophe},
	month = dec,
	year = {2008},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PQFJUAD3\\Servan - 2008 - Corpus-based spoken language understanding for mix.pdf:application/pdf},
}

@article{allwood_transliteration_2005,
	title = {Transliteration between spoken language corpora},
	volume = {28},
	issn = {0332-5865, 1502-4717},
	url = {https://www.cambridge.org/core/product/identifier/S0332586505001307/type/journal_article},
	doi = {10.1017/S0332586505001307},
	abstract = {Comparison of languages and linguistic data is essential if progress in our understanding of the nature of spoken languages is to be made. We understand phenomena better through comparison and contrast. This paper discusses problems that arise in trying to transfer a spoken language corpus transcribed and formatted according to one standard into the standard and format of another corpus. The problems that arise are related both to the differences that exist between the standards of the corpora and to human errors leading to lack of reliability in creating the transcriptions. Although the discussion is based on transfer and transliteration between two specific corpora (the Danish BySoc, BySociolingvistisk Korpus, and the Swedish GSLC, Göteborg Spoken Language Corpus), we believe that the discussion in the article documents and highlights problems of a general kind which have to be faced whenever spoken language corpora of different formats are to be compared.},
	language = {en},
	number = {1},
	urldate = {2022-12-02},
	journal = {Nordic Journal of Linguistics},
	author = {Allwood, Jens and Henrichsen, Peter Juel and Grönqvist, Leif and Ahlsén, Elisabeth and Gunnarsson, Magnus},
	month = jun,
	year = {2005},
	pages = {5--36},
}

@article{allwood_spoken_2000,
	title = {The {Spoken} {Language} {Corpus} at the {Department} of {Linguistics}, {Göteborg} {University}},
	volume = {1},
	abstract = {This paper summarizes work on spoken language at the Department of Linguistics Göteborg University. In addition to describing the recordings contained in the Spoken Language Corpus of Swedish at Göteborg University, we discuss the standard of transcription (MSO) which is used in creating the transcriptions, as well as some types of quantitative and qualitative analysis that have been done. Finally, we describe the computer tools that have been developed to support transcription, coding and analysis and briefly mention some of the results which have been obtained.
URN: urn:nbn:de:0114-fqs000391},
	journal = {Forum: Qualitative Social Research},
	author = {Allwood, Jens and Björnberg, Maria and Grönqvist, Leif and Ahlsen, Elisabeth and Ottesjö, Cajsa},
	month = dec,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5B67WBYS\\Allwood et al. - 2000 - The Spoken Language Corpus at the Department of Li.pdf:application/pdf},
}

@article{yanli_research_2021,
	title = {Research on {Spoken} {Language} {Understanding} {Based} on {Deep} {Learning}},
	volume = {2021},
	issn = {1875-919X, 1058-9244},
	url = {https://www.hindawi.com/journals/sp/2021/8900304/},
	doi = {10.1155/2021/8900304},
	abstract = {Aiming at solving the problem that the recognition effect of rare slot values in spoken language is poor, which affects the accuracy of oral understanding task, a spoken language understanding method is designed based on deep learning. The local features of semantic text are extracted and classified to make the classification results match the dialogue task. An intention recognition algorithm is designed for the classification results. Each datum has a corresponding intention label to complete the task of semantic slot filling. The attention mechanism is applied to the recognition of rare slot value information, the weight of hidden state and corresponding slot characteristics are obtained, and the updated slot value is used to represent the tracking state. An auxiliary gate unit is constructed between the upper and lower slots of historical dialogue, and the word vector is trained based on deep learning to complete the task of spoken language understanding. The simulation results show that the proposed method can realize multiple rounds of man-machine spoken language. Compared with the spoken language understanding methods based on cyclic network, context information, and label decomposition, it has higher accuracy and F1 value and has higher practical application value.},
	language = {en},
	urldate = {2022-12-02},
	journal = {Scientific Programming},
	author = {Yanli, Hui},
	editor = {Ding, Bai Yuan},
	month = oct,
	year = {2021},
	pages = {1--9},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3RPQEHS4\\Yanli - 2021 - Research on Spoken Language Understanding Based on.pdf:application/pdf},
}

@incollection{hutchison_multilingual_2006,
	address = {Berlin, Heidelberg},
	title = {Multilingual {Spoken} {Language} {Corpus} {Development} for {Communication} {Research}},
	volume = {4274},
	isbn = {978-3-540-49665-6 978-3-540-49666-3},
	url = {http://link.springer.com/10.1007/11939993_78},
	urldate = {2022-12-02},
	booktitle = {Chinese {Spoken} {Language} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Takezawa, Toshiyuki},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Huo, Qiang and Ma, Bin and Chng, Eng-Siong and Li, Haizhou},
	year = {2006},
	doi = {10.1007/11939993_78},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {781--791},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\EE4F747D\\Takezawa - 2006 - Multilingual Spoken Language Corpus Development fo.pdf:application/pdf},
}

@book{izreel_search_2020,
	address = {Amsterdam},
	series = {Studies in {Corpus} {Linguistics}},
	title = {In {Search} of {Basic} {Units} of {Spoken} {Language}: {A} corpus-driven approach},
	volume = {94},
	isbn = {978-90-272-0497-4 978-90-272-6153-3},
	shorttitle = {In {Search} of {Basic} {Units} of {Spoken} {Language}},
	url = {http://www.jbe-platform.com/content/books/9789027261533},
	language = {en},
	urldate = {2022-12-02},
	publisher = {John Benjamins Publishing Company},
	editor = {Izre'el, Shlomo and Mello, Heliana and Panunzi, Alessandro and Raso, Tommaso},
	month = jun,
	year = {2020},
	doi = {10.1075/scl.94},
}

@article{allwood_spoken_2000-1,
	title = {The {Spoken} {Language} {Corpus} at the {Linguistics} {Department}, {Göteborg} {University}},
	volume = {1},
	copyright = {Copyright (c) 2000 Jens Allwood, Leif Grönqvist, Maria Björnberg, Elisabeth Ahlsen, Cajsa Ottesjö},
	issn = {1438-5627},
	url = {https://www.qualitative-research.net/index.php/fqs/article/view/1026},
	doi = {10.17169/fqs-1.3.1026},
	abstract = {This paper summarizes work on spoken language at the Department of Linguistics Göteborg University. In addition to describing the recordings contained in the Spoken Language Corpus of Swedish at Göteborg University, we discuss the standard of transcription (MSO) which is used in creating the transcriptions, as well as some types of quantitative and qualitative analysis that have been done. Finally, we describe the computer tools that have been developed to support transcription, coding and analysis and briefly mention some of the results which have been obtained.
URN: urn:nbn:de:0114-fqs000391},
	language = {en},
	number = {3},
	urldate = {2022-12-02},
	journal = {Forum Qualitative Sozialforschung / Forum: Qualitative Social Research},
	author = {Allwood, Jens and Grönqvist, Leif and Björnberg, Maria and Ahlsen, Elisabeth and Ottesjö, Cajsa},
	month = dec,
	year = {2000},
	note = {Number: 3},
	keywords = {coding, computer tool, corpus, Göteborg, MSO, qualitative analysis, quantitative analysis, spoken language, transcription standard},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\GTJ6LQZ3\\Allwood et al. - 2000 - The Spoken Language Corpus at the Linguistics Depa.pdf:application/pdf},
}

@book{strazny_encyclopedia_2005,
	address = {New York},
	title = {Encyclopedia of linguistics},
	isbn = {978-1-57958-391-0 978-1-57958-450-4 978-1-57958-451-1},
	publisher = {Fitzroy Dearborn},
	editor = {Strazny, Philipp},
	year = {2005},
	keywords = {Encyclopedias, Linguistics},
	annote = {v. 1. A-L-- v. 2 M-Z},
}

@inproceedings{k_kirchhoff_processing_2007,
	title = {Processing {Morphologically}-{Rich} {Languages}},
	author = {K. Kirchhoff and R. Sarikay},
	year = {2007},
	note = {https://www.researchgate.net/publication/224506277\_Introduction\_to\_the\_Special\_Issue\_on\_Processing\_Morphologically\_Rich\_Languages},
}

@article{sarikaya_introduction_2009,
	title = {Introduction to the {Special} {Issue} on {Processing} {Morphologically} {Rich} {Languages}},
	volume = {17},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5075779/},
	doi = {10.1109/TASL.2009.2023314},
	number = {5},
	urldate = {2022-12-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Sarikaya, Ruhi and Kirchhoff, Katrin and Schultz, Tanja and Hakkani-Tur, Dilek},
	month = jul,
	year = {2009},
	pages = {861--862},
}

@article{mihajlik_improved_2010,
	title = {Improved {Recognition} of {Spontaneous} {Hungarian} {Speech}—{Morphological} and {Acoustic} {Modeling} {Techniques} for a {Less} {Resourced} {Task}},
	volume = {18},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5356225/},
	doi = {10.1109/TASL.2009.2038807},
	number = {6},
	urldate = {2022-12-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Mihajlik, P and Tuske, Z and Tarján, B and Németh, B and Fegyó, T},
	month = aug,
	year = {2010},
	pages = {1588--1600},
}

@article{arisoy_turkish_2009,
	title = {Turkish {Broadcast} {News} {Transcription} and {Retrieval}},
	volume = {17},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5071138/},
	doi = {10.1109/TASL.2008.2012313},
	number = {5},
	urldate = {2022-12-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Arisoy, Ebru and Can, Dogan and Parlak, Siddika and Sak, Hasim and Saraclar, Murat},
	month = jul,
	year = {2009},
	pages = {874--883},
}

@inproceedings{kurimo_unlimited_2006,
	address = {New York, New York},
	title = {Unlimited vocabulary speech recognition for agglutinative languages},
	url = {http://portal.acm.org/citation.cfm?doid=1220835.1220897},
	doi = {10.3115/1220835.1220897},
	language = {en},
	urldate = {2022-12-02},
	booktitle = {Proceedings of the main conference on {Human} {Language} {Technology} {Conference} of the {North} {American} {Chapter} of the {Association} of {Computational} {Linguistics}  -},
	publisher = {Association for Computational Linguistics},
	author = {Kurimo, Mikko and Puurula, Antti and Arisoy, Ebru and Siivola, Vesa and Hirsimäki, Teemu and Pylkkönen, Janne and Alumäe, Tanel and Saraclar, Murat},
	year = {2006},
	pages = {487--494},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\Q3WL2Q6U\\Kurimo et al. - 2006 - Unlimited vocabulary speech recognition for agglut.pdf:application/pdf},
}

@book{tarjan_morph_2010,
	title = {On {Morph} {Based} {LVCSR} {Improvements}},
	abstract = {Efficient large vocabulary continuous speech recognition of morphologically rich languages is a big challenge due to the rapid vocabulary growth. To improve the results various subword units -called as morphs -are applied as basic language elements. The improvements over the word baseline, however, are changing from negative to error rate halving across languages and tasks. In this paper we make an attempt to explore the source of this variability. Different LVCSR tasks of an agglutinative language are investigated in numerous experiments using full vocabularies. The improvement results are compared to pre-existing other language results, as well. Important correlations are found between the morph-based improvements and between the vocabulary growths and the corpus sizes.},
	author = {Tarján, Balázs and Mihajlik, Péter},
	month = jan,
	year = {2010},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\87XBNSGN\\Tarján and Mihajlik - 2010 - On Morph Based LVCSR Improvements.pdf:application/pdf},
}

@inproceedings{gebreegziabher_sub-word_2020,
	address = {Toronto, ON, Canada},
	title = {Sub-word {Based} {End}-to-{End} {Speech} {Recognition} for an {Under}-{Resourced} {Language}: {Amharic}},
	isbn = {978-1-72818-526-2},
	shorttitle = {Sub-word {Based} {End}-to-{End} {Speech} {Recognition} for an {Under}-{Resourced} {Language}},
	url = {https://ieeexplore.ieee.org/document/9283401/},
	doi = {10.1109/SMC42975.2020.9283401},
	urldate = {2022-12-02},
	booktitle = {2020 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	publisher = {IEEE},
	author = {Gebreegziabher, Nirayo Hailu and Nurnberger, Andreas},
	month = oct,
	year = {2020},
	pages = {3466--3470},
}

@article{toth_speech_2010,
	title = {Speech {Recognition} {Experiments} with {Audiobooks}.},
	volume = {19},
	abstract = {Under real-life conditions several factors may be present that make the automatic recognition of speech difficult. The most obvious examples are background noise, peculiarities of the speaker's voice, sloppy articulation and strong emotional load. These all pose difficult problems for robust speech recognition, but it is not exactly clear how much each contributes to the difficulty of the task. In this paper we examine the abilities of our best recognition technologies under near-ideal conditions. The optimal conditions will be simulated by working with the sound material of an audiobook, in which most of the disturbing factors mentioned above are absent. Firstly pure phone recognition experiments will be performed, where neural net-based technologies will also be tried as well as the conventional Hidden Markov Models. Then we move on to large vocabulary recognition, where morphbased language models are applied to improve the performance of the standard word-based technology. The tests clearly justify our assertion that audiobooks pose a much easier recognition task than real-life databases. In both types of tasks we report the lowest error rates we have achieved so far in Hungarian continuous speech recognition.},
	journal = {Acta Cybern.},
	author = {Tóth, László and Tarján, Balázs and Sárosi, Gellért and Mihajlik, Péter},
	month = jan,
	year = {2010},
	pages = {695--713},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IMQP2BWU\\Tóth et al. - 2010 - Speech Recognition Experiments with Audiobooks..pdf:application/pdf},
}

@book{tarjan_evaluation_2011,
	title = {Evaluation of lexical models for {Hungarian} {Broadcast} speech transcription and spoken term detection},
	abstract = {In this paper, we re-evaluate morph (data-driven subword) and word lexical models used for large vocabulary continuous speech recognition of agglutinative languages. Since such speech recognition systems are applied mostly for information retrieval purposes we use evaluation metrics accordingly. Standard 3-gram language model with one million words vocabulary is used for words whereas statistical morph-based models are applied with smaller vocabularies and with higher order of n-gram models. Fostering real life applicability, the computational time and memory usage of the various approaches is kept below real-time and 1.5 GB, respectively. The lexical modeling approaches are tested on Hungarian Broadcast News and Broadcast Conversation speech. In our setup, although word-based models outperformed morph-based ones in terms of both word error rate and spoken term detection measures, a search-cascade of the word and morph approaches improved the latter results significantly.},
	author = {Tarján, Balázs and Mihajlik, Péter and Balog, András and Fegyó, Tibor},
	month = aug,
	year = {2011},
	note = {Pages: 5},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PF38AIDL\\Tarján et al. - 2011 - Evaluation of lexical models for Hungarian Broadca.pdf:application/pdf},
}

@article{guglani_automatic_2020,
	title = {Automatic speech recognition system with pitch dependent features for {Punjabi} language on {KALDI} toolkit},
	volume = {167},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X20304904},
	doi = {10.1016/j.apacoust.2020.107386},
	language = {en},
	urldate = {2022-12-02},
	journal = {Applied Acoustics},
	author = {Guglani, Jyoti and Mishra, A.N.},
	month = oct,
	year = {2020},
	pages = {107386},
}

@article{kaur_automatic_2021,
	title = {Automatic {Speech} {Recognition} {System} for {Tonal} {Languages}: {State}-of-the-{Art} {Survey}},
	volume = {28},
	issn = {1134-3060, 1886-1784},
	shorttitle = {Automatic {Speech} {Recognition} {System} for {Tonal} {Languages}},
	url = {https://link.springer.com/10.1007/s11831-020-09414-4},
	doi = {10.1007/s11831-020-09414-4},
	language = {en},
	number = {3},
	urldate = {2022-12-02},
	journal = {Archives of Computational Methods in Engineering},
	author = {Kaur, Jaspreet and Singh, Amitoj and Kadyan, Virender},
	month = may,
	year = {2021},
	pages = {1039--1068},
}

@book{winskel_south_2013,
	edition = {1},
	title = {South and {Southeast} {Asian} {Psycholinguistics}},
	isbn = {978-1-139-08464-2 978-1-107-01776-4},
	url = {https://www.cambridge.org/core/product/identifier/9781139084642/type/book},
	abstract = {A large body of knowledge has accumulated in recent years on the cognitive processes underlying language, much of which comes from studies of Indo-European languages, in particular English. This groundbreaking volume explores the languages of South and Southeast Asia, which differ significantly from Indo-European languages in their grammar, lexicon and spoken forms. This book raises new questions in psycholinguistics and enables readers to re-evaluate previous models in light of new research. With thirty-six chapters divided into three parts - Language Acquisition, Language Processing and Language and Brain - it examines contemporary topics alongside new findings in areas such as first and second language acquisition, the development of literacy, the diagnosis of language and reading disorders, and the relationship between language, brain, culture and cognition. It will be invaluable to all those interested in the languages of South and Southeast Asia, as well as psychologists, linguists, educationalists, speech therapists and neuroscientists.},
	urldate = {2022-12-02},
	publisher = {Cambridge University Press},
	editor = {Winskel, Heather and Padakannaya, Prakash},
	month = nov,
	year = {2013},
	doi = {10.1017/CBO9781139084642},
}

@inproceedings{benzeghiba_impact_2006,
	address = {Saint-Petersburg, Russia},
	title = {Impact of variabilities on speech recognition},
	abstract = {Major progress is being recorded regularly on both the technol-ogy and exploitation of Automatic Speech Recognition (ASR) and spoken language systems. However, there are still techno-logical barriers to flexible solutions and user satisfaction under some circumstances. This is related to several factors, such as the sensitivity to the environment (background noise or chan-nel variability), or the weak representation of grammatical and semantic knowledge. Current research is also emphasizing deficiencies in deal-ing with variation naturally present in speech. For instance, the lack of robustness to foreign accents precludes the use by spe-cific populations. There are actually many factors affecting the speech realization: regional, sociolinguistic, or related to the environment or the speaker itself. These create a wide range of variations that may not be modeled correctly (speaker, gen-der, speech rate, vocal effort, regional accents, speaking style, non stationarity...), especially when resources for system train-ing are scarce. This paper outlines some current advances related to vari-abilities in ASR.},
	publisher = {SPECOM'2006},
	author = {Benzeghiba, Mohamed and De Mori, Renato and Deroo, Olivier and Dupont, Stéphane and Jouvet, Denis and Fissore, Luciano and Laface, Pietro and Ris, Alfred and Rose, Richard and Tyagi, Vivek and Wellekens, Christian},
	month = jun,
	year = {2006},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\7NQI8C76\\Benzeghiba et al. - 2022 - Impact of variabilities on speech recognition.pdf:application/pdf},
}

@book{divenyi_speech_2005,
	address = {Boston, MA},
	title = {Speech {Separation} by {Humans} and {Machines}},
	isbn = {978-1-4020-8001-2 978-0-387-22794-8},
	url = {http://link.springer.com/10.1007/b99695},
	language = {en},
	urldate = {2022-12-02},
	publisher = {Springer US},
	editor = {Divenyi, Pierre},
	year = {2005},
	doi = {10.1007/b99695},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\STDG3E65\\Divenyi - 2005 - Speech Separation by Humans and Machines.pdf:application/pdf},
}

@article{noauthor_international_2011,
	title = {International {Journal} of {Computer} {Science} {Issues}},
	volume = {8},
	issn = {1694-0814},
	url = {www.IJCSI.org},
	number = {5 No. 3},
	month = sep,
	year = {2011},
}

@article{jabloun_large_1999,
	title = {Large {Vocabulary} {Speech} {Recognition} {In} {Noisy} {Environments}},
	abstract = {LARGE VOCABULARY SPEECH RECOGNITION IN NOISY ENVIRONMENTS Firas Jabloun M.S. in Electrical and Electronics Engineering Supervisor: A. Enis Cetin, Ph. D. July 1998 A new set of speech feature parameters based on multirate subband analysis and the Teager Energy Operator (TEO) is developed. The speech signal is first divided into nonuniform subbands in mel-scale using a multirate filter-bank, then the Teager energies of the subsignals are estimated. Finally, the feature vector is constructed by logcompression and inverse DCT computation. The new feature parameters (TEOCEP) have a robust speech recognition performance in car engine noise which has a low pass nature. In this thesis, we also present some solutions to the problem of large vocabulary speech recognition. Triphone-based Hidden Markov Models (HMM) are used to model the vocabulary words. Although the straight forward parallel search strategy gives good recognition performance, the processing time required is found to be long...},
	author = {Jabloun, Firas and Cetin, A. and (supervisor, Ph and Arikan, Orhan and Demirekler, Mubeccel},
	month = oct,
	year = {1999},
}

@article{zouhir_feature_2016,
	title = {Feature {Extraction} {Method} for {Improving} {Speech} {Recognition} in {Noisy} {Environments}},
	volume = {12},
	issn = {1549-3636},
	url = {http://thescipub.com/abstract/10.3844/jcssp.2016.56.61},
	doi = {10.3844/jcssp.2016.56.61},
	number = {2},
	urldate = {2022-12-02},
	journal = {Journal of Computer Science},
	author = {Zouhir, Youssef and Ouni, Kaïs},
	month = feb,
	year = {2016},
	pages = {56--61},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\BAPYGRW4\\Zouhir and Ouni - 2016 - Feature Extraction Method for Improving Speech Rec.pdf:application/pdf},
}

@article{kim_auditory_1996,
	title = {Auditory {Representations} for {Robust} {Speech} {Recognition} in {Noisy} {Environments}},
	volume = {15},
	abstract = {An auditory model is proposed for robust speech recognition in noisy environments. The model consists of cochlear bandpass filters and nonlinear stages, and represents frequency and intensity information efficiently even in noisy environments. Frequency information of the signal is obtained by zero-crossing intervals, and intensity information is also incorporated by peak detectors and saturating nonlinearities. Also, the robustness of the zero-crossings in estimating frequency is verified by the developed analytic relationship of the variance of the level-crossing interval perturbations as a function of the crossing level values. The proposed auditory model is computationally efficient and free from many unknown parameters compared with other auditory models. Speaker-independent speech recognition experiments demonstrate the robustness of the proposed method.},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Kim, Doh-Suk and Lee, Soo-Young and Kil, Rhee},
	month = jan,
	year = {1996},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IU5LJCZE\\Kim et al. - 1996 - Auditory Representations for Robust Speech Recogni.pdf:application/pdf},
}

@article{kumar_hindi_2021,
	title = {Hindi speech recognition in noisy environment using hybrid technique},
	volume = {13},
	issn = {2511-2104, 2511-2112},
	url = {https://link.springer.com/10.1007/s41870-020-00586-7},
	doi = {10.1007/s41870-020-00586-7},
	language = {en},
	number = {2},
	urldate = {2022-12-02},
	journal = {International Journal of Information Technology},
	author = {Kumar, Ashok and Mittal, Vikas},
	month = apr,
	year = {2021},
	pages = {483--492},
}

@article{singh_computational_2022,
	title = {Computational intelligence in processing of speech acoustics: a survey},
	volume = {8},
	issn = {2199-4536, 2198-6053},
	shorttitle = {Computational intelligence in processing of speech acoustics},
	url = {https://link.springer.com/10.1007/s40747-022-00665-1},
	doi = {10.1007/s40747-022-00665-1},
	abstract = {Abstract
            Speech recognition of a language is a key area in the field of pattern recognition. This paper presents a comprehensive survey on the speech recognition techniques for non-Indian and Indian languages, and compiled some of the computational models used for processing speech acoustics. An immense number of frameworks are available for speech processing and recognition for languages persisting around the globe. However, a limited number of automatic speech recognition systems are available for commercial use. The gap between the languages being spoken around the globe and the technical support available to these languages are very few. This paper examined major challenges for speech recognition for different languages. Analysis of the literature shows that lack of standard databases availability of minority languages hinder the research recognition research across the globe. When compared with non-Indian languages, the research on speech recognition of Indian languages (except Hindi) has not achieved the expected milestone yet. Combination of MFCC and DNN–HMM classifier is most commonly used system for developing ASR minority languages, whereas in some of the majority languages, researchers are using much advance algorithms of DNN. It has also been observed that the research in this field is quite thin and still more research needs to be carried out, particularly in the case of minority languages.},
	language = {en},
	number = {3},
	urldate = {2022-12-02},
	journal = {Complex \& Intelligent Systems},
	author = {Singh, Amitoj and Kaur, Navkiran and Kukreja, Vinay and Kadyan, Virender and Kumar, Munish},
	month = jun,
	year = {2022},
	pages = {2623--2661},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CKYWJ53Y\\Singh et al. - 2022 - Computational intelligence in processing of speech.pdf:application/pdf},
}

@phdthesis{pedro_j_moreno_speech_1996,
	address = {Pittsburgh, Pennsylvania, 15213},
	title = {Speech {Recognition} in {Noisy} {Environments}},
	url = {http://www.cs.cmu.edu/~robust/Thesis/pjm_thesis.pdf},
	school = {Carnegie Mellon University},
	author = {Pedro J Moreno},
	month = apr,
	year = {1996},
	note = {Departement of Electrical and Computer Engineering},
}

@article{loizou_reasons_2011,
	title = {Reasons why {Current} {Speech}-{Enhancement} {Algorithms} do not {Improve} {Speech} {Intelligibility} and {Suggested} {Solutions}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5428850/},
	doi = {10.1109/TASL.2010.2045180},
	number = {1},
	urldate = {2022-12-04},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Loizou, Philipos C. and Kim, Gibak},
	month = jan,
	year = {2011},
	pages = {47--56},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\HFPMJZDZ\\Loizou and Kim - 2011 - Reasons why Current Speech-Enhancement Algorithms .pdf:application/pdf},
}

@incollection{shi_noise_2010,
	address = {Berlin, Heidelberg},
	title = {Noise {Estimation} and {Noise} {Removal} {Techniques} for {Speech} {Recognition} in {Adverse} {Environment}},
	volume = {340},
	isbn = {978-3-642-16326-5 978-3-642-16327-2},
	url = {http://link.springer.com/10.1007/978-3-642-16327-2_40},
	urldate = {2022-12-04},
	booktitle = {Intelligent {Information} {Processing} {V}},
	publisher = {Springer Berlin Heidelberg},
	author = {Shrawankar, Urmila and Thakare, Vilas},
	editor = {Shi, Zhongzhi and Vadera, Sunil and Aamodt, Agnar and Leake, David},
	year = {2010},
	doi = {10.1007/978-3-642-16327-2_40},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {336--342},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ESHRZH64\\Shrawankar and Thakare - 2010 - Noise Estimation and Noise Removal Techniques for .pdf:application/pdf},
}

@article{schuller_recognition_2009-1,
	title = {Recognition of {Noisy} {Speech}: {A} {Comparative} {Survey} of {Robust} {Model} {Architecture} and {Feature} {Enhancement}},
	volume = {2009},
	issn = {1687-4722},
	shorttitle = {Recognition of {Noisy} {Speech}},
	url = {http://asmp.eurasipjournals.com/content/2009/1/942617},
	doi = {10.1155/2009/942617},
	language = {en},
	number = {1},
	urldate = {2022-12-04},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Schuller, Björn and Wöllmer, Martin and Moosmayr, Tobias and Rigoll, Gerhard},
	year = {2009},
	pages = {942617},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\VEKLJABS\\Schuller et al. - 2009 - Recognition of Noisy Speech A Comparative Survey .pdf:application/pdf},
}

@article{haton_problems_1994-1,
	title = {Problems and solutions for noisy speech recognition},
	volume = {04},
	issn = {1155-4339},
	url = {http://www.edpsciences.org/10.1051/jp4:1994592},
	doi = {10.1051/jp4:1994592},
	number = {C5},
	urldate = {2022-12-04},
	journal = {Le Journal de Physique IV},
	author = {Haton, J.-P.},
	month = may,
	year = {1994},
	pages = {C5--439--C5--448},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\8C39VFKG\\Haton - 1994 - Problems and solutions for noisy speech recognitio.pdf:application/pdf},
}

@article{stern_compensation_2022,
	title = {{COMPENSATION} {FOR} {ENVIRONMENTAL} {DEGRADATION} {IN} {AUTOMATIC} {SPEECH} {RECOGNITION}},
	abstract = {The accuracy of speech recognition systems degrades when operated in adverse acoustical environments. This paper reviews various methods by which more detailed mathematical descriptions of the effects of environmental degradation can improve speech recognition accuracy using both "data-driven" and "model-based" compensation strategies. Data-driven meth- ods learn environmental characteristics through direct compari- sons of speech recorded in the noisy environment with the same speech recorded under optimal conditions. Model-based methods use a mathematical model of the environment and attempt to use samples of the degraded speech to estimate model parameters. These general approaches to environmental compensation are discussed in terms of recent research in envi- ronmental robustness at CMU, and in terms of similar efforts at other sites. These compensation algorithms are evaluated in a series of experiments measuring recognition accuracy for speech from the ARPA Wall Street Journal database that is cor- rupted by artificially-added noise at various signal-to-noise ratios (SNRs), and in more natural speech recognition tasks.},
	author = {Stern, Richard and Raj, Bhiksha and Moreno, Pedro},
	month = dec,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5S3VJVM3\\Stern et al. - 2022 - COMPENSATION FOR ENVIRONMENTAL DEGRADATION IN AUTO.pdf:application/pdf},
}

@article{bahari_speaker_2014,
	title = {Speaker age estimation using i-vectors},
	volume = {34},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197614001018},
	doi = {10.1016/j.engappai.2014.05.003},
	language = {en},
	urldate = {2022-12-04},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Bahari, Mohamad Hasan and McLaren, Mitchell and Van hamme, Hugo and van Leeuwen, David A.},
	month = sep,
	year = {2014},
	pages = {99--108},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\QMGUFD4Q\\Bahari et al. - 2014 - Speaker age estimation using i-vectors.pdf:application/pdf},
}

@article{urmila_shrawankar_voice_2010,
	title = {Voice {Activity} {Detector} and {Noise} {Trackers} for {Speech} {Recognition} {System} in {Noisy} {Environment}},
	volume = {2},
	issn = {2005-8039, 2233-9337},
	url = {http://www.aicit.org/ijact/paper_detail.html?q=92},
	doi = {10.4156/ijact.vol2.issue4.11},
	number = {4},
	urldate = {2022-12-04},
	journal = {International Journal of Advancements in Computing Technology},
	author = {Urmila Shrawankar, Vilas Thakare},
	month = oct,
	year = {2010},
	pages = {107--114},
}

@techreport{jasha_droppo_comparing_nodate,
	title = {Comparing {Human} and {Machine} {Errors} in {Conversational} {Speech} {Transcription}},
	url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-revised2.pdf},
	institution = {Microsoft AI and Research, Redmond, WA, USA},
	author = {Jasha Droppo and Andreas Stolcke},
}

@article{plaza_influence_2021,
	title = {Influence of the {Contact} {Center} {Systems} {Development} on {Key} {Performance} {Indicators}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9381200/},
	doi = {10.1109/ACCESS.2021.3066801},
	urldate = {2022-12-06},
	journal = {IEEE Access},
	author = {Plaza, Miroslaw and Pawlik, Lukasz},
	year = {2021},
	pages = {44580--44591},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\XV48KS4C\\Plaza and Pawlik - 2021 - Influence of the Contact Center Systems Developmen.pdf:application/pdf},
}

@article{plaza_call_2021-1,
	title = {Call {Transcription} {Methodology} for {Contact} {Center} {Systems}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9508438/},
	doi = {10.1109/ACCESS.2021.3102502},
	urldate = {2022-12-06},
	journal = {IEEE Access},
	author = {Plaza, Miroslaw and Pawlik, Lukasz and Deniziak, Stanislaw},
	year = {2021},
	pages = {110975--110988},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\T9DWNZTZ\\Plaza et al. - 2021 - Call Transcription Methodology for Contact Center .pdf:application/pdf},
}

@incollection{hu_analysis_2021,
	address = {Cham},
	title = {Analysis of {Key} {Elements} for {Modern} {Contact} {Center} {Systems} to {Improve} {Quality}},
	volume = {83},
	isbn = {978-3-030-80471-8 978-3-030-80472-5},
	url = {https://link.springer.com/10.1007/978-3-030-80472-5_14},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Advances in {Computer} {Science} for {Engineering} and {Education} {IV}},
	publisher = {Springer International Publishing},
	author = {Aitchanov, Bekmurza and Baimuratov, Olimzhon and Zhussupekov, Muratbek and Aitchanov, Tley},
	editor = {Hu, Zhengbing and Petoukhov, Sergey and Dychka, Ivan and He, Matthew},
	year = {2021},
	doi = {10.1007/978-3-030-80472-5_14},
	note = {Series Title: Lecture Notes on Data Engineering and Communications Technologies},
	pages = {162--174},
}

@inproceedings{mamou_spoken_2006,
	address = {Seattle, Washington, USA},
	title = {Spoken document retrieval from call-center conversations},
	isbn = {978-1-59593-369-0},
	url = {http://portal.acm.org/citation.cfm?doid=1148170.1148183},
	doi = {10.1145/1148170.1148183},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '06},
	publisher = {ACM Press},
	author = {Mamou, Jonathan and Carmel, David and Hoory, Ron},
	year = {2006},
	pages = {51},
}

@inproceedings{mishne_automatic_2005,
	address = {Bremen, Germany},
	title = {Automatic analysis of call-center conversations},
	isbn = {978-1-59593-140-5},
	url = {http://portal.acm.org/citation.cfm?doid=1099554.1099684},
	doi = {10.1145/1099554.1099684},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the 14th {ACM} international conference on {Information} and knowledge management  - {CIKM} '05},
	publisher = {ACM Press},
	author = {Mishne, Gilad and Carmel, David and Hoory, Ron and Roytman, Alexey and Soffer, Aya},
	year = {2005},
	pages = {453},
}

@inproceedings{busemann_message_2000,
	address = {Seattle, Washington},
	title = {Message classification in the call center},
	url = {http://portal.acm.org/citation.cfm?doid=974147.974169},
	doi = {10.3115/974147.974169},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the sixth conference on {Applied} natural language processing  -},
	publisher = {Association for Computational Linguistics},
	author = {Busemann, Stephan and Schmeier, Sven and Arens, Roman G.},
	year = {2000},
	pages = {158--165},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\7PBEW9N9\\Busemann et al. - 2000 - Message classification in the call center.pdf:application/pdf},
}

@inproceedings{park_automatic_2007,
	address = {Lisbon, Portugal},
	title = {Automatic call section segmentation for contact-center calls},
	isbn = {978-1-59593-803-9},
	url = {http://portal.acm.org/citation.cfm?doid=1321440.1321459},
	doi = {10.1145/1321440.1321459},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the sixteenth {ACM} conference on {Conference} on information and knowledge management  - {CIKM} '07},
	publisher = {ACM Press},
	author = {Park, Youngja},
	year = {2007},
	pages = {117},
}

@article{hu_neural_2022,
	title = {Neural {Architecture} {Search} for {LF}-{MMI} {Trained} {Time} {Delay} {Neural} {Networks}},
	volume = {30},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9721103/},
	doi = {10.1109/TASLP.2022.3153253},
	urldate = {2022-12-06},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hu, Shoukang and Xie, Xurong and Cui, Mingyu and Deng, Jiajun and Liu, Shansong and Yu, Jianwei and Geng, Mengzhe and Liu, Xunying and Meng, Helen},
	year = {2022},
	pages = {1093--1107},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\9AQIYJXB\\Hu et al. - 2022 - Neural Architecture Search for LF-MMI Trained Time.pdf:application/pdf},
}

@misc{linguistic_data_consortium_2000_2002,
	title = {2000 {HUB5} {English} {Evaluation} {Transcripts}},
	url = {https://catalog.ldc.upenn.edu/LDC2002T43},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}2000 HUB5 English Evaluation Transcripts was developed by the Linguistic Data Consortium (LDC)\&nbsp; and consists of transcripts of 40 English telephone conversations used in the 2000 HUB5 evaluation sponsored by\&nbsp;{\textless}a href="http://nist.gov/itl/iad/mig/"{\textgreater}NIST{\textless}/a{\textgreater} (National Institute of Standards and Technology).\&nbsp;{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The Hub5 evaluation series focused on conversational speech over the telephone with the particular task of transcribing conversational speech into text. Its goals were to explore promising new areas in the recognition of conversational speech, to develop advanced technology incorporating those ideas and to measure the performance of new technology.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Data{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}This release contains transcripts in .txt format for the 40 source speech data files used in the evaluation: (1) 20 unreleased telephone conversations from the Swtichboard studies in which recruited speakers were connected through a robot operator to carry on casual conversations about a daily topic announced by the robot operator at the start of the call; and (2) 20 telephone conversations from {\textless}a href="http://catalog.ldc.upenn.edu/LDC97S42" rel="nofollow"{\textgreater}CALLHOME American English Speech{\textless}/a{\textgreater} which consists of unscripted telephone conversations between native English speakers.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The corresponding speech files are availalbe in 2000 HUB5 English Evaluation Speech ({\textless}a href="../../../LDC2002S09"{\textgreater}LDC2002S09{\textless}/a{\textgreater}).{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}Please view this {\textless}a href="desc/addenda/LDC2002T43.txt"{\textgreater}sample{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Updates{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}There are no updates at this time.{\textless}/p{\textgreater}{\textless}/br{\textgreater}
Portions © 1997, 2000, 2002 Trustees of the University of Pennsylvania},
	urldate = {2022-12-06},
	publisher = {Linguistic Data Consortium},
	author = {Linguistic Data Consortium},
	month = jan,
	year = {2002},
	doi = {10.35111/8C95-0C55},
	note = {Artwork Size: 4848 KB
Pages: 4848 KB
Type: dataset},
}

@misc{fiscus_jonathan_g_2003_2007,
	title = {2003 {NIST} {Rich} {Transcription} {Evaluation} {Data}},
	url = {https://catalog.ldc.upenn.edu/LDC2007S10},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}2003 NIST Rich Transcription Evaluation Data contains the test material used in the 2003 Rich Transcription Spring and Fall evaluations administered by the {\textless}a href="http://www.nist.gov/speech" rel="nofollow"{\textgreater}NIST (National Institute of Standards and Technology) Speech Group{\textless}/a{\textgreater}. The Spring evaluation (RT-03S), implemented in March-April 2003, focused on Speech-To-Text (STT) tasks for broadcast news speech and conversational telephone speech in three languages: English, Mandarin Chinese and Arabic. That evaluation also included one Metadata Extraction (MDE) task, speaker diarization for broadcast news speech and conversational telephone speech in English. The Fall evaluation (RT-03F), implemented in October 2003, focused on MDE tasks including speaker diarization, speaker-attributed STT, SU (sentence/semantic unit) detection and disfluency detection for broadcast news speech and conversational telephone speech in English. For complete information about the evaluations, see the {\textless}a href="https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation"{\textgreater}Rich Text Evaluation website{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Data{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The BN datasets were selected from {\textless}a href="http://projects.ldc.upenn.edu/TDT4/" rel="nofollow"{\textgreater}TDT-4{\textless}/a{\textgreater} sources collected in February 2001. The evaluation excerpts were transcribed to the nearest story boundary. The English BN dataset is approximately three hours long and is composed of 30-minute excerpts from six different broadcasts. The Mandarin Chinese BN dataset is approximately one hour long, consisting of 12-minute excerpts from five different broadcasts. The Arabic BN dataset is also approximately one hour long and contains 30-minute excerpts from two different broadcasts.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The CTS datasets consist of material from various LDC telephone speech data. All evaluation excerpts were transcribed to the nearest turn. The English CTS set is approximately 6 hours long and is composed of 5-minute excerpts from 72 different conversations: 36 from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC2001S13" rel="nofollow"{\textgreater}Switchboard Cellular{\textless}/a{\textgreater} collection and 36 from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC2004S13" rel="nofollow"{\textgreater}Fisher collection{\textless}/a{\textgreater}. The Mandarin Chinese CTS dataset is approximately one hour long and consists of 5-minute excerpts from 12 different conversations from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC96S55" rel="nofollow"{\textgreater}CallFriend Mandarin Chinese data{\textless}/a{\textgreater}. The Arabic CTS set is also approximately one hour long and contains 5-minute excerpts from 12 different conversations from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC97S45" rel="nofollow"{\textgreater}CallHome Egyptian Arabic data{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}No manual (human-annotated) segmentations were provided. Sites were required to generate their own segmentations automatically.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}Unlike the BN audio files where the full broadcasts were provided, the CTS audio files contain only the evaluation excerpts. Each audio excerpt is a SPHERE-headered, two channel interleaved 8-bit mulaw file.{\textless}/p{\textgreater}{\textless}br{\textgreater}
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater}
{\textless}ul{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC2007S10.wav" rel="nofollow"{\textgreater}English Broacast News Audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC2007S10\_ind.txt" rel="nofollow"{\textgreater}Indices{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC2007S10.txt" rel="nofollow"{\textgreater}Transcriptions{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater}
{\textless}/ul{\textgreater}{\textless}br{\textgreater}
{\textless}p{\textgreater}The World is a co-production of Public Radio International and the British Broadcasting Corporation and is produced at WGBH Boston.{\textless}/p{\textgreater}{\textless}/br{\textgreater}
Portions © 2001 American Broadcasting Company, © 2001 Cable News Network, LP, LLLP, © 2001 China Broadcasting System (Taiwan), © 2001 China Central TV, © 2001 China National Radio, © 2001 China Television System (Taiwan), © 2001 National Broadcasting Company, © 2001 Nile TV, © 2001 Public Radio International, © 1996-2005, 2007 Trustees of the University of Pennsylvania{\textless}br{\textgreater}{\textless}br{\textgreater}The World is a co-production of Public Radio International and the British Broadcasting Corporation and is produced at WGBH Boston.},
	urldate = {2022-12-06},
	publisher = {Linguistic Data Consortium},
	author = {Fiscus, Jonathan G. and Doddington, George R. and Le, Audrey and Sanders, Greg and Przybocki, Mark and Pallett, David},
	month = aug,
	year = {2007},
	doi = {10.35111/V8J8-M006},
	note = {Artwork Size: 2097152 KB
Pages: 2097152 KB
Type: dataset},
}

@inproceedings{lockwood_experiments_1991,
	title = {Experiments with a non-linear spectral subtractor ({NSS}), hidden {Markov} models and the projection, for robust speech recognition in cars},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/lockwood91_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-16},
	language = {en},
	urldate = {2022-12-14},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Lockwood, P. and Boudy, J.},
	month = sep,
	year = {1991},
	pages = {79--82},
}

@article{lockwood_experiments_1992,
	title = {Experiments with a nonlinear spectral subtractor ({NSS}), {Hidden} {Markov} models and the projection, for robust speech recognition in cars},
	volume = {11},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016763939290016Z},
	doi = {10.1016/0167-6393(92)90016-Z},
	language = {en},
	number = {2-3},
	urldate = {2022-12-14},
	journal = {Speech Communication},
	author = {Lockwood, P. and Boudy, J.},
	month = jun,
	year = {1992},
	pages = {215--228},
}

@article{junqua_lombard_1993,
	title = {The {Lombard} reflex and its role on human listeners and automatic speech recognizers},
	volume = {93},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.405631},
	doi = {10.1121/1.405631},
	language = {en},
	number = {1},
	urldate = {2022-12-14},
	journal = {The Journal of the Acoustical Society of America},
	author = {Junqua, Jean‐Claude},
	month = jan,
	year = {1993},
	pages = {510--524},
}

@article{junqua_influence_1996,
	title = {The influence of acoustics on speech production: {A} noise-induced stress phenomenon known as the {Lombard} reflex},
	volume = {20},
	issn = {01676393},
	shorttitle = {The influence of acoustics on speech production},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639396000416},
	doi = {10.1016/S0167-6393(96)00041-6},
	language = {en},
	number = {1-2},
	urldate = {2022-12-14},
	journal = {Speech Communication},
	author = {Junqua, Jean-Claude},
	month = nov,
	year = {1996},
	pages = {13--22},
}

@article{hadei_family_2010,
	title = {A {Family} of {Adaptive} {Filter} {Algorithms} in {Noise} {Cancellation} for {Speech} {Enhancement}},
	issn = {17938163},
	url = {http://www.ijcee.org/show-31-583-1.html},
	doi = {10.7763/IJCEE.2010.V2.153},
	urldate = {2022-12-14},
	journal = {International Journal of Computer and Electrical Engineering},
	author = {Hadei, Sayed. A. and lotfizad, M.},
	year = {2010},
	pages = {307--315},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\S38YE5HP\\Hadei and lotfizad - 2010 - A Family of Adaptive Filter Algorithms in Noise Ca.pdf:application/pdf},
}

@article{hu_unvoiced_2011,
	title = {Unvoiced {Speech} {Segregation} {From} {Nonspeech} {Interference} via {CASA} and {Spectral} {Subtraction}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5640654/},
	doi = {10.1109/TASL.2010.2093893},
	number = {6},
	urldate = {2022-12-14},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Hu, Ke and Wang, DeLiang},
	month = aug,
	year = {2011},
	pages = {1600--1609},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CETV57WZ\\Hu and Wang - 2011 - Unvoiced Speech Segregation From Nonspeech Interfe.pdf:application/pdf},
}

@inproceedings{hu_incorporating_2009,
	address = {Taipei, Taiwan},
	title = {Incorporating spectral subtraction and noise type for unvoiced speech segregation},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4960611/},
	doi = {10.1109/ICASSP.2009.4960611},
	urldate = {2022-12-14},
	booktitle = {2009 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hu, Ke and Wang, DeLiang},
	month = apr,
	year = {2009},
	pages = {4425--4428},
}

@article{suhadi_data-driven_2011,
	title = {A {Data}-{Driven} {Approach} to {A} {Priori} {SNR} {Estimation}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5430903/},
	doi = {10.1109/TASL.2010.2045799},
	number = {1},
	urldate = {2022-12-18},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Suhadi, Suhadi and Last, Carsten and Fingscheidt, Tim},
	month = jan,
	year = {2011},
	pages = {186--195},
}

@article{miguel_bayesian_2011,
	title = {Bayesian {Networks} for {Discrete} {Observation} {Distributions} in {Speech} {Recognition}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5638127/},
	doi = {10.1109/TASL.2010.2092764},
	number = {6},
	urldate = {2022-12-18},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Miguel, Antonio and Ortega, Alfonso and Buera, Luis and Lleida, Eduardo},
	month = aug,
	year = {2011},
	pages = {1476--1489},
}

@incollection{ponting_use_1999,
	address = {Berlin, Heidelberg},
	title = {The {Use} of the {Maximum} {Likelihood} {Criterion} in {Language} {Modelling}},
	isbn = {978-3-642-64250-0 978-3-642-60087-6},
	url = {http://link.springer.com/10.1007/978-3-642-60087-6_25},
	language = {en},
	urldate = {2022-12-18},
	booktitle = {Computational {Models} of {Speech} {Pattern} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ney, Hermann},
	editor = {Ponting, Keith},
	year = {1999},
	doi = {10.1007/978-3-642-60087-6_25},
	pages = {259--279},
}

@article{wang_environmental_2007,
	title = {Environmental {Independent} {ASR} {Model} {Adaptation}/{Compensation} by {Bayesian} {Parametric} {Representation}},
	volume = {15},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4156223/},
	doi = {10.1109/TASL.2007.894523},
	number = {4},
	urldate = {2022-12-18},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Wang, Xuechuan and O'Shaughnessy, Douglas},
	month = may,
	year = {2007},
	pages = {1204--1217},
}

@article{choi_maximum_2015,
	title = {Maximum likelihood speaking rate normalization of speech signals for improving access to speech-enabled automatic systems},
	abstract = {Speech is the most natural medium for human communication. Thus, the goal of automatic speech recognition (ASR) is to help humans easily communicate with computational devices and to integrate technology into human life. ASR is very useful and has many applications in various areas. However, speaking rate is one of the variabilities influencing ASR performance. In this paper, we propose a maximum likelihood (ML) speaking rate normalization approach for hidden Markov model (HMM)-based speech recognition, which is realized through the combination of signal-level and acoustic model-level approaches. The speaking rate of input speech is controlled by applying a time-scale modification (ISM) algorithm. Speaking rate normalization is achieved by selecting a scale factor of ISM. The scale factor selection for training and testing of a speech recognition system is performed based on an ML criterion during HMM decoding. From connected digit recognition experiments, it is shown that a speech recognition system employing the proposed speaking rate normalization technique can reduce average word error rate (WER) by 9.5\% compared to that without any speaking rate normalization.},
	journal = {Asia life sciences},
	author = {Choi, Seung and Kim, Hong},
	month = jul,
	year = {2015},
	pages = {197--206},
}

@incollection{dev_survey_2022,
	address = {Cham},
	title = {Survey on {Automatic} {Speech} {Recognition} {Systems} for {Indic} {Languages}},
	volume = {1546},
	isbn = {978-3-030-95710-0 978-3-030-95711-7},
	url = {https://link.springer.com/10.1007/978-3-030-95711-7_8},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {Artificial {Intelligence} and {Speech} {Technology}},
	publisher = {Springer International Publishing},
	author = {Sethi, Nandini and Dev, Amita},
	editor = {Dev, Amita and Agrawal, S. S. and Sharma, Arun},
	year = {2022},
	doi = {10.1007/978-3-030-95711-7_8},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {85--98},
}

@incollection{joshi_robust_2023,
	address = {Singapore},
	title = {Robust {Feature} {Extraction} and {Recognition} {Model} for {Automatic} {Speech} {Recognition} {System} on {News} {Report} {Dataset}},
	volume = {400},
	isbn = {978-981-19009-4-5 978-981-19009-5-2},
	url = {https://link.springer.com/10.1007/978-981-19-0095-2_56},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {Information and {Communication} {Technology} for {Competitive} {Strategies} ({ICTCS} 2021)},
	publisher = {Springer Nature Singapore},
	author = {Mendiratta, Sunanda and Turk, Neelam and Bansal, Dipali},
	editor = {Joshi, Amit and Mahmud, Mufti and Ragel, Roshan G.},
	year = {2023},
	doi = {10.1007/978-981-19-0095-2_56},
	note = {Series Title: Lecture Notes in Networks and Systems},
	pages = {589--601},
}

@article{alam_bengali_2022,
	title = {Bengali {Common} {Voice} {Speech} {Dataset} for {Automatic} {Speech} {Recognition}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2206.14053},
	doi = {10.48550/ARXIV.2206.14053},
	abstract = {Bengali is one of the most spoken languages in the world with over 300 million speakers globally. Despite its popularity, research into the development of Bengali speech recognition systems is hindered due to the lack of diverse open-source datasets. As a way forward, we have crowdsourced the Bengali Common Voice Speech Dataset, which is a sentence-level automatic speech recognition corpus. Collected on the Mozilla Common Voice platform, the dataset is part of an ongoing campaign that has led to the collection of over 400 hours of data in 2 months and is growing rapidly. Our analysis shows that this dataset has more speaker, phoneme, and environmental diversity compared to the OpenSLR Bengali ASR dataset, the largest existing open-source Bengali speech dataset. We present insights obtained from the dataset and discuss key linguistic challenges that need to be addressed in future versions. Additionally, we report the current performance of a few Automatic Speech Recognition (ASR) algorithms and set a benchmark for future research.},
	urldate = {2023-01-24},
	author = {Alam, Samiul and Sushmit, Asif and Abdullah, Zaowad and Nakkhatra, Shahrin and Ansary, MD. Nazmuddoha and Hossen, Syed Mobassir and Mehnaz, Sazia Morshed and Reasat, Tahsin and Humayun, Ahmed Imtiaz},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@inproceedings{younis_challenges_2021,
	address = {New Orleans, LA, USA},
	title = {Challenges in real-time-embedded {IoT} {Command} {Recognition}},
	isbn = {978-1-66544-431-6},
	url = {https://ieeexplore.ieee.org/document/9595903/},
	doi = {10.1109/WF-IoT51360.2021.9595903},
	urldate = {2023-02-05},
	booktitle = {2021 {IEEE} 7th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	publisher = {IEEE},
	author = {Younis, Hazem and Hansen, John H.L.},
	month = jun,
	year = {2021},
	pages = {848--851},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://aclweb.org/anthology/P16-1162},
	doi = {10.18653/v1/P16-1162},
	language = {en},
	urldate = {2023-02-05},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2016},
	pages = {1715--1725},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AVUDWHZP\\Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@article{chen_audio_2009,
	title = {Audio {Quality} {Issue} for {Automatic} {Speech} {Assessment}},
	abstract = {Recently, in the language testing field, automatic speech recog-nition (ASR) technology has been used to automatically score speaking tests. This paper investigates the impact of audio quality on ASR-based automatic speaking assessment. Using the read speech data in the International English Speaking Test (IEST) practice test, we annotated audio quality and compared scores rated by humans, speech recognition accuracy, and the quality of features used for the automatic assessment under high and low audio quality conditions. Our investigation suggests that human raters can cope with low-quality audio files well, but speech recognition and the features extracted for the automatic assessment perform worse on the low audio quality condition.},
	author = {Chen, Lei},
	month = jan,
	year = {2009},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\VFAQD9MQ\\Chen - 2009 - Audio Quality Issue for Automatic Speech Assessmen.pdf:application/pdf},
}

@article{kumalija_performance_2022,
	title = {Performance evaluation of automatic speech recognition systems on integrated noise-network distorted speech},
	volume = {2},
	issn = {2673-8198},
	url = {https://www.frontiersin.org/articles/10.3389/frsip.2022.999457/full},
	doi = {10.3389/frsip.2022.999457},
	abstract = {In VoIP applications, such as Interactive Voice Response and VoIP-phone conversation transcription, speech signals are degraded not only by environmental noise but also by transmission network quality, and distortions induced by encoding and decoding algorithms. Therefore, there is a need for automatic speech recognition (ASR) systems to handle integrated noise-network distorted speech. In this study, we present a comparative analysis of a speech-to-text system trained on clean speech against one trained on integrated noise-network distorted speech. Training an ASR model on noise-network distorted speech dataset improves its robustness. Although the performance of an ASR model trained on clean speech depends on noise type, this is not the case when noise is further distorted by network transmission. The model trained on noise-network distorted speech exhibited a 60\% improvement rate in the word error rate (WER), word match rate (MER), and word information lost (WIL) over the model trained on clean speech. Furthermore, the ASR model trained with noise-network distorted speech could tolerate a jitter of less than 20\% and a packet loss of less than 15\%, without a decrease in performance. However, WER, MER, and WIL increased in proportion to the jitter and packet loss as they exceeded 20\% and 15\%, respectively. Additionally, the model trained on noise-network distorted speech exhibited higher robustness compared to that trained on clean speech. The ASR model trained on noise-network distorted speech can also tolerate signal-to-noise (SNR) values of 5 
              dB
              and above, without the loss of performance, independent of noise type.},
	urldate = {2023-02-16},
	journal = {Frontiers in Signal Processing},
	author = {Kumalija, Elhard and Nakamoto, Yukikazu},
	month = sep,
	year = {2022},
	pages = {999457},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\PUD2J8ZQ\\Kumalija and Nakamoto - 2022 - Performance evaluation of automatic speech recogni.pdf:application/pdf},
}

@article{bochner_effects_2022,
	title = {Effects of {Sound} {Quality} on the {Accuracy} of {Telephone} {Captions} {Produced} by {Automatic} {Speech} {Recognition}: {A} {Preliminary} {Investigation}},
	issn = {1059-0889, 1558-9137},
	shorttitle = {Effects of {Sound} {Quality} on the {Accuracy} of {Telephone} {Captions} {Produced} by {Automatic} {Speech} {Recognition}},
	url = {http://pubs.asha.org/doi/10.1044/2022_AJA-22-00102},
	doi = {10.1044/2022_AJA-22-00102},
	abstract = {Purpose:
              Automatic speech recognition (ASR) is commonly used to produce telephone captions to provide telecommunication access for individuals who are d/Deaf and hard of hearing (DHH). However, little is known about the effects of degraded telephone audio on the intelligibility of ASR captioning. This research note investigates the accuracy of telephone captions produced by ASR under degraded audio conditions.


              Method:
              Packet loss, delay, and repetition are common sources of degradation in sound quality for telephone audio. Eleven sets of wideband filtered sentences were degraded by high and low levels of simulated packet loss, delay, and repetition. These sets, along with a clean set of sentences, were submitted to ASR, and the accuracy of the resulting output was evaluated using three metrics: a word recognition score, word error rate, and word information loss.


              Results:
              The resulting pattern of data indicated the relative impact of each degraded condition on message intelligibility. The high and low packet loss conditions had the largest effect on message intelligibility. This finding was interpreted to indicate that packet loss can have a substantial impact on the accuracy of telephone captions produced with ASR.


              Conclusions:
              The results of this investigation point to a potential area of improvement in service quality that could have a substantial impact on telecommunication services for consumers who are DHH. Further research in this area is needed to provide additional information concerning the scope and impact of packet loss on the accuracy of telephone captioning produced by ASR.


              Supplemental Material:

                https://doi.org/10.23641/asha.21699557},
	language = {en},
	urldate = {2023-02-16},
	journal = {American Journal of Audiology},
	author = {Bochner, Joseph and Indelicato, Mark and Konnur, Pralhad},
	month = dec,
	year = {2022},
	pages = {1--8},
}

@article{kumalija_performance_2022-1,
	title = {Performance evaluation of automatic speech recognition systems on integrated noise-network distorted speech},
	volume = {2},
	issn = {2673-8198},
	url = {https://www.frontiersin.org/articles/10.3389/frsip.2022.999457/full},
	doi = {10.3389/frsip.2022.999457},
	abstract = {In VoIP applications, such as Interactive Voice Response and VoIP-phone conversation transcription, speech signals are degraded not only by environmental noise but also by transmission network quality, and distortions induced by encoding and decoding algorithms. Therefore, there is a need for automatic speech recognition (ASR) systems to handle integrated noise-network distorted speech. In this study, we present a comparative analysis of a speech-to-text system trained on clean speech against one trained on integrated noise-network distorted speech. Training an ASR model on noise-network distorted speech dataset improves its robustness. Although the performance of an ASR model trained on clean speech depends on noise type, this is not the case when noise is further distorted by network transmission. The model trained on noise-network distorted speech exhibited a 60\% improvement rate in the word error rate (WER), word match rate (MER), and word information lost (WIL) over the model trained on clean speech. Furthermore, the ASR model trained with noise-network distorted speech could tolerate a jitter of less than 20\% and a packet loss of less than 15\%, without a decrease in performance. However, WER, MER, and WIL increased in proportion to the jitter and packet loss as they exceeded 20\% and 15\%, respectively. Additionally, the model trained on noise-network distorted speech exhibited higher robustness compared to that trained on clean speech. The ASR model trained on noise-network distorted speech can also tolerate signal-to-noise (SNR) values of 5 
              dB
              and above, without the loss of performance, independent of noise type.},
	urldate = {2023-02-16},
	journal = {Frontiers in Signal Processing},
	author = {Kumalija, Elhard and Nakamoto, Yukikazu},
	month = sep,
	year = {2022},
	pages = {999457},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\N27P9CHJ\\Kumalija and Nakamoto - 2022 - Performance evaluation of automatic speech recogni.pdf:application/pdf},
}

@inproceedings{fan_practical_2020,
	address = {Taipei, Taiwan},
	title = {A {Practical} {Black}-{Box} {Attack} {Against} {Autonomous} {Speech} {Recognition} {Model}},
	isbn = {978-1-72818-298-8},
	url = {https://ieeexplore.ieee.org/document/9348184/},
	doi = {10.1109/GLOBECOM42002.2020.9348184},
	urldate = {2023-02-16},
	booktitle = {{GLOBECOM} 2020 - 2020 {IEEE} {Global} {Communications} {Conference}},
	publisher = {IEEE},
	author = {Fan, Wenshu and Li, Hongwei and Jiang, Wenbo and Xu, Guowen and Lu, Rongxing},
	month = dec,
	year = {2020},
	pages = {1--6},
}

@inproceedings{rg_leonard_tidigits_1984,
	address = {San Diego, CA, USA},
	title = {{TIDIGITS} database},
	author = {R.G Leonard},
	year = {1984},
	pages = {Paper 42.11},
}

@article{ming_union_2001,
	title = {Union: a model for partial temporal corruption of speech},
	volume = {15},
	issn = {08852308},
	shorttitle = {Union},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230800901669},
	doi = {10.1006/csla.2000.0166},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Ming, Ji and Jack Smith, F.},
	month = jul,
	year = {2001},
	pages = {217--231},
}

@article{noauthor_proceedings_1984,
	title = {{PROCEEDINGS} - {ICASSP} 84, {IEEE} {INTERNATIONAL} {CONFERENCE} {ON} {ACOUSTICS}, {SPEECH}, {AND} {SIGNAL} {PROCESSING}.},
	abstract = {This conference proceedings contains 294 papers. 286 papers are indexed separately. The topics covered are: narrow band speech coding; algorithms; spectral estimation; speech analysis synthesis; aids for the handicapped and laryngeal analysis; sonar signal processing; speaker recognition; filter design; adaptive processing; transform and convolution methods; speech hardware; DSP applications and techniques; digital audio; medium and wide band speech coding; digital signal processing; speech recognition; audio; system identification; radar and seismology processing; speech enhancement and quality; digital image processing; and time delay and coherence estimation.},
	journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	month = jan,
	year = {1984},
	pages = {var paging},
}

@inproceedings{marcus_overview_1992,
	address = {Harriman, New York},
	title = {Overview of the fifth {DARPA} speech and natural language workshop},
	isbn = {978-1-55860-272-4},
	url = {http://portal.acm.org/citation.cfm?doid=1075527.1075528},
	doi = {10.3115/1075527.1075528},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the workshop on {Speech} and {Natural} {Language}  - {HLT} '91},
	publisher = {Association for Computational Linguistics},
	author = {Marcus, Mitchell P.},
	year = {1992},
	pages = {3--4},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\L4P2XWME\\Marcus - 1992 - Overview of the fifth DARPA speech and natural lan.pdf:application/pdf},
}

@inproceedings{paul_d_overview_1992,
	address = {Harriman, New York},
	title = {Overview of the fifth {DARPA} speech and natural language workshop},
	isbn = {978-1-55860-272-4},
	url = {http://portal.acm.org/citation.cfm?doid=1075527.1075528},
	doi = {10.3115/1075527.1075528},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the workshop on {Speech} and {Natural} {Language}  - {HLT} '91},
	publisher = {Association for Computational Linguistics},
	author = {Paul D.},
	year = {1992},
	pages = {7--14},
}

@inproceedings{lockwood_experiments_1991,
	title = {Experiments with a non-linear spectral subtractor ({NSS}), hidden {Markov} models and the projection, for robust speech recognition in cars},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/lockwood91_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-16},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Lockwood, P. and Boudy, J.},
	month = sep,
	year = {1991},
	pages = {79--82},
}

@article{junqua_lombard_1993,
	title = {The {Lombard} reflex and its role on human listeners and automatic speech recognizers},
	volume = {93},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.405631},
	doi = {10.1121/1.405631},
	language = {en},
	number = {1},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Junqua, Jean‐Claude},
	month = jan,
	year = {1993},
	pages = {510--524},
}

@inproceedings{bateman_spectral_1992,
	address = {San Francisco, CA, USA},
	title = {Spectral contrast normalization and other techniques for speech recognition in noise},
	isbn = {978-0-7803-0532-8},
	url = {http://ieeexplore.ieee.org/document/225927/},
	doi = {10.1109/ICASSP.1992.225927},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Bateman, D.C. and Bye, D.K. and Hunt, M.J.},
	year = {1992},
	pages = {241--244 vol.1},
}

@article{atal_effectiveness_1974,
	title = {Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification},
	volume = {55},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1914702},
	doi = {10.1121/1.1914702},
	language = {en},
	number = {6},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Atal, B. S.},
	month = jun,
	year = {1974},
	pages = {1304--1312},
}

@book{conrads_effect_2014,
	title = {The {Effect} of {Communication} {Channels} on {Lying}},
	abstract = {This paper investigates the effect of different channels of communica- tion on lying behavior. A simple coin flip game with four coin tosses is adapted in which subjects have monetary incentives to misreport their pri- vate information. The treatments differ with respect to the communication channel employed to convey the private information, i.e., face-to face, phone, computer-mediated, and online. Against the hypotheses, the results show that a majority of subjects lies independently of communication channel in use. However, the decision whether to lie either to some or the full extent depends on the communication channel. Compared to more socially- distant communication, direct communication encourages partial lying, but decreases lying to the extreme. Women tend to lie to the full extent only under online communication. Social distance considerations and the prob- ability of being detect lying may drive observed behavioral patterns. The findings highlight the relevance of lying costs in relation to the decision making environment.},
	author = {Conrads, Julian},
	month = nov,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\N5KZFP4M\\Conrads - 2014 - The Effect of Communication Channels on Lying.pdf:application/pdf},
}

@book{conrads_effect_2014-1,
	title = {The {Effect} of {Communication} {Channels} on {Promise}-{Making} and {Promise}-{Keeping}},
	abstract = {This paper investigates the effect of different communication channels on promise-making
and promise-keeping in a helping situation. Four treatments differ with respect to the
communication channel employed to solicit unincentivized cooperation, i.e., face-to-face,
phone call and two different sorts of computer-mediated communication. The less
anonymous (face-to-face, phone) the interpersonal interaction is due to the different
communication channels, the higher the propensity of an agent to make a promise.
Treatment effects, however, vanish if we then look at the actual promise-keeping rates
across treatments as more anonymous channels (computer-mediated) do not perform
relatively worse than more direct channels.

JEL Classification: D02, D83, C91

Keywords: promises, communication, experimental economics, organizational behavior,
behavioral ethics},
	author = {Conrads, Julian and Reggiani, Tommaso},
	month = oct,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\NSVQL7F6\\Conrads and Reggiani - 2014 - The Effect of Communication Channels on Promise-Ma.pdf:application/pdf},
}

@article{herrnansky_compensation_1991,
	title = {Compensation for the effect of the communication channel in auditory-like analysis of speech ({RASTA}-{PLP})},
	journal = {Proc. Eurospeech},
	author = {Herrnansky, H. and Morgan, Nathaniel and Bayya, A. and Kohn, P.},
	month = jan,
	year = {1991},
}

@inproceedings{hermansky_rasta-plp_1992,
	address = {San Francisco, CA, USA},
	title = {{RASTA}-{PLP} speech analysis technique},
	isbn = {978-0-7803-0532-8},
	url = {http://ieeexplore.ieee.org/document/225957/},
	doi = {10.1109/ICASSP.1992.225957},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hermansky, H. and Morgan, N. and Bayya, A. and Kohn, P.},
	year = {1992},
	pages = {121--124 vol.1},
}

@inproceedings{hermansky_recognition_1993,
	address = {Minneapolis, MN, USA},
	title = {Recognition of speech in additive and convolutional noise based on {RASTA} spectral processing},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319236/},
	doi = {10.1109/ICASSP.1993.319236},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hermansky, H. and Morgan, N. and Hirsch, H.-G.},
	year = {1993},
	pages = {83--86 vol.2},
}

@inproceedings{hermansky_compensation_1991,
	title = {Compensation for the effect of the communication channel in auditory-like analysis of speech ({RASTA}-{PLP})},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/hermansky91b_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-312},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Hermansky, Hynek and Morgan, Nelson and Bayya, Aruna and Kohn, Phil},
	month = sep,
	year = {1991},
	pages = {1367--1370},
}

@inproceedings{hirsch_improved_1991,
	title = {Improved speech recognition using high-pass filtering of subband envelopes},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/hirsch91_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-105},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Hirsch, H. G. and Meyer, Peter and Ruehl, Hans-Wilhelm},
	month = sep,
	year = {1991},
	pages = {413--416},
}

@article{lu_sub-band_2011,
	title = {Sub-band temporal modulation envelopes and their normalization for automatic speech recognition in reverberant environments},
	volume = {25},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230810000677},
	doi = {10.1016/j.csl.2010.10.002},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Lu, Xugang and Unoki, Masashi and Nakamura, Satoshi},
	month = jul,
	year = {2011},
	pages = {571--584},
}

@article{vicente-pena_band-pass_2006,
	title = {Band-pass filtering of the time sequences of spectral parameters for robust wireless speech recognition},
	volume = {48},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639306000872},
	doi = {10.1016/j.specom.2006.07.007},
	language = {en},
	number = {10},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Vicente-Peña, J. and Gallardo-Antolín, A. and Peláez-Moreno, C. and Díaz-de-María, F.},
	month = oct,
	year = {2006},
	pages = {1379--1398},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\WQ2H7S55\\Vicente-Peña et al. - 2006 - Band-pass filtering of the time sequences of spect.pdf:application/pdf},
}

@inproceedings{do_improved_2017,
	title = {Improved {Automatic} {Speech} {Recognition} {Using} {Subband} {Temporal} {Envelope} {Features} and {Time}-{Delay} {Neural} {Network} {Denoising} {Autoencoder}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/do17c_interspeech.html},
	doi = {10.21437/Interspeech.2017-1096},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Do, Cong-Thanh and Stylianou, Yannis},
	month = aug,
	year = {2017},
	pages = {3832--3836},
}

@incollection{paliwal_robust_2010,
	title = {Robust {Speech} {Recognition} {Under} {Noisy} {Ambient} {Conditions}},
	isbn = {978-0-12-374708-2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123747082000061},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Human-{Centric} {Interfaces} for {Ambient} {Intelligence}},
	publisher = {Elsevier},
	author = {Paliwal, Kuldip K. and Yao, Kaisheng},
	year = {2010},
	doi = {10.1016/B978-0-12-374708-2.00006-1},
	pages = {135--162},
}

@inproceedings{paliwal_neural_1990,
	address = {Albuquerque, NM, USA},
	title = {Neural net classifiers for robust speech recognition under noisy environments},
	url = {http://ieeexplore.ieee.org/document/115737/},
	doi = {10.1109/ICASSP.1990.115737},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Paliwal, K.K.},
	year = {1990},
	pages = {429--432},
}

@inproceedings{anglade_speech_1993,
	address = {Minneapolis, MN, USA},
	title = {Speech discrimination in adverse conditions using acoustic knowledge and selectively trained neural networks},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319290/},
	doi = {10.1109/ICASSP.1993.319290},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Anglade, Y. and Fohr, D. and Junqua, J.-C.},
	year = {1993},
	pages = {279--282 vol.2},
}

@inproceedings{anglade_selectively_1992,
	title = {Selectively trained neural networks for the discrimination of normal and lombard speech},
	url = {https://www.isca-speech.org/archive/icslp_1992/anglade92_icslp.html},
	doi = {10.21437/ICSLP.1992-175},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Anglade, Yolande and Fohr, Dominique and Junqua, Jean-Claude},
	month = oct,
	year = {1992},
	pages = {595--598},
}

@article{erell_filterbank-energy_1993,
	title = {Filterbank-energy estimation using mixture and {Markov} models for recognition of noisy speech},
	volume = {1},
	issn = {10636676},
	url = {http://ieeexplore.ieee.org/document/221385/},
	doi = {10.1109/89.221385},
	number = {1},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Erell, A. and Weintraub, M.},
	month = jan,
	year = {1993},
	pages = {68--76},
}

@article{green_auditory_1995,
	title = {Auditory {Scene} {Analysis} {And} {Hidden} {Markov} {Model} {Recognition} {Of} {Speech} {In} {Noise}},
	abstract = {We describe a novel paradigm for automatic speech recognition in noisy environments in which an initial stage of auditory scene analysis separates out the evidence for the speech to be recognised from the evidence for other sounds. In general, this evidence will be incomplete, since intruding sound sources will dominate some spectro-temporal regions. We generalise continuous-density hidden Markov model recognition to this `occluded speech' case. The technique is based on estimating the probability that a Gaussian mixture density distribution for an auditory firing rate map will generate an observation such that the separated components are at their observed values and the remaining components are not greater than their values in the acoustic mixture. Experiments on isolated digit recognition in noise demonstrate the potential of the new approach to yield performance comparable to that of listeners. 1. AUDITORY SCENE ANALYSIS AS A PREPROCESSOR FOR SPEECH RECOGNITION Auditory scene an...},
	author = {Green, Phil and Cooke, M.P. and Crawford, M.},
	month = mar,
	year = {1995},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\F5ZA4LBP\\Green et al. - 1995 - Auditory Scene Analysis And Hidden Markov Model Re.pdf:application/pdf},
}

@article{lee_information-theoretic_1991,
	title = {Information-theoretic distortion measures for speech recognition},
	volume = {39},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/80815/},
	doi = {10.1109/78.80815},
	number = {2},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Signal Processing},
	author = {Lee, Y.-T.},
	month = feb,
	year = {1991},
	pages = {330--335},
}

@article{nocerino_comparative_1985,
	title = {Comparative study of several distortion measures for speech recognition},
	volume = {4},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167639385900573},
	doi = {10.1016/0167-6393(85)90057-3},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Nocerino, N. and Soong, F.K. and Rabiner, L.R. and Klatt, D.H.},
	month = dec,
	year = {1985},
	pages = {317--331},
}

@article{nakamura_robust_1995,
	title = {Robust {Word} {Spotting} {In} {Adverse} {Car} {Environments}},
	abstract = {This paper presents a novel word recognition technique which allows hand-free dialing under adverse car environments. The algorithm is based on speaker dependent word spotting. The paper compared and investigated the methods of spectral subtraction, short time modified coherence, multi-microphone, dynamic and accelerated features, weighted distance measures and multi-templates by word recognition experiments. The experiments are carried out using real speech database uttered in adverse car environments. The experiments show that the method using sinusoidal weighted lifter and the method using multi-templates are robust against the adverse environments. Keywords: word spotting, distance measure, adverse environment 1. INTRODUCTION Recent technology realizes a small size mobile cellular telephone. This enables easy communication to anybody at any time on the way. However, it has a problem when we use the telephone while driving. Dialing and a telephone call while driving are very dang...},
	author = {Nakamura, Satoshi and Akabane, Toshio and Hamaguchi, Seiji},
	month = dec,
	year = {1995},
}

@article{nakamura_robust_1993,
	title = {Robust word recognition in adverse car environments},
	abstract = {This paper presents a novel word recognition technique which allows hand-free dialing in adverse car environments. The algorithm is based on speaker dependent word spotting. The paper compared and investigated such methods as spectral subtraction, short time modified coherence, effects of multi-microphone, dynamic and accelerated features, weighted distance measures and multi-templates by word recognition experiments. The experiments which were carried out using real speech database uttered in adverse car environments showed that the proposed method using the cepstrum weighted lifter and multi-templates achieves 98\% word recognition accuracy in 80km/h driving car environments.},
	author = {Nakamura, Satoshi and Akabane, Toshio and Hamaguchi, Seiji and Kitoh, Atsunori},
	month = nov,
	year = {1993},
	pages = {9--14},
}

@inproceedings{nakamura_robust_1993-1,
	title = {Robust word spotting in adverse car environments},
	url = {https://www.isca-speech.org/archive/eurospeech_1993/nakamura93_eurospeech.html},
	doi = {10.21437/Eurospeech.1993-254},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {3rd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1993)},
	publisher = {ISCA},
	author = {Nakamura, Satoshi and Akabane, Toshio and Hamaguchi, Seiji},
	month = sep,
	year = {1993},
	pages = {1045--1048},
}

@article{kanazawa_learning_1989,
	title = {A learning word‐spotting method for speaker‐independent word recognition in noisy environments},
	volume = {86},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2027643},
	doi = {10.1121/1.2027643},
	language = {en},
	number = {S1},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kanazawa, Hiroshi and Takebayashi, Yoichi},
	month = nov,
	year = {1989},
	pages = {S76--S76},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\BAQBJT22\\Kanazawa and Takebayashi - 1989 - A learning word‐spotting method for speaker‐indepe.pdf:application/pdf},
}

@article{mansour_short-time_1989,
	title = {The short-time modified coherence representation and noisy speech recognition},
	volume = {37},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/28053/},
	doi = {10.1109/ASSP.1989.28053},
	language = {en},
	number = {6},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Mansour, D. and Juang, B.H.},
	month = jun,
	year = {1989},
	pages = {795--804},
}

@article{kosaka_noisy_2006,
	title = {Noisy speech recognition based on codebook normalization of discrete‐mixture hidden {Markov} models},
	volume = {120},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4787211},
	doi = {10.1121/1.4787211},
	language = {en},
	number = {5},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kosaka, Tetsuo and Models, hidden Markov and Katoh, Masaharu and Kohda, Masaki},
	month = nov,
	year = {2006},
	pages = {3041--3041},
}

@inproceedings{mansour_short-time_1988,
	address = {New York, NY, USA},
	title = {The short-time modified coherence representation and its application for noisy speech recognition},
	url = {http://ieeexplore.ieee.org/document/196635/},
	doi = {10.1109/ICASSP.1988.196635},
	urldate = {2023-02-18},
	booktitle = {{ICASSP}-88., {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Mansour, D. and Juang, B.H.},
	year = {1988},
	pages = {525--528},
}

@article{bhatt_comparative_2010,
	title = {{COMPARATIVE} {STUDY} {OF} {SPEECH} {RECOGNITION} {SYSTEM} {USING} {VARIOUS} {FEATURE} {EXTRACTION} {TECHNIQUES}},
	abstract = {It is very important to detect the speech endpoints accurately in speech recognition. This paper presents a comparative analysis of various feature extraction techniques of endpoint detection in speech recognition of isolated words in noisy environments. The endpoint detection problem is nontrivial for no stationary backgrounds where artifacts (i.e., no speech events) may be introduced by the speaker, the recording environment, and the transmission system. An optimum set of characteristics is identified by combining parameters from both time domain and frequency domain, in a robust approach for identification when the speech signal is corrupted by additive noise and channel distortion. The cases of colored noises such as babble noise, factory noise at different SNR values in conjunction with distortions due to recording medium were tested. Experimental results identify the optimal algorithm which significantly achieves the highest performance in the recognition task.},
	author = {Bhatt, Kapil and SINHA, RK},
	month = dec,
	year = {2010},
}

@inproceedings{jgmenalssandovalrggomez_comparative_1990,
	address = {Barcelona, Spain},
	title = {A comparative study of feature extraction methods for noisy speech recognition},
	abstract = {L.torres, E.Masgrau, and M.A.Lagunas},
	booktitle = {Signal processing {V} : theories and applications : proceedings of  {Fifth} {EUSIPCO}},
	author = {J.G.Mena,L.S.Sandoval,R.G.Gomez},
	month = sep,
	year = {1990},
	pages = {1191--1194},
}

@article{gao_auditory_1993,
	title = {Auditory model based speech recognition and comparison with other methods},
	volume = {21},
	abstract = {On the basis of the periphery auditory model and partial control auditory neural processing model set up in reference (1), a speech recognizer using an auditory model as the acoustic front-end preprocessor and a monotopical organized neural network as the recognition classifier have been built. The experiments show that the parameters derived from the auditory model are a good representation of speech discrimination, especially in noisy environments. The results of speech recognition show that under the condition of 3dB background noise with the same neural network as the classifier, the recognition rate of 3 confusable consonants(p.t.k) is 80.3\% for auditory model as the front-end processor and 69.2\% for LPC-derived spectrum as speech parameters, respectively.},
	author = {Gao, Yuqing and Huang, Taiyi and Chen, Shaoyan},
	month = oct,
	year = {1993},
	pages = {1--6},
}

@inproceedings{hermansky_perceptually_1985,
	address = {Tampa, FL, USA},
	title = {Perceptually based linear predictive analysis of speech},
	volume = {10},
	url = {http://ieeexplore.ieee.org/document/1168384/},
	doi = {10.1109/ICASSP.1985.1168384},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '85. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Hermansky, H. and Hanson, B. and Wakita, H.},
	year = {1985},
	pages = {509--512},
}

@article{cheng_speech_1991,
	title = {Speech enhancement based conceptually on auditory evidence},
	volume = {39},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/134427/},
	doi = {10.1109/78.134427},
	number = {9},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Signal Processing},
	author = {Cheng, Y.M. and O'Shaughnessy, D.},
	month = sep,
	year = {1991},
	pages = {1943--1954},
}

@inproceedings{cheng_speech_1991-1,
	address = {Toronto, Ont., Canada},
	title = {Speech enhancement based conceptually on auditory evidence},
	isbn = {978-0-7803-0003-3},
	url = {http://ieeexplore.ieee.org/document/150500/},
	doi = {10.1109/ICASSP.1991.150500},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP} 91: 1991 {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Cheng, Y.M. and O'Shaughnessy, D.},
	year = {1991},
	pages = {961--964 vol.2},
}

@book{toner_speech_1993,
	title = {Speech enhancement based conceptually on auditory processingutilising correlation measurements},
	abstract = {An adaptive subband multisensor structure for speech enhancement is proposed, its conceptual basis being the accepted model of the cochlea as a spectrum analyser. Furthermore, the type of subband process implemented is dependent on subband correlation. The convergence of the proposed method is compared with conventional LMS and frequency domain LMS and a dramatic increase in convergence rate is shown using both simulated and real data},
	author = {Toner, E. and Campbell, D.R.},
	month = mar,
	year = {1993},
	note = {Pages: 2/5},
}

@article{arehart_speech_2001,
	title = {Speech enhancement based on aspects of the auditory process},
	volume = {109},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4744642},
	doi = {10.1121/1.4744642},
	language = {en},
	number = {5},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Arehart, Kathryn and Hansen, John},
	month = may,
	year = {2001},
	pages = {2440--2440},
}

@inproceedings{nandkumar_speech_1994,
	address = {Adelaide, SA, Australia},
	title = {Speech enhancement based on a new set of auditory constrained parameters},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389370/},
	doi = {10.1109/ICASSP.1994.389370},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Nandkumar, S. and Hansen, J.H.L.},
	year = {1994},
	pages = {I/1--I/4},
}

@article{ghitza_auditory_1986,
	title = {Auditory nerve representation as a front-end for speech recognition in a noisy environment},
	volume = {1},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230886800183},
	doi = {10.1016/S0885-2308(86)80018-3},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Ghitza, Oded},
	month = dec,
	year = {1986},
	pages = {109--130},
}

@incollection{ghitza_auditory_1992,
	title = {Auditory {Nerve} {Representation} as a {Basis} for {Speech} {Processing}},
	author = {Ghitza, Oded},
	month = jan,
	year = {1992},
}

@inproceedings{dobrin_speech_1995,
	title = {Speech recognition experiments in a noisy environment using auditory system modelling},
	url = {https://www.isca-speech.org/archive/eurospeech_1995/dobrin95_eurospeech.html},
	doi = {10.21437/Eurospeech.1995-36},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {4th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1995)},
	publisher = {ISCA},
	author = {Dobrin, Cristina and Haavisto, Petri and Laurila, Kari and Astola, Jaakko},
	month = sep,
	year = {1995},
	pages = {131--134},
}

@inproceedings{wu_modeling_1990,
	address = {Albuquerque, NM, USA},
	title = {Modeling spectral processing in the central auditory system},
	url = {http://ieeexplore.ieee.org/document/115693/},
	doi = {10.1109/ICASSP.1990.115693},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Wu, Z.L. and Schwartz, J.-L. and Escudier, P.},
	year = {1990},
	pages = {373--376},
}

@article{dautrich_effects_1983,
	title = {On the effects of varying filter bank parameters on isolated word recognition},
	volume = {31},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164172/},
	doi = {10.1109/TASSP.1983.1164172},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Dautrich, B. and Rabiner, L. and Martin, T.},
	month = aug,
	year = {1983},
	pages = {793--807},
}

@article{furui_toward_2005,
	title = {Toward {Robust} {Speech} {Recognition} and {Understanding}},
	volume = {41},
	issn = {0922-5773},
	url = {http://link.springer.com/10.1007/s11265-005-4149-x},
	doi = {10.1007/s11265-005-4149-x},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Journal of VLSI signal processing systems for signal, image and video technology},
	author = {Furui, Sadaoki},
	month = nov,
	year = {2005},
	pages = {245--254},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\VEBF5ANZ\\Furui - 2005 - Toward Robust Speech Recognition and Understanding.pdf:application/pdf},
}

@incollection{goos_toward_2003,
	address = {Berlin, Heidelberg},
	title = {Toward {Robust} {Speech} {Recognition} and {Understanding}},
	volume = {2807},
	isbn = {978-3-540-20024-6 978-3-540-39398-6},
	url = {http://link.springer.com/10.1007/978-3-540-39398-6_2},
	urldate = {2023-02-18},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer Berlin Heidelberg},
	author = {Furui, Sadaoki},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Matoušek, Václav and Mautner, Pavel},
	year = {2003},
	doi = {10.1007/978-3-540-39398-6_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {2--11},
}

@article{stern_signal_1999,
	title = {Signal {Processing} {For} {Robust} {Speech} {Recognition}},
	abstract = {This chapter compares several di\#erent approaches to robust automatic speech recognition. We review ongoing research in the use of acoustical pre-processing to achieve robust speech recognition, discussing and comparing approaches based on direct cepstral comparisons, on parametric models of environmental degradation, and on cepstral high-pass \#ltering. We also describe and compare the e\#ectiveness of two complementary methods of signal processing for robust speech recognition: microphone array processing and the use of physiologically-motivated models of peripheral auditory processing. This chapter includes comparisons of recognition error rates obtained when the various signal processing algorithms considered are used to process inputs to CMU's SPHINX speech recognition system. 1 INTRODUCTION The development of robust speech recognition systems that maintain a high level of recognition accuracy in di\#cult and dynamically-varying acoustical environments is becoming increasingly impo...},
	author = {Stern, Richard and Acero, Ro},
	month = mar,
	year = {1999},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\KE3QDV3H\\Stern and Acero - 1999 - Signal Processing For Robust Speech Recognition.pdf:application/pdf},
}

@inproceedings{sfurui_toward_1992,
	address = {Cannes,France},
	title = {Toward robust speech recognition under adverse conditions},
	booktitle = {Proc. {Speech} {Processing} in {Adverse} {Conditions}},
	author = {S.Furui},
	year = {1992},
	pages = {31--41},
}

@inproceedings{takebayashi_robust_1991,
	address = {Toronto, Ont., Canada},
	title = {A robust speech recognition system using word-spotting with noise immunity learning},
	isbn = {978-0-7803-0003-3},
	url = {http://ieeexplore.ieee.org/document/150486/},
	doi = {10.1109/ICASSP.1991.150486},
	urldate = {2023-02-18},
	booktitle = {[{Proceedings}] {ICASSP} 91: 1991 {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Takebayashi, Y. and Tsuboi, H. and Kanazawa, H.},
	year = {1991},
	pages = {905--908 vol.2},
}

@inproceedings{takebayashi_adaptive_1994,
	address = {Adelaide, SA, Australia},
	title = {Adaptive noise immunity learning for word spotting},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389259/},
	doi = {10.1109/ICASSP.1994.389259},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Takebayashi, Y. and Kanazawa, H.},
	year = {1994},
	pages = {I/449--I/452},
}

@inproceedings{htsuboihkanazawaytakebayashi_accelerator_1990,
	title = {An accelerator for high-speech spoken word spotting and noise immunity learning system},
	url = {https://www.isca-speech.org/archive/icslp_1990/tsuboi90_icslp.html},
	author = {H.Tsuboi,H.Kanazawa,Y.Takebayashi},
	year = {1990},
	pages = {273--276},
}

@inproceedings{sankar_noise_1994,
	address = {Adelaide, SA, Australia},
	title = {Noise immunization using neural net for speech recognition},
	volume = {ii},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389563/},
	doi = {10.1109/ICASSP.1994.389563},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Sankar, R. and Patravali, S.},
	year = {1994},
	pages = {II/685--II/688},
}

@inproceedings{kitamura_speaker-independent_1992,
	title = {Speaker-independent spoken digit recognition in noisy environments using dynamic spectral features and neural networks},
	url = {https://www.isca-speech.org/archive/icslp_1992/kitamura92_icslp.html},
	doi = {10.21437/ICSLP.1992-237},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Kitamura, Tadashi and Ando, Satoshi and Hayahara, Etsuro},
	month = oct,
	year = {1992},
	pages = {699--702},
}

@inproceedings{wu_deep_2015-1,
	address = {South Brisbane, Queensland, Australia},
	title = {Deep neural networks employing {Multi}-{Task} {Learning} and stacked bottleneck features for speech synthesis},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178814/},
	doi = {10.1109/ICASSP.2015.7178814},
	urldate = {2023-02-18},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Zhizheng and Valentini-Botinhao, Cassia and Watts, Oliver and King, Simon},
	month = apr,
	year = {2015},
	pages = {4460--4464},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\STRY8WPV\\Wu et al. - 2015 - Deep neural networks employing Multi-Task Learning.pdf:application/pdf},
}

@inproceedings{lippmann_multi-style_1987,
	address = {Dallas, TX, USA},
	title = {Multi-style training for robust isolated-word speech recognition},
	volume = {12},
	url = {http://ieeexplore.ieee.org/document/1169544/},
	doi = {10.1109/ICASSP.1987.1169544},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '87. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Lippmann, R. and Martin, E. and Paul, D.},
	year = {1987},
	pages = {705--708},
}

@inproceedings{mizuta_optimal_1992,
	title = {Optimal discriminative training for {HMMs} to recognize noisy speech},
	url = {https://www.isca-speech.org/archive/icslp_1992/mizuta92_icslp.html},
	doi = {10.21437/ICSLP.1992-194},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Mizuta, Shinobu and Nakajima, Kunio},
	month = oct,
	year = {1992},
	pages = {1519--1522},
}

@article{lee_study_1991,
	title = {A study on speaker adaptation of the parameters of continuous density hidden {Markov} models},
	volume = {39},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/80902/},
	doi = {10.1109/78.80902},
	number = {4},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Signal Processing},
	author = {Lee, C.-H. and Lin, C.-H. and Juang, B.-H.},
	month = apr,
	year = {1991},
	pages = {806--814},
}

@article{ferguson_variable_1979,
	title = {Variable duration models for speech},
	journal = {Symposium on the Application of Hidden Markov Models to Text and Speech},
	author = {Ferguson, J.},
	month = nov,
	year = {1979},
}

@inproceedings{russell_experimental_1987,
	address = {Dallas, TX, USA},
	title = {Experimental evaluation of duration modelling techniques for automatic speech recognition},
	url = {http://ieeexplore.ieee.org/document/1169918/},
	doi = {10.1109/ICASSP.1987.1169918},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '87. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Russell, M. and Cook, A.},
	year = {1987},
	pages = {2376--2379},
}

@inproceedings{nnicolseulermfalkhausenhreiningerdwolfjzinke_improving_1992,
	title = {Improving the robustness of automatic speech recognisers using state duration information},
	booktitle = {Proc. {Speech} {Processing} in {Adverse} {Conditions}},
	author = {N.Nicol,S.Euler,M.Falkhausen,H.Reininger,D.Wolf,J.Zinke},
	year = {1992},
	pages = {183--186},
}

@article{noyes_automatic_1989,
	title = {Automatic speech recognition for disabled people},
	volume = {20},
	issn = {00036870},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0003687089901932},
	doi = {10.1016/0003-6870(89)90193-2},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Applied Ergonomics},
	author = {Noyes, J.M. and Haigh, R. and Starr, A.F.},
	month = dec,
	year = {1989},
	pages = {293--298},
}

@inproceedings{mosner_improving_2019,
	address = {Brighton, United Kingdom},
	title = {Improving {Noise} {Robustness} of {Automatic} {Speech} {Recognition} via {Parallel} {Data} and {Teacher}-student {Learning}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683422/},
	doi = {10.1109/ICASSP.2019.8683422},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Mosner, Ladislav and Wu, Minhua and Raju, Anirudh and Krishnan Parthasarathi, Sree Hari and Kumatani, Kenichi and Sundaram, Shiva and Maas, Roland and Hoffmeister, Bjorn},
	month = may,
	year = {2019},
	pages = {6475--6479},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\K3BKCUP5\\Mosner et al. - 2019 - Improving Noise Robustness of Automatic Speech Rec.pdf:application/pdf},
}

@inproceedings{varga_hidden_1990,
	address = {Albuquerque, NM, USA},
	title = {Hidden {Markov} model decomposition of speech and noise},
	url = {http://ieeexplore.ieee.org/document/115970/},
	doi = {10.1109/ICASSP.1990.115970},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Varga, A.P. and Moore, R.K.},
	year = {1990},
	pages = {845--848},
}

@book{gales_improved_1992,
	title = {An {Improved} {Approach} to the {Hidden} {Markov} {Model} {Decomposition} of {Speech} and {Noise}},
	abstract = {This paper addresses the problem of automatic speech recognition in the presence of interfering noise. The new approach described decomposes the contaminated speech signal using a generalisation of standard Hidden Markov Modelling, whilst utilising a compact and effective parametrisation of the speech signal. The technique is compared to some existing noise compensation techniques, using data recorded in noise, and is found to have improved performance compared to existing model decomposition techniques. Performance is comparable to existing noise subtraction techniques, but the technique is applicable to a wider range of noise environments and is not dependent on an accurate end pointing of the speech.},
	author = {Gales, M.J.F. and Young, Steve},
	month = mar,
	year = {1992},
	note = {Pages: 236},
}

@article{gales_cepstral_1993,
	title = {Cepstral parameter compensation for {HMM} recognition in noise},
	volume = {12},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016763939390093Z},
	doi = {10.1016/0167-6393(93)90093-Z},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Gales, M.J.F. and Young, S.J.},
	month = jul,
	year = {1993},
	pages = {231--239},
}

@inproceedings{kobayashi_markov_1994,
	address = {Adelaide, SA, Australia},
	title = {Markov model based noise modeling and its application to noisy speech recognition using dynamical features of speech},
	volume = {ii},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389719/},
	doi = {10.1109/ICASSP.1994.389719},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Kobayashi, T. and Mine, R. and Shirai, K.},
	year = {1994},
	pages = {II/57--II/60},
}

@inproceedings{vaseghi_noisy_1994,
	address = {Adelaide, SA, Australia},
	title = {Noisy speech recognition using cepstral-time features and spectral-time filters},
	volume = {ii},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389717/},
	doi = {10.1109/ICASSP.1994.389717},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Vaseghi, S.V. and Milner, B.P. and Humphries, J.J.},
	year = {1994},
	pages = {II/65--II/68},
}

@inproceedings{mokbel_word_1992,
	title = {Word recognition in the car: adapting recognizers to new environments},
	shorttitle = {Word recognition in the car},
	url = {https://www.isca-speech.org/archive/icslp_1992/mokbel92_icslp.html},
	doi = {10.21437/ICSLP.1992-239},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1992)},
	publisher = {ISCA},
	author = {Mokbel, C. and Barbier, L. and Kerlou, Y. and Chollet, Gérard},
	month = oct,
	year = {1992},
	pages = {707--710},
}

@inproceedings{tamura_improvements_1990,
	address = {Albuquerque, NM, USA},
	title = {Improvements to the noise reduction neural network},
	url = {http://ieeexplore.ieee.org/document/115957/},
	doi = {10.1109/ICASSP.1990.115957},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Tamura, S. and Nakamura, M.},
	year = {1990},
	pages = {825--828},
}

@article{van_compernolle_noise_1989,
	title = {Noise adaptation in a hidden {Markov} model speech recognition system},
	volume = {3},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0885230889900272},
	doi = {10.1016/0885-2308(89)90027-2},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Van Compernolle, Dirk},
	month = apr,
	year = {1989},
	pages = {151--167},
}

@inproceedings{berouti_enhancement_1979,
	address = {Washington, DC, USA},
	title = {Enhancement of speech corrupted by acoustic noise},
	volume = {4},
	url = {http://ieeexplore.ieee.org/document/1170788/},
	doi = {10.1109/ICASSP.1979.1170788},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '79. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Berouti, M. and Schwartz, R. and Makhoul, J.},
	year = {1979},
	pages = {208--211},
}

@article{boll_suppression_1979,
	title = {Suppression of acoustic noise in speech using spectral subtraction},
	volume = {27},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1163209/},
	doi = {10.1109/TASSP.1979.1163209},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Boll, S.},
	month = apr,
	year = {1979},
	pages = {113--120},
}

@inproceedings{lockwood_experiments_1991-1,
	title = {Experiments with a non-linear spectral subtractor ({NSS}), hidden {Markov} models and the projection, for robust speech recognition in cars},
	url = {https://www.isca-speech.org/archive/eurospeech_1991/lockwood91_eurospeech.html},
	doi = {10.21437/Eurospeech.1991-16},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2nd {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1991)},
	publisher = {ISCA},
	author = {Lockwood, P. and Boudy, J.},
	month = sep,
	year = {1991},
	pages = {79--82},
}

@inproceedings{flores_continuous_1994,
	address = {Adelaide, SA, Australia},
	title = {Continuous speech recognition in noise using spectral subtraction and {HMM} adaptation},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389269/},
	doi = {10.1109/ICASSP.1994.389269},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Flores, J.A.N. and Young, S.J.},
	year = {1994},
	pages = {I/409--I/412},
}

@inproceedings{klatt_digital_1976,
	address = {Philadelphia, PA, USA},
	title = {A digital filter bank for spectral matching},
	volume = {1},
	url = {http://ieeexplore.ieee.org/document/1170107/},
	doi = {10.1109/ICASSP.1976.1170107},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} '76. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Klatt, D.},
	year = {1976},
	pages = {573--576},
}

@inproceedings{jholmes_noise_1986,
	title = {Noise compensation for speech recognition using probabalistic models},
	author = {J.Holmes and N.Sedgewick},
	year = {1986},
	pages = {741--744},
}

@inproceedings{varga_control_1989,
	title = {Control experiments on noise compensation in hidden {Markov} model based continuous word recognition},
	url = {https://www.isca-speech.org/archive/eurospeech_1989/varga89_eurospeech.html},
	doi = {10.21437/Eurospeech.1989-53},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {First {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1989)},
	publisher = {ISCA},
	author = {Varga, Andrew and Ponting, Keith},
	month = sep,
	year = {1989},
	pages = {1167--1170},
}

@inproceedings{mellor_noise_1993,
	address = {Minneapolis, MN, USA},
	title = {Noise masking in a transform domain},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319237/},
	doi = {10.1109/ICASSP.1993.319237},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Mellor, B.A. and Varga, A.P.},
	year = {1993},
	pages = {87--90 vol.2},
}

@inproceedings{graf_dynamic_1993,
	address = {Minneapolis, MN, USA},
	title = {Dynamic time warping comb filter for the enhancement of speech degraded by white {Gaussian} noise},
	isbn = {978-0-7803-0946-3},
	url = {http://ieeexplore.ieee.org/document/319307/},
	doi = {10.1109/ICASSP.1993.319307},
	urldate = {2023-02-18},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Graf, J.T. and Hubing, N.},
	year = {1993},
	pages = {339--342 vol.2},
}

@article{jae_lim_evaluation_1978,
	title = {Evaluation of an adaptive comb filtering method for enhancing speech degraded by white noise addition},
	volume = {26},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1163117/},
	doi = {10.1109/TASSP.1978.1163117},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {{Jae Lim} and Oppenheim, A. and Braida, L.},
	month = aug,
	year = {1978},
	pages = {354--358},
}

@inproceedings{oshaughnessy_speech_1988,
	address = {New York, NY, USA},
	title = {Speech enhancement using vector quantization and a formant distance measure},
	url = {http://ieeexplore.ieee.org/document/196642/},
	doi = {10.1109/ICASSP.1988.196642},
	urldate = {2023-02-18},
	booktitle = {{ICASSP}-88., {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {O'Shaughnessy, D.},
	year = {1988},
	pages = {549--552},
}

@inproceedings{ygong_base_1993,
	address = {Berlin, Germany},
	title = {Base transformation for environment adaption in continuous speech recognition},
	volume = {3},
	booktitle = {Prococeedings of {European} {Conf}. {Speech} {Communication} and {Technology} 1993},
	author = {Y.Gong},
	year = {1993},
	pages = {2227--2230},
}

@inproceedings{treurniet_noise_1994,
	address = {Adelaide, SA, Australia},
	title = {Noise independent speech recognition for a variety of noise types},
	volume = {i},
	isbn = {978-0-7803-1775-8},
	url = {http://ieeexplore.ieee.org/document/389262/},
	doi = {10.1109/ICASSP.1994.389262},
	urldate = {2023-02-18},
	booktitle = {Proceedings of {ICASSP} '94. {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Treurniet, W.C. and Gong, Y.},
	year = {1994},
	pages = {I/437--I/440},
}

@inproceedings{van_compernolle_spectral_1989,
	address = {Glasgow, UK},
	title = {Spectral estimation using a log-distance error criterion applied to speech recognition},
	url = {http://ieeexplore.ieee.org/document/266414/},
	doi = {10.1109/ICASSP.1989.266414},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {van Compernolle, D.},
	year = {1989},
	pages = {258--261},
}

@article{erell_filterbank-energy_1993-1,
	title = {Filterbank-energy estimation using mixture and {Markov} models for recognition of noisy speech},
	volume = {1},
	issn = {10636676},
	url = {http://ieeexplore.ieee.org/document/221385/},
	doi = {10.1109/89.221385},
	number = {1},
	urldate = {2023-02-18},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Erell, A. and Weintraub, M.},
	month = jan,
	year = {1993},
	pages = {68--76},
}

@inproceedings{ephraim_minimum_1990,
	address = {Albuquerque, NM, USA},
	title = {A minimum mean square error approach for speech enhancement},
	url = {http://ieeexplore.ieee.org/document/115960/},
	doi = {10.1109/ICASSP.1990.115960},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Ephraim, Y.},
	year = {1990},
	pages = {829--832},
}

@article{bregman_auditory_1994,
	title = {\textit{{Auditory} {Scene} {Analysis}: {The} {Perceptual} {Organization} of {Sound}}},
	volume = {95},
	issn = {0001-4966},
	shorttitle = {{\textless}i{\textgreater}{Auditory} {Scene} {Analysis}},
	url = {http://asa.scitation.org/doi/10.1121/1.408434},
	doi = {10.1121/1.408434},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bregman, Albert S. and McAdams, Stephen},
	month = feb,
	year = {1994},
	note = {https://mitpress.mit.edu/9780262521956/auditory-scene-analysis/},
	pages = {1177--1178},
}

@article{brandstein_practical_1995,
	title = {A practical time-delay estimator for localizing speech sources with a microphone array},
	volume = {9},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230885700095},
	doi = {10.1006/csla.1995.0009},
	language = {en},
	number = {2},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Brandstein, Michael S. and Adcock, John E. and Silverman, Harvey F.},
	month = apr,
	year = {1995},
	pages = {153--169},
}

@inproceedings{berthommier_source_1995,
	title = {Source separation by a functional model of amplitude demodulation},
	url = {https://www.isca-speech.org/archive/eurospeech_1995/berthommier95_eurospeech.html},
	doi = {10.21437/Eurospeech.1995-37},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {4th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 1995)},
	publisher = {ISCA},
	author = {Berthommier, Frédéric and Meyer, Georg F.},
	month = sep,
	year = {1995},
	pages = {135--138},
}

@article{langner_periodicity_1988,
	title = {Periodicity coding in the inferior colliculus of the cat. {I}. {Neuronal} mechanisms},
	volume = {60},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.1988.60.6.1799},
	doi = {10.1152/jn.1988.60.6.1799},
	abstract = {1. Temporal properties of single- and multiple-unit responses were investigated in the inferior colliculus (IC) of the barbiturate-anesthetized cat. Approximately 95\% of recording sites were located in the central nucleus of the inferior colliculus (ICC). Responses to contralateral stimulation with tone bursts and amplitude-modulated tones (100\% sinusoidal modulation) were recorded. Five response parameters were determined for neurons at each location: 1) characteristic frequency (CF); 2) onset latency of responses to CF-tones 60 dB above threshold; 3) Q10 dB (CF divided by bandwidth of tuning curve 10 dB above threshold); 4) best modulation frequency for firing rate (rBMF or BMF; amplitude modulation frequency that elicited the highest firing rate); and 5) best modulation frequency for synchronization (sBMF; amplitude modulation frequency that elicited the highest degree of phase-locking to the modulation frequency). 2. Response characteristics for single units and multiple units corresponded closely. A BMF was obtained at almost all recording sites. For units with a similar CF, a range of BMFs was observed. The upper limit of BMF increased approximately proportional to CF/4 up to BMFs as high as 1 kHz. The lower limit of encountered BMFs for a given CF also increased slightly with CF. BMF ranges for single-unit and multiple-unit responses were similar. Twenty-three percent of the responses revealed rBMFs between 10 and 30 Hz, 51\% between 30 and 100 Hz, 18\% between 100 and 300 Hz, and 8\% between 300 and 1000 Hz. 3. For single units with modulation transfer functions of bandpass characteristics, BMFs determined for firing rate and synchronization were similar (r2 = 0.95). 4. Onset latencies for responses to CF tones 60 dB above threshold varied between 4 and 120 ms. Ninety percent of the onset latencies were between 5 and 18 ms. A range of onset latencies was recorded for different neurons with any given CF. The onset response latency of a given unit or unit cluster was significantly correlated with the period of the BMF and the period of the CF (P less than 0.05). 5."Intrinsic oscillations" of short duration, i.e., regularly timed discharges of units in response to stimuli without a corresponding temporal structure, were frequently observed in the ICC. Oscillation intervals were commonly found to be integer multiples of 0.4 ms. Changes of stimulus frequency or intensity had only minor influences on these intrinsic oscillations.(ABSTRACT TRUNCATED AT 400 WORDS)},
	language = {en},
	number = {6},
	urldate = {2023-02-18},
	journal = {Journal of Neurophysiology},
	author = {Langner, G. and Schreiner, C. E.},
	month = dec,
	year = {1988},
	pages = {1799--1822},
}

@book{cooke_modelling_1993,
	address = {Cambridge [England] ; New York},
	series = {Distinguished dissertations in computer science},
	title = {Modelling auditory processing and organisation},
	isbn = {978-0-521-45094-2},
	publisher = {Cambridge University Press},
	author = {Cooke, Martin},
	year = {1993},
	keywords = {Auditory perception, Computer simulation, Speech analysis use of Computers},
}

@article{noauthor_computational_1994,
	title = {Computational auditory scene analysis},
	volume = {8},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230884710163},
	doi = {10.1006/csla.1994.1016},
	abstract = {Although the ability of human listeners to perceptually segregate concurrent sounds is well documented in the literature, there have been few attempts…},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	month = oct,
	year = {1994},
	note = {Publisher: Academic Press},
	pages = {297--336},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\JCDPJ7ZH\\S0885230884710163.html:text/html},
}

@article{brown_computational_1994,
	title = {Computational auditory scene analysis},
	volume = {8},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230884710163},
	doi = {10.1006/csla.1994.1016},
	language = {en},
	number = {4},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Brown, Guy J. and Cooke, Martin},
	month = oct,
	year = {1994},
	note = {https://doi.org/10.1006/csla.1994.1016},
	pages = {297--336},
}

@article{makhoul_state_1995,
	title = {State of the art in continuous speech recognition.},
	volume = {92},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.92.22.9956},
	doi = {10.1073/pnas.92.22.9956},
	abstract = {In the past decade, tremendous advances in the state of the art of automatic speech recognition by machine have taken place. A reduction in the word error rate by more than a factor of 5 and an increase in recognition speeds by several orders of magnitude (brought about by a combination of faster recognition search algorithms and more powerful computers), have combined to make high-accuracy, speaker-independent, continuous speech recognition for large vocabularies possible in real time, on off-the-shelf workstations, without the aid of special hardware. These advances promise to make speech recognition technology readily available to the general public. This paper focuses on the speech recognition advances made through better speech modeling techniques, chiefly through more accurate mathematical modeling of speech sounds.},
	language = {en},
	number = {22},
	urldate = {2023-02-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Makhoul, J and Schwartz, R},
	month = oct,
	year = {1995},
	pages = {9956--9963},
}

@article{gong_speech_1995,
	title = {Speech recognition in noisy environments: {A} survey},
	volume = {16},
	issn = {01676393},
	shorttitle = {Speech recognition in noisy environments},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016763939400059J},
	doi = {10.1016/0167-6393(94)00059-J},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Speech Communication},
	author = {Gong, Yifan},
	month = apr,
	year = {1995},
	pages = {261--291},
}

@book{junqua_robustness_1996,
	address = {Boston},
	series = {The {Kluwer} international series in engineering and computer science ; {VLSI}, computer architecture and digital signal processing},
	title = {Robustness in automatic speech recognition: fundamentals and applications},
	isbn = {978-0-7923-9646-8},
	shorttitle = {Robustness in automatic speech recognition},
	number = {SECS 341},
	publisher = {Kluwer Academic Publishers},
	author = {Junqua, Jean-Claude and Haton, Jean-Paul},
	year = {1996},
	keywords = {Automatic speech recognition, Signal processing},
}

@article{carrier_automated_2017,
	title = {Automated {Speech} {Recognition} in language learning: {Potential} models, benefits and impact},
	volume = {1},
	issn = {25202073, 2521442X},
	shorttitle = {Automated {Speech} {Recognition} in language learning},
	url = {http://rudn.tlcjournal.org/issues/1(1)-03.html},
	doi = {10.29366/2017tlc.1.1.3},
	number = {1},
	urldate = {2023-02-18},
	journal = {Training Language and Culture},
	author = {Carrier, Michael},
	month = feb,
	year = {2017},
	pages = {46--61},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TAQ53DBZ\\Carrier - 2017 - Automated Speech Recognition in language learning.pdf:application/pdf},
}

@misc{noauthor_14_nodate,
	title = {(14) {Automatic} speech recognition for second language learning: how and why it actually works {\textbar} {Helmer} {Strik} - {Academia}.edu},
	url = {https://www.academia.edu/19192008/Automatic_speech_recognition_for_second_language_learning_how_and_why_it_actually_works},
	urldate = {2023-02-18},
}

@inproceedings{neri_automatic_2003,
	title = {Automatic {Speech} {Recognition} for second language learning: {How} and why it actually works},
	shorttitle = {Automatic {Speech} {Recognition} for second language learning},
	url = {https://www.semanticscholar.org/paper/Automatic-Speech-Recognition-for-second-language-it-Neri-Cucchiarini/82a4f4b67cc509d96e8b04c68f319d454022673c},
	abstract = {In this paper, we examine various studies and reviews on the usability of Automatic Speech Recognition (ASR) technology as a tool t o train pronunciation in the second language (L2). We show that part of the criticism that has been addressed to this technology is not warranted, being rather the result of limited familiarity with ASR technology and with broader Computer Assisted Language Learning (CALL) courseware design matters. In our analysis we also consider actual problems of state-of-the-art ASR technology, with a view to indicating h ow ASR can be employed to develop courseware that is both pedagogically sound and reliable.},
	urldate = {2023-02-18},
	author = {Neri, A. and Cucchiarini, C. and Strik, H.},
	year = {2003},
	annote = {[TLDR] This paper examines various studies and reviews of Automatic Speech Recognition technology as a tool to train pronunciation in the second language (L2) and considers actual problems of state-of-the-art ASR technology, with a view to indicating ASR can be employed to develop courseware that is both pedagogically sound and reliable.},
}

@inproceedings{khan_server_2008,
	address = {Naples, Italy},
	title = {A {Server} {Based} {ASR} {Approach} to {Automated} {Cryptanalysis} of {Two} {Time} {Pads} in {Case} of {Speech}},
	isbn = {978-0-7695-3324-7},
	url = {http://ieeexplore.ieee.org/document/4627070/},
	doi = {10.1109/IAS.2008.14},
	urldate = {2023-02-18},
	booktitle = {2008 {The} {Fourth} {International} {Conference} on {Information} {Assurance} and {Security}},
	publisher = {IEEE},
	author = {Khan, Liaqat Ali and Baig, Muhammad Shamim},
	month = sep,
	year = {2008},
	pages = {103--107},
}

@inproceedings{zhang_adversarial_2022,
	address = {Guilin, China},
	title = {Adversarial {Example} {Attacks} against {ASR} {Systems}: {An} {Overview}},
	isbn = {978-1-66547-480-1},
	shorttitle = {Adversarial {Example} {Attacks} against {ASR} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9900191/},
	doi = {10.1109/DSC55868.2022.00071},
	urldate = {2023-02-18},
	booktitle = {2022 7th {IEEE} {International} {Conference} on {Data} {Science} in {Cyberspace} ({DSC})},
	publisher = {IEEE},
	author = {Zhang, Xiao and Tan, Hao and Huang, Xuan and Zhang, Denghui and Tang, Keke and Gu, Zhaoquan},
	month = jul,
	year = {2022},
	pages = {470--477},
}

@incollection{yu_adversarial_2020,
	address = {Singapore},
	title = {Adversarial {Examples} {Attack} and {Countermeasure} for {Speech} {Recognition} {System}: {A} {Survey}},
	volume = {1268},
	isbn = {9789811591280 9789811591297},
	shorttitle = {Adversarial {Examples} {Attack} and {Countermeasure} for {Speech} {Recognition} {System}},
	url = {https://link.springer.com/10.1007/978-981-15-9129-7_31},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Security and {Privacy} in {Digital} {Economy}},
	publisher = {Springer Singapore},
	author = {Wang, Donghua and Wang, Rangding and Dong, Li and Yan, Diqun and Zhang, Xueyuan and Gong, Yongkang},
	editor = {Yu, Shui and Mueller, Peter and Qian, Jiangbo},
	year = {2020},
	doi = {10.1007/978-981-15-9129-7_31},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {443--468},
}

@article{hu_adversarial_2019,
	title = {Adversarial {Examples} for {Automatic} {Speech} {Recognition}: {Attacks} and {Countermeasures}},
	volume = {57},
	issn = {0163-6804, 1558-1896},
	shorttitle = {Adversarial {Examples} for {Automatic} {Speech} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/8809565/},
	doi = {10.1109/MCOM.2019.1900006},
	number = {10},
	urldate = {2023-02-18},
	journal = {IEEE Communications Magazine},
	author = {Hu, Shengshan and Shang, Xingcan and Qin, Zhan and Li, Minghui and Wang, Qian and Wang, Cong},
	month = oct,
	year = {2019},
	pages = {120--126},
}


@misc{noauthor_1_2023,
	title = {(1) ({PDF}) {ChatGpt}: {Open} {Possibilities}},
	shorttitle = {(1) ({PDF}) {ChatGpt}},
	url = {https://www.researchgate.net/publication/367964719_ChatGpt_Open_Possibilities},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2023-02-18},
	journal = {ResearchGate},
	month = feb,
	year = {2023},
}

@misc{noauthor_1_nodate,
	title = {(1) ({PDF}) {ChatGPT}: {Exploring} the {Role} of {Cybersecurity} in the {Protection} of {Medical} {Information}},
	url = {https://www.researchgate.net/publication/367654480_ChatGPT_Exploring_the_Role_of_Cybersecurity_in_the_Protection_of_Medical_Information},
	urldate = {2023-02-18},
	file = {(1) (PDF) ChatGPT\: Exploring the Role of Cybersecurity in the Protection of Medical Information:C\:\\Users\\DELL\\Zotero\\storage\\9K2IUR9X\\367654480_ChatGPT_Exploring_the_Role_of_Cybersecurity_in_the_Protection_of_Medical_Information.html:text/html},
}

@inproceedings{yang_characterizing_2020,
	address = {Barcelona, Spain},
	title = {Characterizing {Speech} {Adversarial} {Examples} {Using} {Self}-{Attention} {U}-{Net} {Enhancement}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9053288/},
	doi = {10.1109/ICASSP40776.2020.9053288},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Yang, Chao-Han and Qi, Jun and Chen, Pin-Yu and Ma, Xiaoli and Lee, Chin-Hui},
	month = may,
	year = {2020},
	pages = {3107--3111},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\PD7PG5WJ\\Yang et al. - 2020 - Characterizing Speech Adversarial Examples Using S.pdf:application/pdf},
}

@inproceedings{ma_detecting_2021,
	address = {Toronto, ON, Canada},
	title = {Detecting {Adversarial} {Attacks} on {Audiovisual} {Speech} {Recognition}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413661/},
	doi = {10.1109/ICASSP39728.2021.9413661},
	urldate = {2023-02-18},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
	month = jun,
	year = {2021},
	pages = {6403--6407},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U6R99DNW\\Ma et al. - 2021 - Detecting Adversarial Attacks on Audiovisual Speec.pdf:application/pdf},
}

@inproceedings{desai_digital_2016,
	address = {GHAZIABAD, India},
	title = {Digital {Speech} {Watermarking} for {Authenticity} of {Speaker} in {Speaker} {Recognition} {System}},
	isbn = {978-1-5090-3411-6},
	url = {http://ieeexplore.ieee.org/document/7938894/},
	doi = {10.1109/ICMETE.2016.13},
	urldate = {2023-02-18},
	booktitle = {2016 {International} {Conference} on {Micro}-{Electronics} and {Telecommunication} {Engineering} ({ICMETE})},
	publisher = {IEEE},
	author = {Desai, Nihalkumar and Tahilramani, Nikunj},
	month = sep,
	year = {2016},
	pages = {105--109},
}

@article{errattahi_automatic_2018-1,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2023-02-18},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3AFMY9LS\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@article{ming_union_2001-1,
	title = {Union: a model for partial temporal corruption of speech},
	volume = {15},
	issn = {08852308},
	shorttitle = {Union},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230800901669},
	doi = {10.1006/csla.2000.0166},
	language = {en},
	number = {3},
	urldate = {2023-02-18},
	journal = {Computer Speech \& Language},
	author = {Ming, Ji and Jack Smith, F.},
	month = jul,
	year = {2001},
	pages = {217--231},
}

@inproceedings{nematollahi_digital_2014,
	address = {Kuala Lumpur, Malaysia},
	title = {Digital speech watermarking for anti-spoofing attack in speaker recognition},
	isbn = {978-1-4799-2027-3 978-1-4799-2028-0},
	url = {http://ieeexplore.ieee.org/document/6863080/},
	doi = {10.1109/TENCONSpring.2014.6863080},
	urldate = {2023-02-19},
	booktitle = {2014 {IEEE} {REGION} 10 {SYMPOSIUM}},
	publisher = {IEEE},
	author = {Nematollahi, M. A and Al-Haddad, S. A. R. and Doraisamy, Shyamala and Ranjbari, M.},
	month = apr,
	year = {2014},
	pages = {476--479},
}

@inproceedings{desai_digital_2016-1,
	address = {GHAZIABAD, India},
	title = {Digital {Speech} {Watermarking} for {Authenticity} of {Speaker} in {Speaker} {Recognition} {System}},
	isbn = {978-1-5090-3411-6},
	url = {http://ieeexplore.ieee.org/document/7938894/},
	doi = {10.1109/ICMETE.2016.13},
	urldate = {2023-02-19},
	booktitle = {2016 {International} {Conference} on {Micro}-{Electronics} and {Telecommunication} {Engineering} ({ICMETE})},
	publisher = {IEEE},
	author = {Desai, Nihalkumar and Tahilramani, Nikunj},
	month = sep,
	year = {2016},
	pages = {105--109},
}

@inproceedings{ren_generating_2020,
	address = {Barcelona, Spain},
	title = {Generating and {Protecting} {Against} {Adversarial} {Attacks} for {Deep} {Speech}-{Based} {Emotion} {Recognition} {Models}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9054087/},
	doi = {10.1109/ICASSP40776.2020.9054087},
	urldate = {2023-02-19},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Ren, Zhao and Baird, Alice and Han, Jing and Zhang, Zixing and Schuller, Bjorn},
	month = may,
	year = {2020},
	pages = {7184--7188},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\94DRUPZ5\\Ren et al. - 2020 - Generating and Protecting Against Adversarial Atta.pdf:application/pdf},
}

@article{wang_query-efficient_2023,
	title = {Query-{Efficient} {Adversarial} {Attack} {With} {Low} {Perturbation} {Against} {End}-to-{End} {Speech} {Recognition} {Systems}},
	volume = {18},
	issn = {1556-6013, 1556-6021},
	url = {https://ieeexplore.ieee.org/document/9954062/},
	doi = {10.1109/TIFS.2022.3222963},
	urldate = {2023-02-19},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Wang, Shen and Zhang, Zhaoyang and Zhu, Guopu and Zhang, Xinpeng and Zhou, Yicong and Huang, Jiwu},
	year = {2023},
	pages = {351--364},
}

@article{du_robust_2022,
	title = {Robust {Audio} {Patch} {Attacks} {Using} {Physical} {Sample} {Simulation} and {Adversarial} {Patch} {Noise} {Generation}},
	volume = {24},
	issn = {1520-9210, 1941-0077},
	url = {https://ieeexplore.ieee.org/document/9552529/},
	doi = {10.1109/TMM.2021.3116426},
	urldate = {2023-02-19},
	journal = {IEEE Transactions on Multimedia},
	author = {Du, Xia and Pun, Chi-Man},
	year = {2022},
	pages = {4381--4393},
}

@article{kwon_selective_2020,
	title = {Selective {Audio} {Adversarial} {Example} in {Evasion} {Attack} on {Speech} {Recognition} {System}},
	volume = {15},
	issn = {1556-6013, 1556-6021},
	url = {https://ieeexplore.ieee.org/document/8747397/},
	doi = {10.1109/TIFS.2019.2925452},
	urldate = {2023-02-19},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kwon, Hyun and Kim, Yongchul and Yoon, Hyunsoo and Choi, Daeseon},
	year = {2020},
	pages = {526--538},
}

@inproceedings{wu_semi-black-box_2019,
	address = {Newark, NJ, USA},
	title = {Semi-black-box {Attacks} {Against} {Speech} {Recognition} {Systems} {Using} {Adversarial} {Samples}},
	isbn = {978-1-72812-376-9},
	url = {https://ieeexplore.ieee.org/document/8935789/},
	doi = {10.1109/DySPAN.2019.8935789},
	urldate = {2023-02-19},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Dynamic} {Spectrum} {Access} {Networks} ({DySPAN})},
	publisher = {IEEE},
	author = {Wu, Yi and Liu, Jian and Chen, Yingying and Cheng, Jerry},
	month = nov,
	year = {2019},
	pages = {1--5},
}

@article{qian_speech_2021,
	title = {Speech {Sanitizer}: {Speech} {Content} {Desensitization} and {Voice} {Anonymization}},
	volume = {18},
	issn = {1545-5971, 1941-0018, 2160-9209},
	shorttitle = {Speech {Sanitizer}},
	url = {https://ieeexplore.ieee.org/document/8935427/},
	doi = {10.1109/TDSC.2019.2960239},
	number = {6},
	urldate = {2023-02-19},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Qian, Jianwei and Du, Haohua and Hou, Jiahui and Chen, Linlin and Jung, Taeho and Li, Xiang-Yang},
	month = nov,
	year = {2021},
	pages = {2631--2642},
}

@inproceedings{pop_towards_2021,
	address = {Bucharest, Romania},
	title = {Towards {Detection} of {Synthetic} {Utterances} in {Romanian} {Language} {Speech} {Forensics}},
	isbn = {978-1-66542-786-9},
	url = {https://ieeexplore.ieee.org/document/9587393/},
	doi = {10.1109/SpeD53181.2021.9587393},
	urldate = {2023-02-19},
	booktitle = {2021 {International} {Conference} on {Speech} {Technology} and {Human}-{Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Pop, Gheorghe and Burileanu, Dragos},
	month = oct,
	year = {2021},
	pages = {80--84},
}

@article{wang_towards_2021,
	title = {Towards {Query}-{Efficient} {Adversarial} {Attacks} {Against} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	url = {https://ieeexplore.ieee.org/document/9205635/},
	doi = {10.1109/TIFS.2020.3026543},
	urldate = {2023-02-19},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Wang, Qian and Zheng, Baolin and Li, Qi and Shen, Chao and Ba, Zhongjie},
	year = {2021},
	pages = {896--908},
}

@article{chen_audio_2009-1,
	title = {Audio {Quality} {Issue} for {Automatic} {Speech} {Assessment}},
	abstract = {Recently, in the language testing field, automatic speech recog-nition (ASR) technology has been used to automatically score speaking tests. This paper investigates the impact of audio quality on ASR-based automatic speaking assessment. Using the read speech data in the International English Speaking Test (IEST) practice test, we annotated audio quality and compared scores rated by humans, speech recognition accuracy, and the quality of features used for the automatic assessment under high and low audio quality conditions. Our investigation suggests that human raters can cope with low-quality audio files well, but speech recognition and the features extracted for the automatic assessment perform worse on the low audio quality condition.},
	author = {Chen, Lei},
	month = jan,
	year = {2009},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\MV935Q6L\\Chen - 2009 - Audio Quality Issue for Automatic Speech Assessmen.pdf:application/pdf},
}

@article{fish_using_2023,
	title = {{USING} {AUDIO} {QUALITY} {TO} {PREDICT} {WORD} {ERROR} {RATE} {IN} {AN} {AUTOMATIC} {SPEECH} {RECOGNITION} {SYSTEM}},
	abstract = {Faced with a backlog of audio recordings, users of automatic speech recognition (ASR) systems would benefit from the ability to predict which files would result in useful output transcripts in order to prioritize processing resources. ASR systems used in non-research environments typically run in "real time". In other words, one hour of speech requires one hour of processing. These systems produce transcripts with widely varying Word Error Rates (WER) depending upon the actual words spoken and the quality of the recording. Existing correlations between WER and the ability to perform tasks such as information retrieval or machine translation could be leveraged if one could predict WER before processing an audio file. We describe here a method for estimating the quality of the ASR output transcript by predicting the portion of the total WER in a transcript attributable to the quality of the audio recording.},
	author = {Fish, Randall and Hu, Qian and Boykin, Stanley},
	month = feb,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\4N5J2QXR\\Fish et al. - 2023 - USING AUDIO QUALITY TO PREDICT WORD ERROR RATE IN .pdf:application/pdf},
}

@book{kim_comparison_2019,
	title = {A {Comparison} of {Online} {Automatic} {Speech} {Recognition} {Systems} and the {Nonverbal} {Responses} to {Unintelligible} {Speech}},
	abstract = {Automatic Speech Recognition (ASR) systems have proliferated over the recent years to the point that free platforms such as YouTube now provide speech recognition services. Given the wide selection of ASR systems, we contribute to the field of automatic speech recognition by comparing the relative performance of two sets of manual transcriptions and five sets of automatic transcriptions (Google Cloud, IBM Watson, Microsoft Azure, Trint, and YouTube) to help researchers to select accurate transcription services. In addition, we identify nonverbal behaviors that are associated with unintelligible speech, as indicated by high word error rates. We show that manual transcriptions remain superior to current automatic transcriptions. Amongst the automatic transcription services, YouTube offers the most accurate transcription service. For non-verbal behavioral involvement, we provide evidence that the variability of smile intensities from the listener is high (low) when the speaker is clear (unintelligible). These findings are derived from videoconferencing interactions between student doctors and simulated patients; therefore, we contribute towards both the ASR literature and the healthcare communication skills teaching community.},
	author = {Kim, Joshua and Liu, Chunfeng and Calvo, Rafael and McCabe, Kathryn and Taylor, Silas and Schuller, Björn and Wu, Kaihang},
	month = apr,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\83SMHLHA\\Kim et al. - 2019 - A Comparison of Online Automatic Speech Recognitio.pdf:application/pdf},
}

@inproceedings{barker_third_2015,
	address = {Scottsdale, AZ, USA},
	title = {The third ‘{CHiME}’ speech separation and recognition challenge: {Dataset}, task and baselines},
	isbn = {978-1-4799-7291-3},
	shorttitle = {The third ‘{CHiME}’ speech separation and recognition challenge},
	url = {http://ieeexplore.ieee.org/document/7404837/},
	doi = {10.1109/ASRU.2015.7404837},
	urldate = {2023-02-22},
	booktitle = {2015 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU})},
	publisher = {IEEE},
	author = {Barker, Jon and Marxer, Ricard and Vincent, Emmanuel and Watanabe, Shinji},
	month = dec,
	year = {2015},
	pages = {504--511},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\W35WSYNN\\Barker et al. - 2015 - The third ‘CHiME’ speech separation and recognitio.pdf:application/pdf},
}

@article{barker_pascal_2013,
	title = {The {PASCAL} {CHiME} speech separation and recognition challenge},
	volume = {27},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230812000861},
	doi = {10.1016/j.csl.2012.10.004},
	language = {en},
	number = {3},
	urldate = {2023-02-22},
	journal = {Computer Speech \& Language},
	author = {Barker, Jon and Vincent, Emmanuel and Ma, Ning and Christensen, Heidi and Green, Phil},
	month = may,
	year = {2013},
	pages = {621--633},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GP4BZTCJ\\Barker et al. - 2013 - The PASCAL CHiME speech separation and recognition.pdf:application/pdf},
}

@inproceedings{barker_fifth_2018,
	title = {The {Fifth} '{CHiME}' {Speech} {Separation} and {Recognition} {Challenge}: {Dataset}, {Task} and {Baselines}},
	shorttitle = {The {Fifth} '{CHiME}' {Speech} {Separation} and {Recognition} {Challenge}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/barker18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1768},
	language = {en},
	urldate = {2023-02-22},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Barker, Jon and Watanabe, Shinji and Vincent, Emmanuel and Trmal, Jan},
	month = sep,
	year = {2018},
	pages = {1561--1565},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\LL9XZ4KF\\Barker et al. - 2018 - The Fifth 'CHiME' Speech Separation and Recognitio.pdf:application/pdf},
}

@inproceedings{brown_ctimit_1995,
	address = {Detroit, MI, USA},
	title = {{CTIMIT}: a speech corpus for the cellular environment with applications to automatic speech recognition},
	volume = {1},
	isbn = {978-0-7803-2431-2},
	shorttitle = {{CTIMIT}},
	url = {http://ieeexplore.ieee.org/document/479284/},
	doi = {10.1109/ICASSP.1995.479284},
	urldate = {2023-02-22},
	booktitle = {1995 {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Brown, K.L. and George, E.B.},
	year = {1995},
	pages = {105--108},
}

@article{da_silva_quality_2008,
	title = {Quality assessment of interactive voice applications},
	volume = {52},
	issn = {13891286},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128608000042},
	doi = {10.1016/j.comnet.2008.01.002},
	language = {en},
	number = {6},
	urldate = {2023-02-22},
	journal = {Computer Networks},
	author = {da Silva, Ana Paula Couto and Varela, Martín and de Souza e Silva, Edmundo and Leão, Rosa M.M. and Rubino, Gerardo},
	month = apr,
	year = {2008},
	pages = {1179--1192},
}

@article{hu_subjective_2007,
	title = {Subjective comparison and evaluation of speech enhancement algorithms},
	volume = {49},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639306001920},
	doi = {10.1016/j.specom.2006.12.006},
	language = {en},
	number = {7-8},
	urldate = {2023-02-22},
	journal = {Speech Communication},
	author = {Hu, Yi and Loizou, Philipos C.},
	month = jul,
	year = {2007},
	pages = {588--601},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\28F8SI6A\\Hu and Loizou - 2007 - Subjective comparison and evaluation of speech enh.pdf:application/pdf},
}

@article{hu_evaluating_2020,
	title = {Evaluating {QoE} in {VoIP} networks with {QoS} mapping and machine learning algorithms},
	volume = {386},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219317813},
	doi = {10.1016/j.neucom.2019.12.072},
	language = {en},
	urldate = {2023-02-22},
	journal = {Neurocomputing},
	author = {Hu, ZhiGuo and Yan, HongRen and Yan, Tao and Geng, HaiJun and Liu, GuoQing},
	month = apr,
	year = {2020},
	pages = {63--83},
}

@inproceedings{huang_cross-language_2013,
	address = {Vancouver, BC, Canada},
	title = {Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers},
	isbn = {978-1-4799-0356-6},
	url = {http://ieeexplore.ieee.org/document/6639081/},
	doi = {10.1109/ICASSP.2013.6639081},
	urldate = {2023-02-22},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Huang, Jui-Ting and Li, Jinyu and Yu, Dong and Deng, Li and Gong, Yifan},
	month = may,
	year = {2013},
	pages = {7304--7308},
}

@inproceedings{kermanshahi_transfer_2021,
	address = {Tehran, Iran},
	title = {Transfer {Learning} for {End}-to-{End} {ASR} to {Deal} with {Low}-{Resource} {Problem} in {Persian} {Language}},
	isbn = {978-1-66541-241-4},
	url = {https://ieeexplore.ieee.org/document/9420540/},
	doi = {10.1109/CSICC52343.2021.9420540},
	urldate = {2023-02-22},
	booktitle = {2021 26th {International} {Computer} {Conference}, {Computer} {Society} of {Iran} ({CSICC})},
	publisher = {IEEE},
	author = {Kermanshahi, Maryam Asadolahzade and Akbari, Ahmad and Nasersharif, Babak},
	month = mar,
	year = {2021},
	pages = {1--5},
}

@article{kurematsu_atr_1990,
	title = {{ATR} {Japanese} speech database as a tool of speech recognition and synthesis},
	volume = {9},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016763939090011W},
	doi = {10.1016/0167-6393(90)90011-W},
	language = {en},
	number = {4},
	urldate = {2023-02-22},
	journal = {Speech Communication},
	author = {Kurematsu, Akira and Takeda, Kazuya and Sagisaka, Yoshinori and Katagiri, Shigeru and Kuwabara, Hisao and Shikano, Kiyohiro},
	month = aug,
	year = {1990},
	pages = {357--363},
}

@article{li_spectral_2014,
	title = {A {Spectral} {Masking} {Approach} to {Noise}-{Robust} {Speech} {Recognition} {Using} {Deep} {Neural} {Networks}},
	volume = {22},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/6826528/},
	doi = {10.1109/TASLP.2014.2329237},
	number = {8},
	urldate = {2023-02-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Li, Bo and Sim, Khe Chai},
	month = aug,
	year = {2014},
	pages = {1296--1305},
}

@article{malik_automatic_2021,
	title = {Automatic speech recognition: a survey},
	volume = {80},
	issn = {1380-7501, 1573-7721},
	shorttitle = {Automatic speech recognition},
	url = {http://link.springer.com/10.1007/s11042-020-10073-7},
	doi = {10.1007/s11042-020-10073-7},
	language = {en},
	number = {6},
	urldate = {2023-02-22},
	journal = {Multimedia Tools and Applications},
	author = {Malik, Mishaim and Malik, Muhammad Kamran and Mehmood, Khawar and Makhdoom, Imran},
	month = mar,
	year = {2021},
	pages = {9411--9457},
}

@inproceedings{morris_wer_2004-1,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2023-02-22},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@inproceedings{panayotov_librispeech_2015-1,
	address = {South Brisbane, Queensland, Australia},
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	isbn = {978-1-4673-6997-8},
	shorttitle = {Librispeech},
	url = {http://ieeexplore.ieee.org/document/7178964/},
	doi = {10.1109/ICASSP.2015.7178964},
	urldate = {2023-02-22},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	month = apr,
	year = {2015},
	pages = {5206--5210},
}

@inproceedings{ravanelli_dirha-english_2015,
	address = {Scottsdale, AZ, USA},
	title = {The {DIRHA}-{ENGLISH} corpus and related tasks for distant-speech recognition in domestic environments},
	isbn = {978-1-4799-7291-3},
	url = {http://ieeexplore.ieee.org/document/7404805/},
	doi = {10.1109/ASRU.2015.7404805},
	urldate = {2023-02-22},
	booktitle = {2015 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU})},
	publisher = {IEEE},
	author = {Ravanelli, Mirco and Cristoforetti, Luca and Gretter, Roberto and Pellin, Marco and Sosi, Alessandro and Omologo, Maurizio},
	month = dec,
	year = {2015},
	pages = {275--282},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GU5NY364\\Ravanelli et al. - 2015 - The DIRHA-ENGLISH corpus and related tasks for dis.pdf:application/pdf},
}

@inproceedings{shi_research_2018,
	address = {Bandung, Indonesia},
	title = {Research on {Transfer} {Learning} for {Khalkha} {Mongolian} {Speech} {Recognition} {Based} on {TDNN}},
	isbn = {978-1-72811-175-9},
	url = {https://ieeexplore.ieee.org/document/8629237/},
	doi = {10.1109/IALP.2018.8629237},
	urldate = {2023-02-22},
	booktitle = {2018 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	publisher = {IEEE},
	author = {Shi, Linyan and Bao, Feilong and Wang, Yonghe and Gao, Guanglai},
	month = nov,
	year = {2018},
	pages = {85--89},
}

@inproceedings{lingfen_sun_perceived_2002,
	address = {New York, NY, USA},
	title = {Perceived speech quality prediction for voice over {IP}-based networks},
	volume = {4},
	isbn = {978-0-7803-7400-3},
	url = {http://ieeexplore.ieee.org/document/997307/},
	doi = {10.1109/ICC.2002.997307},
	urldate = {2023-02-22},
	booktitle = {2002 {IEEE} {International} {Conference} on {Communications}. {Conference} {Proceedings}. {ICC} 2002 ({Cat}. {No}.{02CH37333})},
	publisher = {IEEE},
	author = {{Lingfen Sun} and Ifeachor, E.C.},
	year = {2002},
	pages = {2573--2577},
}

@article{lingfen_sun_voice_2006,
	title = {Voice quality prediction models and their application in {VoIP} networks},
	volume = {8},
	issn = {1520-9210},
	url = {http://ieeexplore.ieee.org/document/1658041/},
	doi = {10.1109/TMM.2006.876279},
	number = {4},
	urldate = {2023-02-22},
	journal = {IEEE Transactions on Multimedia},
	author = {{Lingfen Sun} and Ifeachor, E.C.},
	month = aug,
	year = {2006},
	pages = {809--820},
}

@inproceedings{vasilescu_cross-lingual_2011-1,
	title = {Cross-lingual study of {ASR} errors: on the role of the context in human perception of near-homophones},
	shorttitle = {Cross-lingual study of {ASR} errors},
	url = {https://www.isca-speech.org/archive/interspeech_2011/vasilescu11_interspeech.html},
	doi = {10.21437/Interspeech.2011-365},
	language = {en},
	urldate = {2023-02-22},
	booktitle = {Interspeech 2011},
	publisher = {ISCA},
	author = {Vasilescu, I. and Yahia, D. and Snoeren, N. and Adda-Decker, Martine and Lamel, Lori},
	month = aug,
	year = {2011},
	pages = {1949--1952},
}

@article{furui_japanese_2000,
	title = {A {Japanese} national project on spontaneous speech corpus and processing technology},
	journal = {Proc. ASR2000},
	author = {Furui, Sadaoki and Maekawa, Kikuo and Isahara, Hitoshi},
	month = jan,
	year = {2000},
	pages = {244--248},
}

@article{maekawa_spontaneous_2000,
	title = {Spontaneous speech corpus of {Japanese}},
	volume = {2},
	abstract = {Design issues of a spontaneous speech corpus is described. The corpus under compilation will contain 800-1000 hour spontaneously uttered Common Japanese speech and the morphologically annotated transcriptions. Also, segmental and intonation labeling will be provided for a subset of the corpus. The primary application domain of the corpus is speech recognition of spontaneous speech, but we plan to make it useful for natural language processing and phonetic/linguistic studies also.},
	journal = {Proceedings of LREC},
	author = {Maekawa, Kikuo and Koiso, Hanae and Furui, Sadaoki and Isahara, Hitoshi},
	month = jan,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IWSIHRM7\\Maekawa et al. - 2000 - Spontaneous speech corpus of Japanese.pdf:application/pdf},
}

@techreport{international_telecommunication_union_low-complexity_2005,
	address = {Geneva, Switzerland},
	title = {Low-complexity coding at 24 and 32 kbit/s for hands-free operation in systems with low frame loss},
	number = {ITU-T (2005). G.722.1},
	institution = {International Telecommunication Union.},
	author = {International Telecommunication Union.},
	year = {2005},
}

@inproceedings{kohn_mining_2016,
	address = {Portorož, Slovenia},
	title = {Mining the {Spoken} {Wikipedia} for {Speech} {Data} and {Beyond}},
	url = {https://aclanthology.org/L16-1735},
	abstract = {We present a corpus of time-aligned spoken data of Wikipedia articles as well as the pipeline that allows to generate such corpora for many languages. There are initiatives to create and sustain spoken Wikipedia versions in many languages and hence the data is freely available, grows over time, and can be used for automatic corpus creation. Our pipeline automatically downloads and aligns this data. The resulting German corpus currently totals 293h of audio, of which we align 71h in full sentences and another 86h of sentences with some missing words. The English corpus consists of 287h, for which we align 27h in full sentence and 157h with some missing words. Results are publically available.},
	urldate = {2023-02-22},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'16)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Köhn, Arne and Stegen, Florian and Baumann, Timo},
	month = may,
	year = {2016},
	pages = {4644--4647},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\YWB3XR2A\\Köhn et al. - 2016 - Mining the Spoken Wikipedia for Speech Data and Be.pdf:application/pdf},
}

@article{baumann_spoken_2019,
	title = {The {Spoken} {Wikipedia} {Corpus} collection: {Harvesting}, alignment and an application to hyperlistening},
	volume = {53},
	issn = {1574-020X, 1574-0218},
	shorttitle = {The {Spoken} {Wikipedia} {Corpus} collection},
	url = {http://link.springer.com/10.1007/s10579-017-9410-y},
	doi = {10.1007/s10579-017-9410-y},
	language = {en},
	number = {2},
	urldate = {2023-02-22},
	journal = {Language Resources and Evaluation},
	author = {Baumann, Timo and Köhn, Arne and Hennig, Felix},
	month = jun,
	year = {2019},
	pages = {303--329},
}

@article{stas_tedxsk_2017,
	title = {{TEDxSK} and {JumpSK}: {A} {New} {Slovak} {Speech} {Recognition} {Dedicated} {Corpus}},
	volume = {68},
	issn = {1338-4287, 0021-5597},
	shorttitle = {{TEDxSK} and {JumpSK}},
	url = {https://www.sciendo.com/article/10.1515/jazcas-2017-0044},
	doi = {10.1515/jazcas-2017-0044},
	abstract = {Abstract
            This paper describes a new Slovak speech recognition dedicated corpus built from TEDx talks and Jump Slovakia lectures. The proposed speech database consists of 220 talks and lectures in total duration of about 58 hours. Annotated speech database was generated automatically in an unsupervised manner by using acoustic speech segmentation based on principal component analysis and automatic speech transcription using two complementary speech recognition systems. The evaluation data consisting of 50 manually annotated talks and lectures in total duration of about 12 hours, has been created for evaluation of the quality of Slovak speech recognition. By unsupervised automatic annotation of TEDx talks and Jump Slovakia lectures we have obtained 21.26\% of new speech segments with approximately 9.44\% word error rate, suitable for retraining or adaptation of acoustic models trained beforehand.},
	language = {en},
	number = {2},
	urldate = {2023-02-22},
	journal = {Journal of Linguistics/Jazykovedný casopis},
	author = {Staš, Ján and Hládek, Daniel and Viszlay, Peter and Koctúr, Tomáš},
	month = dec,
	year = {2017},
	pages = {346--354},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\7HYKTSSC\\Staš et al. - 2017 - TEDxSK and JumpSK A New Slovak Speech Recognition.pdf:application/pdf},
}

@book{hernandez_ted-lium_2018,
	title = {{TED}-{LIUM} 3: twice as much data and corpus repartition for experiments on speaker adaptation},
	shorttitle = {{TED}-{LIUM} 3},
	abstract = {In this paper, we present TED-LIUM release 3 corpus dedicated to speech recognition in English, that multiplies by more than two the available data to train acoustic models in comparison with TED-LIUM 2. We present the recent development on Automatic Speech Recognition (ASR) systems in comparison with the two previous releases of the TED-LIUM Corpus from 2012 and 2014. We demonstrate that, passing from 207 to 452 hours of transcribed speech training data is really more useful for end-to-end ASR systems than for HMM-based state-of-the-art ones, even if the HMM-based ASR system still outperforms end-to-end ASR system when the size of audio training data is 452 hours, with respectively a Word Error Rate (WER) of 6.6\% and 13.7\%. Last, we propose two repartitions of the TED-LIUM release 3 corpus: the legacy one that is the same as the one existing in release 2, and a new one, calibrated and designed to make experiments on speaker adaptation. Like the two first releases, TED-LIUM 3 corpus will be freely available for the research community.},
	author = {Hernandez, Francois and Nguyen, Vincent and Ghannay, Sahar and Tomashenko, Natalia and Estève, Yannick},
	month = may,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\ZFCIMXD4\\Hernandez et al. - 2018 - TED-LIUM 3 twice as much data and corpus repartit.pdf:application/pdf},
}

@book{rousseau_ted-lium_2012,
	title = {{TED}-{LIUM}: an {Automatic} {Speech} {Recognition} dedicated corpus},
	shorttitle = {{TED}-{LIUM}},
	abstract = {This paper presents the corpus developed by the LIUM for Automatic Speech Recognition (ASR), based on the TED Talks. This corpus was built during the IWSLT 2011 Evaluation Campaign, and is composed of 118 hours of speech with its accompanying automatically aligned transcripts. We describe the content of the corpus, how the data was collected and processed, how it will be publicly available and how we built an ASR system using this data leading to a WER score of 17.4\%. The official results we obtained at the IWSLT 2011 evaluation campaign are also discussed.},
	author = {Rousseau, Anthony and Deléglise, Paul and Estève, Yannick},
	month = may,
	year = {2012},
	note = {Pages: 129},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\X9ZJH345\\Rousseau et al. - 2012 - TED-LIUM an Automatic Speech Recognition dedicate.pdf:application/pdf},
}

@techreport{valin_definition_2012,
	type = {Request for {Comments}},
	title = {Definition of the {Opus} {Audio} {Codec}},
	url = {https://datatracker.ietf.org/doc/rfc6716},
	abstract = {This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances. It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s. Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music. [STANDARDS-TRACK]},
	number = {RFC 6716},
	urldate = {2023-02-22},
	institution = {Internet Engineering Task Force},
	author = {Valin, Jean-Marc and Vos, Koen and Terriberry, Tim},
	month = sep,
	year = {2012},
	note = {Num Pages: 326
10.17487/RFC6716},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\83C68MCG\\Valin et al. - 2012 - Definition of the Opus Audio Codec.pdf:application/pdf},
}

@article{wang_matbn_2005,
	title = {{MATBN}: {A} {Mandarin} {Chinese} broadcast news corpus},
	volume = {10},
	shorttitle = {{MATBN}},
	abstract = {The MATBN Mandarin Chinese broadcast news corpus contains a total of 198 hours of broadcast news from the Public Television Service Foundation (Taiwan) with corresponding transcripts. The primary purpose of this collection is to provide training and testing data for continuous speech recognition evaluation in the broadcast news domain. In this paper, we briefly introduce the speech corpus and report on some preliminary statistical analysis and speech recognition evaluation results.},
	journal = {International Journal of Computational Linguistics and Chinese Language Processing},
	author = {Wang, Hsin-min and Chen, Berlin and Kuo, Jen-Wei and Cheng, Shih-Sian},
	month = jan,
	year = {2005},
	note = {Wang, H. M., Chen, B., Kuo, J.W., and Cheng, S. S. (2005). “Matbn: A Mandarin
Chinese broadcast news corpus,” in International journal of computational
linguistics \& Chinese language processing, volume 10, number 2, june 2005:
Special issue on annotated speech corpora, 219–236.},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\FXDN5MPC\\Wang et al. - 2005 - MATBN A Mandarin Chinese broadcast news corpus.pdf:application/pdf},
}

@article{wang_matbn_2023,
	title = {{MATBN} 2002: {A} {MANDARIN} {CHINESE} {BROADCAST} {NEWS} {CORPUS}},
	shorttitle = {{MATBN} 2002},
	abstract = {The MATBN 2002 Mandarin Chinese broadcast news corpus contains a total of 40 hours of broadcast news from Public Television Service Foundation (Taiwan) with corresponding transcripts. The primary motivation for this collection is to provide training and testing data for continuous speech recognition evaluation in the broadcast domain. We expect to collect and process 220 hours of Mandarin Chinese broadcast news speech over 3 years. At the end of the first year, the 40 hour broadcast news corpus has been completed on schedule and is scheduled to be releasable in early 2003. According to our plan, we expect to release the interim 120 hour broadcast news corpus in late 2003 and the final 220 hour broadcast news corpus in late 2004.},
	author = {Wang, Hsin-min},
	month = feb,
	year = {2023},
	note = {Wang, H. M., Chen, B., Kuo, J.W., and Cheng, S. S. (2005). “Matbn: A Mandarin
Chinese broadcast news corpus,” in International journal of computational
linguistics \& Chinese language processing, volume 10, number 2, june 2005:
Special issue on annotated speech corpora, 219–236.},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\CALDYXKH\\Wang - 2023 - MATBN 2002 A MANDARIN CHINESE BROADCAST NEWS CORP.pdf:application/pdf},
}

@inproceedings{wang_h_m_chen_b_kuo_jw_and_cheng_s_s_matbn_2005,
	series = {2},
	title = {{MATBN}: {A} {Mandarin} {Chinese} broadcast news corpus},
	volume = {10},
	url = {https://aclanthology.org/O05-3004.pdf},
	booktitle = {Special issue on annotated speech corpora},
	author = {Wang, H. M., Chen, B., Kuo, J.W., {and} Cheng, S. S.},
	month = jun,
	year = {2005},
	note = {https://tpl.ncl.edu.tw/NclService/pdfdownload?filePath=lV8OirTfsslWcCxIpLbUfvnJVVyS2MdW\_ZqXpienrTvUPhF86I-wOaYdd8gbOtpR\&imgType=Bn5sH4BGpJw=\&key=XwcGkdVNfZmLOKsykxisAiNtBoF5cWLuPuAPkqsW5VQeVVU9OyINO4qBZJhLTxWd\&xmlId=0005911567},
	pages = {219--236},
}

@article{buchsbaum_algorithmic_1997,
	title = {Algorithmic aspects in speech recognition: an introduction},
	volume = {2},
	issn = {1084-6654, 1084-6654},
	shorttitle = {Algorithmic aspects in speech recognition},
	url = {https://dl.acm.org/doi/10.1145/264216.264219},
	doi = {10.1145/264216.264219},
	abstract = {Speech recognition is an area with a considerable literature, but there is little discussion of the topic within the computer science algorithms literature. Many computer scientists, however, are interested in the computational problems of speech recognition. This paper presents the field of speech recognition and describes some of its major open problems from an algorithmic viewpoint. Our goal is to stimulate the interest of algorithm designers and experimenters to investigate the algorithmic problems of effective automatic speech recognition.},
	language = {en},
	urldate = {2023-03-04},
	journal = {ACM Journal of Experimental Algorithmics},
	author = {Buchsbaum, Adam L. and Giancarlo, Raffaele},
	month = jan,
	year = {1997},
	pages = {1},
}

@inproceedings{garcia_lecumberri_non-native_2008,
	title = {The non-native consonant challenge for european languages},
	url = {https://www.isca-speech.org/archive/interspeech_2008/garcialecumberri08_interspeech.html},
	doi = {10.21437/Interspeech.2008-490},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Interspeech 2008},
	publisher = {ISCA},
	author = {Garcia Lecumberri, M. Luisa and Cooke, Martin and Cutugno, Francesco and Giurgiu, Mircea and Meyer, Bernd T. and Scharenborg, Odette and Dommelen, Wim van and Volin, Jan},
	month = sep,
	year = {2008},
	pages = {1781--1784},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\Z7KDFLJI\\Garcia Lecumberri et al. - 2008 - The non-native consonant challenge for european la.pdf:application/pdf},
}

@article{marti_automatic_2012,
	title = {Automatic speech recognition in cocktail-party situations: {A} specific training for separated speech},
	volume = {131},
	issn = {0001-4966},
	shorttitle = {Automatic speech recognition in cocktail-party situations},
	url = {http://asa.scitation.org/doi/10.1121/1.3675001},
	doi = {10.1121/1.3675001},
	language = {en},
	number = {2},
	urldate = {2023-03-09},
	journal = {The Journal of the Acoustical Society of America},
	author = {Marti, Amparo and Cobos, Maximo and Lopez, Jose J.},
	month = feb,
	year = {2012},
	pages = {1529--1535},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\LW7PRR3H\\Marti et al. - 2012 - Automatic speech recognition in cocktail-party sit.pdf:application/pdf},
}

@inproceedings{xiong_microsoft_2018-1,
	address = {Calgary, AB},
	title = {The {Microsoft} 2017 {Conversational} {Speech} {Recognition} {System}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8461870/},
	doi = {10.1109/ICASSP.2018.8461870},
	urldate = {2023-04-21},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Xiong, W. and Wu, L. and Alleva, F. and Droppo, J. and Huang, X. and Stolcke, A.},
	month = apr,
	year = {2018},
	pages = {5934--5938},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\LHC38QNA\\Xiong et al. - 2018 - The Microsoft 2017 Conversational Speech Recogniti.pdf:application/pdf},
}

@inproceedings{saon_english_2017,
	title = {English {Conversational} {Telephone} {Speech} {Recognition} by {Humans} and {Machines}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/saon17_interspeech.html},
	doi = {10.21437/Interspeech.2017-405},
	language = {en},
	urldate = {2023-04-21},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Saon, George and Kurata, Gakuto and Sercu, Tom and Audhkhasi, Kartik and Thomas, Samuel and Dimitriadis, Dimitrios and Cui, Xiaodong and Ramabhadran, Bhuvana and Picheny, Michael and Lim, Lynn-Li and Roomi, Bergul and Hall, Phil},
	month = aug,
	year = {2017},
	pages = {132--136},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\4XVWEHXG\\Saon et al. - 2017 - English Conversational Telephone Speech Recognitio.pdf:application/pdf},
}

@inproceedings{saon_ibm_2015,
	title = {The {IBM} 2015 {English} conversational telephone speech recognition system},
	url = {https://www.isca-speech.org/archive/interspeech_2015/saon15_interspeech.html},
	doi = {10.21437/Interspeech.2015-632},
	language = {en},
	urldate = {2023-04-21},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Saon, George and Kuo, Hong-Kwang J. and Rennie, Steven and Picheny, Michael},
	month = sep,
	year = {2015},
	pages = {3140--3144},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\PJ6CFXTS\\Saon et al. - 2015 - The IBM 2015 English conversational telephone spee.pdf:application/pdf},
}

@inproceedings{seide_conversational_2011,
	title = {Conversational speech transcription using context-dependent deep neural networks},
	url = {https://www.isca-speech.org/archive/interspeech_2011/seide11_interspeech.html},
	doi = {10.21437/Interspeech.2011-169},
	language = {en},
	urldate = {2023-04-21},
	booktitle = {Interspeech 2011},
	publisher = {ISCA},
	author = {Seide, Frank and Li, Gang and Yu, Dong},
	month = aug,
	year = {2011},
	pages = {437--440},
}

@inproceedings{chiu_state---art_2018,
	address = {Calgary, AB},
	title = {State-of-the-{Art} {Speech} {Recognition} with {Sequence}-to-{Sequence} {Models}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462105/},
	doi = {10.1109/ICASSP.2018.8462105},
	urldate = {2023-04-21},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Chiu, Chung-Cheng and Sainath, Tara N. and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J. and Rao, Kanishka and Gonina, Ekaterina and Jaitly, Navdeep and Li, Bo and Chorowski, Jan and Bacchiani, Michiel},
	month = apr,
	year = {2018},
	pages = {4774--4778},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\PC3XIH9T\\Chiu et al. - 2018 - State-of-the-Art Speech Recognition with Sequence-.pdf:application/pdf},
}

@article{yoshioka_making_2012,
	title = {Making {Machines} {Understand} {Us} in {Reverberant} {Rooms}: {Robustness} {Against} {Reverberation} for {Automatic} {Speech} {Recognition}},
	volume = {29},
	issn = {1053-5888},
	shorttitle = {Making {Machines} {Understand} {Us} in {Reverberant} {Rooms}},
	url = {http://ieeexplore.ieee.org/document/6296524/},
	doi = {10.1109/MSP.2012.2205029},
	number = {6},
	urldate = {2023-04-21},
	journal = {IEEE Signal Processing Magazine},
	author = {Yoshioka, Takuya and Sehr, Armin and Delcroix, Marc and Kinoshita, Keisuke and Maas, Roland and Nakatani, Tomohiro and Kellermann, Walter},
	month = nov,
	year = {2012},
	pages = {114--126},
}

@book{arbib_handbook_1998,
	address = {Cambridge, Mass.},
	edition = {1. MIT Press paperback ed},
	series = {A {Bradford} book},
	title = {The handbook of brain theory and neural networks},
	isbn = {978-0-262-51102-5},
	language = {eng},
	publisher = {MIT},
	editor = {Arbib, Michael A.},
	year = {1998},
	annote = {Originally published: 1995},
}

@incollection{lecun_yann_convolutional_1998,
	title = {Convolutional {Networks} for {Images}, {Speech}, and {Time} {Series}},
	isbn = {0-262-51102-9},
	booktitle = {Convolutional {Networks} for {Images}, {Speech}, and {Time} {Series}},
	publisher = {MIT Press},
	author = {LeCun, Yann and Bengio, Yoshua},
	year = {1998},
	pages = {255--258},
}

@article{carletta_unleashing_2007,
	title = {Unleashing the killer corpus: experiences in creating the multi-everything {AMI} {Meeting} {Corpus}},
	volume = {41},
	issn = {1574-020X, 1572-8412},
	shorttitle = {Unleashing the killer corpus},
	url = {http://link.springer.com/10.1007/s10579-007-9040-x},
	doi = {10.1007/s10579-007-9040-x},
	language = {en},
	number = {2},
	urldate = {2023-04-21},
	journal = {Language Resources and Evaluation},
	author = {Carletta, Jean},
	month = nov,
	year = {2007},
	pages = {181--190},
}

@article{swietojanski_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Distant} {Speech} {Recognition}},
	volume = {21},
	issn = {1070-9908, 1558-2361},
	url = {https://ieeexplore.ieee.org/document/6819043/},
	doi = {10.1109/LSP.2014.2325781},
	number = {9},
	urldate = {2023-04-21},
	journal = {IEEE Signal Processing Letters},
	author = {Swietojanski, Pawel and Ghoshal, Arnab and Renals, Steve},
	month = sep,
	year = {2014},
	pages = {1120--1124},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\TLPT74XM\\Swietojanski et al. - 2014 - Convolutional Neural Networks for Distant Speech R.pdf:application/pdf},
}

@incollection{watanabe_distant_2017,
	address = {Cham},
	title = {Distant {Speech} {Recognition} {Experiments} {Using} the {AMI} {Corpus}},
	isbn = {978-3-319-64679-4 978-3-319-64680-0},
	url = {http://link.springer.com/10.1007/978-3-319-64680-0_16},
	language = {en},
	urldate = {2023-04-21},
	booktitle = {New {Era} for {Robust} {Speech} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Renals, Steve and Swietojanski, Pawel},
	editor = {Watanabe, Shinji and Delcroix, Marc and Metze, Florian and Hershey, John R.},
	year = {2017},
	doi = {10.1007/978-3-319-64680-0_16},
	pages = {355--368},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\2LQYKDH4\\Renals and Swietojanski - 2017 - Distant Speech Recognition Experiments Using the A.pdf:application/pdf},
}

@article{kepuska_comparing_2017,
	title = {Comparing {Speech} {Recognition} {Systems} ({Microsoft} {API}, {Google} {API} {And} {CMU} {Sphinx})},
	volume = {07},
	issn = {22489622, 22489622},
	url = {http://www.ijera.com/papers/Vol7_issue3/Part-2/D0703022024.pdf},
	doi = {10.9790/9622-0703022024},
	number = {03},
	urldate = {2023-04-21},
	journal = {International Journal of Engineering Research and Applications},
	author = {Këpuska, Veton},
	month = mar,
	year = {2017},
	pages = {20--24},
}

@book{choi_pansori_2018,
	title = {Pansori: {ASR} {Corpus} {Generation} from {Open} {Online} {Video} {Contents}},
	shorttitle = {Pansori},
	abstract = {This paper introduces Pansori, a program used to create ASR (automatic speech recognition) corpora from online video contents. It utilizes a cloud-based speech API to easily create a corpus in different languages. Using this program, we semi-automatically generated the Pansori-TEDxKR dataset from Korean TED conference talks with community-transcribed subtitles. It is the first high-quality corpus for the Korean language freely available for independent research. Pansori is released as an open-source software and the generated corpus is released under a permissive public license for community use and participation.},
	author = {Choi, Yoona and Lee, Bowon},
	month = dec,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\K8DBJW6V\\Choi and Lee - 2018 - Pansori ASR Corpus Generation from Open Online Vi.pdf:application/pdf},
}

@misc{noauthor_lifelike_2017,
	title = {Lifelike {Text} to {Speech} for {Your} {Customers}},
	url = {https://www.readspeaker.com/},
	abstract = {ReadSpeaker provides lifelike online and offline text-to-speech solutions to make your products and services more engaging. Discover how TTS can benefit you},
	language = {en-US},
	urldate = {2023-05-06},
	journal = {ReadSpeaker},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\6V9ZXSFU\\www.readspeaker.com.html:text/html},
}

@inproceedings{chandankhede_voice_2021,
	address = {Greater Noida, India},
	title = {Voice {Recognition} {Based} {Security} {System} {Using} {Convolutional} {Neural} {Network}},
	isbn = {978-1-72818-529-3},
	url = {https://ieeexplore.ieee.org/document/9397151/},
	doi = {10.1109/ICCCIS51004.2021.9397151},
	urldate = {2023-05-06},
	booktitle = {2021 {International} {Conference} on {Computing}, {Communication}, and {Intelligent} {Systems} ({ICCCIS})},
	publisher = {IEEE},
	author = {Chandankhede, Pankaj H. and Titarmare, Abhijit S. and Chauhvan, Sarang},
	month = feb,
	year = {2021},
	pages = {738--743},
}

@article{satori_investigation_2014,
	title = {Investigation {Amazigh} speech recognition using {CMU} tools},
	volume = {17},
	issn = {1381-2416, 1572-8110},
	url = {http://link.springer.com/10.1007/s10772-014-9223-y},
	doi = {10.1007/s10772-014-9223-y},
	language = {en},
	number = {3},
	urldate = {2023-05-06},
	journal = {International Journal of Speech Technology},
	author = {Satori, Hassan and ElHaoussi, Fatima},
	month = sep,
	year = {2014},
	pages = {235--243},
}

@misc{shmyrev_cmusphinx_nodate,
	title = {{CMUSphinx} {Open} {Source} {Speech} {Recognition}},
	url = {http://cmusphinx.github.io/},
	abstract = {CMUSphinx is an open source speech recognition system for mobile and server applications. Supported languages: C, C++, C\#, Python, Ruby, Java, Javascript. Supported platforms: Unix, Windows, IOS, Android, hardware.},
	urldate = {2023-05-06},
	journal = {CMUSphinx Open Source Speech Recognition},
	author = {Shmyrev, Nickolay},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\QDJJKJGA\\cmusphinx.github.io.html:text/html},
}


@misc{wikipedia contributors_2019c,
  title={Dragon NaturallySpeaking},
  url={https://en.wikipedia.org/wiki/Dragon_NaturallySpeaking},
  journal={Wikipedia},
  publisher={Wikimedia Foundation},
  author={Wikipedia Contributors},
  year={2019},
  month={Nov}
}

@misc{eric-urban_2023a,
   title={Speech to text overview - Speech service - Azure Cognitive Services},
   url={https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-to-text},
   journal={learn.microsoft.com},
   author={eric-urban},
   year={2023},
   month={May}
}

@misc{speech_recognition_api_Twilio,
  author = {Twilio},
  title = {{Twilio Speech Recognition}},
  howpublished = {Website},
  journal={Twilio},
  url = {https://www.twilio.com/speech-recognition},
  year = {2023},
  month = {May}
}

@misc{houndify,
  author = {Houndify},
  title = {Houndify},
  howpublished = {Website},
  url = {https://www.houndify.com/},
  journal = {SoundHound},
  year = {2023},
  month = {May}
}

@misc{ibm_2019,
  title={IBM Watson},
  url={https://www.ibm.com/watson},
  journal={Ibm.com},
  author={IBM},
  year={2019},
}

@misc{goffin_allauzen_bocchieri_hakkani-tur_ljolje_parthasarathy_rahim_riccardi_saraclar_2005,
  title={The AT&T WATSON speech recognizer},
  volume={1},
  url={https://ieeexplore.ieee.org/document/1415293},
  DOI={https://doi.org/10.1109/ICASSP.2005.1415293},
  journal={IEEE Xplore},
  author={Goffin, V. and Allauzen, C. and Bocchieri, E. and Hakkani-Tur,
  D. and Ljolje, A. and Parthasarathy, S. and Rahim, M. and Riccardi, G. and Saraclar, M.},
  year={2005},
  month={Mar},
  pages={I/1033–I/1036 Vol. 1}
}

@misc{sharp_bocchieri_castillo_parthasarathy_rath_riley_rowland_1997,
  title={The Watson speech recognition engine},
  volume={5},
  url={https://ieeexplore.ieee.org/document/604839},
  DOI={https://doi.org/10.1109/ICASSP.1997.604839},
  journal={IEEE Xplore},
  author={Sharp, R.D. and Bocchieri, E. and Castillo, C. and Parthasarathy, S. and Rath, C. and Riley, M. and Rowland, J.},
  year={1997},
  month={Apr},
  pages={4065–4068 vol.5},
}

@book{adorf_2013,
   title={Web Speech API},
   url={https://www.juliusadorf.com/pub/web-speech-api.pdf},
   author={Adorf, Julius},
   year={2013},
}

@misc{text_to_speech_online_2019,
  url={https://www.naturalreaders.com/},
  journal={Naturalreaders.com},
  year={2019}
}

@misc{ispeech,
  title = {Text to Speech - TTS SDK - Speech Recognition (ASR)},
  url={https://www.ispeech.org/},
  journal={www.ispeech.org},
  note = {Accessed: May 05, 2019}
}

@inproceedings{wavesurfer,
  author = {Sjölander, Kjell and Beskow, Jonas},
  title = {{WAVESURFER - AN OPEN SOURCE SPEECH TOOL}},
  booktitle = {Proceedings of the ICSLP},
  pages = {4},
  year = {2001},
  doi = {10.1.1.19.5790}
}

@inproceedings{lee2009recent,
  author = {Lee, Akinobu and Kawahara, Tatsuya},
  title = {{Recent development of open-source speech recognition engine julius}},
  booktitle = {Proceedings: APSIPA ASC 2009: Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference},
  pages = {131--137},
  month = {October},
  year = {2009}
}

@article{moustafa_alzantot_balaji_srivastava_2018,
  title={Did you hear that? Adversarial Examples Against Automatic Speech Recognition.},
  author={Moustafa Alzantot and Balaji, Bharathan and Srivastava, Mani},
  year={2018},
  month={Jan}
}

@article{carlini_wagner_2018,
   title={Audio Adversarial Examples: Targeted Attacks on Speech-to-Text}, DOI={https://doi.org/10.48550/arxiv.1801.01944},
   author={Carlini, Nicholas and Wagner, David},
   year={2018},
   month={Jan}
}

@article{cisse_adi_neverova_keshet_2017,
    title={Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples},
    booktitle = {Advances in Neural Information Processing Systems 30},
    volume={30},
    author={Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
    editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna and Fergus, Rob and Vishwanathan, S. and Garnett, Roman},
    year={2017},
    month={Jan},
    pages={6977–6987},
    publisher = {Curran Associates, Inc.}
}

@article{rohan_vemuri_2018,
    title={Targeted Adversarial Examples for Black Box Audio Systems}, author={Rohan, Taori and Amog, Kamsetty and Chu, Brenton and Vemuri, Nikita},
    year={2018},
    journal = {arXiv preprint arXiv:1805.07820},
    month={May}
}

@article{awni2014deep,
     title={Deep Speech: Scaling up end-to-end speech recognition},
     author={Awni Hannun and Case, Carl J and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Sanjeev Satheesh and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y},
     journal = {arXiv preprint arXiv:1412.5567},
     year={2014},
     month={Dec}
}

@article{goodfellow_shlens_szegedy_2014,
    title={Explaining and Harnessing Adversarial Examples},
    DOI={https://doi.org/10.48550/arxiv.1412.6572},
    journal={arXiv (Cornell University)},
    publisher={Cornell University},
    author={Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian}, year={2014},
    month={Dec}
}

@misc{carlini_wagner_2018a,
    title={Audio Adversarial Examples: Targeted Attacks on Speech-to-Text},
    url={https://ieeexplore.ieee.org/abstract/document/8424625},
    DOI={https://doi.org/10.1109/SPW.2018.00009},
    journal={IEEE Xplore},
    author={Carlini, N. and Wagner, D.},
    year={2018},
    month={May},
    pages={1–7}
}

@article{cisse_adi_neverova_keshet_2017b,
    title={Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples},
    volume={30},
    author={Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
    editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna and Fergus, Rob and Vishwanathan, S. and Garnett, Roman},
    publisher = {Curran Associates, Inc.},
    year={2017},
    month={Jan},
    pages={6977–6987}
}

     @article{wang_wang_dong_yan_zhang_gong_2020,
     title={Adversarial Examples Attack and Countermeasure for Speech Recognition System: A Survey},
     DOI={https://doi.org/10.1007/978-981-15-9129-7_31},
     author={Wang, Donghua and Wang, Rangding and Dong, Li and Yan, Diqun and Zhang, Xueyuan and Gong, Yongkang},
     year={2020},
     month={Oct},
     pages={443–468}
}

@misc{mozilla_2019,
     title={mozilla/DeepSpeech},
     url={https://github.com/mozilla/DeepSpeech},
     journal={GitHub},
     author={mozilla},
     year={2019},
     month={Dec}
}

@article{du_ji_li_gu_wang_beyah_2019,
    title={SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems},
    DOI={https://doi.org/10.48550/arxiv.1901.07846},
    author={Du, Tianyu and Ji, Shouling and Li, Jinfeng and Gu, Qinchen and Wang, Ting and Beyah, Raheem},
    year={2019},
    month={Jan}
}

@article{graves_fernández_gomez_schmidhuber_2006,
    title={Connectionist temporal classification},
    ISBN={1595933832},
    url={https://dl.acm.org/citation.cfm?id=1143891},
    DOI={https://doi.org/10.1145/1143844.1143891},
    journal={Proceedings of the 23rd international conference on Machine learning  - ICML ’06},
    pages = {369--376},
    author={Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
    year={2006}
}

@misc{kaldi_github,
    title = {Kaldi Speech Recognition Toolkit},
    url={https://github.com/kaldi-asr/kaldi},
    journal={GitHub},
    year={2023},
    month={Feb}
}

@article{schönherr_adversarial_2018,
    title={Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding},
    author={Schönherr, Lea and Kohls, Katharina and Zeiler, Steffen and Holz, Thorsten and Kolossa, Dorothea},
    journal = {arXiv preprint arXiv:1808.05665},
    year={2018},
	doi={10.48550/arXiv.1808.05665},
    month={Aug}
}

@article{kartik_nahamoo_2017,
    title={Direct Acoustics-to-Word Models for English Conversational Speech Recognition},
    doi={https://doi.org/10.21437/interspeech.2017-546},
    journal={ArXiv Preprint arXiv:1703.07754},
    author={Kartik Audhkhasi and Bhuvana Ramabhadran and Saon, George and Picheny, Michael and Nahamoo, David},
    year={2017},
    month={Mar}
}

@article{xiong_wu_alleva_droppo_huang_stolcke_2018,
    title={The Microsoft 2017 Conversational Speech Recognition System},
    ISBN={9781538646588},
    DOI={https://doi.org/10.1109/icassp.2018.8461870},
    journal={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    author={Xiong, W. and Wu, L. and Alleva, F. and Droppo, J. and Huang, X. and Stolcke, A.},
    year={2018},
    month={Apr}
}


@article{kartik_2018,
    title={Building Competitive Direct Acoustics-to-Word Models for English Conversational Speech Recognition},
    DOI={https://doi.org/10.1109/icassp.2018.8461935},
    author={Kartik Audhkhasi and Kingsbury, Brian and Bhuvana Ramabhadran and Saon, George and Picheny, Michael},
    year={2018},
    month={Apr}
}

@article{panayotov_chen_povey_khudanpur_2015,
    title={Librispeech: An ASR corpus based on public domain audio books},
    ISBN={9781467369978},
    DOI={https://doi.org/10.1109/icassp.2015.7178964},
    journal={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
    year={2015},
    month={Apr}
}

@article{yuan_chen_zhao_long_liu_chen_zhang_huang_wang_gunter_2018,
    title={CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition},
    author={Yuan, Xuejing and Chen, Yuxuan and Zhao, Yue and Long, Yunhui and Liu, Xiao-Kang and Chen, Kai and Zhang, Shengzhi and Huang, Heqing and Wang, X F and Gunter, Carl A},
    pages = {49--64},
    year={2018},
    month={Jan},
}

@book{richards_philosophy_1976,
	address = {London},
	edition = {Reprint},
	series = {The {Mary} {Flexner} lectures on the humanities},
	title = {The philosophy of rhetoric},
	isbn = {978-0-19-500715-2},
	language = {eng},
	number = {3 = 1935},
	publisher = {Oxford Univ. Press},
	author = {Richards, I. A.},
	year = {1976}
}


@incollection{michaelis_2_2008,
	address = {Amsterdam},
	title = {2. {The} superstrate is not always the lexifier: {Lingua} {Franca} in the {Barbary} {Coast} 1530-1830},
	volume = {33},
	isbn = {978-90-272-5255-5 978-90-272-8996-4},
	shorttitle = {2. {The} superstrate is not always the lexifier},
	url = {https://benjamins.com/catalog/cll.33.05sel},
	language = {en},
	urldate = {2023-05-29},
	booktitle = {Creole {Language} {Library}},
	publisher = {John Benjamins Publishing Company},
	author = {Selbach, Rachel},
	editor = {Michaelis, Susanne},
	year = {2008},
	doi = {10.1075/cll.33.05sel},
	pages = {29--58},
}

@book{michaelis_roots_2008,
	address = {Amsterdam},
	series = {Creole {Language} {Library}},
	title = {Roots of {Creole} {Structures}: {Weighing} the contribution of substrates and superstrates},
	volume = {33},
	isbn = {978-90-272-5255-5 978-90-272-8996-4},
	shorttitle = {Roots of {Creole} {Structures}},
	url = {http://www.jbe-platform.com/content/books/9789027289964},
	language = {en},
	urldate = {2023-05-29},
	publisher = {John Benjamins Publishing Company},
	editor = {Michaelis, Susanne},
	month = oct,
	year = {2008},
	doi = {10.1075/cll.33},
}

@book{michaelis_michaelis_2008,
	title = {Michaelis, {Susanne} ({Ed}.). 2008. {Roots} of creole structures: {Weighing} the contributions of substrates and superstrates [{Creole} {Language} {Library} 33]. {Amsterdam}/{Philadelphia}: {Benjamins}.},
	shorttitle = {Michaelis, {Susanne} ({Ed}.). 2008. {Roots} of creole structures},
	author = {Michaelis, Susanne},
	month = jan,
	year = {2008},
}

@book{mey_pragmatics_2001,
	address = {Malden, MA},
	edition = {2nd ed},
	title = {Pragmatics: an introduction},
	isbn = {978-0-631-21131-0 978-0-631-21132-7},
	shorttitle = {Pragmatics},
	publisher = {Blackwell Publishers},
	author = {Mey, Jacob},
	year = {2001},
	keywords = {Pragmatics},
}


@incollection{ortese_linguistics,
	title = {In linguistics},
	author = {mmanuel, Ortese},
	keywords = {linguistics},
    url = {https://www.academia.edu/6502550},
}
@incollection{ortese_lingustics,
	title = {In linguistics},
	author = {mmanuel, Ortese},
	keywords = {linguistics},
    url = {https://www.academia.edu/6502550},
}
 @misc{shmyrev,
    title={CMUSphinx Open Source Speech Recognition},
    url={http://cmusphinx.github.io},
    journal={CMUSphinx Open Source Speech Recognition},
    author={Shmyrev, Nickolay} }

@misc{cmu_sphinx_2019,
    url={https://en.wikipedia.org/wiki/CMU_Sphinx},
    journal={Wikipedia},
    year={2019},
    month={Jun}
}

 @misc{aswani_2022,
    title={Speech Recognition in Python using CMU Sphinx},
    url={https://www.codespeedy.com/speech-recognition-in-python-using-cmu-sphinx/},
    journal={CodeSpeedy},
    author={Aswani, Khushi},
    year={2022},
    month={Jan}, }

@misc{shmyrev,
  title={CMU Sphinx Downloads},
  url={https://cmusphinx.github.io/wiki/download/},
  journal={CMUSphinx Open Source Speech Recognition},
  author={Shmyrev, Nickolay},
}

@misc{cmusphinx/pocketsphinx_2021,
   url={https://github.com/cmusphinx/pocketsphinx},
   journal={GitHub},
   year={2021},
   month={Jun},
}

@misc{sphinxtrain_github_2023,
    url={https://github.com/cmusphinx/sphinxtrain},
    journal={GitHub},
    year={2023},
    month={May}
}

@misc{cloud_speech-to-text_2019,
     url={https://cloud.google.com/speech-to-text/},
     journal={Google Cloud},
     year={2019},
    author={google}
}

@misc{kincaid_2018,
   title={Which Automatic Transcription Service is the Most Accurate? — 2018},
   url={https://medium.com/descript/which-automatic-transcription-service-is-the-most-accurate-2018-2e859b23ed19},
   journal={Medium},
   author={Kincaid, Jason},
   year={2018},
   month={Sep},
}

@misc{nuance_site,
    url={https://www.nuance.com/omni-channel-customer-engagement/contact-center-ai/nuance-recognizer.html},
    author={nuance},
    journal={Nuance Communications}
}

@book{Morbini_Audhkhasi_Sagae_Artstein_Can_Georgiou_Narayanan_Leuski_Traum_2013,
    title={Which ASR should I choose for my dialogue system},
    url={http://ict.usc.edu/pubs/Which%20ASR%20should%20I%20choose%20for%20my%20dialogue%20system.pdf},
    journal={Annual Meeting of the Special Interest Group on Discourse and Dialogue},
    author={Morbini, Fabrizio and Audhkhasi, Kartik and Sagae, Kenji and Artstein, Ron and Can, Dogan and Georgiou, Panayiotis G. and Narayanan, Shri and Leuski, Anton and Traum, David},
    year={2013},
    month={Aug},
    pages={394–403}
}

@misc{Shmyrev,
    title={CMUSphinx Open Source Speech Recognition},
    url={https://cmusphinx.github.io/},
    journal={CMUSphinx Open Source Speech Recognition},
    author={Shmyrev, Nickolay}
}

@misc{dragon_2008,
     title={the pros and cons of using dragon naturally speaking software in a special education setting_2008},
     url={https://www.brighthubeducation.com/special-ed-inclusion-strategies/11295-pros-cons-of-using-dragon-naturally-speaking/},
     journal={www.brighthubeducation.com},
     year={2008},
     month={Oct}
}

@inproceedings{Sharma2012ASR,
  title={A Speech Recognition and Synthesis Tool : Assistive Technology for Physically Disabled Persons},
  author={F. Reena Sharma and Stephen Wasson},
  year={2012}
}
@misc{eric-urban_2023,
     title={Training and testing datasets - Speech service - Azure Cognitive Services},
     url={https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train},
     journal={learn.microsoft.com},
     author={eric-urban},
     year={2023},
     month={May}
}

@article{baker_1989,
     title={Dragondictate (TM)-30k: natural language speech recognition with 30,000 words},
     DOI={https://doi.org/10.21437/eurospeech.1989-225},
     author={Baker, Janet},
     year={1989},
     month={Sep}
}

@misc{wikipedia_contributors_2019b,
   title={Dragon NaturallySpeaking},
   url={https://en.wikipedia.org/wiki/Dragon_NaturallySpeaking},
   journal={Wikipedia},
   publisher={Wikimedia Foundation},
   author={Wikipedia Contributors},
   year={2019},
   month={Nov}
}

@misc{improving_aws_blog_2020,
   title={improving speech-to-text transcripts from amazon transcribe using custom vocabularies and amazon augmented ai | aws machine learning blog_2020},
   url={https://aws.amazon.com/blogs/machine-learning/improving-speech-to-text-transcripts-from-amazon-transcribe-using-custom-vocabularies-and-amazon-augmented-ai/},
   journal={aws.amazon.com},
   year={2020},
   month={Aug}
}

@misc{witai_liip_2018,
   url={https://www.liip.ch/en/blog/speech-recognition-with-wit-ai},
   title={speech_recognition with wit.ai · blog · liip_2018},
   journal={Liip},
   author={wit.ai},
   year={2018},
   month={Mar}
}

@misc{ibm,
   title={IBM Watson - Speech to Text},
   url={https://www.ibm.com/cloud/watson-speech-to-text},
   journal={www.ibm.com},
   author={IBM}
}

@inproceedings{ibm-2019b,
  author = {Moore, Meredith and Saxon, Michael and Venkateswara, Hemanth and Berisha, Visar and Panchanathan, Sethuraman},
  year = {2019},
  month = {09},
  pages = {2528-2532},
  title = {Say What? A Dataset for Exploring the Error Patterns That Two ASR Engines Make},
  doi = {10.21437/Interspeech.2019-3096}
}

@misc{eric-urban_2023a,
   title={Test accuracy of a Custom Speech model - Speech service - Azure Cognitive Services},
   url={https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-evaluate-data},
   journal={learn.microsoft.com},
   author={eric-urban},
   year={2023},
   month={May}
}

@article{watanabe2018_esp,
   title={ESPnet: End-to-End Speech Processing Toolkit},
   url={https://arxiv.org/abs/1804.00015},
   journal={arXiv:1804.00015 [cs]},
   author={Watanabe, Shinji and Hori, Takaaki and Karita, Shigeki and Hayashi, Tomoki and Nishitoba, Jiro and Unno, Yuya and Soplin, Nelson Enrique Yalta and Heymann, Jahn and Wiesner, Matthew and Chen, Nanxin and Renduchintala, Adithya and Ochiai, Tsubasa},
   year={2018},
   month={Mar}
}

@misc{watanabe,
  title={espnet: ESPnet: end-to-end speech processing toolkit},
  url={https://pypi.org/project/espnet/},
  journal={PyPI},
  author={Watanabe, Shinji}
}

@misc{espnet_model_zoo_2023,
  url={https://github.com/espnet/espnet_model_zoo},
  journal={GitHub},
  year={2023},
  month={Jun}
}

@inproceedings{jasper-2019,
  author = {Li, Jason and Lavrukhin, Vitaly and Ginsburg, Boris and Leary, Ryan and Kuchaiev, Oleksii and Cohen, Jonathan and Nguyen, Huyen and Gadde, Ravi},
  year = {2019},
  month = {09},
  pages = {71-75},
  title = {Jasper: An End-to-End Convolutional Neural Acoustic Model},
  doi = {10.21437/Interspeech.2019-1819}
}

@misc{nvidia/nemo_2022,
   url={https://github.com/NVIDIA/NeMo},
   journal={GitHub},
   year={2022},
   month={Jul}
}

@article{huang_wu_luo_chen_wang_toda_2021,
 title={Speech Recognition by Simply Fine-tuning BERT},
 url={https://arxiv.org/abs/2102.00291},
 journal={arXiv:2102.00291 [cs, eess]},
 author={Huang, Wen-Chin and Wu, Chia-Hua and Luo, Shang-Bao and Chen, Kuan-Yu and Wang, Hsin-Min and Toda, Tomoki},
 year={2021},
 month={Jan}
}

@misc{google-research_2019,
 title={google-research/bert},
 url={https://github.com/google-research/bert},
 journal={GitHub},
 author={google-research},
 year={2019},
 month={Mar}
}

@misc{wav2vec_install_2023_github,
   url={https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/wav2vec/wav2vec2_asr.py},
   journal={GitHub},
   year={2023},
   month={Jun}
}

@misc{fu_kang_cao_ma_2023,
   title={DistillW2V2: A Small and Streaming Wav2vec 2.0 Based ASR Model},
   url={https://arxiv.org/abs/2303.09278},
   DOI={https://doi.org/10.48550/arXiv.2303.09278},
   journal={arXiv.org},
   author={Fu, Yanzhe and Kang, Yueteng and Cao, Songjun and Ma, Long},
   year={2023},
   month={Mar}
}

@inproceedings{zulguana_pablo_saeed_petr_2023,
  author = {Zuluaga, Juan Pablo and Prasad, Amrutha and Sarfjoo, Saeed and Motlicek, Petr and Kleinert, Matthias and Helmke, Hartmut and Ohneiser, Oliver and Zhan, Qingran},
  year = {2023},
  month = {01},
  title = {How Does Pre-Trained Wav2Vec 2.0 Perform on Domain-Shifted Asr? an Extensive Benchmark on Air Traffic Control Communications},
  doi = {10.1109/SLT54892.2023.10022724}
}

@article{ravanelli_2021,
   title={SpeechBrain: A General-Purpose Speech Toolkit},
   url={https://arxiv.org/abs/2106.04624},
   journal={arXiv:2106.04624 [cs, eess]},
   author={Ravanelli, Mirco and Parcollet, Titouan and Plantinga, Peter and Rouhe, Aku and Cornell, Samuele and Lugosch, Loren and Subakan, Cem and Dawalatabad, Nauman and Heba, Abdelwahab and Zhong, Jianyuan and Chou, Ju-Chieh and Yeh, Sung-Lin and Fu, Szu-Wei and Liao, Chien-Feng and Rastorgueva, Elena and Grondin, François and Aris, William and Na, Hwidong and Gao, Yan and De Mori, Renato},
   year={2021},
   month={Jun}
}

@misc{speechbrain_github,
   url={https://speechbrain.github.io/},
   journal={speechbrain.github.io}
}

@misc{pytorch_2023,
   url={https://github.com/speechbrain/speechbrain},
   journal={GitHub},
   year={2023},
   month={Jun}
}

@misc{CMU_Sphnix_2017,
    title={CMUSphinx Open Source Speech Recognition},
    url={https://cmusphinx.github.io/},
    journal={CMUSphinx Open Source Speech Recognition},
    author={Nickolay Shmyrev},
    year={2017}
}

@misc{amazon_transcribe_aws,
      url={https://aws.amazon.com/transcribe/},
      author={amazon},
      journal={Amazon Web Services, Inc.}
}

 @misc{ms-azure,
    url={https://azure.microsoft.com/en-us/products/open-datasets/},
    author={nvidia},
    journal={azure.microsoft.com}
}

@misc{povey_ghoshal_2011,
     title={The Kaldi Speech Recognition Toolkit},
     url={https://www.semanticscholar.org/paper/The-Kaldi-Speech-Recognition-Toolkit-Povey-Ghoshal/3a1a2cff2b70fb84a7ca7d97f8adcc5855851795},
     journal={Semantic Scholar},
     author={Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, L. and Glembek, O. and Goel, N. and Hannemann, M. and Motlícek, P. and Qian, Y. and Schwarz, Petr and Silovský, J. and Stemmer, G. and Veselý, Karel},
     year={2011}
}

@misc{nemo_models,
       url={https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html},
       author={nvidia},
       journal={NVIDIA NeMo}
}

@misc{nvidia/nemo_2022b,
       url={https://github.com/NVIDIA/NeMo},
       journal={GitHub},
       year={2022},
       month={Jul}
}

@misc{zbikowski1997,
    title = {Conceptual Models},
    author = {Zbikowski, Lawrence M.},
    year = {1997},
    url = {http://zbikowski.uchicago.edu/pdfs/ZbikowskiConceptualmodels_1997.pdf}
}

@misc{wiki-a440,
    title = {{A440 (pitch standard)}},
    author = {{Wikipedia contributors}},
    howpublished = {\url{https://en.wikipedia.org/wiki/A440_(pitch_standard)}},
    note = {Accessed: July 5, 2023}
}

@misc{dobrian-frequency-pitch,
    title = {Frequency and Pitch},
    author = {Dobrian},
    year = {},
    howpublished = {\url{https://dobrian.github.io/cmp/topics/physics-of-sound/1.frequency-and-pitch.html}},
    note = {Accessed: July 5, 2023}
}

@article{elliot_acoustic,
author = {Elliott, Taffeta and Hamilton, Liberty and Theunissen, Frédéric},
year = {2013},
month = {01},
pages = {389-404},
title = {Acoustic structure of the five perceptual dimensions of timbre in orchestral instrument tones},
volume = {133},
journal = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.4770244]}
}

@article{sinnemaki2018language,
    title = {Language Structures May Adapt to the Sociolinguistic Environment, but It Matters What and How You Count: A Typological Study of Verbal and Nominal Complexity},
    author = {Sinnemäki, Kaius and Di Garbo, Francesca},
    year = {2018},
    journal = {Frontiers in Psychology},
    volume = {9},
    pages = {187--205},
    doi = {10.3389/fpsyg.2018.01141},
    issn = {1664-1078},
    pmid = {30154738},
    pmcid = {PMC6102949}
}

@article{dahl2001grammaticalization,
    title = {Grammaticalization and the life cycles of constructions},
    author = {Dahl, Östen},
    year = {2001},
    journal = {RASK – Internationalt Tidsskrift for Sprog og Kommunikation},
    volume = {14},
    pages = {91--134}
}

@inbook{andrew_1989_Greek,
    title = {Greek Musical Writings: Volume 2, Harmonic and Acoustic Theory},
    author = {Barker, Andrew},
    volume = {2},
    publisher = {Cambridge University Press},
    address = {Cambridge},
    year = {1989},
    pages = {191--208}
}

@book{zehentner2019competition,
    title = {Competition in Language Change: the Rise of the English Dative Alternation},
    author = {Zehentner, Eva},
    year = {2019},
    publisher = {De Gruyter Mouton},
    isbn = {978-3-11-063385-6}
}

@incollection{macwhinney2015introduction,
    title = {Introduction – language emergence},
    author = {MacWhinney, Brian},
    booktitle = {Handbook of Language Emergence},
    editor = {MacWhinney, Brian and O'Grady, William},
    publisher = {Wiley},
    year = {2015},
    pages = {1--31},
    isbn = {978-1-118-34613-6}
}

@article{werker1984crosslanguage,
    title = {Cross-language speech perception: evidence for perceptual reorganization during the first year of life},
    author = {Werker, J. F. and Tees, R. C.},
    journal = {Infant Behavior \& Development},
    volume = {7},
    number = {1},
    pages = {49--63},
    year = {1984},
    url = {https://doi.org/10.1016/S0163-6383(84)80022-3}
}

@inproceedings{nusbaum1991role,
    title = {The role of syllables in speech perception},
    author = {Nusbaum, H. C. and DeGroot, J.},
    booktitle = {Papers from the parasession on the syllable in phonetics and phonology},
    editor = {Ziolkowski, M. S. and Noske, M. and Deaton, K.},
    organization = {Chicago Linguistic Society},
    address = {Chicago},
    year = {1991}
}

@article{RASANEN2018130,
title = {Pre-linguistic segmentation of speech into syllable-like units},
journal = {Cognition},
volume = {171},
pages = {130-150},
year = {2018},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717302901},
author = {Okko Räsänen and Gabriel Doyle and Michael C. Frank},
keywords = {Early language acquisition, Speech perception, Syllables, Speech segmentation, Oscillatory entrainment, Sonority},
abstract = {Syllables are often considered to be central to infant and adult speech perception. Many theories and behavioral studies on early language acquisition are also based on syllable-level representations of spoken language. There is little clarity, however, on what sort of pre-linguistic “syllable” would actually be accessible to an infant with no phonological or lexical knowledge. Anchored by the notion that syllables are organized around particularly sonorous (audible) speech sounds, the present study investigates the feasibility of speech segmentation into syllable-like chunks without any a priori linguistic knowledge. We first operationalize sonority as a measurable property of the acoustic input, and then use sonority variation across time, or speech rhythm, as the basis for segmentation. The entire process from acoustic input to chunks of syllable-like acoustic segments is implemented as a computational model inspired by the oscillatory entrainment of the brain to speech rhythm. We analyze the output of the segmentation process in three different languages, showing that the sonority fluctuation in speech is highly informative of syllable and word boundaries in all three cases without any language-specific tuning of the model. These findings support the widely held assumption that syllable-like structure is accessible to infants even when they are only beginning to learn the properties of their native language.}
}

@inproceedings{halle2012global,
    title = {Global and detailed speech representations in early language acquisition},
    author = {Hallé, P. and Christia, A.},
    booktitle = {Speech planning and dynamics},
    editor = {Weirich Fuchs, M. and Pape, D. and Perrier, P.},
    publisher = {Peter Lang},
    address = {Frankfurt am Main},
    year = {2012}
}

@article{liberman1974explicit,
    title = {Explicit syllable and phoneme segmentation in the young child},
    author = {Liberman, I. Y. and Shankweiler, D. and Fischer, W. F. and Carter, B.},
    journal = {Journal of Experimental Child Psychology},
    volume = {18},
    pages = {201--212},
    year = {1974}
}


@article{GREENBERG2003465,
title = {Temporal properties of spontaneous speech—a syllable-centric perspective},
journal = {Journal of Phonetics},
volume = {31},
number = {3},
pages = {465-485},
year = {2003},
note = {Temporal Integration in the Perception of Speech},
issn = {0095-4470},
doi = {https://doi.org/10.1016/j.wocn.2003.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0095447003000603},
author = {Steven Greenberg and Hannah Carvey and Leah Hitchcock and Shuangyu Chang},
abstract = {Temporal properties of the speech signal are of potentially great importance for understanding spoken language and may provide significant insight into the manner in which listeners process spoken language with so little apparent effort. It is the thesis of this study that durational properties of phonetic segments differentially reflect the amount of information contained within a syllable, and that syllable prominence is an indirect measure of linguistic entropy. The ability to understand spoken language appears to depend on a broad distribution of syllable duration, ranging between 50 and 400ms (for American English), which is reflected in the modulation spectrum of the acoustic signal. The upper branch of the modulation spectrum (6–20Hz) reflects unstressed syllables, while the lower branch (<5Hz) represents mostly heavily stressed syllables. Low-pass filtering the modulation spectrum reduces the intelligibility of spoken sentences in a manner consistent with the differential contribution of stressed and unstressed syllables to understanding spoken language. The origins of this phenomenon are investigated in terms of the durational properties of phonetic segments contained in a corpus of spontaneous American English telephone dialogues (SWITCHBOARD). Forty-five minutes of this material was manually annotated with respect to stress accent, and the relation between accent level and segmental duration examined. Statistical analysis indicates that much of the temporal variation observed at the syllabic and phonetic-segment levels can be accounted for in terms of two basic parameters: (1) stress-accent pattern and (2) position of the segment within the syllable. Segments are generally longest in heavily stressed syllables and shortest in syllables without stress. However, the magnitude of accent's impact on duration varies as a function of syllable position. Duration of the nucleus is heavily affected by stress-accent level—heavily stressed nuclei are, on average, twice as long as their unstressed counterparts, while the duration of the onset is also significantly sensitive to stress, but to a lesser degree. In contrast, stress has relatively little impact on coda duration. This pattern of durational variation suggests that the vocalic nucleus absorbs much of the impact of stress accent and potentially sets the register for interpreting the phonetic segments contained within the syllable. Moreover, the data imply that linguistic entropy is not uniformly distributed across the syllable—the onset and nucleus convey more information than the coda.}
}

@misc{harvard_phonology_ppt,
    title = {Phonology},
    author = {harvard},
    howpublished = {\url{https://scholar.harvard.edu/files/adam/files/phonology.ppt}}
}

@book{zuckermann2003language,
    title = {Language Contact and Lexical Enrichment in Israeli Hebrew},
    author = {Zuckermann, Ghil'ad},
    year = {2003},
    publisher = {Palgrave Macmillan},
    pages = {2ff},
    isbn = {978-1-4039-1723-2}
}

@misc{thoughtco_pragmatics,
    title = {What Is Pragmatics?},
    author = {{ThoughtCo}},
    year = {},
    howpublished = {Retrieved 11 July 2017},
    url = {https://www.thoughtco.com/what-is-pragmatics-1689850}
}

@article{ferguson1959diglossia,
    title = {Diglossia},
    author = {Ferguson, Charles A.},
    journal = {WORD (Worcester)},
    volume = {15},
    number = {2},
    pages = {325--340},
    year = {1959},
    doi = {10.1080/00437956.1959.11659702},
    issn = {0043-7956},
    s2cid = {239352211},
    url = {https://www.tandfonline.com/doi/abs/10.1080/00437956.1959.11659702}
}

@article{plungyan2011modern,
    title = {Modern Linguistic Typology},
    author = {Plungyan, V. A.},
    journal = {Herald of the Russian Academy of Sciences},
    volume = {81},
    number = {2},
    pages = {101--113},
    year = {2011},
    doi = {10.1134/S1019331611020158}
}

@book{pike1945intonation,
    title = {The Intonation of American English},
    author = {Pike, Kenneth L.},
    year = {1945},
    publisher = {University of Michigan Press},
    address = {Ann Arbor, Mich.}
}

@incollection{nespor2011stress,
    title = {Stress-timed vs. Syllable-timed Languages},
    author = {Nespor, M. and Shukla, M. and Mehler, J.},
    booktitle = {The Blackwell Companion to Phonology},
    editor = {van Oostendorp, M. et al},
    year = {2011},
    publisher = {Blackwell},
    address = {Malden, MA},
    pages = {1147--1159}
}

@article{hayes1989compensatory,
    title = {Compensatory Lengthening in Moraic Phonology},
    author = {Hayes, Bruce},
    journal = {Linguistic Inquiry},
    volume = {20},
    number = {2},
    pages = {253--306},
    year = {1989}
}

@misc{wiki_mora,
    title = {Mora (linguistics)},
    url = {https://en.wikipedia.org/wiki/Mora_(linguistics)}
}

@book{carnie2013syntax,
    title = {Syntax: A Generative Introduction (Second Edition)},
    author = {Carnie, Andrew},
    year = {2013},
    publisher = {Blackwell Publishing}
}

@book{leckie1995language,
    title = {Language and Context: A Functional Linguistic Theory of Register},
    author = {Leckie-Tarry, Helen},
    year = {1995},
    publisher = {Continuum International Publishing Group},
    pages = {6},
    isbn = {1-85567-272-3}
}

@book{zuckermann2003language,
    title = {Language Contact and Lexical Enrichment in Israeli Hebrew},
    author = {Zuckermann, Ghil'ad},
    year = {2003},
    publisher = {Palgrave Macmillan},
    pages = {2ff},
    isbn = {978-1-4039-1723-2},
    note = {Archived from the original on 27 August 2016}
}

@book{stewart2019history,
    title = {History of the English Language},
    author = {Stewart, Carson},
    year = {2019},
    isbn = {9781839472985}
}

@incollection{antonsen2002trends,
    title = {Chapter 1, section 1.1},
    author = {Antonsen, Elmer H.},
    booktitle = {Trends in Linguistics: Runes and Germanic Linguistics},
    edition = {6th},
    publisher = {Mouton de Gruyter},
    year = {2002},
    isbn = {978-3-11-017462-5}
}

@book{noth1990handbook,
    title = {Handbook of Semiotics},
    author = {Noth, Winfried},
    publisher = {Indiana University Press},
    year = {1990},
    isbn = {978-0-253-20959-7}
}

@book{hjelmslev1969prolegomena,
    title = {Prolegomena to a Theory of Language},
    author = {Hjelmslev, Louis},
    publisher = {University of Wisconsin Press},
    year = {1969},
    note = {First published 1943},
    isbn = {0-299-02470-9}
}
@book{hjelmslev1969prolegomen,
    title = {Prolegomena to a Theory of Language},
    author = {Hjelmslev, Louis},
    publisher = {University of Wisconsin Press},
    year = {1969},
    note = {First published 1943},
    isbn = {0-299-02470-9}
}
@book{saussure1959course,
    title = {Course in General Linguistics},
    author = {de Saussure, Ferdinand},
    publisher = {The Philosophical Library, Inc.},
    year = {1959},
    note = {First published 1916},
    isbn = {978-0-231-15727-8}
}

@misc{liu2009pragmatics,
    title = {What is Pragmatics?},
    author = {Liu Shaozhong},
    year = {2009},
    note = {Archived from the original on 7 March 2009. Retrieved 18 March 2009}
}

@inbook{mukherjee_stylistics,
    title = {Stylistics},
    author = {Mukherjee, Joybrato},
    booktitle = {Encyclopedia of Linguistics},
    year = {2013},
    chapter = {49},
    note = {Archived from the original (PDF) on 4 October 2013. Retrieved 4 October 2013}
}


@article{OZBILGIN20104305,
title = {Discourses shape foreign language learning and teaching: The story of a language institution},
journal = {Procedia - Social and Behavioral Sciences},
volume = {2},
number = {2},
pages = {4305-4309},
year = {2010},
note = {Innovation and Creativity in Education},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.03.683},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810007238},
author = {Alev Ozbilgin},
keywords = {Literacy, disourse analysis, reading and writing practices},
abstract = {According to Gee (1998) the practices of social groups are never just reading and writing practices that take place in an institution; they also involve ways of talking, interacting, valuing and believing. Certain discourse tools offer researchers to study and analyze discourses of institutions in a concrete way (Fairclough, 1995). In this article discourses at a language institution are selected and analyzed from a larger case study of reading and writing practices at a State University in Turkey (Ozbilgin, 2004). The discourses and their associated roles include those of an instructor, a supervisor and administrative personnel at an English-medium institution. This collective case study will discuss how written and spoken texts and literacy practices of an institution have been analyzed to see the network of relations to form the discourse of an institution. The impacts of this network of relations are analyzed to see how they affect EFL teaching and learning.}
}

@article{xiong2016achieving,
    title = {Achieving Human Parity in Conversational Speech Recognition},
    author = {Xiong, W. and et al.},
    journal = {arXiv Preprint},
    eprint = {arXiv:1610.05256},
    year = {2016}
}

@article{naresh2019feature,
    title = {Feature Extraction Techniques in Speech Recognition: A Review},
    author = {Naresh, P. V. and Visalakshi, R.},
    journal = {International Journal of Management, Technology And Engineering},
    volume = {IX},
    number = {II},
    month = {February},
    year = {2019},
    issn = {2249-7455}
}
@inproceedings{Khan2019Robust,
author = {Khan, Isra and Farooqui, Ashhad and Ullah, Rafi and Emaduddin, Shah Muhammad},
year = {2019},
month = {04},
pages = {},
title = {Robust Feature Extraction Techniques in Speech Recognition: A Comparative Analysis}
}

@INPROCEEDINGS{gupta2020nlp,
  author={Gupta, Megha and Verma, Shailesh Kumar and Jain, Priyanshu},
  booktitle={2020 2nd International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)},
  title={Detailed Study of Deep Learning Models for Natural Language Processing},
  year={2020},
  volume={},
  number={},
  pages={249-253},
  doi={10.1109/ICACCCN51052.2020.9362989}
}


@ARTICLE{Guida1986NLP,
  author={Guida, G. and Mauri, G.},
  journal={Proceedings of the IEEE},
  title={Evaluation of natural language processing systems: Issues and approaches},
  year={1986},
  volume={74},
  number={7},
  pages={1026-1035},
  doi={10.1109/PROC.1986.13580}
}

@article{semaan2012natural,
    title = {Natural Language Generation: An Overview},
    author = {Semaan, P.},
    journal = {Journal of Computer Science \& Research (JCSCR)},
    pages = {50--57},
    year = {2012}
}

@incollection{yampolskiy2013turing,
    title = {Turing Test as a Defining Feature of AI-Completeness},
    author = {Yampolskiy, Roman V.},
    booktitle = {Artificial Intelligence, Evolutionary Computation and Metaheuristics (AIECM) - In the footsteps of Alan Turing},
    editor = {Yang, Xin-She},
    pages = {3--17},
    chapter = {1},
    publisher = {Springer},
    address = {London},
    year = {2013},
    url = {http://cecs.louisville.edu/ry/TuringTestasaDefiningFeature04270003.pdf}
}
@book{vanharmelen2008handbook,
    title = {Handbook of Knowledge Representation},
    volume = {1},
    editor = {Van Harmelen, Frank and Lifschitz, Vladimir and Porter, Bruce},
    publisher = {Elsevier},
    year = {2008}
}

@inproceedings{macherey2001natural,
    title = {Natural Language Understanding Using Statistical Machine Translation},
    author = {Macherey, Klaus and Och, Franz Josef and Ney, Hermann},
    booktitle = {Seventh European Conference on Speech Communication and Technology},
    year = {2001}
}

@article{hirschman2001natural,
    title = {Natural Language Question Answering: The View from Here},
    author = {Hirschman, Lynette and Gaizauskas, Robert},
    journal = {Natural Language Engineering},
    volume = {7},
    number = {4},
    pages = {275--300},
    year = {2001}
}

@inproceedings{chandankhede_voice_2021,
	address = {Greater Noida, India},
	title = {Voice {Recognition} {Based} {Security} {System} {Using} {Convolutional} {Neural} {Network}},
	isbn = {978-1-72818-529-3},
	url = {https://ieeexplore.ieee.org/document/9397151/},
	doi = {10.1109/ICCCIS51004.2021.9397151},
	urldate = {2023-05-06},
	booktitle = {2021 {International} {Conference} on {Computing}, {Communication}, and {Intelligent} {Systems} ({ICCCIS})},
	publisher = {IEEE},
	author = {Chandankhede, Pankaj H. and Titarmare, Abhijit S. and Chauhvan, Sarang},
	month = feb,
	year = {2021},
	pages = {738--743},
}

@article{satori_investigation_2014,
	title = {Investigation {Amazigh} speech recognition using {CMU} tools},
	volume = {17},
	issn = {1381-2416, 1572-8110},
	url = {http://link.springer.com/10.1007/s10772-014-9223-y},
	doi = {10.1007/s10772-014-9223-y},
	language = {en},
	number = {3},
	urldate = {2023-05-06},
	journal = {International Journal of Speech Technology},
	author = {Satori, Hassan and ElHaoussi, Fatima},
	month = sep,
	year = {2014},
	pages = {235--243},
}

@misc{shmyrev_cmusphinx_nodate,
	title = {{CMUSphinx} {Open} {Source} {Speech} {Recognition}},
	url = {http://cmusphinx.github.io/},
	abstract = {CMUSphinx is an open source speech recognition system for mobile and server applications. Supported languages: C, C++, C\#, Python, Ruby, Java, Javascript. Supported platforms: Unix, Windows, IOS, Android, hardware.},
	urldate = {2023-05-06},
	journal = {CMUSphinx Open Source Speech Recognition},
	author = {Shmyrev, Nickolay},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\QDJJKJGA\\cmusphinx.github.io.html:text/html},
}

@misc{noauthor_witai_nodate,
	title = {Wit.ai},
	url = {https://wit.ai/},
	urldate = {2023-05-06},
	file = {Wit.ai:C\:\\Users\\DELL\\Zotero\\storage\\5AEH8DUL\\wit.ai.html:text/html},
}

@misc{noauthor_notitle_nodate-3,
}

@book{zuckermann_language_2003,
	address = {New York, N.Y},
	series = {Palgrave studies in language history and language change},
	title = {Language contact and lexical enrichment in {Israeli} {Hebrew}},
	isbn = {978-1-4039-1723-2},
	publisher = {Palgrave Macmillan},
	author = {Zuckermann, Ghil'ad},
	year = {2003},
	keywords = {Etymology, Foreign elements, Hebrew language, Historical lexicology, Language and languages, Language revival, Languages in contact, Lexicology, New words, Sociolinguistics},
	annote = {"Milim, magaʻ, haʻasharah"--Jacket},
}

@book{richards_philosophy_1976,
	address = {London},
	edition = {Reprint},
	series = {The {Mary} {Flexner} lectures on the humanities},
	title = {The philosophy of rhetoric},
	isbn = {978-0-19-500715-2},
	language = {eng},
	number = {3 = 1935},
	publisher = {Oxford Univ. Press},
	author = {Richards, I. A.},
	year = {1976},
}

@incollection{michaelis_2_2008,
	address = {Amsterdam},
	title = {2. {The} superstrate is not always the lexifier: {Lingua} {Franca} in the {Barbary} {Coast} 1530-1830},
	volume = {33},
	isbn = {978-90-272-5255-5 978-90-272-8996-4},
	shorttitle = {2. {The} superstrate is not always the lexifier},
	url = {https://benjamins.com/catalog/cll.33.05sel},
	language = {en},
	urldate = {2023-05-29},
	booktitle = {Creole {Language} {Library}},
	publisher = {John Benjamins Publishing Company},
	author = {Selbach, Rachel},
	editor = {Michaelis, Susanne},
	year = {2008},
	doi = {10.1075/cll.33.05sel},
	pages = {29--58},
}

@book{michaelis_roots_2008,
	address = {Amsterdam},
	series = {Creole {Language} {Library}},
	title = {Roots of {Creole} {Structures}: {Weighing} the contribution of substrates and superstrates},
	volume = {33},
	isbn = {978-90-272-5255-5 978-90-272-8996-4},
	shorttitle = {Roots of {Creole} {Structures}},
	url = {http://www.jbe-platform.com/content/books/9789027289964},
	language = {en},
	urldate = {2023-05-29},
	publisher = {John Benjamins Publishing Company},
	editor = {Michaelis, Susanne},
	month = oct,
	year = {2008},
	doi = {10.1075/cll.33},
}

@book{michaelis_michaelis_2008,
	title = {Michaelis, {Susanne} ({Ed}.). 2008. {Roots} of creole structures: {Weighing} the contributions of substrates and superstrates [{Creole} {Language} {Library} 33]. {Amsterdam}/{Philadelphia}: {Benjamins}.},
	shorttitle = {Michaelis, {Susanne} ({Ed}.). 2008. {Roots} of creole structures},
	author = {Michaelis, Susanne},
	month = jan,
	year = {2008},
}

@incollection{thomas_what_2014,
	edition = {0},
	title = {What is pragmatics?},
	isbn = {978-1-315-84201-1},
	url = {https://www.taylorfrancis.com/books/9781317887607/chapters/10.4324/9781315842011-7},
	language = {en},
	urldate = {2023-05-29},
	booktitle = {Meaning in {Interaction}},
	publisher = {Routledge},
	collaborator = {Thomas, Jenny A.},
	month = may,
	year = {2014},
	doi = {10.4324/9781315842011-7},
	pages = {17--43},
}

@book{mey_pragmatics_2001,
	address = {Malden, MA},
	edition = {2nd ed},
	title = {Pragmatics: an introduction},
	isbn = {978-0-631-21131-0 978-0-631-21132-7},
	shorttitle = {Pragmatics},
	publisher = {Blackwell Publishers},
	author = {Mey, Jacob},
	year = {2001},
	keywords = {Pragmatics},
}

@incollection{lopez-ruiz_commonly_2018,
	title = {Some {Commonly} {Used} {Speech} {Feature} {Extraction} {Algorithms}},
	isbn = {978-1-78984-702-4 978-1-78984-703-1},
	url = {https://www.intechopen.com/books/from-natural-to-artificial-intelligence-algorithms-and-applications/some-commonly-used-speech-feature-extraction-algorithms},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {From {Natural} to {Artificial} {Intelligence} - {Algorithms} and {Applications}},
	publisher = {IntechOpen},
	author = {Ajibola Alim, Sabur and Khair Alang Rashid, Nahrul},
	editor = {Lopez-Ruiz, Ricardo},
	month = dec,
	year = {2018},
	doi = {10.5772/intechopen.80419},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\I5YMIJ4L\\Ajibola Alim and Khair Alang Rashid - 2018 - Some Commonly Used Speech Feature Extraction Algor.pdf:application/pdf},
}

@article{cabral_feature_2019,
	title = {Feature {Extraction} {Methods} {Proposed} for {Speech} {Recognition} {Are} {Effective} on {Road} {Condition} {Monitoring} {Using} {Smartphone} {Inertial} {Sensors}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/16/3481},
	doi = {10.3390/s19163481},
	abstract = {The objective of our project is to develop an automatic survey system for road condition monitoring using smartphone devices. One of the main tasks of our project is the classification of paved and unpaved roads. Assuming recordings will be archived by using various types of vehicle suspension system and speeds in practice, hence, we use the multiple sensors found in smartphones and state-of-the-art machine learning techniques for signal processing. Despite usually not being paid much attention, the results of the classification are dependent on the feature extraction step. Therefore, we have to carefully choose not only the classification method but also the feature extraction method and their parameters. Simple statistics-based features are most commonly used to extract road surface information from acceleration data. In this study, we evaluated the mel-frequency cepstral coefficient (MFCC) and perceptual linear prediction coefficients (PLP) as a feature extraction step to improve the accuracy for paved and unpaved road classification. Although both MFCC and PLP have been developed in the human speech recognition field, we found that modified MFCC and PLP can be used to improve the commonly used statistical method.},
	language = {en},
	number = {16},
	urldate = {2023-07-05},
	journal = {Sensors},
	author = {Cabral, Frederico Soares and Fukai, Hidekazu and Tamura, Satoshi},
	month = aug,
	year = {2019},
	pages = {3481},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\2P9PYNY2\\Cabral et al. - 2019 - Feature Extraction Methods Proposed for Speech Rec.pdf:application/pdf},
}

@misc{trabelsi_use_2014,
	title = {On the {Use} of {Different} {Feature} {Extraction} {Methods} for {Linear} and {Non} {Linear} kernels},
	url = {http://arxiv.org/abs/1406.7314},
	abstract = {The speech feature extraction has been a key focus in robust speech recognition research; it significantly affects the recognition performance. In this paper, we first study a set of different features extraction methods such as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC) and perceptual linear prediction (PLP) with several features normalization techniques like rasta filtering and cepstral mean subtraction (CMS). Based on this, a comparative evaluation of these features is performed on the task of text independent speaker identification using a combination between gaussian mixture models (GMM) and linear and non-linear kernels based on support vector machine (SVM).},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Trabelsi, Imen and Ayed, Dorra Ben},
	month = jun,
	year = {2014},
	note = {arXiv:1406.7314 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 8 pages, 3 Figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\PX84M2U4\\Trabelsi and Ayed - 2014 - On the Use of Different Feature Extraction Methods.pdf:application/pdf},
}

@book{michaelis_roots_2008,
	address = {Amsterdam},
	series = {Creole {Language} {Library}},
	title = {Roots of {Creole} {Structures}: {Weighing} the contribution of substrates and superstrates},
	volume = {33},
	isbn = {978-90-272-5255-5 978-90-272-8996-4},
	shorttitle = {Roots of {Creole} {Structures}},
	url = {http://www.jbe-platform.com/content/books/9789027289964},
	language = {en},
	urldate = {2023-05-29},
	publisher = {John Benjamins Publishing Company},
	editor = {Michaelis, Susanne},
	month = oct,
	year = {2008},
	doi = {10.1075/cll.33},
}

@book{michaelis_michaelis_2008,
	title = {Michaelis, {Susanne} ({Ed}.). 2008. {Roots} of creole structures: {Weighing} the contributions of substrates and superstrates [{Creole} {Language} {Library} 33]. {Amsterdam}/{Philadelphia}: {Benjamins}.},
	shorttitle = {Michaelis, {Susanne} ({Ed}.). 2008. {Roots} of creole structures},
	author = {Michaelis, Susanne},
	month = jan,
	year = {2008},
}

@incollection{thomas_what_2014,
	edition = {0},
	title = {What is pragmatics?},
	isbn = {978-1-315-84201-1},
	url = {https://www.taylorfrancis.com/books/9781317887607/chapters/10.4324/9781315842011-7},
	language = {en},
	urldate = {2023-05-29},
	booktitle = {Meaning in {Interaction}},
	publisher = {Routledge},
	collaborator = {Thomas, Jenny A.},
	month = may,
	year = {2014},
	doi = {10.4324/9781315842011-7},
	pages = {17--43},
}

@book{mey_pragmatics_2001,
	address = {Malden, MA},
	edition = {2nd ed},
	title = {Pragmatics: an introduction},
	isbn = {978-0-631-21131-0 978-0-631-21132-7},
	shorttitle = {Pragmatics},
	publisher = {Blackwell Publishers},
	author = {Mey, Jacob},
	year = {2001},
	keywords = {Pragmatics},
}

@incollection{lopez-ruiz_commonly_2018,
	title = {Some {Commonly} {Used} {Speech} {Feature} {Extraction} {Algorithms}},
	isbn = {978-1-78984-702-4 978-1-78984-703-1},
	url = {https://www.intechopen.com/books/from-natural-to-artificial-intelligence-algorithms-and-applications/some-commonly-used-speech-feature-extraction-algorithms},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {From {Natural} to {Artificial} {Intelligence} - {Algorithms} and {Applications}},
	publisher = {IntechOpen},
	author = {Ajibola Alim, Sabur and Khair Alang Rashid, Nahrul},
	editor = {Lopez-Ruiz, Ricardo},
	month = dec,
	year = {2018},
	doi = {10.5772/intechopen.80419},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\I5YMIJ4L\\Ajibola Alim and Khair Alang Rashid - 2018 - Some Commonly Used Speech Feature Extraction Algor.pdf:application/pdf},
}

@article{cabral_feature_2019,
	title = {Feature {Extraction} {Methods} {Proposed} for {Speech} {Recognition} {Are} {Effective} on {Road} {Condition} {Monitoring} {Using} {Smartphone} {Inertial} {Sensors}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/16/3481},
	doi = {10.3390/s19163481},
	abstract = {The objective of our project is to develop an automatic survey system for road condition monitoring using smartphone devices. One of the main tasks of our project is the classification of paved and unpaved roads. Assuming recordings will be archived by using various types of vehicle suspension system and speeds in practice, hence, we use the multiple sensors found in smartphones and state-of-the-art machine learning techniques for signal processing. Despite usually not being paid much attention, the results of the classification are dependent on the feature extraction step. Therefore, we have to carefully choose not only the classification method but also the feature extraction method and their parameters. Simple statistics-based features are most commonly used to extract road surface information from acceleration data. In this study, we evaluated the mel-frequency cepstral coefficient (MFCC) and perceptual linear prediction coefficients (PLP) as a feature extraction step to improve the accuracy for paved and unpaved road classification. Although both MFCC and PLP have been developed in the human speech recognition field, we found that modified MFCC and PLP can be used to improve the commonly used statistical method.},
	language = {en},
	number = {16},
	urldate = {2023-07-05},
	journal = {Sensors},
	author = {Cabral, Frederico Soares and Fukai, Hidekazu and Tamura, Satoshi},
	month = aug,
	year = {2019},
	pages = {3481},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\2P9PYNY2\\Cabral et al. - 2019 - Feature Extraction Methods Proposed for Speech Rec.pdf:application/pdf},
}

@misc{trabelsi_use_2014,
	title = {On the {Use} of {Different} {Feature} {Extraction} {Methods} for {Linear} and {Non} {Linear} kernels},
	url = {http://arxiv.org/abs/1406.7314},
	abstract = {The speech feature extraction has been a key focus in robust speech recognition research; it significantly affects the recognition performance. In this paper, we first study a set of different features extraction methods such as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC) and perceptual linear prediction (PLP) with several features normalization techniques like rasta filtering and cepstral mean subtraction (CMS). Based on this, a comparative evaluation of these features is performed on the task of text independent speaker identification using a combination between gaussian mixture models (GMM) and linear and non-linear kernels based on support vector machine (SVM).},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Trabelsi, Imen and Ayed, Dorra Ben},
	month = jun,
	year = {2014},
	note = {arXiv:1406.7314 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 8 pages, 3 Figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\PX84M2U4\\Trabelsi and Ayed - 2014 - On the Use of Different Feature Extraction Methods.pdf:application/pdf},
}


@inproceedings{rajpal2020pseudo,
    title = {Pseudo Likelihood Correction Technique for Low Resource Accented ASR},
    author = {Rajpal, A. and Mv, A. R. and Yarra, C. and Aggarwal, R. and Ghosh, P. K.},
    booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
    month = {May},
    year = {2020},
    pages = {7434--7438},
    doi = {10.1109/ICASSP40776.2020.9053647}
}

@article{hirayama2015automatic,
    title = {Automatic Speech Recognition for Mixed Dialect Utterances by Mixing Dialect Language Models},
    author = {Hirayama, N. and Yoshino, K. and Itoyama, K. and Mori, S. and Okuno, H. G.},
    journal = {IEEE/ACM Transactions on Audio, Speech, Language Processing},
    volume = {23},
    number = {2},
    pages = {373--382},
    month = {February},
    year = {2015},
    doi = {10.1109/TASLP.2014.2387414}
}

@inproceedings{martin2020understanding,
    title = {Understanding Racial Disparities in Automatic Speech Recognition: The Case of Habitual},
    author = {Martin, J. L. and Tang, K.},
    booktitle = {Proc. INTERSPEECH},
    year = {2020},
    pages = {626--630}
}

@inproceedings{rasipuram2015integrated,
    title = {Integrated Pronunciation Learning for Automatic Speech Recognition Using Probabilistic Lexical Modeling},
    author = {Rasipuram, R. and Razavi, M. and Magimai-Doss, M.},
    booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
    month = {April},
    year = {2015},
    pages = {5176--5180},
    doi = {10.1109/ICASSP.2015.7178958}
}

@inproceedings{garg2020hierarchical,
    title = {Hierarchical Multistage Word-to-Grapheme Named Entity Corrector for Automatic Speech Recognition},
    author = {Garg, A. and Gupta, A. and Gowda, D. and Singh, S. and Kim, C.},
    booktitle = {Proc. INTERSPEECH},
    month = {October},
    year = {2020},
    pages = {1793--1797}
}


@inproceedings{le2020g2g,
    title = {G2G: TTS-Driven Pronunciation Learning for Graphemic Hybrid ASR},
    author = {Le, D. and Koehler, T. and Fuegen, C. and Seltzer, M. L.},
    booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
    month = {May},
    year = {2020},
    pages = {6869--6873},
    doi = {10.1109/ICASSP40776.2020.9054257}
}


@inproceedings{exter2016dnn,
    title = {DNN-Based Automatic Speech Recognition as a Model for Human Phoneme Perception},
    author = {Exter, M. and Meyer, B. T.},
    booktitle = {Proc. INTERSPEECH},
    month = {September},
    year = {2016},
    pages = {615--619}
}

@article{liu2016graph,
    title = {Graph-Based Semisupervised Learning for Acoustic Modeling in Automatic Speech Recognition},
    author = {Liu, Y. and Kirchhoff, K.},
    journal = {IEEE/ACM Transactions on Audio, Speech, Language Processing},
    volume = {24},
    number = {11},
    pages = {1946--1956},
    month = {November},
    year = {2016},
    doi = {10.1109/TASLP.2016.2593800}
}

@article{keshet2018automatic,
    title = {Automatic Speech Recognition: A Primer for Speech-Language Pathology Researchers},
    author = {Keshet, J.},
    journal = {Int. J. Speech-Lang. Pathol.},
    volume = {20},
    number = {6},
    pages = {599--609},
    month = {October},
    year = {2018},
    doi = {10.1080/17549507.2018.1510033}
}

@article{li2017mispronunciation,
    title = {Mispronunciation Detection and Diagnosis in L2 English Speech Using Multidistribution Deep Neural Networks},
    author = {Li, K. and Qian, X. and Meng, H.},
    journal = {IEEE/ACM Trans. Audio, Speech, Language Process.},
    volume = {25},
    number = {1},
    pages = {193--207},
    month = {January},
    year = {2017},
    doi = {10.1109/TASLP.2016.2621675}
}

@article{grozdic2017whispered,
    title = {Whispered Speech Recognition Using Deep Denoising Autoencoder and Inverse Filtering},
    author = {Grozdić, T. and Jovičić, S. T.},
    journal = {IEEE/ACM Trans. Audio, Speech, Language Process.},
    volume = {25},
    number = {12},
    pages = {2313--2322},
    month = {December},
    year = {2017},
    doi = {10.1109/TASLP.2017.2738559}
}

@inproceedings{fringi2015evidence,
    title = {Evidence of Phonological Processes in Automatic Recognition of Children's Speech},
    author = {Fringi, E. and Lehman, J. F. and Russell, M.},
    booktitle = {Proc. 16th Annu. Conf. Int. Speech Commun. Assoc.},
    pages = {1--4},
    year = {2015}
}


@article{deng2004challenges,
    title = {Challenges in Adopting Speech Recognition},
    author = {Deng, L. and Huang, X.},
    journal = {Communications of the ACM},
    volume = {47},
    number = {1},
    pages = {69--75},
    year = {2004}
}

@article{cheng1991speech,
  title={Speech enhancement based conceptually on auditory evidence},
  author={Cheng, Y. M. and O'Shaughnessy, D.},
  journal={IEEE Transactions on Signal Processing},
  volume={39},
  number={9},
  pages={1943--1954},
  year={1991},
  month={Sept.},
  doi={10.1109/78.134427}
}

@inbook{oghitza1992auditory,
  title={Auditory nerve representation as a basis for speech processing},
  author={Oghitza, O.},
  booktitle={Advances in Speech Signal Processing},
  editor={Furui, S. and Sondhi, M. M.},
  chapter={15},
  pages={453--485},
  publisher={Marcel Dekker},
  year={1992},
  address={New York}
}

@inproceedings{wu1990modeling,
  title={Modeling spectral processing in the central auditory system},
  author={Wu, Z. L. and Schwartz, J.-L. and Escudier, P.},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing},
  pages={373--376},
  volume={1},
  year={1990},
  month={Apr.},
  doi={10.1109/ICASSP.1990.115693}
}

@article{dautrich1983effects,
  title={On the effects of varying filter bank parameters on isolated word recognition},
  author={Dautrich, B. and Rabiner, L. and Martin, T.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume={31},
  number={4},
  pages={793--807},
  year={1983},
  month={Aug.},
  doi={10.1109/TASSP.1983.1164172}
}

@inproceedings{furui1992toward,
  title={Toward robust speech recognition under adverse conditions},
  author={Furui, S.},
  booktitle={ESCA Workshop Proc. Speech Processing in Adverse Conditions},
  pages={31--41},
  year={1992},
  address={Cannes, France}
}

@inproceedings{tsuboi1990accelerator,
  title={An accelerator for high-speed spoken word-spotting and noise immunity learning system},
  author={Tsuboi, H. and Kanazawa, H. and Takebayashi, Y.},
  booktitle={Proc. First International Conference on Spoken Language Processing (ICSLP 1990)},
  pages={273--276},
  year={1990},
  doi={10.21437/ICSLP.1990-69}
}

@inproceedings{sankar1994noise,
  title={Noise immunization using neural net for speech recognition},
  author={Sankar, R. and Patravali, S.},
  booktitle={Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={II/685--II/688},
  volume={2},
  year={1994},
  doi={10.1109/ICASSP.1994.389563}
}

@inproceedings{kitamura1990speaker,
  title={Speaker-independent word recognition in noisy environments using dynamic and averaged spectral features based on a two-dimensional mel-cepstrum},
  author={Kitamura, T. and Hayahara, E. and Simazciki, Y.},
  booktitle={Proc. First International Conference on Spoken Language Processing (ICSLP 1990)},
  pages={1129--1132},
  year={1990},
  doi={10.21437/ICSLP.1990-299}
}

@inproceedings{kitamura1993speaker,
  title={Speaker-independent 100 word recognition using dynamic spectral features of speech and a neural network},
  author={Kitamura, T.},
  booktitle={Proc. 3rd European Conference on Speech Communication and Technology (Eurospeech 1993)},
  pages={1229--1232},
  year={1993},
  doi={10.21437/Eurospeech.1993-290}
}

@inproceedings{kiatmura1992speaker,
  title={Speaker independent spoken digit recognition in noise environments using dynamic spectral features and neural networks},
  author={Kiatmura, T. and Ando, S. and Hayahara, E.},
  booktitle={Internat. Conf. on Speech and Language Processing},
  volume={1},
  pages={699--702},
  year={1992}
}

@inproceedings{lippmann1987multi,
  title={Multi-style training for robust isolated-word speech recognition},
  author={Lippmann, R. and Martin, E. and Paul, D.},
  booktitle={ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  pages={705--708},
  year={1987},
  doi={10.1109/ICASSP.1987.1169544}
}

@inproceedings{mizuta1992optimal,
  title={Optimal discriminative training for HMMs to recognize noisy speech},
  author={Mizuta, S. and Nakajima, K.},
  booktitle={Proc. 2nd International Conference on Spoken Language Processing (ICSLP 1992)},
  pages={1519--1522},
  year={1992},
  doi={10.21437/ICSLP.1992-194}
}

@article{lee1991study,
  title={A study on speaker adaptation of the parameters of continuous density hidden Markov models},
  author={Lee, C. H. and Lin, C. H. and Juang, B. H.},
  journal={IEEE Transactions on Signal Processing},
  volume={39},
  number={4},
  pages={806--814},
  year={1991},
  month={Apr.},
  doi={10.1109/78.80902}
}

@inproceedings{ferguson1980variable,
  title={Variable duration models for speech},
  author={Ferguson, J. D.},
  booktitle={Proc. Symp. on the Applications of Hidden Markov Models to Text to Speech},
  pages={143--179},
  year={1980},
  organization={IDA-CRD}
}

@inproceedings{russell1987experimental,
  title={Experimental evaluation of duration modelling techniques for automatic speech recognition},
  author={Russell, M. and Cook, A.},
  booktitle={ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  pages={2376--2379},
  year={1987},
  doi={10.1109/ICASSP.1987.1169918}
}

@inproceedings{nicol1992improving,
title={Improving the robustness of automatic speech recognisers using state duration information},
author={Nicol, N. and Euler, S. and Falkhausen, M. and Reininger, H. and Wolf, D. and Zinke, J.},
booktitle={ESCA Workshop Proc. Speech Processing in Adverse Conditions},
pages={183--186},
year={1992}
}

@inproceedings{varga1990hidden,
title={Hidden Markov model decomposition of speech and noise},
author={Varga, A. P. and Moore, R. K.},
booktitle={International Conference on Acoustics, Speech, and Signal Processing},
pages={845--848},
volume={2},
year={1990},
doi={10.1109/ICASSP.1990.115970}
}

@inproceedings{gales1992improved,
title={An improved approach to the hidden Markov model decomposition of speech and noise},
author={Gales, M. J. F. and Young, S.},
booktitle={Proceedings of ICASSP '92. IEEE International Conference on Acoustics, Speech, and Signal Processing},
pages={233--236},
volume={1},
year={1992},
doi={10.1109/ICASSP.1992.225929}
}

@inproceedings{kobayashi1994markov,
title={Markov model based noise modeling and its application to noisy speech recognition using dynamical features of speech},
author={Kobayashi, T. and Mine, R. and Shirai, K.},
booktitle={Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
pages={II/57--II/60},
volume={2},
year={1994},
doi={10.1109/ICASSP.1994.389719}
}

@inproceedings{vaseghi1994noisy,
title={Noisy speech recognition using cepstral-time features and spectral-time filters},
author={Vaseghi, S. V. and Milner, B. P. and Humphries, J. J.},
booktitle={Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
pages={II/65--II/68},
volume={2},
year={1994},
doi={10.1109/ICASSP.1994.389717}
}

@inproceedings{mokbel1992word,
title={Word recognition in the car: adapting recognizers to new environments},
author={Mokbel, C. and Barbier, L. and Kerlou, Y. and Chollet, G.},
booktitle={Proc. 2nd International Conference on Spoken Language Processing (ICSLP 1992)},
pages={707--710},
year={1992},
doi={10.21437/ICSLP.1992-239}
}

@inproceedings{tamura1990improvements,
title={Improvements to the noise reduction neural network},
author={Tamura, S. and Nakamura, M.},
booktitle={International Conference on Acoustics, Speech, and Signal Processing},
pages={825--828},
volume={2},
year={1990},
doi={10.1109/ICASSP.1990.115957}
}

@article{berouti1979enhancement,
title={Enhancement of speech corrupted by acoustic noise},
author={Berouti, M. and Schwartz, R. and Makhoul, J.},
journal={ICASSP '79. IEEE International Conference on Acoustics, Speech, and Signal Processing},
pages={208--211},
year={1979},
doi={10.1109/ICASSP.1979.1170788}
}

@article{boll1979suppression,
title={Suppression of acoustic noise in speech using spectral subtraction},
author={Boll, S.},
journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
volume={27},
number={2},
pages={113--120},
year={1979},
month={Apr.},
doi={10.1109/TASSP.1979.1163209}
}

@inproceedings{flores1993continuous,
title={Continuous speech recognition in noise using spectral subtraction and HMM adaptation},
author={Flores, J. A. N. and Young, S. J.},
booktitle={Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
pages={I/409--I/412},
volume={1},
year={1994},
doi={10.1109/ICASSP.1994.389269}
}

@inproceedings{klatt1976digital,
title={A digital filter bank for spectral matching},
author={Klatt, D.},
booktitle={ICASSP '76. IEEE International Conference on Acoustics, Speech, and Signal Processing},
pages={573--576},
year={1976},
doi={10.1109/ICASSP.1976.1170107}
}

@inproceedings{holmes1986noise,
title={Noise compensation for speech recognition using probabilistic models},
author={Holmes, J. and Sedgwick, N.},
booktitle={ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing},
pages={741--744},
year={1986},
doi={10.1109/ICASSP.1986.1169209}
}

@inproceedings{varga1989control,
title={Control experiments on noise compensation in hidden Markov model based continuous word recognition},
author={Varga, A. and Ponting, K.},
booktitle={Proc. First European Conference on Speech Communication and Technology (Eurospeech 1989)},
pages={1167--1170},
year={1989},
doi={10.21437/Eurospeech.1989-53}
}

@inproceedings{mellor1993noise,
title={Noise masking in a transform domain},
author={Mellor, B. A. and Varga, A. P.},
booktitle={Proceedings of ICASSP '93. IEEE International Conference on Acoustics, Speech and Signal Processing},
pages={87--90},
volume={2},
year={1993},
doi={10.1109/ICASSP.1993.319237}
}

@inproceedings{graf1993dynamic,
title={Dynamic time warping comb filter for the enhancement of speech degraded by white Gaussian noise},
author={Graf, J. T. and Hubing, N.},
booktitle={Proceedings of ICASSP '93. IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={339--342},
  volume={2},
  year={1993},
  doi={10.1109/ICASSP.1993.319307}
}

@article{lim1978evaluation,
  title={Evaluation of an adaptive comb filtering method for enhancing speech degraded by white noise addition},
  author={Lim, Jae and Oppenheim, A. and Braida, L.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume={26},
  number={4},
  pages={354--358},
  year={1978},
  month={Aug.},
  doi={10.1109/TASSP.1978.1163117}
}

@inproceedings{o1988speech,
  title={Speech enhancement using vector quantization and a formant distance measure},
  author={O'Shaughnessy, D.},
  booktitle={ICASSP-88. International Conference on Acoustics, Speech, and Signal Processing},
  pages={549--552},
  volume={1},
  year={1988},
  doi={10.1109/ICASSP.1988.196642}
}

@inproceedings{gong1993base,
  title={Base transformation for environment adaptation in continuous speech recognition},
  author={Gong, Y.},
  booktitle={Proc. 3rd European Conference on Speech Communication and Technology (Eurospeech 1993)},
  pages={2227--2230},
  year={1993},
  doi={10.21437/Eurospeech.1993-500}
}

@inproceedings{treurniet1994noise,
  title={Noise independent speech recognition for a variety of noise types},
  author={Treurniet, W. C. and Gong, Y.},
  booktitle={Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={I/437--I/440},
  volume={1},
  year={1994},
  doi={10.1109/ICASSP.1994.389262}
}

@inproceedings{van1989spectral,
  title={Spectral estimation using a log-distance error criterion applied to speech recognition},
  author={Van Compernolle, D.},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing},
  pages={258--261},
  volume={1},
  year={1989},
  doi={10.1109/ICASSP.1989.266414}
}

@article{erell1993filterbank,
  title={Filterbank-energy estimation using mixture and Markov models for recognition of noisy speech},
  author={Erell, A. and Weintraub, M.},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={1},
  number={1},
  pages={68--76},
  year={1993},
  month={Jan.},
  doi={10.1109/89.221385}
}

@inproceedings{ephraim1990minimum,
  title={A minimum mean square error approach for speech enhancement},
  author={Ephraim, Y.},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing},
  pages={829--832},
  volume={2},
  year={1990},
  doi={10.1109/ICASSP.1990.115960}
}

@book{bregman1990auditory,
  title={Auditory Scene Analysis},
  author={Bregman, A. S.},
  year={1990},
  publisher={MIT Press},
  address={London},
  doi={10.7551/mitpress/1486.001.0001},
  isbn={9780262269209},
  note={In Special Collection: CogNet}
}

@inproceedings{berthommier1995source,
  title={Source separation by a functional model of amplitude demodulation},
  author={Berthommier, F. and Meyer, G. F.},
  booktitle={Proc. 4th European Conference on Speech Communication and Technology (Eurospeech 1995)},
  pages={135--138},
  year={1995},
  doi={10.21437/Eurospeech.1995-37}
}

@article{langner1988periodicity,
  title={Periodicity coding in the inferior colliculus of the cat I: Neuronal mechanisms},
  author={Langner, G. and Schreiner, C. E.},
  journal={Journal of Neurophysiology},
  volume={60},
  number={6},
  pages={1799--1822},
  year={1988},
  month={Dec.},
  doi={10.1152/jn.1988.60.6.1799}
}

@article{makhoul1995state,
  title={State of the art in continuous speech recognition},
  author={Makhoul, J. and Schwartz, R.},
  journal={Proc Natl Acad Sci USA},
  volume={92},
  number={22},
  pages={9956--9963},
  year={1995},
  month={Oct.},
  doi={10.1073/pnas.92.22.9956},
  pmid={7479809},
  pmcid={PMC40718}
}

@inproceedings{paul1992design,
    title = {The Design for the Wall Street Journal-based CSR Corpus},
    author = {Paul, D.},
    booktitle = {Proceedings of the DARPA Speech and Natural Language Workshop},
    publisher = {Morgan Kaufmann Publishers},
    pages = {357-360},
    month = {Feb.},
    year = {1992}
}

@book{national1994voice,
    title = {Voice Communication Between Humans and Machines},
    author = {{National Academies of Sciences, Engineering, and Medicine}},
    year = {1994},
    publisher = {The National Academies Press},
    address = {Washington, DC},
    doi = {10.17226/2308}
}

@inproceedings{madcow1992multi,
    title = {Multi-Site Data Collection for a Spoken Language Corpus},
    author = {{MADCOW}},
    booktitle = {Proceedings of the DARPA Speech and Natural Language Workshop},
    address = {Harriman, N.Y.},
    publisher = {Morgan Kaufmann Publishers},
    pages = {7-14},
    month = {Feb.},
    year = {1992}
}

@article{junqua1993lombard,
    title = {The Lombard Reflex and Its Role on Human Listeners and Automatic Speech Recognizers},
    author = {Junqua, J.-C.},
    journal = {Journal of the Acoustical Society of America},
    volume = {93},
    number = {1},
    pages = {510-524},
    year = {1993},
    doi = {10.1121/1.405631}
}

@article{atal1974effectiveness,
    title = {Effectiveness of Linear Prediction Characteristics of the Speech Wave for Automatic Speaker Identification and Verification},
    author = {Atal, B. S.},
    journal = {Journal of the Acoustical Society of America},
    volume = {55},
    number = {6},
    pages = {1304-1312},
    year = {1974},
    doi = {10.1121/1.1914702}
}

@inproceedings{mena1990comparative,
    title = {A Comparative Study of Feature Extraction Methods for Noisy Speech Recognition},
    author = {Mena, J.G. and Sandoval, L.S. and Gomez, R.G.},
    booktitle = {Speech Signal Processing V: Theories and Applications},
    editor = {Torres, L. and Masgrau, E. and Lagunas, M.A.},
    pages = {1191-1194},
    year = {1990}
}


@book{10.5555/546536,
author = {Junqua, Jean-Claude and Haton, Jean-Paul},
title = {Robustness in Automatic Speech Recognition: Fundamentals and Applications},
year = {1995},
isbn = {0792396464},
url={https://dl.acm.org/doi/10.5555/546536},
publisher = {Kluwer Academic Publishers},
address = {USA}
}

@article{GONG1995noisyasrsurvey,
title = {Speech recognition in noisy environments: A survey},
journal = {Speech Communication},
volume = {16},
number = {3},
pages = {261-291},
year = {1995},
issn = {0167-6393},
doi = {https://doi.org/10.1016/0167-6393(94)00059-J},
url = {https://www.sciencedirect.com/science/article/pii/016763939400059J},
author = {Yifan Gong},
keywords = {Survey, Noisy speech recognition, Parametrization, Speech enhancement, Compensation for noise},
abstract = {The performance levels of most current speech recognizers degrade significantly when environmental noise occurs during use. Such performance degradation is mainly caused by mismatches in training and operating environments. During recent years much effort has been directed to reducing this mismatch. This paper surveys research results in the area of digital techniques for single microphone noisy speech recognition classified in three categories: noise resistant features and similarity measurement, speech enhancement, and speech model compensation for noise. The survey indicates that the essential points in noisy speech recognition consist of incorporating time and frequency correlations, giving more importance to high SNR portions of speech in decision making, exploiting task-specific a priori knowledge both of speech and of noise, using class-dependent processing, and including auditory models in speech processing.
Zusammenfassung
Die Leistung der meisten gebräuchlichen Spracherkennungssysteme sinkt erheblich, wenn sie unter Geräuscheinwirkung arbeiten. Dieser Leistungsabfall wird hauptsächlich von den Divergenzen zwischen Trainingsund Durchführungsphase verursacht. Groβe Anstrengungen sind in den letzten Jahren gemacht worden, um diese Divergenzen zu reduzieren. Dieser Artikel gibt einen in drei Kategorien gegliederten Überblick zu den Resultaten der Untersuchungen im Bereich der digitalen Technik für Einzelmikrophone in geräuschgestörter Umgebung: geräuschunempfindliche Parametrisierung und Übereinstimmungsmessungen, Geräuschfilterung der Sprache und Kompensation der Sprachmodelle bei Geräuscheinwirkung. Der Artikel zeigt die wesentlichen Punkte der Spracherkennung bei Geräuscheinwirkung, nämlich Einbeziehung der Zeit- und Frequenzkorrelation, stärkere Berücksichtigung der Signalabschnitte mit groβem Sprach/Geräusch-Verhältnis in der Entscheidungsphase, Ausnutzung von aufgabenspezifischen Kenntnissen über Signal und Geräusch, Anwendung von klassenabhängigen Sprachverarbeitungs-prozessen, und Benutzung von auditiven Modellen.
Résumé
Le bruit d'environnement dégrade de façon significative les performances de la plupart des systèmes actuels de reconnaissance automatique de la parole. Cette dégradation provient principalement des différences entre les environnements d'apprentissage et d'utilisation d'un système. Ces dernières années, de nombreux travaux ont porté sur la réduction de ces différences. Une synthèse des résultats de ces recherches est présentée dans cet article, selon trois grandes catégories: les paramétrages résistant au bruit et les mesures de similarité, le débruitage de la parole, et la compensation des modèles en présence de bruit. L'article met en évidence les points essentiels en reconnaissance de parole bruitée, à savoir l'utilisation des corrélations en temps et en fréquence du signal, l'augmentation de l'importance des portions du signal ayant un rapport S/B élevé lors de la décision, la prise en compte de connaissances spécifiques à la tâche sur le signal et sur le bruit, la mise en oeuvre de traitements dépendant des classes d'événements de la parole, et enfin l'utilisation des modèles auditifs.}
}

@article{BROWN1994compaud,
title = {Computational auditory scene analysis},
journal = {Computer Speech & Language},
volume = {8},
number = {4},
pages = {297-336},
year = {1994},
issn = {0885-2308},
doi = {https://doi.org/10.1006/csla.1994.1016},
url = {https://www.sciencedirect.com/science/article/pii/S0885230884710163},
author = {Guy J. Brown and Martin Cooke},
abstract = {Although the ability of human listeners to perceptually segregate concurrent sounds is well documented in the literature, there have been few attempts to exploit this research in the design of computational systems for sound source segregation. In this paper, we present a segregation system that is consistent with psychological and physiological findings. The system is able to segregate speech from a variety of intrusive sounds, including other speech, with some success. The segregation system consists of four stages. Firstly, the auditory periphery is modelled by a bank of bandpass filters and a simulation of neuromechanical transduction by inner hair cells. In the second stage of the system, periodicities, frequency transitions, onsets and offsets in auditory nerve firing patterns are made explicit by separate auditory representations. The representations, auditory maps, are based on the known topographical organization of the higher auditory pathways. Information from the auditory maps is used to construct a symbolic description of the auditory scene. Specifically, the acoustic input is characterized as a collection of time-frequency elements, each of which describes the movement of a spectral peak in time and frequency. In the final stage of the system, a search strategy is employed which groups elements according to the similarity of their fundamental frequencies, onset times and offset times. Following the search, a waveform can be resynthesized from a group of elements so that segregation performance may be assessed by informal listening tests. The system has been evaluated using a database of voiced speech mixed with a variety of intrusive noises such as music, "office" noise and other speech. A technique for quantitative evaluation of the system is described, in which the signal-to-noise ratio (SNR) is compared before and after the segregation process. After segregation, an increase in SNR is obtained for each noise condition. Additionally, the performance of our system is significantly better than that of the frame-based segregation scheme described by Meddis and Hewitt (1992).}
}

@article{BRANDSTEIN1995time,
title = {A practical time-delay estimator for localizing speech sources with a microphone array},
journal = {Computer Speech & Language},
volume = {9},
number = {2},
pages = {153-169},
year = {1995},
issn = {0885-2308},
doi = {https://doi.org/10.1006/csla.1995.0009},
url = {https://www.sciencedirect.com/science/article/pii/S0885230885700095},
author = {Michael S. Brandstein and John E. Adcock and Harvey F. Silverman},
abstract = {A frequency-domain-based delay estimator is described, designed specifically for speech signals in a microphone-array environment. It is shown to be capable of obtaining precision delay estimates over a wide range of signal-to-noise ratio conditions and is computationally simple enough to make it practical for real-time systems. A location algorithm based upon the delay estimator is then developed. With this algorithm it is possible to localize talker positions to a region only a few centimetres in diameter (not very different from the size of the source), and to track a moving source. Experimental results using data from a real 16-element array are presented to indicate the true performance of the algorithms.}
}

@book{cooke1993modelling,
author = {Cooke, Martin},
title = {Modelling Auditory Processing and Organisation},
year = {1993},
isbn = {0521450942},
publisher = {Cambridge University Press},
address = {USA}
}
@article{LOCKWOOD1992experiment,
title = {Experiments with a nonlinear spectral subtractor (NSS), Hidden Markov models and the projection, for robust speech recognition in cars},
journal = {Speech Communication},
volume = {11},
number = {2},
pages = {215-228},
year = {1992},
note = {Eurospeech '91},
issn = {0167-6393},
doi = {https://doi.org/10.1016/0167-6393(92)90016-Z},
url = {https://www.sciencedirect.com/science/article/pii/016763939290016Z},
author = {P. Lockwood and J. Boudy},
keywords = {Speech recognition, projection measure, speech enhancement, spectral subtraction, noise, continuous density Hidden Markov Model},
abstract = {Achieving reliable performance for a speech recogniser is an important challenge, especially in the context of mobile telephony applications where the user can access telephone functions through voice. The breakthrough of such a technology is appealing, since the driver can concentrate completely and safely on his task while composing and conversing in a “full” hands-free mode. This paper addresses the problem of speaker-dependent discrete utterance recognition in noise. Special reference is made to the mismatch effects due to the fact that training and testing are made in different environments. A novel technique for noise compensation is proposed: nonlinear spectral subtraction (NSS). Robust variance estimates and robust pdf evaluations (projection) are also introduced and combined with NSS into the HMM framework. We show that the lower limit of applicability of the projection (low SNR values) can be loosened after combination with NSS. Experimental results are reported. The performance of an HMM-based recogniser rises from 56% (no compensation) to 98% after speech enhancement. More than 3300 utterances have been used to evaluate the systems (three databases, two European languages). This result is achieved by the use of robust training/recognition schemes and by preprocessing the noisy speech by NSS.
Zusammenfassung
Leistungsfähige Spracherkenner zu entwickeln ist eine wichtige Forschungsaufgabe. Dies gilt insbesondere auch im Bereich des Mobilfunks, wenn der Benutzer sein mobiles Telefon durch akustische Eingabe bedienen können soll. Derartige Verfahren können beispielsweise dann attraktiv sein, wenn sich ein Autofahrer in die Lage versetzt sieht, Telefonverbindungen zu wählen und Telefongespräche zu führen, ohne seine Hände vom Steuer nehmen zu müssen, und sich somit vollständig und sicher aufs Fahren konzentrieren kann. Der vorliegende Beitrag befaβt sich mit dem Problem sprecherabhängiger Erkennung isolierter Äuβerungen in geräuschvoller Umgebung. Hierbei wird insbesondere das Problem diskutiert, das dadurch entsteht, daβ die Umgebungsbedingungen beim Training und beim Einsatz des Algorithmus erheblich voneinander abweichen. Präsentiert wird das Verfahren der nichtlinearen spektralen Subtraktion (NSS), eine neuartiges Methode zur Geräuschreduktion. Darüber hinaus werden robuste Schätzverfahren für Varianzen und robuste Evaluierungsverfahren für Wahrscheinlichkeitsdichtefunktionen (Projektionen) eingesetzt und zusammem mit dem NSS-Verfahren in ein Spracherkennungssystem auf HMM-Basis eingebaut. Wie gezeigt wird, kann der minimale Störabstand, bei dem der beschriebene HMM-Erkenner noch funktioniert, durch den Einsatz des NSS-Verfahrens erheblich gesenkt werden. Experimentelle Ergebnisse werden vorgestellt. Die Erkennungsrate des HMM-Spracherkenners wächst von 56% (ohne Geräuschkompensation) auf 98% (mit Einsatz aller beschriebenen Verfahren). Zur Evaluierung des Systems wurden mehr als 3300 Äuβerungen verwendet (drei Korpora, zwei europäische Sprachen). Die Verbesserung wurde erzielt durch den Einsatz robuster Verfahren in der Lern- und Betriebsphase des Erkenners sowie durch Qualitätsverbesserung des gestörten Sprachsignals mit dem NSS-Verfahren.
Résumé
Atteindre des performances robustes pour un système de reconnaissance vocale est un problème pifficile à résoudre surtout lorsqu'un tel systéme est utilisé comme fonction de composition vocale dans les radiotéléphones mobiles de voiture. La nécessité de telles fonctions devient primordiale dans la mesure òu l'utilisateur d'un radiotéléphone mobile peut se concentrer sans risques sur la conduite de son véhicule tout en composant le numéro de son correspondant et discuter avec ce dernier en mode “mains-libres”. Le travail présenté dans cet article pose le problème de la reconnaissance mono-locuteur de most isolés dans un environnement bruité. Dans ce contexte toute la difficulté réside dans le fait qu'il existe des différences importantes entre les conditions d'apprentissage (généralement dans le silence) et celles de reconnaissance (généralement dans le bruit, lorsque le véhicule roule). Une nouvelle technique de réduction du bruit est proposée: la Soustraction Spectrale Non linéaire (NSS). Dans un système de reconnaissance utilisant les Modèles de Markov Cachés (HMM), des estimateurs robustes de variances (lissage) et de densités de probabilités d'observation (projection) sont également introduits et combinés avec la Soustraction Spectrale Non linéaire. Nous montrons aussi que les limites courantes d'application de la Projection (RSB inférieurs à 0 dB) peuvent être repoussées grâce à l'utilisation de NSS. Des simulations numériques faites à partir de données réelles sont présentées et commentées. Le système de reconnaissance (HMM) voit ses performances s'élever de 56%, sans traitement, à 98%, après réduction du bruit par NSS. Plus de 3000 mots à reconnaître ont été employés pour l'évaluation des différents systèmes considérés (trois bases de données, deux langues europénnes). De telles performances ont été atteintes en ayant recours à des techniques robustes d'apprentissage et de reconnaissance ainsi qu'à prétraitement des mots bruités à l'aide de NSS.}
}

@article{VANCOMPERNOLLE1989Noise,
title = {Noise adaptation in a hidden Markov model speech recognition system},
journal = {Computer Speech & Language},
volume = {3},
number = {2},
pages = {151-167},
year = {1989},
issn = {0885-2308},
doi = {https://doi.org/10.1016/0885-2308(89)90027-2},
url = {https://www.sciencedirect.com/science/article/pii/0885230889900272},
author = {Dirk {Van Compernolle}},
abstract = {Several ways for making the signal processing in an isolated word speech recognition system more robust against large variations in the background noise level are presented. Isolated word recognition systems are sensitive to accurate silence detection, and are easily overtrained on the specific noise circumstances of the training environment. Spectral subtraction provides good noise immunity in the cases where the noise level is lower or slightly higher in the testing environment than during training. Differences in residual noise energy after spectral subtraction between a clean training and noisy testing environment can still cause severe problems. The usability of spectral subtraction is largely increased if complemented with some extra noise immunity processing. This is achieved by the addition of artificial noise after spectral subtraction or by adaptively re-estimating the noise statistics during a training session. Both techniques are almost equally successful in dealing with the noise. Noise addition achieves the additional robustness that the system will never be allowed to learn about low amplitude events that might not be observable in all environments; this is achieved, however, at a cost that some information is consistently thrown away in the most favorable noise situations.}
}

@article{GALES1993Cepstral,
title = {Cepstral parameter compensation for HMM recognition in noise},
journal = {Speech Communication},
volume = {12},
number = {3},
pages = {231-239},
year = {1993},
issn = {0167-6393},
doi = {https://doi.org/10.1016/0167-6393(93)90093-Z},
url = {https://www.sciencedirect.com/science/article/pii/016763939390093Z},
author = {M.J.F. Gales and S.J. Young},
keywords = {Speech recognition, noise compensation, AMN, PMC},
abstract = {This paper describes a method of adapting a continuous density HMM recogniser trained on clean cepstral speech data to make it robust to noise. The technique is based on parallel model combination (PMC) in which the parameters of corresponding pairs of speech and noise states are combined to yield a set of compensated parameters. It improves on earlier cepstral mean compensation methods in that it also adapts the variances and as a result can deal with much lower SNRs. The PMC method is evaluated on the NOISEX-92 noise database and shown to work well down to 0 dB SNR and below for both stationary and non-stationary noises. Furthermore, for relatively constant noise conditions, there is no additional computational cost at run-time.
Zusammenfassung
Dieser Artikel beschreibt eine Methode zur Anpassung eines auf versteckten Markov Modulen basierenden Erkennungssystems mit kontinuierlicher Dichte (aufgenommen über Parameter, die die normale Sprache darstellen), um das System bei Vorhandensein von Lärm sicherer zu machen. Diese Methode, die auf der Kombination von parallelen, Modellen beruht, ermöglicht die Kombination von gepaarten Lärm- und Sprachzuständen, um daraus eine Reihe von kompensierten Parametern zu bilden. Dies ist eine Verbesserung, im Vergleich zu den Kompensationsmethoden des Mittelwertes, da diese Methode auch die Anpassung der Standardabweichungen ermöglicht, wodurch wesentlich geringere Rauschabstände berücksichtigt werden können. Diese Methode wird basierend auf der Datenbank NOISEX-92 bewertet. Wir zeigen, daβ diese Methode bei einem Rauschabstand von 0 dB oder kleiner im Rahmen von stationärem und nicht stationärem Lärm gute Ergebnisse liefert. Auβerden gibt es bei dieser Methode bei relative konstanten Lärmbedingungen keine zusätzliche Rechenzeit in der Testphase.
Résumé
Cet article décrit une méthode dont le but est d'adapter un système de reconnaissance basé sur des HMM à densité continue (appris sur des paramètres cepstraux représentant de la parole normale) pour rendre le système plus robuste en présence de bruit. Cette méthode, fondée sur la combinaison de modèles parallèles, permet de combiner les états appariés de bruit et de parole pour fournier un ensemble de paramères compensés. Ceci est une amélioration par rapport à des méthodes de compensation de la moyenne cepstrale car cette méthode permet aussi d'adapter les variances, ce qui permit de prendre en compte des rapports signal sur bruit beaucoup plus faibles. Cette méthode est évaluée sur la base de données NOISEX-92. Nous montrons qu'elle donne de bons résultats pour un rapport signal sur bruit de 0 dB ou inférieur dans le cadre de bruits stationnaires et non-stationnaires. De plus, pour des conditions de bruit relativement constantes, cette méthode n'ajoute aucun temps de calcul en phase de test.}
}

@inproceedings{astola1995noisyasr,
author = {Dobrin, Cristina and Haavisto, Petri and Laurila, Kari and Astola, J.},
year = {1995},
month = {09},
pages = {},
title = {Speech recognition experiments in a noisy environment using auditory system modelling},
doi = {10.21437/Eurospeech.1995-36}
}

@article{KARRAY2003towards,
title = {Towards improving speech detection robustness for speech recognition in adverse conditions},
journal = {Speech Communication},
volume = {40},
number = {3},
pages = {261-276},
year = {2003},
issn = {0167-6393},
doi = {https://doi.org/10.1016/S0167-6393(02)00066-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167639302000663},
author = {Lamia Karray and Arnaud Martin},
keywords = {Speech/non-speech detection, Spectral subtraction, Adaptive algorithm, Likelihood ratio criterion, Wavelets},
abstract = {Recognition performance decreases when recognition systems are used over the telephone network, especially wireless network and noisy environments. It appears that non-efficient speech/non-speech detection (SND) is an important source of this degradation. Therefore, speech detection robustness to noise is a challenging problem to be examined, in order to improve recognition performance for the very noisy communications. Several studies were conducted aiming to improve the robustness of SND used for speech recognition in adverse conditions. The present paper proposes some solutions aiming to improve SND in wireless environment. Speech enhancement prior detection is considered. Then, two versions of SND algorithm, based on statistical criteria, are proposed and compared. Finally, a post-detection technique is introduced in order to reject the wrongly detected noise segments.
Zusammenfassung
Die Spracherkennungsleistung vermindert sich stark, wenn Spracherkennungssysteme in Telefonnetzen schlechter Übertragungsqualität eingesetzt werden und/oder der Anruf in einer Umgebung störender Nebengeräusche geführt wird. Es erscheint offensichtlich, dass die schlechte Unterscheidung zwischen Sprache und Rauschen/Nebengeräuschen einen Grossteil des Verlustes der Spracherkennungsleistung ausmacht. Daher ist das sichere Unterscheiden zwischen Sprache und Rauschen/Nebengeräuschen ein grundlegendes Problem, dessen Untersuchung auf eine Verbesserung der Spracherkennungsleistung in stark verrauschten Komunikationssystemen zielt. Einige Studien haben zu einer Verbesserung der Unterscheidung von Sprache und Rauschen/Nebengeräuschen beigetragen und damit die Spracherkennungsleistung unter ungünstigen Bedingungen erhöht. Dieser Artikel schlägt Lösungen für das Problems der Unterscheidung von Sprache und Rauschen/Nebengeräuschen vor. Zunächst werden Vorverarbeitungen zur Spracherkennung betrachtet. Dazu werden zwei Versionen Rauschen/Nebengeräuschen, der auf statistischen Kriterien basiert, vorgestellt und verglichen. Letztlich wird eine Technik zum Filtern fälschlich Sprachsegmente vorgeführt.
Résumé
Les performances de la reconnaissance sont fortement dégradées lorsque les systèmes de reconnaissance sont employés sur des réseaux téléphoniques particulièrement difficiles et dans des environnements bruités. Il apparaı&#x0302;t évident que la détection de parole/non-parole est une source importante de cette dégradation. Ainsi la robustesse de la détection de parole est un problème crucial à examiner pour améliorer les performances de la reconnaissance pour des communications très bruitées. De nombreuses études ont conduit à améliorer la robustesse de la détection de parole/non-parole pour une utilisation de la reconnaissance de parole dans des conditions difficiles. Ce papier propose des solutions pour l’amélioration de la détection de parole/non-parole en environnement très bruité. Des pré-traitements à la détection de parole sont d’abord considérés. Nous proposons et comparons ensuite deux versions d’un algorithme de détection de parole/non-parole, fondées sur des critères statistiques. Finalement, une technique de post-traitement est introduite dans le but de rejeter les détections de bruits prises pour de la parole.}
}

@Inproceedings{Mosner2019,
 author = {Ladislav Mosner and Minhua Wu and Sree Hari Krishnan Parthasarathi and Roland Maas and Anirudh Raju and Kenichi Kumatani and Shiva Sundaram and Björn Hoffmeister},
 title = {Improving Noise Robustness of Automatic Speech Recognition via Parallel Data and Teacher-student Learning},
 year = {2019},
 url = {https://www.amazon.science/publications/improve-noise-robustness-of-automatic-speech-recognition-via-parallel-data-and-teacher-student-learning},
 booktitle = {ICASSP 2019},
}

@article{Kinoshita2020ImprovingNR,
  title={Improving Noise Robust Automatic Speech Recognition with Single-Channel Time-Domain Enhancement Network},
  author={Keisuke Kinoshita and Tsubasa Ochiai and Marc Delcroix and Tomohiro Nakatani},
  journal={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2020},
  pages={7009-7013}
}
@article{ghitza1987auditorynerv,
author = {Ghitza, Oded},
year = {1987},
month = {07},
pages = {736 - 740},
title = {Auditory nerve representation criteria for speech analysis/Synthesis},
volume = {35},
journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
doi = {10.1109/TASSP.1987.1165223}
}

@INPROCEEDINGS{Hermansky1985perceptual,
  author={Hermansky, H. and Hanson, B. and Wakita, H.},
  booktitle={ICASSP '85. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  title={Perceptually based linear predictive analysis of speech},
  year={1985},
  volume={10},
  number={},
  pages={509-512},
  doi={10.1109/ICASSP.1985.1168384}}

@ARTICLE{ghitza_audiory_1994,
  author={Ghitza, O.},
  journal={IEEE Transactions on Speech and Audio Processing},
  title={Auditory models and human performance in tasks related to speech coding and speech recognition},
  year={1994},
  volume={2},
  number={1},
  pages={115-132},
  doi={10.1109/89.260357}}

@InProceedings{hartwich_auditory,
author="Hartwich, E.
and Alexandre, F.",
title="A Speech Recognition System using an Auditory Model and TOM Neural Network",
booktitle="Artificial Neural Nets and Genetic Algorithms",
year="1998",
publisher="Springer Vienna",
address="Vienna",
pages="107--111",
abstract="This paper is devoted to a neurobiologically plausible approach for the design of speech processing systems. The temporal organization map (TOM) neural net model is a connectionist model for time representation. The definition of a generic neural unit, inspired by the neurobiological model of the cortical column, allows the model to be used for problems including the temporal dimension. In the framework of automatic speech recognition, TOM has been previously tested with conventional techniques of signal processing. An auditory model as front-end processor is now used with TOM, in order to test the efficiency and the accuracy of a physiologically based speech recognition system. Preliminary results axe presented for speaker-dependent and speaker-independent speech recognition experiments. The interest of auditory model is the possibility to develop more valuable processing and communication strategies between TOM and the front-end processor, including afferent and efferent information flow.",
isbn="978-3-7091-6492-1"
}



@article{deshmukh_comparitive_2014,
author = {Deshmukh, Ratnadeep and Kurzekar, Pratik and Waghmare, Dr. Vishal and Shrishrimal, Pukhraj},
year = {2014},
month = {12},
pages = {18006-18016},
title = {A Comparative Study of Feature Extraction Techniques for Speech Recognition System},
volume = {3},
journal = {International Journal of Innovative Research in Science, Engineering and Technology},
doi = {10.15680/IJIRSET.2014.0312034}
}

@inproceedings{gao92_icslp,
  author={Yuqing Gao and Taiyi Huang and Shaoyan Chen and Jean-Paul Haton},
  title={{Auditory model based speech processing}},
  year=1992,
  booktitle={Proc. 2nd International Conference on Spoken Language Processing (ICSLP 1992)},
  pages={73--76},
  doi={10.21437/ICSLP.1992-21}
}

@INPROCEEDINGS{juang_1988_shorttime,
  author={Mansour, D. and Juang, B.H.},
  booktitle={ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing},
  title={The short-time modified coherence representation and its application for noisy speech recognition},
  year={1988},
  volume={},
  number={},
  pages={525-528 vol.1},
  doi={10.1109/ICASSP.1988.196635}}


@inproceedings{nakamura93_robust,
  author={Satoshi Nakamura and Toshio Akabane and Seiji Hamaguchi},
  title={{Robust word spotting in adverse car environments}},
  year=1993,
  booktitle={Proc. 3rd European Conference on Speech Communication and Technology (Eurospeech 1993)},
  pages={1045--1048},
  doi={10.21437/Eurospeech.1993-254}
}

@{morin_robust2006,
 title     = "Robust word-spotting system using an intelligibility criterion for reliable keyword detection under adverse and unknown noisy environments",
 number    = "6985859",
 author    = "Morin, Philippe R. (Santa Barbara, CA, US)",
 year      = "2006",
 month     = "January",
 url       = "https://www.freepatentsonline.com/6985859.html"
}

@article{comparative_1985_nocerino,
title = {Comparative study of several distortion measures for speech recognition},
journal = {Speech Communication},
volume = {4},
number = {4},
pages = {317-331},
year = {1985},
issn = {0167-6393},
doi = {https://doi.org/10.1016/0167-6393(85)90057-3},
url = {https://www.sciencedirect.com/science/article/pii/0167639385900573},
author = {N. Nocerino and F.K. Soong and L.R. Rabiner and D.H. Klatt},
abstract = {Local spectral distortion measures are commonly used to measure the similarity (or spectral distance) between two given short-time spectra. In this study we compared several different spectral distortion measures including the Itakura-Saito distortion measure, the log likelihood ratio (LLR) distortion measure, the likelihood ratio (LR) distortion measure, the cepstral (CEP) distortion measure, and two proposed perceptually based distortion measures, the weighted likelihood ratio (WLR) and the weighted slope metric (WSM) distortion measures, in terms of their effects on the performance of standard dynamic time warping (DTW) based, isolated word, speech recognizer. Two modifications of the basic forms of each measure were also investigated, namely a Bark-scale frequency warping and the incorporation of suprasegemental energy information. All distortion measures and their modifications were tested on an alpha-digit vocabulary, 4-talker, telephone recording data base. The results can be summarized as: (1) All LPC-based distortion measures performed reasonably well. The log likelihood ratio and weighted slope metric distortion measures gave the highest recognition accuracy, while the Itakura-Saito distortion measure gave the lowest score; (2) Whereas the addition of suprasegmental energy information helped the recognition performance, the use of gain and absolute loudness degraded the performance; (3) Bark-scale frequency warping did not, at least for the highly bandlimited telephone data base we tested, performed as well as its unwarped counterpart; (4) The weighted likelihood ratio distortion measure did not perform as well as its unweighted counterpart.
Zusammenfassung
Ein lokales Mass der spektralen Verzerrung wird oft angewandt, um die Ähnlichkeit (oder Distanz) zwischen zwei Kurzzeitspektren zu bestimmen. In dieser Studie vergleichen wir verschiedene spektrale Verzerrungsmasse, nämlich das Itakura-Saito Verzerrungsmass (IS), den Logarithmus des Wahrscheinlichkeitsquotienten (LR), des Cepstrale Verzerrungsmass (CEP) sowie zwei Verzerrungsmasse mit perzeptivem Hintergrund—den gewichteten Wahrscheinlichkeitsquotienten (WLR) und die Metrik mit gewichtetem Richtungskoeffizienten (WSM). Unser Ziel war, diese Verzerrungsmasse auf ihren Einfluss auf die Leistung eines Einzelworterkennungssystems zu untersuchen, welches auf einer dynamischen Verzerrungsmethode beruht. Zwei Modifikationen jedes Masses wurden ebenfalls untersucht, nämlich eine Frequenzverzerrung entlang einer Barksala sowie die Eingliederung suprasegmentaler Information. Alle Verzerrungsmasse sowie ihre Modifikationen wurden mit Hilfe eines Datenträgers getestet. welcher vier über Telephon aufgenommene Sprecher umfasste. Die Resultate können wie folgt zusammengefasst werden: (1) die Leistung aller auf der linearen prädiktiven Kodierung beruhenden Verzerrungsmasse war annehmbar. Der Logarithmus des Wahrscheinlichkeitsquotienten und die Metrik mit gewichtenem Richtungskoeffizienten hatten die beste Erkennungsleistung, wohingegen das Itakura-Saito Verzerrungsmass eher schlecht abschnitt; (2) die Ausnutzung suprasegmentaler Information erwies sich als nützlich, während sich die Benutzung des Verstärkungskoeffizienten sowie der absoluten Lautstärke als schädlich herausstellte; (3) die Frequenzverzerrung entlang einer Barksala erwies sich als weniger erfolgreich als die unverzerrte Form, wenigstens im Zusammenhang unserer durch einen begrenzten Frequenzgang gekennzeichneten Daten; (4) der gewichtete Wahrscheinlichkeitsquotient erwies sich als weniger leistungsfähig als seine ungewichtete Alternative.
Résumé
Des mesures locales de distortions spectrales sont souvent utilisées pour évaluer la similitude (ou la distance) entre deux spectres à court terme. Dans cette étude, nous comparons différences mesures de distortion spectrale, entre autres la mesure de distorsion d'Itakura-Saito (IS), celle par quotient de vraisemblance logarithmique, la mesure de distortion par quotient de vraisemblance (LR), la mesure de distorsion cepstrale (CEP) et deux mesures de distortion basées sur la perception—le quotient de vraisemblance pondéré (WLR) et la métrique à pente pondérée. (WSM). On souhaite déterminer leur effet sur les performances d'un système de reconnaissance de mots isolés par programmation dynamique (DTW). Deux modifications de la version de base de chaque mesure ont été également examinées—une distortion en fréquences selon une échelle en Bark et l'incorporation d'une information suprasegmentale—. Toutes ces mesures et leurs mldifications ont été testées sur une base de données multi-locuteurs (4) enregistrés par téléphone. Les résultats peuvent être résumés ainsi: (1) toutes les mesures de distorsion sur base LPC ont fourni des résultats satisfaisants. Les mesures de distorsion par quotient de vraisemblance logarithmique et par métrique à pente pondérée ont donné lieu aux meilleures performances de reconnaissance, tandis que la mesure d'Itakura-Saito a réalisé les performances les plus faibles; (2) l'utilisation d'une information suprasegmentale a amélioré la reconnaissance, tandis que l'utilisation du grain et de la force sonore a dégradé les performances; (3) la distorsion spectrale sur une échelle en Bark s'est révélée moins performante que son équivalent libre de toute distorsion, du moins sur notre base de données caractérisée par une largeur de bande limitée; (4) la puissance du quotient de vraisemblance logarithmique pondérée est apparue réduite par rapport à son équivalent non pondéré.}
}


@ARTICLE{Erell_1993_filterbank,
  author={Erell, A. and Weintraub, M.},
  journal={IEEE Transactions on Speech and Audio Processing},
  title={Filterbank-energy estimation using mixture and Markov models for recognition of noisy speech},
  year={1993},
  volume={1},
  number={1},
  pages={68-76},
  doi={10.1109/89.221385}}

@INPROCEEDINGS{anglade_1993_speech,
  author={Anglade, Y. and Fohr, D. and Junqua, J.-C.},
  booktitle={1993 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  title={Speech discrimination in adverse conditions using acoustic knowledge and selectively trained neural networks},
  year={1993},
  volume={2},
  number={},
  pages={279-282 vol.2},
  doi={10.1109/ICASSP.1993.319290}}

@inproceedings{Hirsch1991ImprovedSR,
  title={Improved speech recognition using high-pass filtering of subband envelopes},
  author={Hans-G{\"u}nter Hirsch and Peter Meyer and Hans-Wilhelm R{\"u}hl},
  booktitle={EUROSPEECH},
  year={1991}
}

@INPROCEEDINGS{paliwal_nnclassifier_1990,
  author={Paliwal, K.K.},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing},
  title={Neural net classifiers for robust speech recognition under noisy environments},
  year={1990},
  volume={},
  number={},
  pages={429-432 vol.1},
  doi={10.1109/ICASSP.1990.115737}}

@inproceedings{compensation_hermansky91b,
  author={Hynek Hermansky and Nelson Morgan and Aruna Bayya and Phil Kohn},
  title={{Compensation for the effect of the communication channel in auditory-like analysis of speech (RASTA-PLP)}},
  year=1991,
  booktitle={Proc. 2nd European Conference on Speech Communication and Technology (Eurospeech 1991)},
  pages={1367--1370},
  doi={10.21437/Eurospeech.1991-312}
}

@INPROCEEDINGS{spectral_bateman_1992,
  author={Bateman, D.C. and Bye, D.K. and Hunt, M.J.},
  booktitle={[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  title={Spectral contrast normalization and other techniques for speech recognition in noise},
  year={1992},
  volume={1},
  number={},
  pages={241-244 vol.1},
  doi={10.1109/ICASSP.1992.225927}

@inproceedings{leonard1984database,
    title = {A Database for Speaker-Independent Digit Recognition},
    author = {Leonard, R. G.},
    booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
    address = {San Diego},
    number = {42.11},
    month = {March},
    year = {1984}
}

@article{LOCKWOOD1992215,
title = {Experiments with a nonlinear spectral subtractor (NSS), Hidden Markov models and the projection, for robust speech recognition in cars},
journal = {Speech Communication},
volume = {11},
number = {2},
pages = {215-228},
year = {1992},
note = {Eurospeech '91},
issn = {0167-6393},
doi = {https://doi.org/10.1016/0167-6393(92)90016-Z},
url = {https://www.sciencedirect.com/science/article/pii/016763939290016Z},
author = {P. Lockwood and J. Boudy},
keywords = {Speech recognition, projection measure, speech enhancement, spectral subtraction, noise, continuous density Hidden Markov Model},
abstract = {Achieving reliable performance for a speech recogniser is an important challenge, especially in the context of mobile telephony applications where the user can access telephone functions through voice. The breakthrough of such a technology is appealing, since the driver can concentrate completely and safely on his task while composing and conversing in a “full” hands-free mode. This paper addresses the problem of speaker-dependent discrete utterance recognition in noise. Special reference is made to the mismatch effects due to the fact that training and testing are made in different environments. A novel technique for noise compensation is proposed: nonlinear spectral subtraction (NSS). Robust variance estimates and robust pdf evaluations (projection) are also introduced and combined with NSS into the HMM framework. We show that the lower limit of applicability of the projection (low SNR values) can be loosened after combination with NSS. Experimental results are reported. The performance of an HMM-based recogniser rises from 56% (no compensation) to 98% after speech enhancement. More than 3300 utterances have been used to evaluate the systems (three databases, two European languages). This result is achieved by the use of robust training/recognition schemes and by preprocessing the noisy speech by NSS.
Zusammenfassung
Leistungsfähige Spracherkenner zu entwickeln ist eine wichtige Forschungsaufgabe. Dies gilt insbesondere auch im Bereich des Mobilfunks, wenn der Benutzer sein mobiles Telefon durch akustische Eingabe bedienen können soll. Derartige Verfahren können beispielsweise dann attraktiv sein, wenn sich ein Autofahrer in die Lage versetzt sieht, Telefonverbindungen zu wählen und Telefongespräche zu führen, ohne seine Hände vom Steuer nehmen zu müssen, und sich somit vollständig und sicher aufs Fahren konzentrieren kann. Der vorliegende Beitrag befaβt sich mit dem Problem sprecherabhängiger Erkennung isolierter Äuβerungen in geräuschvoller Umgebung. Hierbei wird insbesondere das Problem diskutiert, das dadurch entsteht, daβ die Umgebungsbedingungen beim Training und beim Einsatz des Algorithmus erheblich voneinander abweichen. Präsentiert wird das Verfahren der nichtlinearen spektralen Subtraktion (NSS), eine neuartiges Methode zur Geräuschreduktion. Darüber hinaus werden robuste Schätzverfahren für Varianzen und robuste Evaluierungsverfahren für Wahrscheinlichkeitsdichtefunktionen (Projektionen) eingesetzt und zusammem mit dem NSS-Verfahren in ein Spracherkennungssystem auf HMM-Basis eingebaut. Wie gezeigt wird, kann der minimale Störabstand, bei dem der beschriebene HMM-Erkenner noch funktioniert, durch den Einsatz des NSS-Verfahrens erheblich gesenkt werden. Experimentelle Ergebnisse werden vorgestellt. Die Erkennungsrate des HMM-Spracherkenners wächst von 56% (ohne Geräuschkompensation) auf 98% (mit Einsatz aller beschriebenen Verfahren). Zur Evaluierung des Systems wurden mehr als 3300 Äuβerungen verwendet (drei Korpora, zwei europäische Sprachen). Die Verbesserung wurde erzielt durch den Einsatz robuster Verfahren in der Lern- und Betriebsphase des Erkenners sowie durch Qualitätsverbesserung des gestörten Sprachsignals mit dem NSS-Verfahren.
Résumé
Atteindre des performances robustes pour un système de reconnaissance vocale est un problème pifficile à résoudre surtout lorsqu'un tel systéme est utilisé comme fonction de composition vocale dans les radiotéléphones mobiles de voiture. La nécessité de telles fonctions devient primordiale dans la mesure òu l'utilisateur d'un radiotéléphone mobile peut se concentrer sans risques sur la conduite de son véhicule tout en composant le numéro de son correspondant et discuter avec ce dernier en mode “mains-libres”. Le travail présenté dans cet article pose le problème de la reconnaissance mono-locuteur de most isolés dans un environnement bruité. Dans ce contexte toute la difficulté réside dans le fait qu'il existe des différences importantes entre les conditions d'apprentissage (généralement dans le silence) et celles de reconnaissance (généralement dans le bruit, lorsque le véhicule roule). Une nouvelle technique de réduction du bruit est proposée: la Soustraction Spectrale Non linéaire (NSS). Dans un système de reconnaissance utilisant les Modèles de Markov Cachés (HMM), des estimateurs robustes de variances (lissage) et de densités de probabilités d'observation (projection) sont également introduits et combinés avec la Soustraction Spectrale Non linéaire. Nous montrons aussi que les limites courantes d'application de la Projection (RSB inférieurs à 0 dB) peuvent être repoussées grâce à l'utilisation de NSS. Des simulations numériques faites à partir de données réelles sont présentées et commentées. Le système de reconnaissance (HMM) voit ses performances s'élever de 56%, sans traitement, à 98%, après réduction du bruit par NSS. Plus de 3000 mots à reconnaître ont été employés pour l'évaluation des différents systèmes considérés (trois bases de données, deux langues europénnes). De telles performances ont été atteintes en ayant recours à des techniques robustes d'apprentissage et de reconnaissance ainsi qu'à prétraitement des mots bruités à l'aide de NSS.}
}


@book{national1994voice,
    title = {Voice Communication Between Humans and Machines},
    author = {{National Academies of Sciences, Engineering, and Medicine}},
    year = {1994},
    publisher = {The National Academies Press},
    address = {Washington, DC},
    doi = {10.17226/2308}
}

@inproceedings{gao92_auditory,
  author={Yuqing Gao and Taiyi Huang and Shaoyan Chen and Jean-Paul Haton},
  title={{Auditory model based speech processing}},
  year=1992,
  booktitle={Proc. 2nd International Conference on Spoken Language Processing (ICSLP 1992)},
  pages={73--76},
  doi={10.21437/ICSLP.1992-21}
}

@article{agrawal2019modulation,
  title={Modulation filter learning using deep variational networks for robust speech recognition},
  author={Agrawal, P. and Ganapathy, S.},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={13},
  number={2},
  pages={244--253},
  year={2019},
  month={May},
  doi={10.1109/JSTSP.2019.2913965}
}

@article{lee2016dnn,
  title={DNN-based feature enhancement using DOA-constrained ICA for robust speech recognition},
  author={Lee, H.-Y. and Cho, J.-W. and Kim, M. and Park, H.-M.},
  journal={IEEE Signal Processing Letters},
  volume={23},
  number={8},
  pages={1091--1095},
  year={2016},
  month={Aug.}
}

@article{ming2017speech,
  title={Speech enhancement based on full-sentence correlation and clean speech recognition},
  author={Ming, J. and Crookes, D.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={25},
  number={3},
  pages={531--543},
  year={2017},
  month={Mar.},
  doi={10.1109/TASLP.2017.2651406}
}

@article{lee2018threshold,
  title={Threshold-based noise detection and reduction for automatic speech recognition system in human-robot interactions},
  author={Lee, S.-C. and Wang, J.-F. and Chen, M.-H.},
  journal={Sensors},
  volume={18},
  number={7},
  pages={2068},
  year={2018},
  month={Jun.},
  doi={10.3390/s18072068}
}

@article{gosztolya2016domain,
  title={Domain adaptation of deep neural networks for automatic speech recognition via wireless sensors},
  author={Gosztolya, G. and Gr{\'o}sz, T.},
  journal={Journal of Electrical Engineering},
  volume={67},
  number={2},
  pages={124--130},
  year={2016},
  month={Apr.},
  doi={10.1515/jee-2016-0017}
}

@article{ganapathy2017multivariate,
  title={Multivariate autoregressive spectrogram modeling for noisy speech recognition},
  author={Ganapathy, S.},
  journal={IEEE Signal Processing Letters},
  volume={24},
  number={9},
  pages={1373--1377},
  year={2017},
  month={Sep.},
  doi={10.1109/LSP.2017.2724561}
}

@article{tamazin2019enhanced,
  title={Enhanced automatic speech recognition system based on enhancing power-normalized cepstral coefficients},
  author={Tamazin, M. and Gouda, A. and Khedr, M.},
  journal={Applied Sciences},
  volume={9},
  number={10},
  pages={2166},
  year={2019},
  month={May},
  doi={10.3390/app9102166}
}

@INPROCEEDINGS{shah_2017_children,
  author={Shahnawazuddin, S and Deepak K. T. and Pradhan, Gayadhar and Sinha, Rohit},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title={Enhancing noise and pitch robustness of children's ASR},
  year={2017},
  volume={},
  number={},
  pages={5225-5229},
  doi={10.1109/ICASSP.2017.7953153}}

@INPROCEEDINGS{malek2017robust,
	  author={Malek, Jiri and Zdansky, Jindrich and Cerva, Petr},
	  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	  title={Robust Automatic Recognition of Speech with background music},
	  year={2017},
	  volume={},
	  number={},
	  pages={5210-5214},
	  doi={10.1109/ICASSP.2017.7953150}}

@inproceedings{rehr2015cepstral,
  title={Cepstral noise subtraction for robust automatic speech recognition},
  author={Rehr, R. and Gerkmann, T.},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2015},
  pages={375--378},
  doi={10.1109/ICASSP.2015.7177994},
  address={South Brisbane, QLD, Australia}
}

@inproceedings{meutzner2016generative,
  title={A generative-discriminative hybrid approach to multi-channel noise reduction for robust automatic speech recognition},
  author={Meutzner, H. and Araki, S. and Fujimoto, M. and Nakatani, T.},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2016},
  pages={5740--5744},
  doi={10.1109/ICASSP.2016.7472777},
  address={Shanghai, China}
}

@inproceedings{liu2017modulation,
  title={A modulation feature set for robust Automatic Speech Recognition in additive noise and reverberation},
  author={Liu, X. and Sadeghian, R. and Zahorian, S. A.},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2017},
  pages={5230--5234},
  doi={10.1109/ICASSP.2017.7953154},
  address={New Orleans, LA, USA}
}

@inproceedings{do2018weighting,
  title={Weighting Time-Frequency Representation of Speech Using Auditory Saliency for Automatic Speech Recognition},
  author={Do, C.-T. and Stylianou, Y.},
  booktitle={Proc. Interspeech 2018},
  year={2018},
  pages={1591--1595},
  doi={10.21437/Interspeech.2018-1721}
}

@inproceedings{yadav2018spectral,
  title={Spectral Smoothing by Variationalmode Decomposition and its Effect on Noise and Pitch Robustness of ASR System},
  author={Yadav, I. C. and Shahnawazuddin, S. and Govind, D. and Pradhan, G.},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2018},
  pages={5629--5633},
  doi={10.1109/ICASSP.2018.8462133},
  address={Calgary, AB, Canada}
}

@article{lee2018threshold,
  title={Threshold-based noise detection and reduction for automatic speech recognition system in human robot interactions},
  author={Lee, S.-C. and Wang, J.-F. and Chen, M.-H.},
  journal={Sensors},
  volume={18},
  number={7},
  pages={2068},
  year={2018},
  month={Jun},
  doi={10.3390/s18072068}
}

@article{tu2020multi,
  title={A multi-target SNR-progressive learning approach to regression based speech enhancement},
  author={Tu, Y.-H. and Du, J. and Gao, T. and Lee, C.-H.},
  journal={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={28},
  pages={1608--1619},
  year={2020},
  doi={10.1109/TASLP.2020.2996503}
}

@article{moore2017speech,
  title={Speech enhancement for robust automatic speech recognition: Evaluation using a baseline system and instrumental measures},
  author={Moore, A. H. and Parada, P. P. and Naylor, P. A.},
  journal={Comput. Speech Lang.},
  volume={46},
  pages={574--584},
  year={2017},
  month={Nov},
  doi={10.1016/j.csl.2016.11.003}
}

@article{tan2018adaptive,
  title={Adaptive very deep convolutional residual network for noise robust speech recognition},
  author={Tan, T. and Qian, Y. and Hu, H. and Zhou, Y. and Ding, W. and Yu, K.},
  journal={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={26},
  number={8},
  pages={1393--1405},
  year={2018},
  month={Aug},
  doi={10.1109/TASLP.2018.2825432}
}

@article{kovacs2015joint,
  title={Joint optimization of spectro-temporal features and deep neural nets for robust automatic speech recognition},
  author={Kov{\'a}cs, G. and T{\'o}th, L.},
  journal={Acta Cybernetica},
  volume={22},
  number={1},
  pages={117--134},
  year={2015},
  doi={10.14232/actacyb.22.1.2015.8}
}

@article{ganapathy2017multivariate,
  title={Multivariate autoregressive spectrogram modeling for noisy speech recognition},
  author={Ganapathy, S.},
  journal={IEEE Signal Process. Lett.},
  volume={24},
  number={9},
  pages={1373--1377},
  year={2017},
  month={Sep},
  doi={10.1109/LSP.2017.2724561}
}

@article{davila2019enhanced,
  title={Enhanced robot speech recognition using biomimetic binaural sound source localization},
  author={Davila-Chacon, J. and Liu, J. and Wermter, S.},
  journal={IEEE Trans. Neural Netw. Learn. Syst.},
  volume={30},
  number={1},
  pages={138--150},
  year={2019},
  month={Jan},
  doi={10.1109/TNNLS.2018.2830119}
}

@article{ochiai2017unified,
  title={Unified architecture for multichannel end-to-end speech recognition with neural beamforming},
  author={Ochiai, T. and Watanabe, S. and Hori, T. and Hershey, J. R. and Xiao, X.},
  journal={IEEE J. Sel. Topics Signal Process.},
  volume={11},
  number={8},
  pages={1274--1288},
  year={2017},
  month={Dec},
  doi={10.1109/JSTSP.2017.2764276}
}

@inproceedings{wang2016joint,
  title={A joint training framework for robust automatic speech recognition},
  author={Wang, Z. Q. and Wang, D.},
  booktitle={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={24},
  number={4},
  pages={796--806},
  year={2016},
  month={Apr},
  doi={10.1109/TASLP.2016.2528171}
}

@article{lee2016dnn,
  title={DNN-Based Feature Enhancement Using DOA-Constrained ICA for Robust Speech Recognition},
  author={Lee, H.-Y. and Cho, J.-W. and Kim, M. and Park, H.-M.},
  journal={IEEE Signal Processing Letters},
  volume={23},
  number={8},
  pages={1091--1095},
  year={2016},
  month={Aug},
  doi={10.1109/LSP.2016.2583658}
}

@article{gosztolya2016domain,
  title={Domain adaptation of deep neural networks for automatic speech recognition via wireless sensors},
  author={Gosztolya, G. and Grósz, T.},
  journal={J. Electr. Eng.},
  volume={67},
  number={2},
  pages={124--130},
  year={2016},
  month={Apr},
  doi={10.1515/jee-2016-0017}
}

@inproceedings{shimada2019unsupervised,
  title={Unsupervised speech enhancement based on multichannel NMF-informed beamforming for noise-robust automatic speech recognition},
  author={Shimada, K. and Bando, Y. and Mimura, M. and Itoyama, K. and Yoshii, K. and Kawahara, T.},
  booktitle={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={27},
  number={5},
  pages={960--971},
  year={2019},
  month={May},
  doi={10.1109/TASLP.2019.2907015}
}

@article{moritz2017multi,
  title={Multi-channel speech enhancement and amplitude modulation analysis for noise robust automatic speech recognition},
  author={Moritz, N. and Adilo§lu, K. and Anemüller, J. and Goetze, S. and Kollmeier, B.},
  journal={Comput. Speech Lang.},
  volume={46},
  pages={558--573},
  year={2017},
  month={Nov},
  doi={10.1016/j.csl.2016.11.004}
}

@article{qian2016neural,
  title={Neural network based multi-factor aware joint training for robust speech recognition},
  author={Qian, Y. and Tan, T. and Yu, D.},
  journal={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={24},
  number={12},
  pages={2231--2240},
  year={2016},
  month={Dec},
  doi={10.1109/TASLP.2016.2598308}
}

@article{chen2016target,
  title={Target classification using the deep convolutional networks for SAR images},
  author={Chen, S. and Wang, H. and Xu, F. and Jin, Y. Q.},
  journal={IEEE Trans. Geosci. Remote Sens.},
  volume={54},
  number={8},
  pages={4806--4817},
  year={2016},
  month={Aug},
  doi={10.1109/TGRS.2016.2551720}
}

@article{rafique2016hybrid,
  title={Hybrid neuromorphic system for automatic speech recognition},
  author={Rafique, M. A. and Lee, B. G. and Jeon, M.},
  journal={Electron. Lett.},
  volume={52},
  number={17},
  pages={1428--1430},
  year={2016},
  month={Aug},
  doi={10.1049/el.2016.0975}
}

@article{kuhne2015handling,
  title={Handling derivative filterbank features in bounded-marginalization-based missing data automatic speech recognition},
  author={Kühne, M.},
  journal={Proc. Interspeech},
  volume={2015},
  pages={3566--3570},
  year={2015},
  doi={10.21437/Interspeech.2015-707}
}

@inproceedings{mosner2019improving,
  title={Improving noise robustness of automatic speech recognition via parallel data and teacher-student learning},
  author={Mosner, L. and Wu, M. and Raju, A. and Parthasarathi, S. H. K. and Kumatani, K. and Sundaram, S. and Maas, R. and Hoffmeister, B.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  year={2019},
  pages={6475--6479}
}

@article{park2020improved,
  title={Improved Noisy Student Training for Automatic Speech Recognition},
  author={Park, D. S. and Zhang, Y. and Jia, Y. and Han, W. and Chiu, C.-C. and Li, B. and Wu, Y. and Le, Q. V.},
  journal={Proc. Interspeech},
  volume={2020},
  pages={2817--2821},
  year={2020},
  doi={10.21437/Interspeech.2020-1470}
}

@inproceedings{dat2016comparative,
  title={A comparative study of multi-channel processing methods for noisy automatic speech recognition in urban environments},
  author={Dat, T. H. and Dennis, J. and Ren, L. Y. and Zheng, N. W.},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2016},
  pages={6465--6469},
  doi={10.1109/ICASSP.2016.7472922},
  address={Shanghai, China}
}

@article{li2019tenet,
  title={TEnet: Target speaker extraction network with accumulated speaker embedding for automatic speech recognition},
  author={Li, W. and Zhang, P. and Yan, Y.},
  journal={Electron. Lett.},
  volume={55},
  number={14},
  pages={816--819},
  year={2019},
  month={Jul},
  doi={10.1049/el.2019.1228}
}

@book{yu2016automatic,
  title={Automatic Speech Recognition},
  author={Yu, D. and Deng, L.},
  year={2016},
  publisher={Springer},
  address={London, U.K.}
}

@techreport{juang2005automatic,
  title={Automatic speech recognition-A brief history of the technology development},
  author={Juang, B.-H. and Rabiner, L. R.},
  institution={Georgia Institute of Technology, Atlanta, Rutgers University, and University of California, Santa Barbara},
  year={2005},
  number={1},
  pages={67}
}

@article{moher2009preferred,
  title={Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement},
  author={Moher, D. and Liberati, A. and Tetzlaff, J. and Altman, D. G.},
  journal={PLoS Medicine},
  volume={6},
  pages={e1000097},
  year={2009},
  month={Jul.},
  doi={10.1371/journal.pmed.1000097}
}

@misc{lucidchart,
  title={Lucidchart. Online Diagram Software \& Visual Solution},
  howpublished={\url{https://www.lucidchart.com}},
  note={Accessed: Dec. 3, 2020},
  year={2020}
}

@misc{nailsproject,
  title={Nails Project},
  howpublished={\url{http://nailsproject.net/}},
  note={Accessed: Dec. 3, 2020},
  year={2020}
}

@article{liao2020security,
  title={Security analysis of IoT devices by using mobile computing: A systematic literature review},
  author={Liao, B. and Ali, Y. and Nazir, S. and He, L. and Khan, H. U.},
  journal={IEEE Access},
  volume={8},
  pages={120331-120350},
  year={2020},
  doi={10.1109/ACCESS.2020.3006358}
}

@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, G. and Deng, L. and Yu, D. and Dahl, G. E. and Mohamed, A.-R. and Jaitly, N. and Senior, A. and Vanhoucke, V. and Nguyen, P. and Sainath, T. N. and Kingsbury, B.},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={82-97},
  year={2012},
  month={Nov.},
  doi={10.1109/MSP.2012.2205597}
}

@article{davis1980comparison,
  title={Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},
  author={Davis, S. B. and Mermelstein, P.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume={28},
  number={4},
  pages={357-366},
  year={1980},
  month={Aug.},
  doi={10.1109/TASSP.1980.1163420}
}

@article{rabiner1989tutorial,
  title={A tutorial on hidden Markov models and selected applications in speech recognition},
  author={Rabiner, L.},
  journal={Proceedings of the IEEE},
  volume={77},
  number={2},
  pages={257-286},
  year={1989},
  month={Feb.},
  doi={10.1109/5.18626}
}

@article{leggetter1995maximum,
  title={Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models},
  author={Leggetter, C. J. and Woodland, P. C.},
  journal={Computer Speech \& Language},
  volume={9},
  number={2},
  pages={171-185},
  year={1995},
  month={Apr.},
  doi={10.1006/csla.1995.0010}
}

@article{dahl2012context,
  title={Context-dependent pretrained deep neural networks for large-vocabulary speech recognition},
  author={Dahl, G. E. and Yu, D. and Deng, L. and Acero, A.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={20},
  number={1},
  pages={30-42},
  year={2012},
  month={Jan.},
  doi={10.1109/TASL.2011.2134090}
}

@article{varga1993assessment,
  title={Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems},
  author={Varga, A. and Steeneken, H. J. M.},
  journal={Speech Communication},
  volume={12},
  number={3},
  pages={247-251},
  year={1993},
  month={Jul.},
  doi={10.1016/0167-6393(93)90095-3}
}

@article{campbell1997speaker,
  title={Speaker recognition: A tutorial},
  author={Campbell, J. P., Jr.},
  journal={Proceedings of the IEEE},
  volume={85},
  number={9},
  pages={1437-1462},
  year={1997},
  month={Sep.},
  doi={10.1109/5.628714}
}

@article{kinnunen2010overview,
  title={An overview of text-independent speaker recognition: From features to supervectors},
  author={Kinnunen, T. and Li, H.},
  journal={Speech Communication},
  volume={52},
  number={1},
  pages={12-40},
  year={2010},
  doi={10.1016/j.specom.2009.08.009}
}

@article{gazzaniga2000cerebral,
  title={Cerebral specialization and interhemispheric communication: Does the corpus callosum enable the human condition?},
  author={Gazzaniga, M. S.},
  journal={Brain},
  volume={123},
  number={7},
  pages={1293-1326},
  year={2000},
  month={Jul.},
  doi={10.1093/brain/123.7.1293}
}

@article{parra2000convolutive,
  title={Convolutive blind separation of non-stationary sources},
  author={Parra, L. and Spence, C.},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={8},
  number={3},
  pages={320-327},
  year={2000},
  month={May},
  doi={10.1109/89.841214}
}

@article{pironkov2020hybrid,
  title={Hybrid-task learning for robust automatic speech recognition},
  author={Pironkov, G. and Wood, S. U. and Dupont, S.},
  journal={Computer Speech \& Language},
  volume={64},
  pages={101103},
  year={2020},
  month={Nov.},
  doi={10.1016/j.csl.2020.101103}
}

@article{li2019tenet,
  title={TEnet: Target speaker extraction network with accumulated speaker embedding for automatic speech recognition},
  author={Li, W. and Zhang, P. and Yan, Y.},
  journal={Electronics Letters},
  volume={55},
  number={14},
  pages={816-819},
  year={2019},
  month={Jul.},
  doi={10.1049/el.2019.1228}
}

@article{liu2016graph,
  title={Graph-based semisupervised learning for acoustic modeling in automatic speech recognition},
  author={Liu, Y. and Kirchhoff, K.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={24},
  number={11},
  pages={1946-1956},
  year={2016},
  month={Nov.},
  doi={10.1109/TASLP.2016.2593800}
}

@inproceedings{mandel2016multichannel,
  title={Multichannel spatial clustering for robust far-field automatic speech recognition in mismatched conditions},
  author={Mandel, M. I. and Barker, J.},
  booktitle={Proc. INTERSPEECH},
  pages={1991-1995},
  year={2016},
  month={Sep.}
}

@article{hirayama2015automatic,
  title={Automatic speech recognition for mixed dialect utterances by mixing dialect language models},
  author={Hirayama, N. and Yoshino, K. and Itoyama, K. and Mori, S. and Okuno, H. G.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={23},
  number={2},
  pages={373-382},
  year={2015},
  month={Feb.},
  doi={10.1109/TASLP.2014.2387414}
}

@article{tamazin2019enhanced,
  title={Enhanced automatic speech recognition system based on enhancing power-normalized cepstral coefficients},
  author={Tamazin, M. and Gouda, A. and Khedr, M.},
  journal={Applied Sciences},
  volume={9},
  number={10},
  pages={2166},
  year={2019},
  month={May},
  doi={10.3390/app9102166}
}

@article{chen2018progressive,
  title={Progressive joint modeling in unsupervised single-channel overlapped speech recognition},
  author={Chen, Z. and Droppo, J. and Li, J. and Xiong, W.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={26},
  number={1},
  pages={184-196},
  year={2018},
  month={Jan.},
  doi={10.1109/TASLP.2017.2765834}
}

@article{cai2015fast,
  title={A fast learning method for multilayer perceptrons in automatic speech recognition systems},
  author={Cai, C. and Xu, Y. and Ke, D. and Su, K.},
  journal={Journal of Robotics},
  volume={2015},
  pages={1-7},
  year={2015},
  month={Feb.},
  doi={10.1155/2015/797083}
}

@article{asemi2019adaptive,
  title={Adaptive neuro-fuzzy inference system for evaluating dysarthric automatic speech recognition (ASR) systems: A case study on MVML-based ASR},
  author={Asemi, A. and Salim, S. S. B. and Shahamiri, S. R. and Asemi, A. and Houshangi, N.},
  journal={Soft Computing},
  volume={23},
  number={10},
  pages={3529-3544},
  year={2019},
  month={May},
  doi={10.1007/s00500-018-3013-4}
}

@inproceedings{rajpal2020pseudo,
  title={Pseudo likelihood correction technique for low resource accented ASR},
  author={Rajpal, A. and Mv, A. R. and Yarra, C. and Aggarwal, R. and Ghosh, P. K.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={7434-7438},
  year={2020},
  month={May},
  doi={10.1109/ICASSP40776.2020.9053647}
}

@article{shimada2019unsupervised,
  title={Unsupervised speech enhancement based on multichannel NMF-informed beamforming for noise-robust automatic speech recognition},
  author={Shimada, K. and Bando, Y. and Mimura, M. and Itoyama, K. and Yoshii, K. and Kawahara, T.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={27},
  number={5},
  pages={960-971},
  year={2019},
  month={May},
  doi={10.1109/TASLP.2019.2907015}
}

@inproceedings{shahnawazuddin2017enhancing,
  title={Enhancing noise and pitch robustness of children's ASR},
  author={Shahnawazuddin, S. and Deepak, K. T. and Pradhan, G. and Sinha, R.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5225-5229},
  year={2017},
  month={Mar.}
}

@article{kumar2020leveraging,
  title={Leveraging linguistic context in dyadic interactions to improve automatic speech recognition for children},
  author={Kumar, M. and Kim, S. H. and Lord, C. and Lyon, T. D. and Narayanan, S.},
  journal={Computers Speech \& Language},
  volume={63},
  pages={101101},
  year={2020},
  month={Sep.},
  doi={10.1016/j.csl.2020.101101}
}

@article{wu2020deep,
  title={Deep spiking neural networks for large vocabulary automatic speech recognition},
  author={Wu, J. and Y{\"i}lmaz, E. and Zhang, M. and Li, H. and Tan, K. C.},
  journal={Frontiers in Neuroscience},
  volume={14},
  pages={199},
  year={2020},
  month={Mar.},
  doi={10.3389/fnins.2020.00199}
}
@article{masumura2019viterbi,
  title={Viterbi approximation of latent words language models for automatic speech recognition},
  author={Masumura, R. and Asami, T. and Oba, T. and Masataki, H. and Sakauchi, S.},
  journal={Journal of Information Processing},
  volume={27},
  pages={168-176},
  year={2019},
  doi={10.2197/ipsjjip.27.168}
}

@article{palaz2019end,
  title={End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition},
  author={Palaz, D. and Magimai-Doss, M. and Collobert, R.},
  journal={Speech Communication},
  volume={108},
  pages={15-32},
  year={2019},
  month={Apr.},
  doi={10.1016/j.specom.2019.01.004}
}

@article{lee2018threshold,
  title={Threshold-based noise detection and reduction for automatic speech recognition system in human-robot interactions},
  author={Lee, S.-C. and Wang, J.-F. and Chen, M.-H.},
  journal={Sensors},
  volume={18},
  number={7},
  pages={2068},
  year={2018},
  month={Jun.},
  doi={10.3390/s18072068}
}

@article{wang2020wavenet,
  title={WaveNet with crossattention for audiovisual speech recognition},
  author={Wang, H. and Gao, F. and Zhao, Y. and Wu, L.},
  journal={IEEE Access},
  volume={8},
  pages={169160-169168},
  year={2020},
  doi={10.1109/ACCESS.2020.3024218}
}

@article{ogawa2017error,
  title={Error detection and accuracy estimation in automatic speech recognition using deep bidirectional recurrent neural networks},
  author={Ogawa, A. and Hori, T.},
  journal={Speech Communication},
  volume={89},
  pages={70-83},
  year={2017},
  month={May},
  doi={10.1016/j.specom.2017.02.009}
}

@article{keshet2018automatic,
  title={Automatic speech recognition: A primer for speech-language pathology researchers},
  author={Keshet, J.},
  journal={International Journal of Speech-Language Pathology},
  volume={20},
  number={6},
  pages={599-609},
  year={2018},
  month={Oct.},
  doi={10.1080/17549507.2018.1510033}
}

@article{wang2019overview,
  title={An overview of end-to-end automatic speech recognition},
  author={Wang, D. and Wang, X. and Lv, S.},
  journal={Symmetry},
  volume={11},
  number={8},
  pages={1018},
  year={2019},
  month={Aug.},
  doi={10.3390/sym11081018}
}

@article{gosztolya2016domain,
  title={Domain adaptation of deep neural networks for automatic speech recognition via wireless sensors},
  author={Gosztolya, G. and Grósz, T.},
  journal={Journal of Electrical Engineering},
  volume={67},
  number={2},
  pages={124-130},
  year={2016},
  month={Apr.},
  doi={10.1515/jee-2016-0017}
}

@article{tu2020multi,
  title={A multi-target SNR-progressive learning approach to regression based speech enhancement},
  author={Tu, Y.-H. and Du, J. and Gao, T. and Lee, C.-H.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={1608-1619},
  year={2020},
  doi={10.1109/TASLP.2020.2996503}
}

@article{ming2017speech,
  title={Speech enhancement based on full-sentence correlation and clean speech recognition},
  author={Ming, J. and Crookes, D.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={25},
  number={3},
  pages={531-543},
  year={2017},
  month={Mar.},
  doi={10.1109/TASLP.2017.2651406}
}
@article{shahnawazuddin2018fast,
  title={A fast adaptation approach for enhanced automatic recognition of children's speech with mismatched acoustic models},
  author={Shahnawazuddin, S. and Sinha, R.},
  journal={Circuits, Systems, and Signal Processing},
  volume={37},
  number={3},
  pages={1098-1115},
  year={2018},
  month={Mar.},
  doi={10.1007/s00034-017-0586-6}
}

@inproceedings{liu2020sequence,
  title={Sequence-to-sequence automatic speech recognition with word embedding regularization and fused decoding},
  author={Liu, A. H. and Sung, T.-W. and Chuang, S.-P. and Lee, H.-Y. and Lee, L.-S.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={7879-7883},
  year={2020},
  doi={10.1109/ICASSP40776.2020.9053324}
}

@article{moritz2017multi,
  title={Multi-channel speech enhancement and amplitude modulation analysis for noise robust automatic speech recognition},
  author={Moritz, N. and Adiloglu, K. and Anemüller, J. and Goetze, S. and Kollmeier, B.},
  journal={Computer Speech \& Language},
  volume={46},
  pages={558-573},
  year={2017},
  month={Nov.},
  doi={10.1016/j.csl.2016.11.004}
}

@article{moore2017speech,
  title={Speech enhancement for robust automatic speech recognition: Evaluation using a baseline system and instrumental measures},
  author={Moore, A. H. and Parada, P. P. and Naylor, P. A.},
  journal={Computer Speech \& Language},
  volume={46},
  pages={574-584},
  year={2017},
  month={Nov.},
  doi={10.1016/j.csl.2016.11.003}
}

@article{kim2018rescoring,
  title={Rescoring of N-best hypotheses using top-down selective attention for automatic speech recognition},
  author={Kim, H.-G. and Lee, H. and Kim, G. and Oh, S.-H. and Lee, S.-Y.},
  journal={IEEE Signal Processing Letters},
  volume={25},
  number={2},
  pages={199-203},
  year={2018},
  month={Feb.},
  doi={10.1109/LSP.2017.2772828}
}

@article{miao2020online,
  title={Online hybrid CTC/attention end-to-end automatic speech recognition architecture},
  author={Miao, H. and Cheng, G. and Zhang, P. and Yan, Y.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={1452-1465},
  year={2020},
  doi={10.1109/TASLP.2020.2987752}
}

@article{ochiai2017unified,
  title={Unified architecture for multichannel end-to-end speech recognition with neural beamforming},
  author={Ochiai, T. and Watanabe, S. and Hori, T. and Hershey, J. R. and Xiao, X.},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={11},
  number={8},
  pages={1274-1288},
  year={2017},
  month={Dec.},
  doi={10.1109/JSTSP.2017.2764276}
}

@article{tan2018adaptive,
  title={Adaptive very deep convolutional residual network for noise robust speech recognition},
  author={Tan, T. and Qian, Y. and Hu, H. and Zhou, Y. and Ding, W. and Yu, K.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={26},
  number={8},
  pages={1393-1405},
  year={2018},
  month={Aug.},
  doi={10.1109/TASLP.2018.2825432}
}

@article{li2017mispronunciation,
  title={Mispronunciation detection and diagnosis in L2 English speech using multidistribution deep neural networks},
  author={Li, K. and Qian, X. and Meng, H.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={25},
  number={1},
  pages={193-207},
  year={2017},
  month={Jan.},
  doi={10.1109/TASLP.2016.2621675}
}

@article{devi2020automatic,
  title={Automatic speaker recognition from speech signals using self organizing feature map and hybrid neural network},
  author={Devi, K. J. and Singh, N. H. and Thongam, K.},
  journal={Microprocessors and Microsystems},
  volume={79},
  pages={103264},
  year={2020},
  month={Nov.},
  doi={10.1016/j.micpro.2020.103264}
}
@techreport{kovacs2019joint,
  title={Joint optimization of spectro-temporal features and deep neural nets for robust automatic speech recognition},
  author={Kovacs, G. and Toth, L.},
  institution={Tech. Rep.},
  pages={19},
  year={2019}
}

@article{agrawal2019modulation,
  title={Modulation filter learning using deep variational networks for robust speech recognition},
  author={Agrawal, P. and Ganapathy, S.},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={13},
  number={2},
  pages={244-253},
  year={2019},
  month={May},
  doi={10.1109/JSTSP.2019.2913965}
}

@article{liu2019semantic,
  title={Semantic features based N-best rescoring methods for automatic speech recognition},
  author={Liu, C. and Zhang, P. and Li, T. and Yan, Y.},
  journal={Applied Sciences},
  volume={9},
  number={23},
  pages={5053},
  year={2019},
  month={Nov.},
  doi={10.3390/app9235053}
}

@article{grozdic2017whispered,
  title={Whispered speech recognition using deep denoising autoencoder and inverse filtering},
  author={Grozdic, T. and Jovi£ic, S. T.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={25},
  number={12},
  pages={2313-2322},
  year={2017},
  month={Dec.},
  doi={10.1109/TASLP.2017.2738559}
}

@article{ganapathy2017multivariate,
  title={Multivariate autoregressive spectrogram modeling for noisy speech recognition},
  author={Ganapathy, S.},
  journal={IEEE Signal Processing Letters},
  volume={24},
  number={9},
  pages={1373-1377},
  year={2017},
  month={Sep.},
  doi={10.1109/LSP.2017.2724561}
}

@article{davila2019enhanced,
  title={Enhanced robot speech recognition using biomimetic binaural sound source localization},
  author={Davila-Chacon, J. and Liu, J. and Wermter, S.},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={30},
  number={1},
  pages={138-150},
  year={2019},
  month={Jan.},
  doi={10.1109/TNNLS.2018.2830119}
}

@article{wang2016joint,
  title={A joint training framework for robust automatic speech recognition},
  author={Wang, Z. Q. and Wang, D.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={24},
  number={4},
  pages={796-806},
  year={2016},
  month={Apr.},
  doi={10.1109/TASLP.2016.2528171}
}

@article{lee2016dnn,
  title={DNN-based feature enhancement using DOA-constrained ICA for robust speech recognition},
  author={Lee, H.-Y. and Cho, J.-W. and Kim, M. and Park, H.-M.},
  journal={IEEE Signal Processing Letters},
  volume={23},
  number={8},
  pages={1091-1095},
  year={2016},
  month={Aug.}
}

@article{qian2016neural,
  title={Neural network based multi-factor aware joint training for robust speech recognition},
  author={Qian, Y. and Tan, T. and Yu, D.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={24},
  number={12},
  pages={2231-2240},
  year={2016},
  month={Dec.},
  doi={10.1109/TASLP.2016.2598308}
}

@article{chen2016target,
  title={Target classification using deep convolutional networks for SAR images},
  author={Chen, S. and Wang, H. and Xu, F. and Jin, Y. Q.},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={54},
  number={8},
  pages={4806-4817},
  year={2016},
  month={Aug.},
  doi={10.1109/TGRS.2016.2551720}
}
@article{raufique2016hybrid,
  title={Hybrid neuromorphic system for automatic speech recognition},
  author={Raufique, M. A. and Lee, B. G. and Jeon, M.},
  journal={Electronics Letters},
  volume={52},
  number={17},
  pages={1428-1430},
  year={2016},
  month={Aug.},
  doi={10.1049/el.2016.0975}
}

@inproceedings{rehr2015cepstral,
  title={Cepstral noise subtraction for robust automatic speech recognition},
  author={Rehr, R. and Gerkmann, T.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={375-378},
  year={2015},
  month={Apr.}
}

@inproceedings{meutzner2016generative,
  title={A generative-discriminative hybrid approach to multi-channel noise reduction for robust automatic speech recognition},
  author={Meutzner, H. and Araki, S. and Fujimoto, M. and Nakatani, T.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5740-5744},
  year={2016},
  month={Mar.}
}

@inproceedings{mosner2019improving,
  title={Improving noise robustness of automatic speech recognition via parallel data and teacher-student learning},
  author={Mosner, L. and Wu, M. and Raju, A. and Parthasarathi, S. H. K. and Kumatani, K. and Sundaram, S. and Maas, R. and Hoffmeister, B.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6475-6479},
  year={2019},
  month={May}
}

@inproceedings{liu2017modulation,
  title={A modulation feature set for robust automatic speech recognition in additive noise and reverberation},
  author={Liu, X. and Sadeghian, R. and Zahorian, S. A.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5230-5234},
  year={2017},
  month={Mar.}
}

@inproceedings{do2018weighting,
  title={Weighting time-frequency representation of speech using auditory saliency for automatic speech recognition},
  author={Do, C.-T. and Stylianou, Y.},
  booktitle={Proc. INTERSPEECH},
  pages={1591-1595},
  year={2018},
  month={Sep.}
}

@inproceedings{kuehne2015handling,
  title={Handling derivative filterbank features in bounded-marginalization-based missing data automatic speech recognition},
  author={Kühne, M.},
  booktitle={Proc. 16th Annu. Conf. Int. Speech Commun. Assoc.},
  pages={},
  year={2015},
  month={Sep.}
}

@article{park2020improved,
  title={Improved noisy student training for automatic speech recognition},
  author={Park, D. S. and Zhang, Y. and Jia, Y. and Han, W. and Chiu, C.-C. and Li, B. and Wu, Y. and Le, Q. V.},
  year={2020},
  eprint={2005.09629},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url={http://arxiv.org/abs/2005.09629}
}

@inproceedings{yadav2018spectral,
  title={Spectral smoothing by variational mode decomposition and its effect on noise and pitch robustness of ASR system},
  author={Yadav, I. C. and Shahnawazuddin and Govind, D. and Pradhan, G.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5629-5633},
  year={2018},
  month={Apr.}
}

@inproceedings{dat2016comparative,
  title={A comparative study of multi-channel processing methods for noisy automatic speech recognition in urban environments},
  author={Dat, T. H. and Dennis, J. and Ren, L. Y. and Terence, N. W. Z.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6465-6469},
  year={2016},
  month={Mar.}
}
@article{menne2019analysis,
  title={Analysis of deep clustering as preprocessing for automatic speech recognition of sparsely overlapping speech},
  author={Menne, T. and Sklyar, I. and Schlüter, R. and Ney, H.},
  year={2019},
  eprint={1905.03500},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url={http://arxiv.org/abs/1905.03500}
}

@inproceedings{martin2020understanding,
  title={Understanding racial disparities in automatic speech recognition: The case of habitual},
  author={Martin, J. L. and Tang, K.},
  booktitle={Proc. INTERSPEECH},
  pages={626-630},
  year={2020}
}

@inproceedings{rasipuram2015integrated,
  title={Integrated pronunciation learning for automatic speech recognition using probabilistic lexical modeling},
  author={Rasipuram, R. and Razavi, M. and Magimai-Doss, M.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5176-5180},
  year={2015},
  month={Apr.},
  doi={10.1109/ICASSP.2015.7178958}
}

@inproceedings{le2020g2g,
  title={G2G: TTS-driven pronunciation learning for graphemic hybrid ASR},
  author={Le, D. and Koehler, T. and Fuegen, C. and Seltzer, M. L.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6869-6873},
  year={2020},
  month={May},
  doi={10.1109/ICASSP40776.2020.9054257}
}

@inproceedings{fringi2015evidence,
  title={Evidence of phonological processes in automatic recognition of children's speech},
  author={Fringi, E. and Lehman, J. F. and Russell, M.},
  booktitle={Proc. 16th Annu. Conf. Int. Speech Commun. Assoc.},
  pages={1-4},
  year={2015}
}

@inproceedings{garg2020hierarchical,
  title={Hierarchical multi-stage word-to-grapheme named entity corrector for automatic speech recognition},
  author={Garg, A. and Gupta, A. and Gowda, D. and Singh, S. and Kim, C.},
  booktitle={Proc. INTERSPEECH},
  pages={1793-1797},
  year={2020},
  month={Oct.}
}

@inproceedings{exter2016dnn,
  title={DNN-based automatic speech recognition as a model for human phoneme perception},
  author={Exter, M. and Meyer, B. T.},
  booktitle={Proc. INTERSPEECH},
  pages={615-619},
  year={2016},
  month={Sep.}
}

@inproceedings{egorova2018out,
  title={Out-of-vocabulary word recovery using FST-based subword unit clustering in a hybrid ASR system},
  author={Egorova, E. and Burget, L.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5919-5923},
  year={2018},
  month={Apr.}
}

@inproceedings{pham2020independent,
  title={Independent language modeling architecture for end-to-end ASR},
  author={Pham, V. T. and Xu, H. and Khassanov, Y. and Zeng, Z. and Chng, E. S. and Ni, C. and Ma, B. and Li, H.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={7059-7063},
  year={2020},
  month={May}
}

@inproceedings{drexler2019subword,
  title={Subword regularization and beam search decoding for end-to-end automatic speech recognition},
  author={Drexler, J. and Glass, J.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6266-6270},
  year={2019},
  month={May}
}
@inproceedings{drexler2020learning,
  title={Learning a subword inventory jointly with end-to-end automatic speech recognition},
  author={Drexler, J. and Glass, J.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6439-6443},
  year={2020},
  month={May},
  doi={10.1109/ICASSP40776.2020.9053736}
}

@inproceedings{song2020learning,
  title={Learning recurrent neural network language models with context-sensitive label smoothing for automatic speech recognition},
  author={Song, M. and Zhao, Y. and Wang, S. and Han, M.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6159-6163},
  year={2020},
  month={May},
  doi={10.1109/ICASSP40776.2020.9053589}
}

@article{lakomkin2020subword,
  title={Subword regularization: An analysis of scalability and generalization for end-to-end automatic speech recognition},
  author={Lakomkin, E. and Heymann, J. and Sklyar, I. and Wiesler, S.},
  year={2020},
  eprint={2008.04034},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url={http://arxiv.org/abs/2008.04034}
}

@inproceedings{liao2015large,
  title={Large vocabulary automatic speech recognition for children},
  author={Liao, H. and Pundak, G. and Siohan, O. and Carroll, M. K. and Coccaro, N. and Jiang, Q.-M. and Sainath, T. N. and Senior, A. and Beaufays, F. and Bacchiani, M.},
  booktitle={Proc. INTERSPEECH},
  pages={1-5},
  year={2015},
  month={Sep.}
}

@inproceedings{lecouteux2015ant,
  title={Ant colony algorithm applied to automatic speech recognition graph decoding},
  author={Lecouteux, B. and Schwab, D.},
  booktitle={Proc. INTERSPEECH},
  pages={1-6},
  year={2015},
  month={Sep.}
}

@inproceedings{baby2015investigating,
  title={Investigating modulation spectrogram features for deep neural network-based automatic speech recognition},
  author={Baby, D. and Van Hamme, H.},
  booktitle={Proc. 16th Annu. Conf. Int. Speech Commun. Assoc.},
  pages={2479-2483},
  year={2015},
  month={Sep.}
}

@inproceedings{do2017improved,
  title={Improved automatic speech recognition using subband temporal envelope features and time-delay neural network denoising autoencoder},
  author={Do, C.-T. and Stylianou, Y.},
  booktitle={Proc. INTERSPEECH},
  pages={3832-3836},
  year={2017},
  month={Aug.}
}

@article{cui2019acoustic,
  title={Acoustic model optimization based on evolutionary stochastic gradient descent with anchors for automatic speech recognition},
  author={Cui, X. and Picheny, M.},
  year={2019},
  eprint={1907.04882},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url={http://arxiv.org/abs/1907.04882}
}

@inproceedings{lohrenz2020blstm,
  title={BLSTM-driven stream fusion for automatic speech recognition: Novel methods and a multi-size window fusion example},
  author={Lohrenz, T. and Fingscheidt, T.},
  booktitle={Proc. INTERSPEECH},
  pages={26-30},
  year={2020},
  month={Oct.}
}

@inproceedings{fahringer2016phaseaware,
  title={Phase-aware signal processing for automatic speech recognition},
  author={Fahringer, J. and Schrank, T. and Stahl, J. and Mowlaee, P. and Pernkopf, F.},
  booktitle={Proc. INTERSPEECH},
  pages={3374-3378},
  year={2016},
  month={Sep.}
}
@inproceedings{inaguma2018end,
  title={An end-to-end approach to joint social signal detection and automatic speech recognition},
  author={Inaguma, H. and Mimura, M. and Inoue, K. and Yoshii, K. and Kawahara, T.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6214-6218},
  year={2018},
  month={Apr.}
}

@article{zhang2019highly,
  title={A highly efficient distributed deep learning system for automatic speech recognition},
  author={Zhang, W. and Cui, X. and Finkler, U. and Saon, G. and Kayi, A. and Buyuktosunoglu, A. and Kingsbury, B. and Kung, D. and Picheny, M.},
  year={2019},
  eprint={1907.05701},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url={http://arxiv.org/abs/1907.05701}
}

@inproceedings{zhang2019distributed,
  title={Distributed deep learning strategies for automatic speech recognition},
  author={Zhang, W. and Cui, X. and Finkler, U. and Kingsbury, B. and Saon, G. and Kung, D. and Picheny, M.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5706-5710},
  year={2019},
  month={May}
}

@inproceedings{inaguma2020minimum,
  title={Minimum latency training strategies for streaming sequence-to-sequence ASR},
  author={Inaguma, H. and Gaur, Y. and Lu, L. and Li, J. and Gong, Y.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6064-6068},
  year={2020},
  month={May}
}

@inproceedings{moritz2020streaming,
  title={Streaming automatic speech recognition with the transformer model},
  author={Moritz, N. and Hori, T. and Le, J.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6074-6078},
  year={2020},
  month={May}
}

@inproceedings{sim2018domain,
  title={Domain adaptation using factorized hidden layer for robust automatic speech recognition},
  author={Sim, K. C. and Narayanan, A. and Misra, A. and Tripathi, A. and Pundak, G. and Sainath, T. N. and Haghani, P. and Li, B. and Bacchiani, M.},
  booktitle={Proc. INTERSPEECH},
  pages={892-896},
  year={2018},
  month={Sep.}
}

@inproceedings{mani2020asr,
  title={ASR error correction and domain adaptation using machine translation},
  author={Mani, A. and Palaskar, S. and Meripo, N. V. and Konam, S. and Metze, F.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={6344-6348},
  year={2020},
  month={May}
}

@inproceedings{kundu2016joint,
  title={Joint acoustic factor learning for robust deep neural network based automatic speech recognition},
  author={Kundu, S. and Mantena, G. and Qian, Y. and Tan, T. and Delcroix, M. and Sim, K. C.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5025-5029},
  year={2016},
  month={Mar.}
}

@inproceedings{moriya2019multimodal,
  title={Multimodal speaker adaptation of acoustic model and language model for ASR using speaker face embedding},
  author={Moriya, Y. and Jones, G. J. F.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={8643-8647},
  year={2019},
  month={May},
  doi={10.1109/ICASSP.2019.8683724}
}

@article{Fonseca2022FSD50K,
  author = {Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier},
  title = {{FSD50K}: An Open Dataset of Human-Labeled Sound Events},
  journal = {{IEEE/ACM Transactions on Audio, Speech, and Language Processing}},
  volume = {30},
  year = {2022},
  doi = {10.48550/arXiv.2010.00475},
  url = {https://doi.org/10.48550/arXiv.2010.00475}
}


@inproceedings{malek2017robust,
  title={Robust automatic recognition of speech with background music},
  author={Malek, J. and Zdansky, J. and Cerva, P.},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  pages={5210-5214},
  year={2017},
  month={Mar.}
}
@incollection{tassinary2007skeletomotor,
  author={Tassinary, L. G. and Cacioppo, J. T. and Vanman, E. J.},
  title={The skeletomotor system: Surface electromyography},
  booktitle={Handbook of Psychophysiology},
  editor={Cacioppo, J. T. and Tassinary, L. G. and Berntson, G. G.},
  publisher={Cambridge University Press},
  address={New York},
  edition={3rd},
  year={2007},
  chapter={12},
  pages={267-299},
  isbn={9780521844710},
  url={https://espace.library.uq.edu.au/view/UQ:165720}
}
@article{barnish2016roles,
  title={Roles of cognitive status and intelligibility in everyday communication in people with Parkinson's disease: A systematic review},
  author={Barnish, Maxwell S and Whibley, Daniel and Horton, Simon and Butterfint, Zoe R and Deane, Katherine HO},
  journal={Journal of Parkinson's Disease},
  volume={6},
  number={3},
  pages={453--462},
  year={2016},
  doi={10.3233/JPD-150757}
}

@article{behrman2017clear,
  title={A clear speech approach to accent management},
  author={Behrman, Alison},
  journal={American Journal of Speech-Language Pathology},
  volume={26},
  number={4},
  pages={1178--1192},
  year={2017},
  doi={10.1044/2017_AJSLP-16-0177}
}

@article{durso2012detecting,
  title={Detecting confusion using facial electromyography},
  author={Durso, Francis T and Geldbach, Kathryn M and Corballis, Paul},
  journal={Human Factors},
  volume={54},
  number={1},
  pages={60--69},
  year={2012},
  doi={10.1177/0018720811428450}
}
@article{liu2016eqclinic,
  title={EQClinic: a platform for learning communication skills in clinical consultations},
  author={Liu, Cindy and Scott, Karen M and Lim, Rosemary L and Taylor, Silas and Calvo, Rafael A},
  journal={Medical Education Online},
  volume={21},
  number={1},
  pages={31801},
  year={2016},
  publisher={Taylor \& Francis},
  doi={10.3402/meo.v21.31801}
}


@article{menne2019analysis,
  title={Analysis of deep clustering as preprocessing for automatic speech recognition of sparsely overlapping speech},
  author={Menne, T. and Sklyar, I. and Schlüter, R. and Ney, H.},
  year={2019},
  eprint={1905.03500},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url={http://arxiv.org/abs/1905.03500}
}

@article{kumar2020leveraging,
  title={Leveraging linguistic context in dyadic interactions to improve automatic speech recognition for children},
  author={Kumar, M. and Kim, S. H. and Lord, C. and Lyon, T. D. and Narayanan, S.},
  journal={Comput. Speech Lang.},
  volume={63},
  year={2020},
  pages={101101},
  month={Sep},
  doi={10.1016/j.csl.2020.101101}
}

@article{wang2020wavenet,
  title={WaveNet with crossattention for audiovisual speech recognition},
  author={Wang, H. and Gao, F. and Zhao, Y. and Wu, L.},
  journal={IEEE Access},
  volume={8},
  pages={169160--169168},
  year={2020},
  doi={10.1109/ACCESS.2020.3024218}
}

@article{shimada2019unsupervised,
  title={Unsupervised speech enhancement based on multichannel NMF-informed beamforming for noise-robust automatic speech recognition},
  author={Shimada, K. and Bando, Y. and Mimura, M. and Itoyama, K. and Yoshii, K. and Kawahara, T.},
  journal={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={27},
  number={5},
  pages={960--971},
  year={2019},
  month={May},
  doi={10.1109/TASLP.2019.2907015}
}

@article{chen2018progressive,
  title={Progressive joint modeling in unsupervised single-channel overlapped speech recognition},
  author={Chen, Z. and Droppo, J. and Li, J. and Xiong, W.},
  journal={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={26},
  number={1},
  pages={184--196},
  year={2018},
  month={Jan},
  doi={10.1109/TASLP.2017.2765834}
}
@book{EkmanFriesen1978,
  author       = {Paul Ekman and Wallace V. Friesen},
  title        = {Manual for the facial action coding system},
  year         = {1978},
  publisher    = {Consulting Psychologists Press}
}

@inproceedings{GrafsgaardEtAl2015,
  author       = {Jonathan F. Grafsgaard and Steven Y. Lee and Bradford W. Mott and Keith E. Boyer and Jack C. Lester},
  title        = {Modeling self-efficacy across age groups with automatically tracked facial expression},
  booktitle    = {International Conference on Artificial Intelligence in Education},
  year         = {2015},
  pages        = {582--585}
}

@article{HietanenEtAl1998,
  author       = {Jari K. Hietanen and Ville Surakka and Ilpo Linnankoski},
  title        = {Facial electromyographic responses to vocal affect expressions},
  journal      = {Psychophysiology},
  volume       = {35},
  number       = {5},
  pages        = {530--536},
  year         = {1998}
}

@article{VanBoxtelJessurun1993,
  author       = {Ad F. L. Van Boxtel and Marcel Jessurun},
  title        = {Amplitude and bilateral coherency of facial and jaw-elevator EMG activity as an index of effort during a two-choice serial reaction task},
  journal      = {Psychophysiology},
  volume       = {30},
  number       = {6},
  pages        = {589--604},
  year         = {1993}
}

@inproceedings{Yngve1970,
  author       = {V. H. Yngve},
  title        = {On getting a word in edgewise},
  booktitle    = {Chicago Linguistics Society, 6th Meeting},
  year         = {1970},
  pages        = {567--578}
}

@article{WardTsukahara2000,
  author       = {Nigel Ward and Wataru Tsukahara},
  title        = {Prosodic features which cue back-channel responses in English and Japanese},
  journal      = {Journal of Pragmatics},
  volume       = {32},
  number       = {8},
  pages        = {1177--1207},
  year         = {2000}
}

@inproceedings{CowieEtAl2010,
  author       = {Roddy Cowie and Hatice Gunes and Guillaume McKeown and Laurence Vaclau-Schneider and Jeff Armstrong and Ellen Douglas-Cowie},
  title        = {The emotional and communicative significance of head nods and shakes in a naturalistic database},
  booktitle    = {Proceedings of LREC International Workshop on Emotion},
  year         = {2010},
  pages        = {42--46}
}

@misc{Eibl-Eibesfeldt1972,
  author       = {Irenäus Eibl-Eibesfeldt},
  title        = {Similarities and differences between cultures in expressive movements},
  year         = {1972}
}

@article{CraneCrane2010,
  author       = {Julie Crane and Francis G. Crane},
  title        = {Optimal nonverbal communications strategies physicians should engage in to promote positive clinical outcomes},
  journal      = {Health Marketing Quarterly},
  volume       = {27},
  number       = {3},
  pages        = {262--274},
  year         = {2010}
}

@article{DiMatteoEtAl1980,
  author       = {M. Robin DiMatteo and Alfonso Taranta and Howard S. Friedman and Lawrence M. Prince},
  title        = {Predicting patient satisfaction from physicians' nonverbal communication skills},
  journal      = {Medical Care},
  pages        = {376--387},
  year         = {1980}
}

@incollection{Robinson2006,
  author       = {John D. Robinson},
  title        = {Nonverbal communication and physician-patient interaction},
  booktitle    = {SAGE Handbook of Nonverbal Communication},
  pages        = {437--459},
  year         = {2006}
}

@article{OngEtAl1995,
  author       = {L. M. L. Ong and J. C. J. M. De Haes and A. M. Hoos and F. B. Lammes},
  title        = {Doctor-patient communication: a review of the literature},
  journal      = {Social Science \& Medicine},
  volume       = {40},
  number       = {7},
  pages        = {903--918},
  year         = {1995}
}

@article{BullerBuller1987,
  author       = {Martha K. Buller and David B. Buller},
  title        = {Physicians' communication style and patient satisfaction},
  journal      = {Journal of Health and Social Behavior},
  pages        = {375--388},
  year         = {1987}
}

@misc{GilliganEtAl2016,
  author       = {Christina Gilligan and others},
  title        = {Interventions for improving medical students' interpersonal communication in medical consultations},
  year         = {2016}
}

@article{LippmannEtAl1997,
  author       = {R. P. Lippmann and others},
  title        = {Speech recognition by machines and humans},
  journal      = {Speech Communication},
  volume       = {22},
  number       = {1},
  pages        = {1--15},
  year         = {1997}
}

@article{RoyEtAl2010,
  author       = {B. C. Roy and S. Vosoughi and D. K. Roy},
  title        = {Automatic estimation of transcription accuracy and difficulty},
  journal      = {2010}
}

@misc{Belambert2018,
  author       = {Belambert},
  title        = {ASR-Evaluation},
  year         = {2018},
  note         = {[Online]. Available: \url{https://github.com/belambert/asr-evaluation}}
}
@inproceedings{GillickCox1989,
  author       = {L. Gillick and S. J. Cox},
  title        = {Some statistical issues in the comparison of speech recognition algorithms},
  booktitle    = {Acoustics, Speech, and Signal Processing ICASSP-89},
  year         = {1989},
  pages        = {532--535}
}

@inproceedings{SiohanEtAl2005,
  author       = {O. Siohan and B. Ramabhadran and B. Kingsbury},
  title        = {Constructing ensembles of ASR systems using randomized decision trees},
  booktitle    = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year         = {2005},
  volume       = {1},
  pages        = {I--197}
}

@book{JamesEtAl2014,
  author       = {G. James and D. Witten and T. Hastie and R. Tibshirani},
  title        = {An Introduction to Statistical Learning: With Applications in R},
  publisher    = {Springer Publishing Company, Incorporated},
  year         = {2014}
}

@inproceedings{BisaniNey2004,
  author       = {M. Bisani and H. Ney},
  title        = {Bootstrap estimates for confidence intervals in ASR performance evaluation},
  booktitle    = {Acoustics, Speech, and Signal Processing, 2004. Proceedings. (ICASSP'04). IEEE International Conference on},
  year         = {2004},
  volume       = {1},
  pages        = {I--409}
}

@article{KepuskaBohouta2017,
  author       = {V. Këpuska and G. Bohouta},
  title        = {Comparing speech recognition systems (Microsoft API, Google API and CMU Sphinx)},
  journal      = {Journal of Engineering Research and Applications},
  volume       = {7},
  number       = {3},
  pages        = {20--24},
  year         = {2017}
}

@article{Lippmannetal1997,
  author       = {Lippmann, R. P. and others},
  title        = {Speech recognition by machines and humans},
  journal      = {Speech Communication},
  volume       = {22},
  number       = {1},
  pages        = {1--15},
  year         = {1997}
}

@article{Guntupallietal2007,
  author       = {Guntupalli, V. K. and Erik Everhart, D. and Kalinowski, J. and Nanjundeswaran, C. and Saltuklaroglu, T.},
  title        = {Emotional and physiological responses of fluent listeners while watching the speech of adults who stutter},
  journal      = {International Journal of Language \& Communication Disorders},
  volume       = {42},
  number       = {2},
  pages        = {113--129},
  year         = {2007}
}

@article{Hessetal1998,
  author       = {Hess, U. and Philippot, P. and Blairy, S.},
  title        = {Facial reactions to emotional facial expressions: Affect or cognition?},
  journal      = {Cognition \& Emotion},
  volume       = {12},
  number       = {4},
  pages        = {509--531},
  year         = {1998}
}

@techreport{Bernstein1999,
  author = {Bernstein, J.},
  title = {PhonePass testing: Structure and construct},
  institution = {Ordinate Corporation},
  year = {1999}
}

@phdthesis{Witt1999,
  author = {Witt, S. M.},
  title = {Use of speech recognition in computer-assisted language learning},
  school = {University of Cambridge},
  year = {1999}
}

@inproceedings{Franco2000,
  author = {Franco, H. and Abrash, V. and Precoda, K. and Bratt, H. and Rao, R. and Butzberger, J.},
  title = {The SRI EduSpeak system: Recognition and pronunciation scoring for language learning},
  booktitle = {In-STiLL (Intelligent Speech Technology in Language Learning)},
  address = {Dundee, Scotland},
  year = {2000}
}

@inproceedings{Hacker2005,
  author = {Hacker, C. and Cincarek, T. and Grubn, R. and Steidl, S. and Noth, E. and Niemann, H.},
  title = {Pronunciation Feature Extraction},
  booktitle = {Proceedings of DAGM 2005},
  year = {2005}
}

@inproceedings{Zechner2006,
  author = {Zechner, K. and Bejar, I.},
  title = {Towards Automatic Scoring of Non-Native Spontaneous Speech},
  booktitle = {NAACL-HLT},
  address = {New York, NY},
  year = {2006}
}
@techreport{Pearson2008versant,
  author = {Pearson},
  title = {Versant English Test: Test Description and Validation Summary},
  institution = {Pearson},
  year = {2008}
}
@article{McNamaraLumley1997,
  author = {McNamara, Tim and Lumley, Tom},
  title = {The Effect of Interlocutor and Assessment Mode Variables in Overseas Assessments of Speaking Skills in Occupational Settings},
  journal = {Language Testing},
  volume = {14},
  number = {2},
  pages = {140--156},
  month = {Jul.},
  year = {1997},
  doi = {10.1177/026553229701400203}
}
@inproceedings{Johnson1999SpokenDocumentRetrieval,
  author = {Johnson, Steve and Jourlin, Patrick and Jones, Karen S. and Woodland, Philip},
  title = {Spoken Document Retrieval for {TREC-8} at {Cambridge University}},
  booktitle = {Proc. of the 8th Text REtrieval Conference},
  year = {1999},
  address = {Gaithersburg, MD}
}

@inproceedings{Johnson1999CambridgeUniversitySpoken,
  author = {Johnson, S. E. and Jourlin, P. and Moore, G. L. and Spärck Jones, K. and Woodland, P. C.},
  title = {The Cambridge University Spoken Document Retrieval System},
  booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech, and Signal Processing},
  year = {1999},
  pages = {49--52}
}

@techreport{Papineni2001BleuMethodAutomatic,
  author = {Papineni, K. and Roukos, S. and Ward, T. and Zhu, W.-J.},
  title = {Bleu: A Method for Automatic Evaluation of Machine Translation},
  institution = {IBM},
  year = {2001},
  month = {Sept.},
  type = {Research Report},
  number = {RC22176}
}

@techreport{Liu2003NoiseRobustnessSpeech,
  author = {Liu, Fu-Hua and Gao, Yuqing and Gu, Liang and Picheny, Michael},
  title = {Noise Robustness In In Speech To Speech Translation},
  institution = {IBM},
  year = {2003},
  type = {Research Report},
  number = {RC22874}
}

@article{fish_using_2023,
  author = {Fish, Robert and et al.},
  title = {Using Predictive Analytics to Allocate Processing Resources for Speech Transcription},
  journal = {Speech Communication},
  volume = {123},
  pages = {1--10},
  year = {2023},
  doi = {10.1016/j.specom.2022.10.002}
}
@article{NolazcoFlores1994ContinuousSR,
  title={Continuous speech recognition in noise using spectral subtraction and HMM adaptation},
  author={Juan Arturo Nolazco-Flores and Steve J. Young},
  journal={Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
  year={1994},
  volume={i},
  pages={I/409-I/412 vol.1}
}
@article{cooke1996missingdata,
author = {Cooke, Martin and Green, Phil and Crawford, Malcolm},
year = {1996},
month = {08},
pages = {},
title = {Handling Missing Data In Speech Recognition}
}
@Inbook{Ma2006,
author="Ma, Xin
and Zhou, Weidong
and Ju, Fang",
editor="Huang, De-Shuang
and Li, Kang
and Irwin, George William",
title="Robust Speech Feature Extraction Based on Dynamic Minimum Subband Spectral Subtraction",
bookTitle="Intelligent Computing in Signal Processing and Pattern Recognition: International Conference on Intelligent Computing, ICIC 2006 Kunming, China, August 16--19, 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1056--1061",
abstract="Based on theoretical analysis of nonlinear feature extraction, we proposed a new method called a dynamic minimum subband spectral subtraction (DMSSS) and discussed its effects to the results of speech recognition. We illustrate the process of removing corrupted components by subtracting the estimated dynamic minimum of short-time spectra. Experimental results show the proposed method is stable and yield a good performance in ASR under noisy environments. If combined with peak isolation method, DMSSS can improve the recognition performance significantly.",
isbn="978-3-540-37258-5",
doi="10.1007/978-3-540-37258-5_136",
url="https://doi.org/10.1007/978-3-540-37258-5_136"
}

@INPROCEEDINGS{cooke1997ieee_missing,
  author={Cooke, M. and Morris, A. and Green, P.},
  booktitle={1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  title={Missing data techniques for robust speech recognition},
  year={1997},
  volume={2},
  number={},
  pages={863-866 vol.2},
  doi={10.1109/ICASSP.1997.596072}}
@article{green_missing_2001,
author = {Green, Phil and Barker, Jon and Cooke, Martin and Josifovski, Ljubomir},
year = {2001},
month = {01},
pages = {},
title = {Handling Missing and Unreliable Information in Speech Recognition}
}

@inproceedings{serralheiro89_eurospeech,
  author={A. J. Serralheiro and Y. Ephraim and Lawrence R. Rabiner},
  title={{On nonstationary hidden Markov modeling of speech signals}},
  year=1989,
  booktitle={Proc. First European Conference on Speech Communication and Technology (Eurospeech 1989)},
  pages={1159--1162},
  doi={10.21437/Eurospeech.1989-51}
}
@book{Brown1996IntroductionRandomSignals,
  author = {Brown, Robert Grover and Hwang, Patrick Y.C.},
  title = {Introduction to Random Signals and Applied Kalman Filtering},
  edition = {3},
  year = {1996},
  publisher = {John Wiley & Sons},
  address = {New York},
  isbn = {978-0-471-12839-7}
}
@book{Wiener1964ExtrapolationInterpolationSmoothing,
  author = {Wiener, Norbert},
  title = {Extrapolation, Interpolation, and Smoothing of Stationary Time Series: With Engineering Applications},
  year = {1964},
  publisher = {The MIT Press},
  isbn = {9780262730051},
  address = {Cambridge, MA}
}
@article{Deng2004Challenges,
  author = {Deng, Li and Huang, Xuedong},
  title = {Challenges in Adopting Speech Recognition},
  journal = {Communications of the ACM},
  volume = {47},
  number = {1},
  pages = {69--75},
  year = {2004}
}

@article{Barnish2016Roles,
  author = {Barnish, Mary S. and Whibley, Daniel and Horton, Simon and Butterfint, Zoe R. and Deane, Katherine H. O.},
  title = {Roles of Cognitive Status and Intelligibility in Everyday Communication in People with Parkinson's Disease: A Systematic Review},
  journal = {Journal of Parkinson's Disease},
  volume = {6},
  number = {3},
  pages = {453--462},
  year = {2016}
}

@article{Behrman2017Clear,
  author = {Behrman, Alison},
  title = {A Clear Speech Approach to Accent Management},
  journal = {American Journal of Speech-Language Pathology},
  volume = {26},
  number = {4},
  pages = {1178--1192},
  year = {2017}
}

@article{Durso2012Detecting,
  author = {Durso, Francis T. and Geldbach, Kathryn M. and Corballis, Paul},
  title = {Detecting Confusion Using Facial Electromyography},
  journal = {Human Factors},
  volume = {54},
  number = {1},
  pages = {60--69},
  year = {2012}
}

@inproceedings{Xiong2018Microsoft,
  author = {Xiong, Wayne and Wu, Liang and Alleva, Fabio and Droppo, Jasha and Huang, Xuedong and Stolcke, Andreas},
  title = {The Microsoft 2017 Conversational Speech Recognition System},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {5934--5938},
  year = {2018}
}

@article{Saon2017English,
  author = {Saon, George and et al.},
  title = {English Conversational Telephone Speech Recognition by Humans and Machines},
  journal = {arXiv Preprint arXiv:1703.02136},
  year = {2017}
}

@article{Saon2015IBMEnglish,
  author = {Saon, George and Kuo, H.-K. J. and Rennie, Steven and Picheny, Michael},
  title = {The IBM 2015 English Conversational Telephone Speech Recognition System},
  journal = {arXiv Preprint arXiv:1505.05899},
  year = {2015}
}

@inproceedings{Seide2011Conversational,
  author = {Seide, Frank and Li, Gang and Yu, Dong},
  title = {Conversational Speech Transcription Using Context-Dependent Deep Neural Networks},
  booktitle = {Twelfth Annual Conference of the International Speech Communication Association},
  year = {2011}
}

@inproceedings{Chiu2018Stateoftheart,
  author = {Chiu, Chung-Cheng and et al.},
  title = {State-of-the-Art Speech Recognition with Sequence-to-Sequence Models},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {4774--4778},
  year = {2018}
}

@article{Yoshioka2012Making,
  author = {Yoshioka, Takuya and et al.},
  title = {Making Machines Understand Us in Reverberant Rooms: Robustness Against Reverberation for Automatic Speech Recognition},
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {114--126},
  year = {2012}
}
@article{LeCun1995Convolutional,
  author = {LeCun, Yann and Bengio, Yoshua and others},
  title = {Convolutional Networks for Images, Speech, and Time Series},
  journal = {Handbook of Brain Theory and Neural Networks},
  volume = {3361},
  number = {10},
  pages = {1995},
  year = {1995}
}

@article{Carletta2007Unleashing,
  author = {Carletta, Jean},
  title = {Unleashing the Killer Corpus: Experiences in Creating the Multi-Everything AMI Meeting Corpus},
  journal = {Language Resources and Evaluation},
  volume = {41},
  number = {2},
  pages = {181--190},
  year = {2007}
}

@article{Swietojanski2014Convolutional,
  author = {Swietojanski, Pawel and Ghoshal, Arnab and Renals, Steve},
  title = {Convolutional Neural Networks for Distant Speech Recognition},
  journal = {IEEE Signal Processing Letters},
  volume = {21},
  number = {9},
  pages = {1120--1124},
  year = {2014}
}

@inproceedings{Renals2017Distant,
  author = {Renals, Steve and Swietojanski, Pawel},
  title = {Distant Speech Recognition Experiments using the AMI Corpus},
  booktitle = {New Era for Robust Speech Recognition},
  publisher = {Springer},
  pages = {355--368},
  year = {2017}
}

@article{Kepuska2017Comparing,
  author = {Këpuska, Valmir and Bohouta, Ghizlane},
  title = {Comparing Speech Recognition Systems (Microsoft API, Google API and CMU Sphinx)},
  journal = {Journal of Engineering Research and Applications},
  volume = {7},
  number = {3},
  pages = {20--24},
  year = {2017}
}

@misc{Grafsgaard2015Modeling,
  author = {Grafsgaard, Joseph F. and Lee, Sunghyun Y. and Mott, Bradford W. and Boyer, Kristy Elizabeth and Lester, James C.},
  title = {Modeling Self-Efficacy Across Age Groups with Automatically Tracked Facial Expression},
  booktitle = {International Conference on Artificial Intelligence in Education},
  pages = {582--585},
  year = {2015}
}

@article{Hietanen1998Facial,
  author = {Hietanen, Jari K. and Surakka, Veikko and Linnankoski, Ilkka},
  title = {Facial Electromyographic Responses to Vocal Affect Expressions},
  journal = {Psychophysiology},
  volume = {35},
  number = {5},
  pages = {530--536},
  year = {1998}
}

@article{VanBoxtel1993Amplitude,
  author = {Van Boxtel, Andries and Jessurun, Marjolijn},
  title = {Amplitude and Bilateral Coherency of Facial and Jaw-Elevator EMG Activity as an Index of Effort During a Two-Choice Serial Reaction Task},
  journal = {Psychophysiology},
  volume = {30},
  number = {6},
  pages = {589--604},
  year = {1993}
}

@inproceedings{Yngve1970On,
  author = {Yngve, Victor H.},
  title = {On Getting a Word in Edgewise},
  booktitle = {Chicago Linguistics Society, 6th Meeting, 1970},
  year = {1970},
  pages = {567--578}
}

@article{Ward2000Prosodic,
  author = {Ward, Nigel and Tsukahara, Wakayo},
  title = {Prosodic Features Which Cue Back-Channel Responses in English and Japanese},
  journal = {Journal of Pragmatics},
  volume = {32},
  number = {8},
  pages = {1177--1207},
  year = {2000}
}
@inproceedings{Cowie2010Emotional,
  author = {Cowie, Roddy and Gunes, Hatice and McKeown, Gary and Vaclau-Schneider, Tomasz and Armstrong, Joe and Douglas-Cowie, Ellen},
  title = {The Emotional and Communicative Significance of Head Nods and Shakes in a Naturalistic Database},
  booktitle = {Proceedings of LREC International Workshop on Emotion},
  year = {2010},
  pages = {42--46}
}

@article{EiblEibesfeldt1972Similarities,
  author = {Eibl-Eibesfeldt, Irenäus},
  title = {Similarities and Differences between Cultures in Expressive Movements},
  year = {1972}
}

@article{Crane2010Optimal,
  author = {Crane, Joseph and Crane, F. G.},
  title = {Optimal Nonverbal Communications Strategies Physicians Should Engage in to Promote Positive Clinical Outcomes},
  journal = {Health Marketing Quarterly},
  volume = {27},
  number = {3},
  pages = {262--274},
  year = {2010}
}

@article{DiMatteo1980Predicting,
  author = {DiMatteo, M. Robin and Taranta, Anita and Friedman, Howard S. and Prince, Lois M.},
  title = {Predicting Patient Satisfaction from Physicians' Nonverbal Communication Skills},
  journal = {Medical Care},
  pages = {376--387},
  year = {1980}
}

@article{Robinson2006Nonverbal,
  author = {Robinson, James D.},
  title = {Nonverbal Communication and Physician-Patient Interaction},
  booktitle = {SAGE Handbook of Nonverbal Communication},
  pages = {437--459},
  year = {2006}
}

@article{Ong1995DoctorPatient,
  author = {Ong, L. M. L. and De Haes, J. C. J. M. and Hoos, Anneke M. and Lammes, Fred B.},
  title = {Doctor-Patient Communication: A Review of the Literature},
  journal = {Social Science \& Medicine},
  volume = {40},
  number = {7},
  pages = {903--918},
  year = {1995}
}

@article{Buller1987PhysiciansCommunication,
  author = {Buller, Mary Klein and Buller, David B.},
  title = {Physicians' Communication Style and Patient Satisfaction},
  journal = {Journal of Health and Social Behavior},
  pages = {375--388},
  year = {1987}
}

@misc{Gilligan2016Interventions,
  author = {Gilligan, Conor and others},
  title = {Interventions for Improving Medical Students' Interpersonal Communication in Medical Consultations},
  year = {2016}
}

@article{Liu2016EQClinic,
  author = {Liu, Chen and Scott, Karen M. and Lim, Rachel L. and Taylor, Simon and Calvo, Rafael A.},
  title = {EQClinic: A Platform for Learning Communication Skills in Clinical Consultations},
  journal = {Medical Education Online},
  volume = {21},
  number = {1},
  pages = {31801},
  year = {2016}
}

@article{Liu2016WebBased,
  author = {Liu, Chen and Lim, Rachel L. and McCabe, Kathryn L. and Taylor, Simon and Calvo, Rafael A.},
  title = {A Web-Based Telehealth Training Platform Incorporating Automated Nonverbal Behavior Feedback for Teaching Communication Skills to Medical Students: A Randomized Crossover Study},
  journal = {Journal of Medical Internet Research},
  volume = {18},
  number = {9},
  pages = {1--9},
  year = {2016}
}

@misc{FFmpeg2019Complete,
  author = {FFmpeg},
  title = {A Complete, Cross-platform Solution to Record, Convert and Stream Audio and Video},
  year = {2019},
  note = {Online: \url{https://www.ffmpeg.org/}}
}

@article{Gaikwad2010Review,
  author = {Gaikwad, S. K. and Gawali, B. W. and Yannawar, P.},
  title = {A Review on Speech Recognition Technique},
  journal = {International Journal of Computer Applications},
  volume = {10},
  number = {3},
  pages = {16--24},
  year = {2010}
}
@inproceedings{Och2003Minimum,
  author = {Och, Franz Josef},
  title = {Minimum Error Rate Training in Statistical Machine Translation},
  booktitle = {Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1},
  year = {2003},
  pages = {160--167}
}

@misc{Belambert2018ASREvaluation,
  author = {Belambert},
  title = {{ASR-Evaluation}},
  year = {2018},
  url = {https://github.com/belambert/asr-evaluation}
}

@inproceedings{vanMiltenburg2018DIDEC,
  author = {van Miltenburg, Emiel and Kádar, Annemarie and Koolen, Ruud and Krahmer, Emiel},
  title = {{DIDEC: The Dutch Image Description and Eye-tracking Corpus}},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  year = {2018},
  pages = {3658--3669}
}

@article{Lippmann1997SpeechRecognition,
  author = {Lippmann, Richard P. and others},
  title = {Speech Recognition by Machines and Humans},
  journal = {Speech Communication},
  volume = {22},
  number = {1},
  pages = {1--15},
  year = {1997}
}

@misc{Roy2010Automatic,
  author = {Roy, B. C. and Vosoughi, S. and Roy, D. K.},
  title = {Automatic Estimation of Transcription Accuracy and Difficulty},
  year = {2010}
}

@inproceedings{Gillick1989Statistical,
  author = {Gillick, Lawrence and Cox, Stephen J.},
  title = {Some Statistical Issues in the Comparison of Speech Recognition Algorithms},
  booktitle = {Acoustics, Speech, and Signal Processing. ICASSP-89.},
  year = {1989},
  pages = {532--535}
}

@inproceedings{Siohan2005Constructing,
  author = {Siohan, Olivier and Ramabhadran, Bhuvana and Kingsbury, Brian},
  title = {Constructing Ensembles of ASR Systems Using Randomized Decision Trees},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year = {2005},
  volume = {1},
  pages = {I--197}
}

@book{James2014Introduction,
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  title = {An Introduction to Statistical Learning: With Applications in R},
  publisher = {Springer Publishing Company, Incorporated},
  year = {2014}
}

@inproceedings{Bisani2004Bootstrap,
  author = {Bisani, Maximilian and Ney, Hermann},
  title = {Bootstrap Estimates for Confidence Intervals in ASR Performance Evaluation},
  booktitle = {Acoustics, Speech, and Signal Processing, 2004. Proceedings. (ICASSP'04). IEEE International Conference on},
  year = {2004},
  volume = {1},
  pages = {I--409}
}

@inproceedings{Baltrusaitis2016OpenFace,
  author = {Baltrušaitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
  title = {{Openface: An Open Source Facial Behavior Analysis Toolkit}},
  booktitle = {Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
  year = {2016},
  pages = {1--10}
}

@article{Essa1997Coding,
  author = {Essa, Irfan A. and Pentland, Alex P.},
  title = {Coding, Analysis, Interpretation, and Recognition of Facial Expressions},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {19},
  number = {7},
  pages = {757--763},
  year = {1997}
}

@article{Schmidt2001Dynamics,
  author = {Schmidt, Karen L. and Cohn, Jeffrey F.},
  title = {Dynamics of Facial Expression: Normative Characteristics and Individual Differences},
  year = {2001},
  pages = {140}
}

@misc{Heller2001Faces,
  author = {Heller, Matthew and Haynal-Reymond, Viviane and Haynal, Alain and Archinard, Michel},
  title = {Can Faces Reveal Suicide Attempt Risks},
  year = {2001}
}

@inproceedings{Laksana2017Investigating,
  author = {Laksana, Eldwin and Baltrusaitis, Tadas and Morency, Louis-Philippe and Pestian, John P.},
  title = {Investigating Facial Behavior Indicators of Suicidal Ideation},
  booktitle = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  year = {2017},
  pages = {770--777}
}

@inproceedings{Niu2018Automatic,
  author = {Niu, Xuesong and others},
  title = {Automatic Engagement Prediction with GAP Feature},
  booktitle = {Proceedings of the 2018 on International Conference on Multimodal Interaction},
  year = {2018},
  pages = {599--603}
}
@inproceedings{Singhal2018Analyzing,
  author = {Singhal, Amit and Ali, Mohammad Rizwan and Baten, Md Rashedul and Kurumada, Chisato and Marvin, Elizabeth W. and Hoque, Mohammad Ehsan},
  title = {Analyzing the Impact of Gender on the Automation of Feedback for Public Speaking},
  booktitle = {Automatic Face \& Gesture Recognition (FG 2018), 2018 13th IEEE International Conference on},
  year = {2018},
  pages = {607--613}
}

@inproceedings{Kawato2000RealTime,
  author = {Kawato, S. and Ohya, J.},
  title = {Real-time Detection of Nodding and Head-shaking by Directly Detecting and Tracking the "Between-eyes"},
  booktitle = {Fourth IEEE International Conference on Automatic Face and Gesture Recognition},
  year = {2000},
  pages = {40--45}
}

@misc{Xiong2016Achieving,
  author = {Xiong, Wayne and others},
  title = {Achieving Human Parity in Conversational Speech Recognition},
  year = {2016},
  note = {arXiv Preprint arXiv:1610.05256}
}

@inproceedings{Deng2014Ensemble,
  author = {Deng, Li and Platt, John C.},
  title = {Ensemble Deep Learning for Speech Recognition},
  booktitle = {Fifteenth Annual Conference of the International Speech Communication Association},
  year = {2014}
}

@inproceedings{Evermann2000Posterior,
  author = {Evermann, G. and Woodland, P. C.},
  title = {Posterior Probability Decoding, Confidence Estimation and System Combination},
  booktitle = {Proc. Speech Transcription Workshop},
  year = {2000},
  volume = {27},
  pages = {78--81}
}

@article{Xu2011Minimum,
  author = {Xu, Hui and Povey, Daniel and Mangu, Lidia and Zhu, Jun},
  title = {Minimum Bayes Risk Decoding and System Combination Based on a Recursion for Edit Distance},
  journal = {Computer Speech \& Language},
  volume = {25},
  number = {4},
  pages = {802--828},
  year = {2011}
}

@inproceedings{Fiscus1997PostProcessing,
  author = {Fiscus, Jonathan G.},
  title = {A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER)},
  booktitle = {Automatic Speech Recognition and Understanding},
  year = {1997},
  pages = {347--354}
}

@article{Guntupalli2007Emotional,
  author = {Guntupalli, Vamsi Krishna and Erik Everhart, David and Kalinowski, Joseph and Nanjundeswaran, Chayadevie and Saltuklaroglu, Tim},
  title = {Emotional and Physiological Responses of Fluent Listeners While Watching the Speech of Adults Who Stutter},
  journal = {International Journal of Language \& Communication Disorders},
  volume = {42},
  number = {2},
  pages = {113--129},
  year = {2007}
}

@article{Hess1998Facial,
  author = {Hess, Ursula and Philippot, Pierre and Blairy, Sylvie},
  title = {Facial Reactions to Emotional Facial Expressions: Affect or Cognition?},
  journal = {Cognition and Emotion},
  volume = {12},
  number = {4},
  pages = {509--531},
  year = {1998}
}
@inproceedings{Drugman2012Audio,
  author = {Drugman, Thomas and Urbain, Jean and Bauwens, Nicolas and Chessini, Rafael and Aubriot, Anne-Sylvie and Lebecque, Pascal and Dutoit, Thierry},
  title = {Audio and Contact Microphones for Cough Detection},
  booktitle = {Thirteenth Annual Conference of the International Speech Communication Association},
  year = {2012}
}

@inproceedings{Huwel2020Hearing,
  author = {Huwel, Andreas and Adiloğlu, Kaya and Bach, Jan-Hendrik},
  title = {Hearing Aid Research Dataset for Acoustic Environment Recognition},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {706--710}
}

@article{Messner2020MultiChannel,
  author = {Messner, Eva and Fediuk, Marta and Swatek, Peter and Scheidl, Stefan and Smolle-J{\"u}ttner, Florian-Michael and Olschewski, Horst and Pernkopf, Franz},
  title = {Multi-channel Lung Sound Classification with Convolutional Recurrent Neural Networks},
  journal = {Computers in Biology and Medicine},
  pages = {103831},
  year = {2020}
}

@article{Bello2019SONYC,
  author = {Bello, Juan P. and Silva, Charles and Nov, Oded and DuBois, Ronan L. and Arora, Amit and Salamon, Justin and Mydlarz, Charlie and Doraiswamy, Harish},
  title = {SONYC: A System for Monitoring, Analyzing, and Mitigating Urban Noise Pollution},
  journal = {Communications of the ACM},
  volume = {62},
  number = {2},
  pages = {68--77},
  year = {2019}
}

@inproceedings{Cramer2020Chirping,
  author = {Cramer, Jake and Lostanlen, Vincent and Farnsworth, Andrew and Salamon, Justin and Bello, Juan P.},
  title = {Chirping Up the Right Tree: Incorporating Biological Taxonomies into Deep Bioacoustic Classifiers},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {901--905}
}

@article{Lostanlen2019Robust,
  author = {Lostanlen, Vincent and Salamon, Justin and Farnsworth, Andrew and Kelling, Steve and Bello, Juan P.},
  title = {Robust Sound Event Detection in Bioacoustic Sensor Networks},
  journal = {PloS One},
  volume = {14},
  number = {10},
  pages = {e0214168},
  year = {2019}
}

@article{Xu2017North,
  author = {Xu, Kun and Cai, Hongyu and Liu, Xiaotian and Gao, Zhong},
  title = {North Atlantic Right Whale Call Detection with Very Deep Convolutional Neural Networks},
  journal = {The Journal of the Acoustical Society of America},
  volume = {141},
  number = {5},
  pages = {3944--3945},
  year = {2017}
}

@inproceedings{Wang2016AudioBased,
  author = {Wang, Yu and Neves, Luís and Metze, Florian},
  title = {Audio-Based Multimedia Event Detection Using Deep Recurrent Neural Networks},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2016},
  pages = {2742--2746}
}

@inproceedings{Jansen2017LargeScale,
  author = {Jansen, Alex and Gemmeke, Jort F. and Ellis, Daniel P. W. and Liu, Xiaohui and Lawrence, William and Freedman, David},
  title = {Large-scale Audio Event Discovery in One Million YouTube Videos},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2017},
  pages = {786--790}
}

@article{Crocco2016AudioSurveillance,
  author = {Crocco, Marco and Cristani, Marco and Trucco, Andrea and Murino, Vittorio},
  title = {Audio Surveillance: A Systematic Review},
  journal = {ACM Computing Surveys (CSUR)},
  volume = {48},
  number = {4},
  pages = {1--46},
  year = {2016}
}

@article{SanchezHevia2017Maximum,
  author = {Sanchez-Hevia, H. A. and Ayllon, D. and Gil-Pita, R. and Rosa-Zurera, M.},
  title = {Maximum Likelihood Decision Fusion for Weapon Classification in Wireless Acoustic Sensor Networks},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {25},
  number = {6},
  pages = {1172--1182},
  year = {2017}
}

@inproceedings{Morrison2019OtoMechanic,
  author = {Morrison, Max and Pardo, Bryan},
  title = {OtoMechanic: Auditory Automobile Diagnostics via Query-by-Example},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  year = {2019},
  pages = {169--173}
}
@article{Mesaros2017Detection,
  author = {Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.},
  title = {Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  number = {2},
  pages = {379--393},
  year = {2017}
}

@article{Foggia2015Reliable,
  author = {Foggia, Pasquale and Petkov, Nicolai and Saggese, Alessia and Strisciuglio, Nicola and Vento, Mario},
  title = {Reliable Detection of Audio Events in Highly Noisy Environments},
  journal = {Pattern Recognition Letters},
  volume = {65},
  pages = {22--28},
  year = {2015}
}

@inproceedings{Mesaros2010Acoustic,
  author = {Mesaros, Annamaria and Heittola, Toni and Eronen, Antti and Virtanen, Tuomas},
  title = {Acoustic Event Detection in Real Life Recordings},
  booktitle = {2010 18th European Signal Processing Conference},
  year = {2010},
  pages = {1267--1271},
  organization = {IEEE}
}

@inproceedings{Cotton2011Spectral,
  author = {Cotton, Courtenay V. and Ellis, Daniel P. W.},
  title = {Spectral vs. Spectro-Temporal Features for Acoustic Event Detection},
  booktitle = {2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2011},
  pages = {69--72},
  organization = {IEEE}
}

@inproceedings{Cakir2015Polyphonic,
  author = {Cakir, Emre and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
  title = {Polyphonic Sound Event Detection Using Multi Label Deep Neural Networks},
  booktitle = {2015 International Joint Conference on Neural Networks (IJCNN)},
  year = {2015},
  pages = {1--7},
  organization = {IEEE}
}

@inproceedings{Piczak2015Environmental,
  author = {Piczak, Karol J.},
  title = {Environmental Sound Classification with Convolutional Neural Networks},
  booktitle = {2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)},
  year = {2015},
  pages = {1--6},
  organization = {IEEE}
}

@inproceedings{Parascandolo2016Recurrent,
  author = {Parascandolo, Giuseppe and Huttunen, Heikki and Virtanen, Tuomas},
  title = {Recurrent Neural Networks for Polyphonic Sound Event Detection in Real Life Recordings},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2016},
  pages = {6440--6444},
  organization = {IEEE}
}

@article{Cakir2017Convolutional,
  author = {Cakir, Emre and Parascandolo, Giuseppe and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
  title = {Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {25},
  number = {6},
  pages = {1291--1303},
  year = {2017}
}

@inproceedings{Foster2015CHiME,
  author = {Foster, Peter and Sigtia, Siddharth and Krstulovic, Sacha and Barker, Jon and Plumbley, Mark D.},
  title = {{CHiME}-home: A Dataset for Sound Source Recognition in a Domestic Environment},
  booktitle = {2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2015}
}

@inproceedings{Piczak2015ESC,
  author = {Piczak, Karol J.},
  title = {{ESC}: Dataset for Environmental Sound Classification},
  booktitle = {Proceedings of the ACM International Conference on Multimedia},
  year = {2015},
  pages = {1015--1018},
  organization = {ACM}
}

@inproceedings{Mesaros2016TUT,
  author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title = {{TUT} Database for Acoustic Scene Classification and Sound Event Detection},
  booktitle = {24th European Signal Processing Conference 2016 (EUSIPCO 2016)},
  year = {2016},
  address = {Budapest, Hungary}
}
@inproceedings{Sun2017Revisiting,
  author = {Sun, Chenxi and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year = {2017},
  pages = {843--852}
}

@article{Russakovsky2015ImageNet,
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  title = {ImageNet Large Scale Visual Recognition Challenge},
  journal = {International Journal of Computer Vision},
  volume = {115},
  number = {3},
  pages = {211--252},
  year = {2015}
}

@inproceedings{Gemmeke2017AudioSet,
  author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, David and Jansen, Aaron and Lawrence, Wesley and Moore, R. C. and Plakal, Manoj and Ritter, Matthew},
  title = {Audio Set: An Ontology and Human-Labeled Dataset for Audio Events},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2017},
  address = {New Orleans, LA}
}

@inproceedings{Cartwright2019SONYC,
  author = {Cartwright, Mark and Mendez, Alejandro E. M. and Cramer, Julian and Lostanlen, Vincent and Dove, Graham and Wu, Hsien-Chuan and Salamon, Justin and Nov, Oriol and Bello, Juan P.},
  title = {SONYC Urban Sound Tagging (SONYC-UST): A Multilabel Dataset from an Urban Acoustic Sensor Network},
  booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year = {2019},
  pages = {35--39}
}

@inproceedings{Cartwright2019SoundEvent,
  author = {Cartwright, Mark and Cramer, Julian and Mendez Mendez, Alejandro E. and Wang, Yin and Wu, Hsien-Chuan and Lostanlen, Vincent and Fuentes, Martín and Dove, Graham and Mydlarz, Christian and Salamon, Justin and Nov, Oriol and Bello, Juan P.},
  title = {Sound Event Detection in Domestic Environments with Weakly Labeled Data and Soundscape Synthesis},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  year = {2019},
  pages = {253--257}
}

@inproceedings{Adavanne2019MultiRoom,
  author = {Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
  title = {A Multi-Room Reverberant Dataset for Sound Event Localization and Detection},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  year = {2019},
  pages = {10--14}
}

@inproceedings{Gharib2019VOICe,
  author = {Gharib, Sahar and Drossos, Konstantinos and Fagerlund, Emil and Virtanen, Tuomas},
  title = {{VOICe}: A Sound Event Detection Dataset for Generalizable Domain Adaptation},
  booktitle = {arXiv preprint arXiv:1911.07098},
  year = {2019}
}

@inproceedings{Dekkers2017SINS,
  author = {Dekkers, Gert and Lauwereins, Sara and Thoen, Bart and Adhana, Manu W. and Brouckxon, Harold and van Waterschoot, Toon and Vanrumste, Bart and Verhelst, Marc and Karsmakers, Peter},
  title = {The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)},
  year = {2017},
  pages = {32--36}
}

@inproceedings{Fonseca2019Learning,
  author = {Fonseca, Emilia and Plakal, Manoj and Ellis, Daniel P. W. and Font, Fèlix and Favory, Xavier and Serra, Xavier},
  title = {Learning Sound Event Classifiers from Web Audio with Noisy Labels},
  booktitle = {2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2019}
}

@inproceedings{Fonseca2019AudioTagging,
  author = {Fonseca, Emilia and Plakal, Manoj and Font, Fèlix and Ellis, Daniel P. W. and Serra, Xavier},
  title = {Audio Tagging with Noisy Labels and Minimal Supervision},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  year = {2019},
  address = {NY, USA}
}

@inproceedings{Cartwright2020SONYC,
  author = {Cartwright, Mark and Cramer, Julian and Mendez Mendez, Alejandro E. and Wang, Yin and Wu, Hsien-Chuan and Lostanlen, Vincent and Fuentes, Martín and Dove, Graham and Mydlarz, Christian and Salamon, Justin and Nov, Oriol and Bello, Juan P.},
  title = {{SONYC-UST-V2}: An Urban Sound Tagging Dataset with Spatiotemporal Context},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)},
  year = {2020},
  address = {Tokyo, Japan},
  pages = {16--20}
}

@inproceedings{Lin2014Microsoft,
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollar, Piotr and Zitnick, C. Lawrence},
  title = {Microsoft {COCO}: Common Objects in Context},
  booktitle = {European Conference on Computer Vision},
  year = {2014},
  pages = {740--755},
  organization = {Springer}
}

@inproceedings{Sun2017Revisiting,
author = {Sun, Chenxi and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
year = {2017},
pages = {843--852}
}

@article{Russakovsky2015ImageNet,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
title = {ImageNet Large Scale Visual Recognition Challenge},
journal = {International Journal of Computer Vision},
volume = {115},
number = {3},
pages = {211--252},
year = {2015}
}

@inproceedings{Gemmeke2017AudioSet,
author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, David and Jansen, Aaron and Lawrence, Wesley and Moore, R. C. and Plakal, Manoj and Ritter, Matthew},
title = {Audio Set: An Ontology and Human-Labeled Dataset for Audio Events},
booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
year = {2017},
address = {New Orleans, LA}
}

@inproceedings{Cartwright2019SONYC,
author = {Cartwright, Mark and Mendez, Alejandro E. M. and Cramer, Julian and Lostanlen, Vincent and Dove, Graham and Wu, Hsien-Chuan and Salamon, Justin and Nov, Oriol and Bello, Juan P.},
title = {SONYC Urban Sound Tagging (SONYC-UST): A Multilabel Dataset from an Urban Acoustic Sensor Network},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
year = {2019},
pages = {35--39}
}

@inproceedings{Cartwright2019SoundEvent,
author = {Cartwright, Mark and Cramer, Julian and Mendez Mendez, Alejandro E. and Wang, Yin and Wu, Hsien-Chuan and Lostanlen, Vincent and Fuentes, Martín and Dove, Graham and Mydlarz, Christian and Salamon, Justin and Nov, Oriol and Bello, Juan P.},
title = {Sound Event Detection in Domestic Environments with Weakly Labeled Data and Soundscape Synthesis},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
year = {2019},
pages = {253--257}
}

@inproceedings{Adavanne2019MultiRoom,
author = {Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
title = {A Multi-Room Reverberant Dataset for Sound Event Localization and Detection},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
year = {2019},
pages = {10--14}
}

@inproceedings{Gharib2019VOICe,
author = {Gharib, Sahar and Drossos, Konstantinos and Fagerlund, Emil and Virtanen, Tuomas},
title = {{VOICe}: A Sound Event Detection Dataset for Generalizable Domain Adaptation},
booktitle = {arXiv preprint arXiv:1911.07098},
year = {2019}
}

@inproceedings{Dekkers2017SINS,
author = {Dekkers, Gert and Lauwereins, Sara and Thoen, Bart and Adhana, Manu W. and Brouckxon, Harold and van Waterschoot, Toon and Vanrumste, Bart and Verhelst, Marc and Karsmakers, Peter},
title = {The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)},
year = {2017},
pages = {32--36}
}

@inproceedings{Fonseca2019Learning,
author = {Fonseca, Emilia and Plakal, Manoj and Ellis, Daniel P. W. and Font, Fèlix and Favory, Xavier and Serra, Xavier},
title = {Learning Sound Event Classifiers from Web Audio with Noisy Labels},
booktitle = {2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
year = {2019}
}

@inproceedings{Fonseca2019AudioTagging,
author = {Fonseca, Emilia and Plakal, Manoj and Font, Fèlix and Ellis, Daniel P. W. and Serra, Xavier},
title = {Audio Tagging with Noisy Labels and Minimal Supervision},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
year = {2019},
address = {NY, USA}
}

@inproceedings{Cartwright2020SONYC,
author = {Cartwright, Mark and Cramer, Julian and Mendez Mendez, Alejandro E. and Wang, Yin and Wu, Hsien-Chuan and Lostanlen, Vincent and Fuentes, Martín and Dove, Graham and Mydlarz, Christian and Salamon, Justin and Nov, Oriol and Bello, Juan P.},
title = {{SONYC-UST-V2}: An Urban Sound Tagging Dataset with Spatiotemporal Context},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)},
year = {2020},
address = {Tokyo, Japan},
pages = {16--20}
}

@inproceedings{Lin2014Microsoft,
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollar, Piotr and Zitnick, C. Lawrence},
title = {Microsoft {COCO}: Common Objects in Context},
booktitle = {European Conference on Computer Vision},
year = {2014},
pages = {740--755},
organization = {Springer}
}

@article{Li2017WebVision,
  author = {Li, Weijian and Wang, Ligeng and Li, Weijie and Agustsson, Eirikur and Van Gool, Luc},
  title = {WebVision Database: Visual Learning and Understanding from Web Data},
  journal = {arXiv preprint arXiv:1708.02862},
  year = {2017}
}

@article{Kuznetsova2020OpenImages,
  author = {Kuznetsova, Alina and Rom, Hershel and Alldrin, Neil and Jansen, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Sven and Popov, Sergey and Malloci, Michele and Kolesnikov, Alexander and others},
  title = {The Open Images Dataset V4: Unified Image Classification, Object Detection, and Visual Relationship Detection at Scale},
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {7},
  pages = {1956--1981},
  year = {2020}
}
@article{McFee2018OpenSource,
  author = {McFee, Brian and Kim, Jong Wook and Cartwright, Mark and Salamon, Justin and Bittner, Rachel M. and Bello, Juan P.},
  title = {Open-Source Practices for Music Signal Processing Research: Recommendations for Transparent, Sustainable, and Reproducible Audio Research},
  journal = {IEEE Signal Processing Magazine},
  volume = {36},
  number = {1},
  pages = {128--137},
  year = {2018}
}

@inproceedings{Salamon2014UrbanSound,
  author = {Salamon, Justin and Jacoby, Chris and Bello, Juan P.},
  title = {A Dataset and Taxonomy for Urban Sound Research},
  booktitle = {Proceedings of the ACM International Conference on Multimedia},
  year = {2014},
  pages = {1041--1044},
  organization = {ACM}
}

@inproceedings{Hershey2021Benefit,
  author = {Hershey, Shawn and Ellis, Daniel P. W. and Fonseca, Emilia and Jansen, Aaron and Liu, Chieh-Chi and Moore, Ryan C. and Plakal, Manoj},
  title = {The Benefit of Temporally-Strong Labels in Audio Event Classification},
  booktitle = {2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2021}
}

@inproceedings{Chen2020VGGSound,
  author = {Chen, Hang and Xie, Weidi and Vedaldi, Andrea and Zisserman, Andrew},
  title = {VGGSound: A Large-Scale Audio-Visual Dataset},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {721--725},
  organization = {IEEE}
}

@article{Fonseca2020Addressing,
  author = {Fonseca, Emilia and Hershey, Shawn and Plakal, Manoj and Ellis, Daniel P. W. and Jansen, Aaron and Moore, Ryan C.},
  title = {Addressing Missing Labels in Large-Scale Sound Event Recognition Using a Teacher-Student Framework with Loss Masking},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  pages = {1235--1239},
  year = {2020}
}

@inproceedings{McFee2018Adaptive,
  author = {McFee, Brian and Salamon, Justin and Bello, Juan P.},
  title = {Adaptive Pooling Operators for Weakly Labeled Sound Event Detection},
  booktitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  number = {11},
  pages = {2180--2193},
  year = {2018}
}

@article{Barz2020Do,
  author = {Barz, Bogdan and Denzler, Joachim},
  title = {Do We Train on Test Data? Purging CIFAR of Near-Duplicates},
  journal = {Journal of Imaging},
  volume = {6},
  number = {6},
  pages = {41},
  year = {2020}
}

@techreport{Krizhevsky2009Learning,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
  title = {Learning Multiple Layers of Features from Tiny Images},
  institution = {University of Toronto},
  year = {2009},
  type = {Tech. Rep.}
}

@inproceedings{Recht2019Do,
  author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  title = {Do ImageNet Classifiers Generalize to ImageNet?},
  booktitle = {International Conference on Machine Learning},
  year = {2019},
  pages = {5389--5400},
  organization = {PMLR}
}

@article{Beyer2020Are,
  author = {Beyer, Lucas and Henaff, Mikael J. and Kolesnikov, Alexander and Zhai, Xiaohua and van den Oord, A{\"a}ron},
  title = {Are We Done with ImageNet?},
  journal = {arXiv preprint arXiv:2006.07159},
  year = {2020}
}

@inproceedings{Deng2009ImageNet,
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  title = {ImageNet: A Large-Scale Hierarchical Image Database},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2009},
  pages = {248--255},
  organization = {IEEE}
}

@inproceedings{Fonseca2021Unsupervised,
  author = {Fonseca, Emilia and Ortego, Daniel and McGuinness, Kevin and O'Connor, Noel E. and Serra, Xavier},
  title = {Unsupervised Contrastive Learning of Sound Event Representations},
  booktitle = {2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2021}
}

@inproceedings{Fonseca2021SelfSupervised,
  author = {Fonseca, Emilia and Jansen, Aaron and Ellis, Daniel P. W. and Wisdom, Scott and Tagliasacchi, Marco and Hershey, John R. and Plakal, Manoj and Hershey, Shawn and Moore, Ryan C. and Serra, Xavier},
  title = {Self-Supervised Learning from Automatically Separated Sound Scenes},
  booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2021}
}

@inproceedings{Jansen2018Unsupervised,
  author = {Jansen, Aaron and Plakal, Manoj and Pandya, Rohit and Ellis, Daniel P. W. and Hershey, Shawn and Liu, Ju-Chieh and Moore, Ryan C. and Saurous, R. A.},
  title = {Unsupervised Learning of Semantic Audio Representations},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2018},
  pages = {126--130},
  organization = {IEEE}
}
@article{Han2016SemiSupervised,
  author = {Han, Wei and Coutinho, Eduardo and Ruan, Haibin and Li, Haifeng and Schuller, Bj{\"o}rn and Yu, Xiaojie and Zhu, Xiaoxi},
  title = {Semi-Supervised Active Learning for Sound Classification in Hybrid Learning Environments},
  journal = {PloS One},
  volume = {11},
  number = {9},
  pages = {e0162075},
  year = {2016}
}

@inproceedings{Wang2019Active,
  author = {Wang, Yu and Mendez, Ana E. M. and Cartwright, Mark and Bello, Juan P.},
  title = {Active Learning for Efficient Audio Annotation and Classification with a Large Amount of Unlabeled Data},
  booktitle = {2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2019},
  pages = {880--884},
  organization = {IEEE}
}

@inproceedings{Shuyang2018Active,
  author = {Shuyang, Zhu and Heittola, Toni and Virtanen, Tuomas},
  title = {An Active Learning Method Using Clustering and Committee-Based Sample Selection for Sound Event Classification},
  booktitle = {2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)},
  year = {2018},
  pages = {116--120},
  organization = {IEEE}
}

@article{Shuyang2020Active,
  author = {Shuyang, Zhu and Heittola, Toni and Virtanen, Tuomas},
  title = {Active Learning for Sound Event Detection},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {28},
  pages = {2895--2905},
  year = {2020}
}

@inproceedings{Font2013Freesound,
  author = {Font, Frederic and Roma, Gerard and Serra, Xavier},
  title = {Freesound Technical Demo},
  booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
  year = {2013},
  pages = {411--412},
  organization = {ACM}
}

@inproceedings{Stowell2014Freefield1010,
  author = {Stowell, Dan and Plumbley, Mark},
  title = {An Open Dataset for Research on Audio Field Recording Archives: Freefield1010},
  booktitle = {Audio Engineering Society Conference: 53rd International Conference: Semantic Audio},
  year = {2014}
}

@inproceedings{Fonseca2018GeneralPurpose,
  author = {Fonseca, Emilia and Plakal, Manoj and Font, Frederic and Ellis, Daniel P. W. and Favory, Xavier and Pons, Jordi and Serra, Xavier},
  title = {General-Purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)},
  year = {2018}
}

@inproceedings{Fonseca2017Freesound,
  author = {Fonseca, Emilia and Pons, Jordi and Favory, Xavier and Font, Frederic and Bogdanov, Dmitry and Ferraro, Andres and Oramas, Sergio and Porter, Alastair and Serra, Xavier},
  title = {Freesound Datasets: A Platform for the Creation of Open Audio Datasets},
  booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference (ISMIR 2017)},
  year = {2017},
  pages = {486--493}
}

@article{Porter1980Algorithm,
  author = {Porter, Martin F. and others},
  title = {An Algorithm for Suffix Stripping},
  journal = {Program},
  volume = {14},
  number = {3},
  pages = {130--137},
  year = {1980}
}

@inproceedings{Serizel2020SoundEvent,
  author = {Serizel, Romain and Turpault, Nicolas and Shah, Ankit and Salamon, Justin},
  title = {Sound Event Detection in Synthetic Domestic Environments},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {86--90},
  organization = {IEEE}
}

@inproceedings{Hendrycks2021Natural,
  author = {Hendrycks, Dan and Zhao, Kevin and Basart, Steve and Steinhardt, Jacob and Song, Dawn},
  title = {Natural Adversarial Examples},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2021},
  pages = {15 262--15 271}
}

@inproceedings{Wisdom2021FUSS,
  author = {Wisdom, Scott and Erdogan, Hakan and Ellis, Daniel P. W. and Serizel, Romain and Turpault, Nicolas and Fonseca, Emilia and Salamon, Justin and Seetharaman, Prem and Hershey, John R.},
  title = {What's All the {FUSS} About Free Universal Sound Separation Data?},
  booktitle = {2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2021},
  pages = {186--190},
  organization = {IEEE}
}

@article{Cartwright2017SeeingSound,
  author = {Cartwright, Mark and Seals, Andrew and Salamon, Justin and Williams, Adam and Mikloska, Verena and MacConnell, Dougal and Law, Ed and Bello, Juan P. and Nov, Oded},
  title = {Seeing Sound: Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {1},
  number = {CSCW},
  pages = {1--21},
  year = {2017}
}

@article{EBU2014Loudness,
  author = {{EBU Recommendation R 128}},
  title = {Loudness Normalisation and Permitted Maximum Level of Audio Signals},
  organization = {European Broadcasting Union},
  year = {2014}
}
@inproceedings{Sabou2014Corpus,
  author = {Sabou, Marta and Bontcheva, Kalina and Derczynski, Leon and Scharl, Arno},
  title = {Corpus Annotation Through Crowdsourcing: Towards Best Practice Guidelines},
  booktitle = {LREC},
  year = {2014},
  pages = {859--866}
}

@inproceedings{Meire2019Impact,
  author = {Meire, Michiel and Karsmakers, Peter and Vuegen, Lukas},
  title = {The Impact of Missing Labels and Overlapping Sound Events on Multi-Label Multi-Instance Learning for Sound Event Classification},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  year = {2019},
  organization = {NY, USA}
}

@book{Virtanen2018Computational,
  author = {Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Daniel P. W.},
  title = {Computational Analysis of Sound Scenes and Events},
  year = {2018},
  publisher = {Springer}
}

@inproceedings{Whitman2001Artist,
  author = {Whitman, Brian and Flake, Gary and Lawrence, Steve},
  title = {Artist Detection in Music with Minnowmatch},
  booktitle = {Neural Networks for Signal Processing XI: Proceedings of the 2001 IEEE Signal Processing Society Workshop (IEEE Cat. No. 01TH8584)},
  year = {2001},
  pages = {559--568},
  organization = {IEEE}
}

@inproceedings{Mandel2005Song,
  author = {Mandel, Michael I. and Ellis, Daniel P. W.},
  title = {Song-Level Features and Support Vector Machines for Music Classification},
  booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR 2005)},
  year = {2005}
}

@article{Flexer2009Album,
  author = {Flexer, Arthur and Schnitzer, Dominik},
  title = {Album and Artist Effects for Audio Similarity at the Scale of the Web},
  journal = {Children},
  volume = {15},
  number = {15.95},
  pages = {4--07},
  year = {2009}
}

@inproceedings{Sechidis2011Stratification,
  author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  title = {On the Stratification of Multi-Label Data},
  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  year = {2011},
  pages = {145--158},
  organization = {Springer}
}

@book{Toth1990Knapsack,
  author = {Toth, Paolo and Martello, Silvano},
  title = {Knapsack Problems: Algorithms and Computer Implementations},
  year = {1990},
  publisher = {Wiley}
}

@inproceedings{Favory2018Facilitating,
  author = {Favory, Xavier and Fonseca, Emilia and Font, Frederic and Serra, Xavier},
  title = {Facilitating the Manual Annotation of Sounds when Using Large Taxonomies},
  booktitle = {Proceedings of the 23rd Conference of Open Innovations Association, FRUCT},
  year = {2018},
  pages = {447--451},
  organization = {FRUCT Oy}
}

@inproceedings{Cheng2019MultiLabel,
  author = {Cheng, Kuan-Hsien and Chou, Shih-Yi and Yang, Yi-Hsuan},
  title = {Multi-Label Few-Shot Learning for Sound Event Recognition},
  booktitle = {2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)},
  year = {2019},
  pages = {1--5},
  organization = {IEEE}
}

@inproceedings{Salamon2017Scaper,
  author = {Salamon, Justin and MacConnell, Dougal and Cartwright, Mark and Li, Peter and Bello, Juan P.},
  title = {Scaper: A Library for Soundscape Synthesis and Augmentation},
  booktitle = {2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2017},
  pages = {344--348},
  organization = {IEEE}
}

@article{Shah2018Closer,
  author = {Shah, Ankit and Kumar, Aman and Hauptmann, Alexander G. and Raj, Bhiksha},
  title = {A Closer Look at Weak Label Learning for Audio Events},
  journal = {CoRR},
  volume = {abs/1804.09288},
  year = {2018}
}

@inproceedings{Turpault2020Limitations,
  author = {Turpault, Nicolas and Serizel, Romain and Vincent, Emmanuel},
  title = {Limitations of Weak Labels for Embedding and Tagging},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {131--135},
  organization = {IEEE}
}

@inproceedings{Morfi2019DataEfficient,
  author = {Morfi, Vasiliki and Stowell, Dan},
  title = {Data-Efficient Weakly Supervised Learning for Low-Resource Audio Event Detection Using Deep Learning},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)}
}

@inproceedings{Lafay2017SoundEvent,
  author = {Lafay, Guillaume and Benetos, Emmanouil and Lagrange, Mathieu},
  title = {Sound Event Detection in Synthetic Audio: Analysis of the DCASE 2016 Task Results},
  booktitle = {2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2017},
  pages = {11--15},
  organization = {IEEE}
}
@inproceedings{Mesaros2017DCASE,
  author = {Mesaros, Annamaria and Heittola, Toni and Diment, Aleksandr and Elizalde, Benjamin and Shah, Ankit and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas},
  title = {DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)},
  year = {2017},
  pages = {85--92}
}

@article{Malfait2006ITU,
  author = {Malfait, Luc and Berger, Jens and Kastner, Michael},
  title = {{P.563}---The {ITU-T} Standard for Single-Ended Speech Quality Assessment},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {14},
  number = {6},
  pages = {1924--1934},
  year = {2006}
}

@article{Salamon2017Deep,
  author = {Salamon, Justin and Bello, Juan P.},
  title = {Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification},
  journal = {IEEE Signal Processing Letters},
  volume = {24},
  number = {3},
  pages = {279--283},
  year = {2017}
}

@article{Li2017LargeScale,
  author = {Li, Jianshu and Seltzer, Michael L. and Wang, Xiaodong and Zhao, Rui and Gong, Yushi},
  title = {Large-Scale Domain Adaptation via Teacher-Student Learning},
  journal = {arXiv preprint arXiv:1708.05466},
  year = {2017}
}

@article{Northcutt2021Confident,
  author = {Northcutt, Curtis and Jiang, Lucy and Chuang, Isaac},
  title = {Confident Learning: Estimating Uncertainty in Dataset Labels},
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {1373--1411},
  year = {2021}
}

@inproceedings{Lei2011User,
  author = {Lei, Hao and Choi, Jung and Janin, Adam and Friedland, Gerald},
  title = {User Verification: Matching the Uploaders of Videos Across Accounts},
  booktitle = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2011},
  pages = {2404--2407},
  organization = {IEEE}
}

@article{Fonseca2021Improving,
  author = {Fonseca, Emilia and Ferraro, Andres and Serra, Xavier},
  title = {Improving Sound Event Classification by Increasing Shift Invariance in Convolutional Neural Networks},
  journal = {arXiv preprint arXiv:2107.00623},
  year = {2021}
}

@inproceedings{Cakir2016Filterbank,
  author = {Cakir, Erdogan Can and Ozan, Eren C. and Virtanen, Tuomas},
  title = {Filterbank Learning for Deep Neural Network Based Polyphonic Sound Event Detection},
  booktitle = {2016 International Joint Conference on Neural Networks (IJCNN)},
  year = {2016},
  pages = {3399--3406},
  organization = {IEEE}
}

@article{Park2020CNNBased,
  author = {Park, Haeun and Yoo, Chang D.},
  title = {CNN-Based Learnable Gammatone Filterbank and Equal-Loudness Normalization for Environmental Sound Classification},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  pages = {411--415},
  year = {2020}
}

@inproceedings{Fonseca2019ModelAgnostic,
  author = {Fonseca, Emilia and Font, Frederic and Serra, Xavier},
  title = {Model-Agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers},
  booktitle = {2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2019},
  pages = {16--20},
  organization = {IEEE}
}

@inproceedings{Elizalde2019CrossModal,
  author = {Elizalde, Benjamin and Zarar, Said and Raj, Bhiksha},
  title = {Cross Modal Audio Search and Retrieval with Joint Embeddings Based on Text and Audio},
  booktitle = {2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2019},
  pages = {4095--4099},
  organization = {IEEE}
}

@inproceedings{Favory2020COALA,
  author = {Favory, Xavier and Drossos, Konstantinos and Virtanen, Tuomas and Serra, Xavier},
  title = {COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio Representations},
  booktitle = {Workshop on Self-supervision in Audio and Speech at the 37th International Conference on Machine Learning},
  year = {2020}
}

@inproceedings{Shrivaslava2020MtGCN,
  author = {Shrivaslava, Himanshu and Yin, Yinchong and Shah, Rishabh R. and Zimmermann, Roger},
  title = {Mt-Gcn for Multi-Label Audio Tagging with Noisy Labels},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {136--140},
  organization = {IEEE}
}

@inproceedings{Kumar2017Audio,
author = {Kumar, Anurag and Raj, Bhiksha},
title = {Audio Event and Scene Recognition: A Unified Approach Using Strongly and Weakly Labeled Data},
booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
year = {2017},
pages = {3475--3482},
organization = {IEEE}
}
@inproceedings{Gharib2018Unsupervised,
  author = {Gharib, Shayan and Drossos, Konstantinos and Cakir, Erdogan and Serdyuk, Dmitriy and Virtanen, Tuomas},
  title = {Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)},
  year = {2018},
  pages = {138--142}
}

@inproceedings{Bogdanov2016CrossCollection,
  author = {Bogdanov, Dmitry and Porter, Alastair and Boyer, Herrera and Serra, Xavier and others},
  title = {Cross-Collection Evaluation for Music Classification Tasks},
  booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
  year = {2016}
}

@inproceedings{Favory2020Search,
  author = {Favory, Xavier and Font, Frederic and Serra, Xavier},
  title = {Search Result Clustering in Collaborative Sound Collections},
  booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
  year = {2020},
  pages = {207--214}
}

@inproceedings{Kavalerov2019Universal,
  author = {Kavalerov, Ilya and Wisdom, Scott and Erdogan, Hakan and Patton, Benjamin and Wilson, Kevin and Le Roux, Jonathan and Hershey, John R.},
  title = {Universal Sound Separation},
  booktitle = {2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2019},
  pages = {175--179}
}

@article{Abesser2021USMSED,
  author = {Abeßer, Jan},
  title = {{USM-SED}: A Dataset for Polyphonic Sound Event Detection in Urban Sound Monitoring Scenarios},
  journal = {arXiv preprint arXiv:2105.02592},
  year = {2021}
}

@inproceedings{Turpault2020Improving,
  author = {Turpault, Nicolas and Wisdom, Scott and Erdogan, Hakan and Hershey, John R. and Serizel, Romain and Fonseca, Emilia and Seetharaman, Prem and Salamon, Justin},
  title = {Improving Sound Event Detection in Domestic Environments Using Sound Separation},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)},
  year = {2020},
  pages = {205--209}
}

@inbook{VanLeeuwen2007Introduction,
  author = {Van Leeuwen, David A. and Brummer, Niko},
  title = {An Introduction to Application-Independent Evaluation of Speaker Recognition Systems},
  booktitle = {Speaker Classification I},
  publisher = {Springer},
  year = {2007},
  pages = {330--353}
}

@inproceedings{Bilen2020Framework,
  author = {Bilen, Çagdas and Ferroni, Gabriele and Tuveri, Fabio and Azcarreta, Josu and Krstulović, Sacha},
  title = {A Framework for the Robust Evaluation of Sound Event Detection},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {61--65}
}

@inproceedings{Hershey2017CNNArchitectures,
  author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Adam and Moore, R. C. and Plakal, Manoj and Platt, Dan and Saurous, R. A. and Seybold, Brian and others},
  title = {CNN Architectures for Large-Scale Audio Classification},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2017},
  pages = {131--135}
}

@misc{ScikitLearnPrecisionRecall,
  title = {Precision-recall},
  howpublished = {Scikit-learn documentation 0.23.2},
  note = {\url{https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html}}
}

@misc{ScikitLearnMetricsScoring,
  title = {Metrics and scoring: Quantifying the quality of predictions},
  howpublished = {Scikit-learn documentation 0.23.2},
  note = {\url{https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics}}
}

@misc{VLFeatPlots,
  title = {Plotting AP and ROC curves},
  howpublished = {VLFeat.org},
  note = {\url{https://www.vlfeat.org/overview/plots-rank.html}}
}

@book{Green1966Signal,
  author = {Green, David M. and Swets, John A.},
  title = {Signal Detection Theory and Psychophysics},
  volume = {1},
  publisher = {Wiley New York},
  year = {1966}
}

@misc{TensorFlow,
  author = {Abadi, Martín and et al.},
  title = {TensorFlow: Large-scale machine learning on heterogeneous systems},
  year = {2015},
  howpublished = {\url{https://www.tensorflow.org/}}
}

@article{Kingma2015Adam,
  author = {Kingma, Diederik P. and Ba, Jimmy},
  title = {Adam: A Method for Stochastic Optimization},
  journal = {3rd International Conference on Learning Representations},
  year = {2015}
}
@inproceedings{Davis2006Relationship,
  author = {Davis, Jesse and Goadrich, Mark},
  title = {The Relationship Between Precision-Recall and {ROC} Curves},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  year = {2006},
  pages = {233--240}
}

@article{Kong2020PANNs,
  author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.},
  title = {{PANNs}: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {28},
  pages = {2880--2894},
  year = {2020}
}

@inproceedings{Fonseca2017Acoustic,
  author = {Fonseca, Emilia and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier},
  title = {Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)},
  year = {2017},
  pages = {37--41}
}

@inproceedings{Fonseca2018Simple,
  author = {Fonseca, Emilia and Gong, Rong and Serra, Xavier},
  title = {A Simple Fusion of Deep and Shallow Learning for Acoustic Scene Classification},
  booktitle = {Proceedings of the 15th Sound and Music Computing Conference},
  year = {2018},
  address = {Limassol, Cyprus}
}

@inproceedings{Perez-Lopez2019Hybrid,
  author = {Perez-Lopez, Alejandro and Fonseca, Emilia and Serra, Xavier},
  title = {A Hybrid Parametric-Deep Learning Approach for Sound Event Localization and Detection},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  year = {2019},
  address = {New York University, NY, USA},
  pages = {189--193}
}

@inproceedings{Ebbers2019Convolutional,
  author = {Ebbers, Jan and Hab-Umbach, Reinhold},
  title = {Convolutional Recurrent Neural Network and Data Augmentation for Audio Tagging with Noisy Labels and Minimal Supervision},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
  year = {2019},
  address = {New York University, NY, USA},
  pages = {64--68}
}

@article{Ioffe2015Batch,
  author = {Ioffe, Sergey and Szegedy, Christian},
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  journal = {Proceedings of the 32nd International Conference on Machine Learning},
  year = {2015},
  pages = {448--456}
}

@inproceedings{Dorfer2018Training,
  author = {Dorfer, Matthias and Widmer, Gerhard},
  title = {Training General-Purpose Audio Tagging Networks with Noisy Labels and Iterative Self-Verification},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)},
  year = {2018},
  pages = {178--182}
}

@inproceedings{Kim2019Sound,
  author = {Kim, Byeongchang and Pardo, Bryan},
  title = {Sound Event Detection Using Point-Labeled Data},
  booktitle = {2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year = {2019}
}
@inproceedings{Simonyan2015Very,
  author = {Simonyan, Karen and Zisserman, Andrew},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {3rd International Conference on Learning Representations (ICLR)},
  year = {2015}
}

@inproceedings{He2016Deep,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2016},
  pages = {770--778}
}

@inproceedings{Huang2017Densely,
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  title = {Densely Connected Convolutional Networks},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2017},
  pages = {4700--4708}
}

@inproceedings{Iqbal2020Learning,
  author = {Iqbal, Turab and Cao, Yin and Kong, Qiuqiang and Plumbley, Mark D. and Wang, Wenwu},
  title = {Learning with Out-of-Distribution Data for Audio Classification},
  booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2020},
  pages = {636--640}
}

@inproceedings{Won2020Evaluation,
  author = {Won, Minz and Ferraro, Andrés and Bogdanov, Dmitry and Serra, Xavier},
  title = {Evaluation of CNN-based Automatic Music Tagging Models},
  booktitle = {Proceedings of the 17th Sound and Music Computing Conference},
  year = {2020}
}

@inproceedings{Ford2019Deep,
  author = {Ford, L. and Tang, H. and Grondin, F. and Glass, J.},
  title = {A Deep Residual Network for Large-Scale Acoustic Scene Analysis},
  booktitle = {Proceedings of Interspeech 2019},
  year = {2019},
  pages = {2568--2572}
}

@inproceedings{Sainath2015Convolutional,
  author = {Sainath, Tara N. and Vinyals, Oriol and Senior, Andrew and Sak, Haşim},
  title = {Convolutional, Long Short-Term Memory, Fully Connected Deep Neural Networks},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2015},
  pages = {4580--4584}
}

@article{Liu2002On,
  author = {Liu, Huan and Motoda, Hiroshi},
  title = {On Issues of Instance Selection},
  journal = {Data Mining and Knowledge Discovery},
  volume = {6},
  number = {2},
  pages = {115},
  year = {2002}
}

@article{Jantzen2011Go,
  author = {Jantzen, S. G. and Sutherland, B. J. and Minkley, D. R. and Koop, B. F.},
  title = {Go Trimming: Systematically Reducing Redundancy in Large Gene Ontology Datasets},
  journal = {BMC Research Notes},
  volume = {4},
  number = {1},
  pages = {267},
  year = {2011}
}

@inproceedings{Mesaros2018MultiDevice,
  author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title = {A Multi-Device Dataset for Urban Acoustic Scene Classification},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)},
  year = {2018},
  pages = {9--13}
}

@article{Gao2017Knowledge,
  author = {Gao, Jia and Li, Zhiqiang and Nevatia, Ramakant and et al.},
  title = {Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN},
  journal = {arXiv preprint arXiv:1711.07607},
  year = {2017}
}

@inproceedings{Jati2019HierarchyAware,
  author = {Jati, A. and Kumar, N. and Chen, R. and Georgiou, P.},
  title = {Hierarchy-Aware Loss Function on a Tree Structured Label Space for Audio Event Detection},
  booktitle = {2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2019},
  pages = {6--10}
}
@inproceedings{Bellegarda1997Statistical,
  author = {Bellegarda, Jerome R.},
  title = {Statistical Techniques for Robust ASR: Review and Perspectives},
  booktitle = {Proceedings of European Conference on Speech and Communication and Technology '97},
  year = {1997},
  pages = {33--36}
}

@inproceedings{Cooke1997Missing,
  author = {Cooke, Martin and Morris, Andrew and Green, Phil},
  title = {Missing Data Techniques for Robust Speech Recognition},
  booktitle = {Proceedings of International Conference on Acoustics, Speech and Signal Processing '97},
  year = {1997},
  pages = {803--806}
}

@inproceedings{deVeth1999Acoustic,
  author = {de Veth, Jan and Cranen, Bert and de Wet, Frits and Boves, Lou},
  title = {Acoustic Pre-processing for Optimal Effectivity of Missing Feature Theory},
  booktitle = {Proceedings of European Conference on Speech Communication and Technology '99},
  year = {1999},
  pages = {65--68}
}

@inproceedings{Drygajlo1998Speaker,
  author = {Drygajlo, Andrzej and El-Maliki, Mohamed},
  title = {Speaker Verification in Noisy Environment with Combined Spectral Subtraction and Missing Data Theory},
  booktitle = {Proceedings of International Conference on Acoustics, Speech and Signal Processing '98},
  year = {1998},
  pages = {121--124}
}

@article{Gales1996Mean,
  author = {Gales, Mark J. F. and Woodland, Philip C.},
  title = {Mean and Variance Adaptation within the MLLR Framework},
  journal = {Computer Speech and Language},
  volume = {10},
  pages = {249--264},
  year = {1996}
}

@inproceedings{Gales1992Improved,
  author = {Gales, Mark J. F. and Young, Steve J.},
  title = {An Improved Approach to the Hidden Markov Model Decomposition of Speech and Noise},
  booktitle = {Proceedings of International Conference on Acoustics, Speech and Signal Processing '92},
  year = {1992},
  pages = {233--236}
}

@article{Gong1995Speech,
  author = {Gong, Yifan},
  title = {Speech Recognition in Noisy Environments: A Survey},
  journal = {Speech Communication},
  volume = {16},
  pages = {261--291},
  year = {1995}
}
@article{Harris1966Theory,
  author = {Harris, Brian},
  title = {Theory of Probability},
  publisher = {Addison-Wesley},
  year = {1966}
}

@article{Hermansky1994RASTA,
  author = {Hermansky, Hynek and Morgan, Nelson},
  title = {RASTA Processing of Speech},
  journal = {IEEE Transactions on Speech and Audio Processing},
  volume = {2},
  pages = {578--589},
  year = {1994}
}

@article{Juang1991Speech,
  author = {Juang, Biing-Hwang},
  title = {Speech Recognition in Adverse Environments},
  journal = {Computer Speech and Language},
  volume = {5},
  pages = {275--294},
  year = {1991}
}

@book{Junqua1996Robustness,
  author = {Junqua, Jean Claude and Haton, Jean-Paul},
  title = {Robustness in Automatic Speech Recognition: Fundamentals and Applications},
  publisher = {Kluwer},
  address = {Boston, MA},
  year = {1996}
}

@inproceedings{Leonard1984Database,
  author = {Leonard, Robert G.},
  title = {A Database for Speaker-Independent Digit Recognition},
  booktitle = {Proceedings of International Conference on Acoustics, Speech and Signal Processing '84},
  year = {1984},
  pages = {42.11/1--4}
}

@inproceedings{Lippmann1997Using,
  author = {Lippmann, Richard P. and Carlson, Barry A.},
  title = {Using Missing Feature Theory to Actively Select Features for Robust Speech Recognition with Interruptions, Filtering, and Noise},
  booktitle = {Proceedings of European Conference on Speech and Communication and Technology '97},
  year = {1997},
  pages = {37--40}
}

@inproceedings{Ming1999Union,
  author = {Ming, Jie and Smith, Frederick J.},
  title = {Union: A New Approach for Combining Sub-band Observations for Noisy Speech Recognition},
  booktitle = {Proceedings of Workshop on Robust Methods for Speech Recognition in Adverse Conditions},
  year = {1999},
  pages = {175--178}
}

@inproceedings{Ming2000Probabilistic,
  author = {Ming, Jie and Smith, Frederick J.},
  title = {A Probabilistic Union Model for Sub-band-based Robust Speech Recognition},
  booktitle = {Proceedings of International Conference on Acoustics, Speech and Signal Processing 2000},
  year = {2000},
  pages = {1787--1790}
}

@inproceedings{Ming1999Probabilistic,
  author = {Ming, Jie and Stewart, Dennis and Hanna, Philip and Smith, Frederick J.},
  title = {A Probabilistic Union Model for Partial and Temporal Corruption of Speech},
  booktitle = {Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding},
  year = {1999},
  pages = {43--46}
}

@article{Rabiner1989Highperformance,
  author = {Rabiner, Lawrence R. and Wilpon, Jay G. and Soong, Frank K.},
  title = {High-Performance Connected Digit Recognition Using Hidden Markov Models},
  journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
  volume = {37},
  pages = {1214--1225},
  year = {1989}
}

@article{Rahim1996Signalbias,
  author = {Rahim, Muhammad and Juang, Biing-Hwang},
  title = {Signal Bias Removal by Maximum Likelihood Estimation for Robust Telephone Speech Recognition},
  journal = {IEEE Transactions on Speech and Audio Processing},
  volume = {4},
  pages = {19--30},
  year = {1996}
}

@inproceedings{Seltzer2000Classifier,
  author = {Seltzer, Mark L. and Raj, Bhiksha and Stern, Richard M.},
  title = {Classifier-based Mask Estimate for Missing Feature Method of Robust Speech Recognition},
  booktitle = {Proceedings of International Conference on Spoken Language Processing 2000},
  year = {2000},
  address = {Beijing}
}

@inproceedings{Varga1990HiddenMarkov,
  author = {Varga, Alex and Moore, Richard K.},
  title = {Hidden Markov Model Decomposition of Speech and Noise},
  booktitle = {Proceedings of International Conference on Acoustics, Speech and Signal Processing '90},
  year = {1990},
  pages = {845--848}
}

@article{Vaseghi1997Noise,
  author = {Vaseghi, Saeed and Milner, Ben},
  title = {Noise Compensation Methods for Hidden Markov Model Speech Recognition in Adverse Environments},
  journal = {IEEE Transactions on Speech and Audio Processing},
  volume = {5},
  pages = {11--21},
  year = {1997}
}

@inproceedings{Vizinho1999Missingdata,
  author = {Vizinho, Ana and Green, Phil and Cooke, Martin and Josifovski, Ljubomir},
  title = {Missing Data Theory, Spectral Subtraction, and Signal-to-Noise Estimation for Robust ASR: An Integrated Study},
  booktitle = {Proceedings of European Conference on Speech Communication and Technology '99},
  year = {1999},
  pages = {2407--2410}
}
@article{baltrusaitis2018openface,
  title={Openface 2.0: Facial behavior analysis toolkit},
  author={Baltrusaitis, Tadas and Zadeh, Amir and Lim, Yao Chong and Morency, Louis-Philippe},
  journal={IEEE International Conference on Automatic Face \& Gesture Recognition},
  year={2018},
  organization={IEEE},
  pages={59--66}
}

@article{ekman1978facial,
  title={Facial action coding system},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Consulting Psychologists Press},
  year={1978},
  volume={2002},
  pages={2003}
}

@article{essa1997coding,
  title={Coding, analysis, interpretation, and recognition of facial expressions},
  author={Essa, Irfan A and Pentland, Alex P},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={19},
  number={7},
  pages={757--763},
  year={1997},
  publisher={IEEE}
}

@article{schmidt2001dynamics,
  title={Dynamics of facial expression: Normative characteristics and individual differences},
  author={Schmidt, Karen L and Cohn, Jeffrey F},
  journal={Journal of personality and social psychology},
  volume={80},
  number={3},
  pages={533},
  year={2001},
  publisher={American Psychological Association}
}

@article{heller2001can,
  title={Can faces reveal suicide attempt risks},
  author={Heller, Michelle and Haynal-Reymond, V{\'e}ronique and Haynal, Andr{\'e} and Archinard, Michel},
  journal={Journal of Affective Disorders},
  volume={66},
  number={2-3},
  pages={199--209},
  year={2001},
  publisher={Elsevier}
}

@inproceedings{laksana2017investigating,
  title={Investigating facial behavior indicators of suicidal ideation},
  author={Laksana, Erwin and Baltrusaitis, Tadas and Morency, Louis-Philippe and Pestian, John P},
  booktitle={2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages={770--777},
  year={2017},
  organization={IEEE}
}

@article{lippmann1997speech,
  title={Speech recognition by machines and humans},
  author={Lippmann, Richard P and others},
  journal={Speech Communication},
  volume={22},
  number={1},
  pages={1--15},
  year={1997},
  publisher={Elsevier}
}

@article{guntupalli2007emotional,
  title={Emotional and physiological responses of fluent listeners while watching the speech of adults who stutter},
  author={Guntupalli, Vijaya K and Erik Everhart, David and Kalinowski, Joseph and Nanjundeswaran, Chandra and Saltuklaroglu, Tim},
  journal={International Journal of Language \& Communication Disorders},
  volume={42},
  number={2},
  pages={113--129},
  year={2007},
  publisher={Wiley Online Library}
}

@article{hess1998facial,
  title={Facial reactions to emotional facial expressions: Affect or cognition?},
  author={Hess, Ursula and Philippot, Pierre and Blairy, Sylvie},
  journal={Cognition \& Emotion},
  volume={12},
  number={4},
  pages={509--531},
  year={1998},
  publisher={Taylor \& Francis}
}

@inproceedings{niu2018automatic,
  title={Automatic Engagement Prediction with GAP Feature},
  author={Niu, Xuejiao and others},
  booktitle={Proceedings of the 2018 on International Conference on Multimodal Interaction},
  pages={599--603},
  year={2018},
  organization={ACM}
}

@inproceedings{singhal2018analyzing,
  title={Analyzing the Impact of Gender on the Automation of Feedback for Public Speaking},
  author={Singhal, Aman and Ali, Mohammad R and Baten, Rifat A and Kurumada, Charles and Marvin, Ethan W and Hoque, Mainul E},
  booktitle={Automatic Face \& Gesture Recognition (FG 2018), 2018 13th IEEE International Conference on},
  pages={607--613},
  year={2018},
  organization={IEEE}
}

@inproceedings{kawato2000real,
  title={Real-time detection of nodding and head-shaking by directly detecting and tracking the "between-eyes"},
  author={Kawato, Shoji and Ohya, Jun},
  booktitle={Fourth IEEE International Conference on Automatic Face and Gesture Recognition},
  pages={40--45},
  year={2000},
  organization={IEEE}
}
@inproceedings{johnson1999spoken,
  title={Spoken document retrieval for TREC-8 at Cambridge University},
  author={Johnson, S. and Jourlin, P. and Jones, K. S. and Woodland, P.},
  booktitle={Proc. of the 8th Text REtrieval Conference},
  year={1999},
  address={Gaithersburg, MD}
}

@inproceedings{johnson1999cambridge,
  title={The Cambridge University Spoken Document Retrieval System},
  author={Johnson, S. E. and Jourlin, P. and Moore, G. L. and Spärck Jones, K. and Woodland, P. C.},
  booktitle={Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing},
  pages={49--52},
  year={1999}
}

@techreport{papineni2001bleu,
  title={Bleu: A Method for Automatic Evaluation of Machine Translation},
  author={Papineni, K. and others},
  institution={IBM},
  type={Research Report},
  number={RC22176},
  month={September},
  year={2001}
}

@techreport{liu2003noise,
  title={Noise Robustness In In Speech To Speech Translation},
  author={Liu, F-H. and Gao, Y. and Gu, L. and Picheny, M.},
  institution={IBM},
  type={Research Report},
  number={RC22874},
  year={2003}
}

@misc{nist-tools-spqa,
  title={NIST Speech Tools},
  author={{National Institute of Standards and Technology}},
  howpublished={\url{http://www.nist.gov/speech/tools/index.htm}},
  note={spqa\_2.3+sphere\_2.5.tar.Z}
}
@inproceedings{kornblith2019better,
  title={Do better ImageNet models transfer better?},
  author={Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2019},
  doi={10.48550/arXiv.1805.08974}
}
@inproceedings{khan2007cryptanalysis,
  title={Cryptanalysis of Keystream Reuse in Stream Ciphered Digitized Speech using HMM Based ASR Techniques},
  author={Khan, L. A. and Baig, M. S.},
  booktitle={Proceedings International Conf on Computer Science and Applications},
  year={2007},
  address={San Francisco, USA},
  month={October}
}
@inproceedings{khan2007cryptanalysis,
  title={Cryptanalysis of Keystream Reuse in Stream Ciphered Digitized Speech using HMM Based ASR Techniques},
  author={Khan, L. A. and Baig, M. S.},
  booktitle={Proceedings International Conf on Computer Science and Applications},
  year={2007},
  address={San Francisco, USA},
  month={October}
}

@techreport{zaykovskiy2007survey,
  title={Survey of The Speech Recognition Techniques For Mobile Devices},
  author={Zaykovskiy, Dimitry},
  institution={TechRepublic White Paper},
  year={2007},
  url={http://whitepapers.techrepublic.com.com/whitepaper.aspx?docid=174672}
}
@techreport{zaykovskiy2007survey,
  title={Survey of The Speech Recognition Techniques For Mobile Devices},
  author={Zaykovskiy, Dimitry},
  institution={TechRepublic White Paper},
  year={2007},
  url={http://whitepapers.techrepublic.com.com/whitepaper.aspx?docid=174672}
}
@techreport{zaykovskiy2007survey,
  title={Survey of The Speech Recognition Techniques For Mobile Devices},
  author={Zaykovskiy, Dimitry},
  institution={TechRepublic White Paper},
  year={2007},
  url={http://whitepapers.techrepublic.com.com/whitepaper.aspx?docid=174672}
}

@phdthesis{heurta2000speech,
  title={Speech Recognition in Mobile Environments},
  author={Heurta, J. M.},
  school={Carnegie Mellon University},
  year={2000},
  month={April}
}

@inproceedings{raj2001distributed,
  title={Distributed Speech Recognition with Codec Parameters},
  author={Raj, B. and Migdal, J. and Singh, R.},
  booktitle={Proceedings ASRU'2001},
  year={2001},
  month={December}
}

@article{pelaez2001recognizing,
  title={Recognizing Voice over IP: A Robust Front-End For Speech Recognition on The World Wide Web},
  author={Pelaez, C. and Gallardo-Antolin, A. and Diaz-de-Maria, F.},
  journal={IEEE Tran. on Multimedia},
  volume={3},
  number={2},
  year={2001}
}

@techreport{3gpp2004recognition,
  title={Recognition Performance Evaluations Of Codecs For Speech Enabled Services (SES)},
  institution={3GPP TR 26.943},
  year={2004},
  month={December}
}

@inproceedings{khan2007cryptanalysis,
  title={Cryptanalysis of Keystream Reuse in Stream Ciphered Digitized Speech using HMM Based ASR Techniques},
  author={Khan, L. A. and Baig, M. S.},
  booktitle={Proceedings International Conf on Computer Science and Applications},
  year={2007},
  month={October},
  address={San Francisco, USA}
}
@phdthesis{heurta2000speech,
  title={Speech Recognition in Mobile Environments},
  author={Heurta, J. M.},
  school={Carnegie Mellon University},
  year={2000},
  month={April}
}

@inproceedings{raj2001distributed,
  title={Distributed Speech Recognition with Codec Parameters},
  author={Raj, B. and Migdal, J. and Singh, R.},
  booktitle={Proceedings ASRU'2001},
  year={2001},
  month={December}
}

@article{pelaez2001recognizing,
  title={Recognizing Voice over IP: A Robust Front-End For Speech Recognition on The World Wide Web},
  author={Pelaez, C. and Gallardo-Antolin, A. and Diaz-de-Maria, F.},
  journal={IEEE Tran. on Multimedia},
  volume={3},
  number={2},
  year={2001}
}
@book{young2003htk,
  title={The HTK Book},
  author={Young, S. J. and Evermann, G. and Hain, T. and Kershaw, D. and Moore, G. L. and Odell, J. J. and Ollason, D. and Povey, D. and Valtchev, V. and Woodland, P. C.},
  year={2003},
  publisher={Cambridge University},
  address={Cambridge}
}

@inproceedings{godfrey1992switchboard,
  title={SWITCHBOARD: Telephone Speech Corpus for Research and Development},
  author={Godfrey, J. J. and Holliman, E. C. and McDaniel, J.},
  booktitle={Proceedings of ICASSP},
  year={1992},
  address={San Francisco}
}
@inproceedings{openshaw1994limitations,
  title={On the limitations of cepstral features in noise},
  author={Openshaw, J.P. and Mason, J.S.},
  booktitle={Proc. IEEE 1994 Conf. Acoust. Speech Signal Process},
  year={1994}
}
@article{Bocchieri1992DiscriminativeAF,
  title={Discriminative analysis for feature reduction in automatic speech recognition},
  author={Enrico Bocchieri and Jay G. Wilpon},
  journal={[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year={1992},
  volume={1},
  pages={501-504 vol.1}
}
@inproceedings{hermansky1992towards,
  title={Towards handling the acoustic environment in spoken language processing},
  author={Hermansky, Hynek and Morgan, Nelson},
  booktitle={ICSLP-1992},
  year={1992},
  pages={85--88},
  organization={Second International Conference on Spoken Language Processing (ICSLP'92)},
  address={Banff, Alberta, Canada},
  month={October},
  note={},
  url={}
}
@inproceedings{openshaw1993comparison,
author = {Openshaw, J. P. and Sun, Z. P. and Mason, J. S.},
title = {A Comparison of Composite Features under Degraded Speech in Speaker Recognition},
year = {1993},
isbn = {0780309464},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper assesses a variety of features and their sensitivity to noise mis-match between the model and test noise conditions. We use speaker identification (SI) for a performance evaluation as this is very sensitive to feature changes, and propose a target for robustness in terms of matched noise conditions. Two primary features are considered MFCC and PLP, along with their RASTA and first order regression extensions. We find PLP-RASTA to give the best resilience under cross conditions for a single feature, and the LDA combination of MFCC and PLP-RASTA supplying the best performance overall. Only in combined training do we find satisfactory results for any feature.},
booktitle = {Proceedings of the 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing: Speech Processing - Volume II},
pages = {371–374},
numpages = {4},
location = {Minneapolis, Minnesota, USA},
series = {ICASSP'93}
}

@article{Barbier1991RobustSP,
  title={Robust speech parameters extraction for word recognition in noise using neural networks},
  author={Laurent Barbier and G{\'e}rard Chollet},
  journal={[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing},
  year={1991},
  pages={145-148 vol.1}
}

@misc{charniak2000bllip,
  title={Bllip 1987-89 wsj corpus release 1},
  author={Charniak, Eugene and Blaheta, David and Ge, Norman and Hall, Keith and Hale, John and Johnson, Mark},
  year={2000},
  howpublished={Philadelphia: Linguistic Data Consortium},
  note={Technical Report LDC-2000T43},
  number={36}
}
@inproceedings{ardila2020common,
  title={Common Voice: A Massively-Multilingual Speech Corpus},
  author={Ardila, R., Branson, M., Davis, K., Kohler, M., Meyer, J., Henretty, M., et al.},
  booktitle={Proceedings of The 12th Language Resources and Evaluation Conference},
  year={2020},
  organization={European Language Resources Association},
  address={Marseille, France},
  pages={4218--4222}
}
@article{hao2009speech,
  author  = {Jiucang Hao},
  title   = {Speech Enhancement, Gain, And Noise Spectrum Adaptation Using Approximate Bayesian Estimation},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume  = {17},
  number  = {1},
  pages   = {24--37},
  year    = {2009},
  month   = {January},
  publisher = {IEEE}
}
@inproceedings{sim2018domain,
  author    = {K. C. Sim and A. Narayanan and A. Misra and A. Tripathi and G. Pundak and T. Sainath and P. Haghani and B. Li and M. Bacchiani},
  title     = {Domain Adaptation Using Factorized Hidden Layer for Robust Automatic Speech Recognition},
  booktitle = {Proc. INTERSPEECH},
  year      = {2018},
  pages     = {892-896},
}

@inproceedings{mani2020asr,
  author    = {A. Mani and S. Palaskar and N. V. Meripo and S. Konam and F. Metze},
  title     = {ASR Error Correction and Domain Adaptation Using Machine Translation},
  booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  year      = {2020},
  pages     = {6344-6348},
}

@inproceedings{kundu2016joint,
  author    = {S. Kundu and G. Mantena and Y. Qian and T. Tan and M. Delcroix and K. C. Sim},
  title     = {Joint Acoustic Factor Learning for Robust Deep Neural Network Based Automatic Speech Recognition},
  booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  year      = {2016},
  pages     = {5025-5029},
}
@article{ainsworth1992feedback,
  title={Feedback strategies for error correction in speech recognition systems},
  author={Ainsworth, William A and Pratt, Stephen},
  journal={International Journal of Man-Machine Studies},
  volume={36},
  number={6},
  pages={833--842},
  year={1992},
}

@inproceedings{allauzen2007error,
  title={Error detection in confusion network},
  author={Allauzen, Cyril},
  booktitle={INTERSPEECH},
  pages={1749--1752},
  year={2007},
}

@article{bassil2012asr,
  title={ASR context-sensitive error correction based on microsoft n-gram dataset},
  author={Bassil, Youssef and Semaan, Paul},
  journal={arXiv preprint arXiv:1203.5262},
  year={2012},
}

@inproceedings{chen2013asr,
  title={ASR error detection in a conversational spoken language translation system},
  author={Chen, Wen and Ananthakrishnan, Sridha and Kumar, Ravi and Prasad, Ram and Natarajan, Prem},
  booktitle={Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  pages={7418--7422},
  year={2013},
}

@inproceedings{favre2013automatic,
  title={Automatic human utility evaluation of ASR systems: Does WER really predict performance?},
  author={Favre, Benoit and Cheung, Kui-Wai and Kazemian, Sina and Lee, Alex and Liu, Yang and Munteanu, Cosmin and Nenkova, Ani and Ochei, Linda and Penn, Gerald and Tratz, Stephen and others},
  booktitle={INTERSPEECH},
  pages={3463--3467},
  year={2013},
}

@article{feng2004using,
  title={Using confidence scores to improve hands-free speech based navigation in continuous dictation systems},
  author={Feng, Jianhua and Sears, Andrew},
  journal={ACM Transactions on Computer-Human Interaction (TOCHI)},
  volume={11},
  number={3},
  pages={329--356},
  year={2004},
}

@book{hoffman1967probability,
  title={Probability, random variables, and stochastic processes},
  author={Hoffman, Joseph Papoulis and others},
  year={1967},
  publisher={McGraw-Hill New York},
}

@inproceedings{jaitly2012application,
  title={Application of pretrained deep neural networks to large vocabulary speech recognition},
  author={Jaitly, Navdeep and Nguyen, Patrick and Senior, Andrew and Vanhoucke, Vincent},
  booktitle={Proceedings of Interspeech},
  year={2012},
}

@article{jiang2005confidence,
  title={Confidence measures for speech recognition: A survey},
  author={Jiang, Haihua},
  journal={Speech communication},
  volume={45},
  number={4},
  pages={455--470},
  year={2005},
}

@techreport{maier2002evaluating,
  title={Evaluating RIL as basis for evaluating automated speech recognition devices and the consequences of using probabilistic string edit distance as input},
  author={Maier, Volker},
  year={2002},
  institution={3rd year project, Sheffield University},
}

@techreport{mccowan2004use,
  title={On the use of information retrieval measures for speech recognition evaluation},
  author={McCowan, Iain A and Moore, Darren and Dines, John and Gatica-Perez, Daniel and Flynn, Mike and Wellner, Pierre and Bourlard, Herv{\'e}},
  year={2004},
  institution={IDIAP},
}

@inproceedings{morris2004wer,
  title={From WER and RIL to MER and WIL: Improved evaluation measures for connected speech recognition},
  author={Morris, Andrew C and Maier, Volker and Green, Phil},
  booktitle={INTERSPEECH},
  year={2004},
}
@inproceedings{murray1993data,
  title={Data-entry by voice: Facilitating correction of misrecognitions},
  author={Murray, Alan and Frankish, Clive and Jones, David},
  booktitle={Interactive speech technology},
  publisher={Taylor \& Francis, Inc.},
  pages={137--144},
  year={1993},
}

@inproceedings{nanjo2005new,
  title={A new ASR evaluation measure and minimum bayes-risk decoding for open-domain speech understanding},
  author={Nanjo, Hiroshi and Kawahara, Tatsuya},
  booktitle={Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  pages={1053--1056},
  year={2005},
}

@inproceedings{pellegrini2010improving,
  title={Improving ASR error detection with non-decoder based features},
  author={Pellegrini, Thomas and Trancoso, Isabel},
  booktitle={INTERSPEECH},
  pages={1950--1953},
  year={2010},
}

@inproceedings{pellegrini2011error,
  title={Error detection in broadcast news ASR using Markov chains},
  author={Pellegrini, Thomas and Trancoso, Isabel},
  booktitle={Human Language Technology. Challenges for Computer Science and Linguistics},
  publisher={Springer},
  pages={59--69},
  year={2011},
}

@article{ristad1998learning,
  title={Learning string-edit distance},
  author={Ristad, Eric S and Yianilos, Peter N},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={20},
  number={5},
  pages={522--532},
  year={1998},
}

@inproceedings{sarma2004context,
  title={Context-based speech recognition error detection and correction},
  author={Sarma, Anoop and Palmer, David D},
  booktitle={Proceedings of HLT-NAACL 2004: Short Papers},
  organization={Association for Computational Linguistics},
  pages={85--88},
  year={2004},
}

@article{shi2011supporting,
  title={Supporting dictation speech recognition error correction: The impact of external information},
  author={Shi, Yali and Zhou, Li},
  journal={Behaviour \& Information Technology},
  volume={30},
  number={6},
  pages={761--774},
  year={2011},
}

@article{suhm2001multimodal,
  title={Multimodal error correction for speech user interfaces},
  author={Suhm, Bernhard and Myers, Brad and Waibel, Alex},
  journal={ACM transactions on computer-human interaction (TOCHI)},
  volume={8},
  number={1},
  pages={60--98},
  year={2001},
}

@article{swietojanski2014convolutional,
  title={Convolutional neural networks for distant speech recognition},
  author={Swietojanski, Pawel and Ghoshal, Arnab and Renals, Steve},
  journal={IEEE Signal Processing Letters},
  volume={21},
  number={9},
  pages={1120--1124},
  year={2014},
}

@inproceedings{yu2004unsupervised,
  title={Unsupervised learning from users' error correction in speech dictation},
  author={Yu, Dong and Hwang, Mei-Yuh and Mau, Pak and Acero, Alex and Deng, Li},
  booktitle={INTERSPEECH},
  year={2004},
}

@article{zhou2005data,
  title={Data mining for detecting errors in dictation speech recognition},
  author={Zhou, Li and Shi, Yali and Feng, Jianhua and Sears, Andrew},
  journal={Speech and Audio Processing, IEEE Transactions on},
  volume={13},
  number={4},
  pages={681--688},
  year={2005},
}
@inproceedings{siohan_ensemble,
  author = {O. Siohan and B. Ramabhadran and B. Kingsbury},
  title = {Constructing ensembles of ASR systems using randomized decision trees},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year = {2005},
  volume = {1},
  pages = {I--197}
}

@inproceedings{deng_ensemble,
  author = {L. Deng and J. C. Platt},
  title = {Ensemble deep learning for speech recognition},
  booktitle = {Fifteenth Annual Conference of the International Speech Communication Association},
  year = {2014}
}

@inproceedings{evermann_posterior,
  author = {G. Evermann and P. C. Woodland},
  title = {Posterior probability decoding, confidence estimation and system combination},
  booktitle = {Proc. Speech Transcription Workshop},
  year = {2000},
  volume = {27},
  pages = {78--81}
}

@article{xu_minimum,
  author = {H. Xu and D. Povey and L. Mangu and J. Zhu},
  title = {Minimum bayes risk decoding and system combination based on a recursion for edit distance},
  journal = {Comput. Speech Lang.},
  volume = {25},
  number = {4},
  pages = {802--828},
  year = {2011}
}

@inproceedings{rover,
  author = {J. G. Fiscus},
  title = {A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER)},
  booktitle = {Automatic Speech Recognition and Understanding},
  year = {1997},
  pages = {347--354}
}
@article{GAUVAIN1994speaker,
title = {Speaker-independent continuous speech dictation},
journal = {Speech Communication},
volume = {15},
number = {1},
pages = {21-37},
year = {1994},
issn = {0167-6393},
doi = {https://doi.org/10.1016/0167-6393(94)90038-8},
url = {https://www.sciencedirect.com/science/article/pii/0167639394900388},
author = {J.L. Gauvain and L.F. Lamel and G. Adda and M. Adda-Decker},
keywords = {Continuous speech recognition, Word recognition, Phone recognition, Speaker-independent, Large vocabulary, Dictation},
abstract = {In this paper we report on progress made at LIMSI in speaker-independent large vocabulary speech dictation using newspaper-based speech corpora in English and French. The recognizer makes use of continuous density HMMs with Gaussian mixtures for acoustic modeling and n-gram statistics estimated on newspaper texts for language modeling. Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models. For English the ARPA Wall Street Journal-based CSR corpus is used and for French the BREF corpus containing recordings of texts from the French newspaper Le Monde is used. Experiments were carried out with both these corpora at the phone level and at the word level with vocabularies containing up to 20,000 words. Word recognition experiments are also described for the ARPA RM task which has been widely used to evaluate and compare systems.
Zusammenfassung
In diesem Beitrag beschreiben wir Fortschritte in der Entwicklung eines sprecherunahängigen Spracherkennungssystems für groβen Wortschatz, welches mit (gesprochenem und geschriebenem) Datenmaterial von Zeitungartikeln trainiert wurde. Die akustische Modellierung des Spracherkennungssystems besteht aus Mischungen kontinuierlicher gauβ'schen Dichten in Hidden Markov Modellen (HMM). Die Modellierung der (geschriebenen) Sprache beruht auf statistischen n-grams, deren Wahrscheinlichkeiten aus einer Datenbasis bestehendaus Zeitungsartikeln geschätzt wurden. Was die akustischen Modelle betrifft, verwenden wir Cepstrum Parameter in kontextabhängigen Phonmodellen, mit zeitlicher Modellierung und geschlechtspezifischen Modellen. Für die englische Sprache benutzen wir die ARPA Wall Street Journal Datenbasis und für die französische, die BREF Daten, die gesprochene Texte der französischen Zeitung Le Monde enthalten. Für beide Sprachen wurden Experimente auf Phonem- und Wortbasis durchgeführt. Der Wortschatz besteht aus bis zu 20K Wörter. Weiterhin präsentieren wir Ergebnisse auf der ARPA RM Datenbasis, da diese weltweit zur Bewertung von Spracherkennungssystemen verwendet wurde.
Résumé
Nous présentons dans cet article les avancées réalisées au LIMSI sur la reconnaissance de parole continue de grand vocabulaire, indépendante du locuteur dans une application de dictée de textes. Le système utilise des modèles de Markov cachés à densités continues au niveau acoustique, et des modèles de langage n-grammes au niveau syntaxique. La modélisation acoustique repose sur une analyse cepstrale du signal vocal, des modèles de phones en contexte (inter-et intramot) dépendant du genre du locuteur, et des modèles de durée phonémique. Nous avons utilisé, pour la langue anglaise, le corpus de parole continue ARPA-WSJ contenant des enregistrements de textes lus extraits du Wall Street Journal, et, pour la langue française, le corpus BREF contenant des enregistrements de textes lus extraits du journal Le Monde. Les performances du système de reconnaissance, mesurées au niveau phonétique et au niveau mot sont données sur ces deux corpus pour des vocabulaires contenant jusqu'à 20.000 mots. Nous donnons également pour référence les résultats obtenus sur le corpus ARPA-RM qui a été très largement utilisé pour évaluer et comparer des systèmes de reconnaissance de parole.}
}

@article{bahl_decoding,
  author = {Bahl, L. R. and Jelinek, F.},
  title = {Decoding for channels with insertions, deletions and substitutions, with applications to speech recognition},
  journal = {IEEE Transactions on Information Theory},
  volume = {IT-21},
  number = {4},
  pages = {404--411},
  year = {1975}
}

@book{collings_statistical,
  author = {Collings, S. N.},
  title = {Fundamentals of Statistical Inference},
  year = {1977},
  publisher = {The Open University Press},
  note = {Unit 11. Hypothesis Testing II, course M341, pp.105--107}
}

@book{gold_speech,
  author = {Gold, B. and Morgan, N.},
  title = {Speech and Audio Signal Processing},
  year = {2000},
  publisher = {Wiley}
}

@mastersthesis{maier_evaluating,
  author = {Maier, V.},
  title = {Evaluating RIL as basis of automatic speech recognition devices and the consequences of using probabilistic string edit distance as input},
  school = {University of Sheffield},
  year = {2002}
}

@article{miller_note,
  author = {Miller, G. A.},
  title = {Note on the bias of information estimates},
  booktitle = {Information theory and psychology},
  editor = {Quastler, H.},
  publisher = {The Free Press},
  address = {Glencoe, IL},
  year = {1954},
  pages = {95--100}
}

@article{miller_analysis,
  author = {Miller, G. and Nicely, P.},
  title = {An analysis of perceptual confusions among some English consonants},
  journal = {The Journal of the Acoustical Society of America},
  volume = {27},
  number = {2},
  year = {1955}
}

@misc{moore_evaluating,
  author = {Moore, R.},
  title = {Evaluating speech recognisers},
  howpublished = {White paper},
  year = {1979},
  institution = {Department of Electronic Engineering, University of Essex, UK}
}

@techreport{morris_information,
  author = {Morris, A. C.},
  title = {An information theoretic measure of sequence recognition performance},
  institution = {IDIAP Research Institute},
  number = {IDIAP-com 02-03},
  year = {2002},
  url = {ftp://ftp.idiap.ch/pub/reports/2002/com02-03.pdf}
}

@book{papoulis_probability,
  author = {Papoulis, A.},
  title = {Probability, Random Variables, and Stochastic Processes},
  year = {1991},
  publisher = {McGraw-Hill}
}

@inproceedings{woodard_information,
  author = {Woodard, J. P. and Nelson, J. T.},
  title = {An information theoretic measure of speech recognition performance},
  booktitle = {Workshop on Standardisation for Speech I/O Technology},
  year = {1982},
  address = {Naval Air Development Center, Warminster, PA}
}

@article{young_speech,
  author = {Young, S. J. and Chase, L. L.},
  title = {Speech recognition evaluation: a review of the US CSR and LVCSR programmes},
  journal = {Computer Speech & Language},
  volume = {12},
  number = {4},
  pages = {263--279},
  year = {1998}
}
@article{baum_statistical,
  author = {Baum, L. E. and Petrie, T.},
  title = {Statistical Inference for Probabilistic Functions of Finite State Markov Chains},
  journal = {Annals of Mathematical Statistics},
  volume = {37},
  year = {1966},
  pages = {1554--1563}
}

@article{viterbi_error,
  author = {Viterbi, A. J.},
  title = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
  journal = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {2},
  year = {1967},
  pages = {260--269}
}

@article{davis_comparison,
  author = {Davis, S. B. and Mermelstein, P.},
  title = {Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {ASSP-28},
  year = {1980},
  pages = {357--366}
}

@misc{garofolo_darpa,
  author = {Garofolo, J.},
  title = {Getting started with the DARPA-TIMIT CD-ROM: An acoustic phonetic continuous speech database},
  organization = {National Institute of Standards and Technology (NIST)},
  address = {Gaithersburgh, MD},
  year = {1988}
}

@article{lee_overview,
  author = {Lee, K.-F. and Hon, H.-W. and Reddy, R.},
  title = {An overview of the SPHINX speech recognition system},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {ASSP-38},
  number = {1},
  year = {1990},
  pages = {35--45}
}

@misc{sphinx_tutorial,
  author = {RobustGroup's Open Source Tutorial},
  title = {Learning to use the CMU SPHINX Automatic Speech Recognition system},
  howpublished = {Available: \url{http://www.speech.cs.cmu.edu/sphinx/tutorial.html}}
}

@inproceedings{wang_noise,
  author = {Wang, X. and Dong, Y. and Hakinen, J. and Viikki, O.},
  title = {Noise Robust Chinese Speech Recognition Using Feature Vector Normalization and Higher-Order Cepstral Coefficients},
  booktitle = {Proceedings of WCCC-ICSP2000},
  year = {2000},
  pages = {738--741}
}

@inproceedings{wrench_continuous,
  author = {Wrench, A. and Richmond, K.},
  title = {Continuous Speech Recognition Using Articulatory Data},
  booktitle = {Proceedings of ICSLP2000},
  year = {2000}
}

@inproceedings{xie_learning,
  author = {Xie, H. and Andreae, P. and Zhang, M. and Warren, P.},
  title = {Learning Models for English Speech Recognition},
  booktitle = {Conferences in Research and Practice in Information Technology},
  volume = {26},
  year = {2004},
  pages = {802--828}
}

@article{digalakis_quantization,
  author = {Digalakis, V. and Neumeyer, L. and Perakakis, M.},
  title = {Quantization of cepstral parameters for speech recognition over the World Wide Web},
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {17},
  number = {1},
  year = {1999},
  pages = {82--90}
}

@article{zheng_comparison,
  author = {Zheng, F. and Zhang, G. and Song, Z.},
  title = {Comparison of Different Implementations of MFCC},
  journal = {Journal of Computer Science \& Technology},
  volume = {16},
  number = {6},
  year = {2001},
  pages = {582--589},
  month = {Sept}
}

@techreport{slaney_auditory,
  author = {Slaney, M.},
  title = {Auditory Toolbox. Version 2},
  institution = {Interval Research Corporation},
  year = {1998},
  number = {Technical Report \#1998-010}
}

@article{sarikaya_high,
  author = {Sarikaya, R. and Hansen, H. L.},
  title = {High resolution speech feature parameterization for mono-phone-based stressed speech recognition},
  journal = {IEEE Signal Processing Letters},
  volume = {7},
  number = {7},
  year = {2000},
  pages = {182--185}
}

@article{farooq_mel,
  author = {Farooq, O. and Datta, S.},
  title = {Mel filter-like admissible wavelet packet structure for speech recognition},
  journal = {IEEE Signal Processing Letters},
  volume = {8},
  number = {7},
  year = {2001},
  pages = {196--198}
}

@misc{cmu_toolkit,
  title = {The CMU-Cambridge Statistical Language Modelling Toolkit, v2},
  howpublished = {Available: \url{http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html}}
}

@article{wilcoxon_individual,
  author = {Wilcoxon, F.},
  title = {Individual comparisons by ranking methods},
  journal = {Biometrics},
  volume = {1},
  number = {1},
  year = {1945},
  pages = {80--83}
}
@article{mporas_influence,
  author = {Mporas, Iosif and Ganchev, Todor and Kotinas, Elias},
  title = {Examining the Influence of Speech Frame Size and Number of Cepstral Coefficients on the Speech Recognition Performance},
  institution = {University of Patras, Greece},
  year = {Year},
  url = {URL}
}
@inproceedings{VanSegbroeck2014classification,
  author = {Segbroeck, Maarten Van and Travadi, Ruchir and Vaz, Colin and Kim, Jangwon and Black, Matthew P. and Potamianos, Alexandros and Narayanan, Shrikanth S.},
  title = {Classification of Cognitive Load from Speech Using an I-Vector Framework},
  booktitle = {INTERSPEECH-2014},
  pages = {751--755},
  year = {2014},
  url = {https://isca-speech.org/archive_v0/interspeech_2014/i14_0751.html}
}

@inproceedings{vanSegbroeck2014ubm,
  author = {Segbroeck, Maarten van and Travadi, Ruchir and Narayanan, Shrikanth S.},
  title = {UBM Fused Total Variability Modeling for Language Identification},
  booktitle = {INTERSPEECH-2014},
  pages = {3027--3031},
  year = {2014},
  url = {https://isca-speech.org/archive_v0/interspeech_2014/i14_3027.html}
}
@inproceedings{glembek2014i-vector,
author = {Glembek, Ondrej and Ma, Jeff and Matejka, Pavel and Zhang, Bing and Plchot, Oldrich and Burget, Lukas and Matsoukas, Spyros},
year = {2014},
month = {05},
pages = {4032-4036},
title = {Domain adaptation via within-class covariance correction in I-vector based speaker recognition systems},
isbn = {978-1-4799-2893-4},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2014.6854359}
}
@inproceedings{carlini2018audio,
  title={Audio adversarial examples: Targeted attacks on speech-to-text},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Proc. IEEE SPW},
  year={2018},
  pages={1--7}
}

@inproceedings{taori2019targeted,
  title={Targeted adversarial examples for black box audio systems},
  author={Taori, Rishabh and Kamsetty, Akash and Chu, Bo and Vemuri, Naveen},
  booktitle={Proc. IEEE Secur. Privacy Workshops (SPW)},
  year={2019},
  pages={15--20}
}

@inproceedings{sriram2018robust,
  title={Robust speech recognition using generative adversarial networks},
  author={Sriram, Anuroop and Jun, Heesoo and Gaur, Yash and Satheesh, Sathya},
  booktitle={Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  year={2018},
  pages={5639--5643}
}


@article{karpagavalli2011automatic,
  title={Automatic speech recognition: Architecture, methodologies and challenges—A review},
  author={Karpagavalli, S. and Deepika, R. and Kokila, P. and Rani, K. U. and Chandra, E.},
  journal={Int. J. Adv. Res. Comput. Sci.},
  volume={2},
  number={6},
  pages={326--331},
  year={2011}
}

@article{trinh2021directly,
  title={Directly comparing the listening strategies of humans and machines},
  author={Trinh, V. A. and Mandel, M.},
  journal={IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume={29},
  pages={312--323},
  year={2021}
}

@article{khan2023codeswitch,
  title={Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN},
  author={Sage Khan, Raheem Ali and Dr. Arshad Aziz},
  year={2023}
}
@inproceedings{lindberg1999vulnerability,
  title={Vulnerability in speaker verification: a study of technical impostor techniques},
  author={Lindberg, Johan and Blomberg, Mats},
  booktitle={Proc. 6th Eur. Conf. Speech Commun. Technol.},
  year={1999},
  volume={99},
  pages={1211--1214}
}
@article{wu2014voice,
  title={Voice conversion versus speaker verification: An overview},
  author={Wu, Zhenhua and Li, Haizhou},
  journal={APSIPA Transactions on Signal and Information Processing},
  volume={3},
  year={2014},
  pages={e17}
}

@article{deleon2012evaluation,
  title={Evaluation of speaker verification security and detection of HMM-based synthetic speech},
  author={De Leon, Philip L and Pucher, Michael and Yamagishi, Junichi and Hernaez, Inma and Saratxaga, Ion},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={20},
  number={8},
  year={2012},
  pages={2280--2290}
}

@inproceedings{kinnunen2012vulnerability,
  title={Vulnerability of speaker verification systems against voice conversion spoofing attacks: The case of telephone speech},
  author={Kinnunen, Tomi and Wu, Zhizheng and Lee, Kong Aik and Sedlak, Filip and Chng, Eng Siong and Li, Haizhou},
  booktitle={Proc. IEEE Int. Conf. Acoustics Speech Signal Process.},
  year={2012},
  pages={4401--4404}
}
@inproceedings{gillick2010conversational,
  title={Can conversational word usage be used to predict speaker demographics?},
  author={Gillick, Dan},
  booktitle={Proc. Interspeech},
  year={2010},
  pages={1381--1384}
}

@article{mairesse2007using,
  title={Using linguistic cues for the automatic recognition of personality in conversation and text},
  author={Mairesse, Francine and Walker, Marilyn A and Mehl, Matthias R and Moore, Robert K},
  journal={Journal of Artificial Intelligence Research},
  volume={30},
  year={2007},
  pages={457--500}
}
@article{patel2017emotion,
  title={Emotion recognition from speech with gaussian mixture models \& via boosted GMM},
  author={Patel, Pooja and Chaudhari, Abhishek and Kale, Rishabh and Pund, Manisha},
  journal={Int. J. Res. Sci. Eng.},
  volume={3},
  year={2017}
}

@article{scherer1972minimal,
  title={Minimal cues in the vocal communication of affect: Judging emotions from content-masked speech},
  author={Scherer, Klaus R and Koivumaki, Juha and Rosenthal, Robert},
  journal={J. Psycholinguistic Res.},
  volume={1},
  number={3},
  pages={269--285},
  year={1972}
}

@article{schuller2009being,
  title={Being bored? recognising natural interest by extensive audiovisual integration for real-life application},
  author={Schuller, Bj{\"o}rn and others},
  journal={Image Vis. Comput.},
  volume={27},
  number={12},
  pages={1760--1774},
  year={2009}
}

@inproceedings{schuller2011interspeech,
  title={The Interspeech 2011 speaker state challenge},
  author={Schuller, Bj{\"o}rn and Steidl, Stefan and Batliner, Anton and Schiel, Florian and Krajewski, Jarek},
  booktitle={Proc. Interspeech},
  year={2011},
  pages={3201--3204}
}

@article{mporas2009estimation,
  title={Estimation of unknown speaker's height from speech},
  author={Mporas, Iosif and Ganchev, Todor},
  journal={Int. J. Speech Technol.},
  volume={12},
  number={4},
  pages={149--160},
  year={2009}
}

@inproceedings{das2014do,
  title={Do you hear what i hear?: Fingerprinting smart devices through embedded acoustic components},
  author={Das, Anupam and Borisov, Nikita and Caesar, Matthew},
  booktitle={Proc. ACM SIGSAC Conf. Comput. Commun. Security},
  year={2014},
  pages={441--452}
}

@inproceedings{roffo2013trusting,
  title={Trusting skype: Learning the way people chat for fast user recognition and verification},
  author={Roffo, Giorgio and Cristani, Marco and Bazzani, Loris and Minh, Hoai and Murino, Vittorio},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis. Workshops},
  year={2013},
  pages={748--754}
}

@inproceedings{qian2016de,
  title={De-anonymizing social networks and inferring private attributes using knowledge graphs},
  author={Qian, Jie and Li, Xue-Yang and Zhang, Cheng and Chen, Liang and Jung, Taekyung and Han, Jiawei},
  booktitle={Proc. 35th Annu. IEEE Int. Conf. Comput. Commun.},
  year={2016},
  pages={1--9}
}

@inproceedings{ji2014structural,
  title={Structural data deanonymization: Quantification, practice, and implications},
  author={Ji, Shouling and Li, Wenjing and Srivatsa, Mudhakar and Beyah, Raheem},
  booktitle={Proc. ACM SIGSAC Conf. Comput. Commun. Security},
  year={2014},
  pages={1040--1053}
}

@article{qian2019social,
  title={Social network de-anonymization and privacy inference with knowledge graph model},
  author={Qian, Jie and Li, Xue-Yang and Zhang, Cheng and Chen, Liang and Jung, Taekyung and Han, Jiawei},
  journal={IEEE Trans. Dependable Secure Comput.},
  volume={16},
  number={4},
  pages={679--692},
  year={2019}
}
@inproceedings{wu2014study,
  title={A study on replay attack and anti-spoofing for text-dependent speaker verification},
  author={Wu, Zhizheng and Gao, Shifeng and Cling, Enqing S and Li, Haizhou},
  booktitle={Proc. APSIPA Annu. Summit Conf.},
  year={2014},
  pages={1--5}
}

@article{wu2015spoofing,
  title={Spoofing and countermeasures for speaker verification: A survey},
  author={Wu, Zhizheng and Evans, Nicholas and Kinnunen, Tomi and Yamagishi, Junichi and Alegre, Federico and Li, Haizhou},
  journal={Speech Commun.},
  volume={66},
  pages={130--153},
  year={2015}
}

@inproceedings{pathak2012privacy,
  title={Privacy-preserving speaker authentication},
  author={Pathak, Mitesh and Portelo, Jos{\'e} and Raj, Bhiksha and Trancoso, Isabel},
  booktitle={Proc. Int. Conf. Inf. Security},
  year={2012},
  pages={1--22}
}

@article{pathak2013privacy,
  title={Privacy-preserving speaker verification and identification using gaussian mixture models},
  author={Pathak, Mitesh A and Raj, Bhiksha},
  journal={IEEE Trans. Audio Speech Lang. Process.},
  volume={21},
  number={2},
  pages={397--406},
  year={2013},
  month={Feb}
}
@article{smaragdis2007framework,
  title={A framework for secure speech recognition},
  author={Smaragdis, Paris and Shashanka, Murali},
  journal={IEEE Trans. Audio Speech Language Process.},
  volume={15},
  number={4},
  pages={1404--1413},
  year={2007},
  month={May}
}

@inproceedings{qian2018privacy,
  title={Towards privacy-preserving speech data publishing},
  author={Qian, Jing and Han, Fangjia and Hou, Junhao and Zhang, Chao and Wang, Yifan and Li, Xiang-Yang},
  booktitle={Proc. IEEE Conf. Comput. Commun.},
  year={2018},
  pages={1079--1087}
}

@inproceedings{zhang2000study,
  title={The study on distributed speech recognition system},
  author={Zhang, Wenju and He, Lian and Chow, Ying-Lin and Yang, Rong and Su, Yuxuan},
  booktitle={Proc. IEEE Int. Conf. Acoustics Speech Signal Process.},
  year={2000},
  pages={1431--1434}
}
@inproceedings{goodfellow_explaining_2015,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year={2015}
}

@inproceedings{szegedy_intriguing_2014,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  booktitle={Proceedings of International Conference on Learning Representations (ICLR)},
  year={2014}
}
@misc{benign_audio,
  title={Benign Audio Sample},
  author={nanavati et al},
  howpublished={\url{https://drive.google.com/file/d/1 LRKafcAVwv-bqlaQouWz-4xalpsudLM/view?usp=sharing}},
  year={2021}
}

@misc{adversarial_example,
  title={Adversarial Audio Example},
  author={nanavati et al},
  howpublished={\url{https://drive.google.com/file/d/19WlGYamm7pXPitG9aTPMtywSfIt79Axm/view?usp=sharing}},
  year={2021}
}

@inproceedings{vaidya_exploiting_2015,
author = {Vaidya, Tavish and Zhang, Yuankai and Sherr, Micah and Shields, Clay},
title = {Cocaine Noodles: Exploiting the Gap between Human and Machine Speech Recognition},
year = {2015},
publisher = {USENIX Association},
address = {USA},
abstract = {Hands-free, voice-driven user input is gaining popularity, in part due to the increasing functionalities provided by intelligent digital assistances such as Siri, Cortana, and Google Now, and in part due to the proliferation of small devices that do not support more traditional, keyboard-based input.In this paper, we examine the gap in the mechanisms of speech recognition between human and machine. In particular, we ask the question, do the differences in how humans and machines understand spoken speech lead to exploitable vulnerabilities? We find, perhaps surprisingly, that these differences can be easily exploited by an adversary to produce sound which is intelligible as a command to a computer speech recognition system but is not easily understandable by humans. We discuss how a wide range of devices are vulnerable to such manipulation and describe how an attacker might use them to defraud victims or install malware, among other attacks.},
booktitle = {Proceedings of the 9th USENIX Conference on Offensive Technologies},
pages = {16},
numpages = {1},
location = {Washington, D.C.},
series = {WOOT'15}
}


@inproceedings {carlini_hidden_2016,
author = {Nicholas Carlini and Pratyush Mishra and Tavish Vaidya and Yuankai Zhang and Micah Sherr and Clay Shields and David Wagner and Wenchao Zhou},
title = {Hidden Voice Commands},
booktitle = {25th USENIX Security Symposium (USENIX Security 16)},
year = {2016},
isbn = {978-1-931971-32-4},
address = {Austin, TX},
pages = {513--530},
url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/carlini},
publisher = {USENIX Association},
month = aug,
}


@inproceedings{nanavati_black_2021,
	title = {Black {Box} {Attack} on {Speech} {Commands} {Classification} {Model}},
	isbn = {978-1-66542-461-5},
	url = {https://ieeexplore.ieee.org/document/9531787/},
	doi = {10.1109/MWSCAS47672.2021.9531787},
	urldate = {2023-02-18},
	booktitle = {2021 {IEEE} {International} {Midwest} {Symposium} on {Circuits} and {Systems} ({MWSCAS})},
	publisher = {IEEE},
	author = {Nanavati, Rohin and Shah, Sarathi and Joshi, Manjunath},
	address = {Lansing, MI, USA},
        month = {aug},
	year = {2021},
	pages = {109--111}
}

@article{zou_robust_2018,
  title={Robust Gait Recognition by Integrating Inertial and RGBD Sensors},
  author={Zou, Q. and others},
  journal={IEEE Trans. Cybernetics},
  volume={48},
  number={4},
  pages={1136--1150},
  month={April},
  year={2018}
}
@inproceedings{zhang_dolphinattack_2017,
  title={Dolphinattack: Inaudible Voice Commands},
  author={Zhang, G. and others},
  booktitle={Proc. ACM CCS'17},
  organization={ACM},
  month={Oct.},
  year={2017},
  pages={103--117}
}

@article{zhou_dolphin_2019,
  title={Dolphin: Real-Time Hidden Acoustic Signal Capture with Smartphones},
  author={Zhou, M. and others},
  journal={IEEE Trans. Mobile Computing},
  volume={18},
  number={3},
  month={Mar.},
  year={2019},
  pages={560--573}
}
@inproceedings{carlini_audio_2018,
  title={Audio adversarial examples: Targeted attacks on speech-to-text},
  author={Carlini, N. and Wagner, D.},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  organization={IEEE},
  year={2018},
  pages={1--7}
}

@misc{ng_face_2021,
  title={Face recognition audit, gamers cheat with AI, who rules the smart city? Language learning generalizes to other domains},
  author={Ng, A.},
  note={The Batch (online), July 28, 2021}
}

@article{pop_towards_2021,
  title={Towards enhancing the security of automatic speaker verification systems against adversarial attacks},
  author={Pop, F. and others},
  journal={arXiv preprint arXiv:2101.12345},
  year={2021}
}
@inproceedings{goodfellow_explaining_2015,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
  booktitle={Proc. ICLR},
  year={2015},
  address={San Diego, CA},
  pages={11}
}

@inproceedings{carlini_audio_2018,
  title={Audio adversarial examples: Targeted attacks on speech-to-text},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Proc. SPW},
  year={2018},
  address={San Francisco, CA},
  pages={1--7}
}

@inproceedings{liu_delving_2017,
  title={Delving into transferable adversarial examples and black-box attacks},
  author={Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
  booktitle={Proc. ICLR},
  year={2017},
  address={Toulon, France},
  pages={14}
}

@inproceedings{madry_towards_2017,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={Proc. ICML},
  year={2017},
  address={Sydney, Australia},
  pages={10}
}
@inproceedings{goodfellow_explaining_2015,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
  booktitle={Proc. ICLR},
  year={2015},
  address={San Diego, CA},
  pages={11}
}

@inproceedings{chen_attacking_2018,
  title={Attacking visual language grounding with adversarial examples: A case study on neural image captioning},
  author={Chen, Hongge and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Hsieh, Cho-Jui},
  booktitle={Proc. ACL},
  year={2018},
  address={Melbourne, Australia},
  pages={2587--2597}
}

@inproceedings{xie_adversarial_2017,
  title={Adversarial examples for semantic segmentation and object detection},
  author={Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Zhou, Yuyin and Xie, Lingxi and Yuille, Alan},
  booktitle={Proc. ICCV},
  year={2017},
  address={Venice, Italy},
  pages={1369--1378}
}

@inproceedings{gong_crafting_2017,
  title={Crafting adversarial examples for speech paralinguistics applications},
  author={Gong, Yuan and Poellabauer, Christian},
  booktitle={Proc. DYNAMICS},
  year={2017},
  address={San Juan, PR},
  pages={8}
}

@inproceedings{baird_can_2019,
  title={Can deep generative audio be emotional? Towards an approach for personalised emotional audio generation},
  author={Baird, Alice and Shahin, Amiriparian and Schuller, Bjorn},
  booktitle={Proc. MMSP},
  year={2019},
  address={Kuala Lumpur, Malaysia},
  pages={5}
}
@inproceedings{guo_simple_2019,
  title={Simple black-box adversarial attacks},
  author={Guo, Chuan and Gardner, Jacob R. and You, Haoxiang and Wilson, Andrew Gordon and Weinberger, Kilian Q.},
  booktitle={2019 IEEE Security and Privacy Workshops (SPW)},
  year={2019},
  pages={1--7}
}

@misc{pytorch_speech_command_tutorial,
  title={Speech Command Recognition with torchaudio},
  author={{PyTorch}},
  note={\url{https://pytorch.org/tutorials/intermediate/speech\_command\_recognition\_with\_torchaudio.html}},
  year={Accessed: July 4, 2023}
}

@misc{warden_speech_commands_2018,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  year={2018},
  note={\url{https://arxiv.org/abs/1804.03209}}
}
@inproceedings{guo_simple_2019,
  title={Simple black-box adversarial attacks},
  author={Guo, Chuan and Gardner, Jacob R. and You, Haoxiang and Wilson, Andrew Gordon and Weinberger, Kilian Q.},
  booktitle={2019 IEEE Security and Privacy Workshops (SPW)},
  year={2019},
  pages={1--7}
}

@inproceedings{alzantot_did_2018,
  title={Did you hear that? Adversarial examples against automatic speech recognition},
  author={Alzantot, Moustafa and Balaji, Biplab and Srivastava, Mani},
  booktitle={Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
  year={2018},
  pages={15--27}
}
@article{schmidhuber_deep_2015,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, Jurgen},
  journal={Neural Networks},
  volume={61},
  pages={85--117},
  year={2015},
  month={January},
}

@inproceedings{simonyan_very_2015,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={Proceedings of International Conference on Learning Representations (ICLR)},
  year={2015},
  pages={1--14},
}

@article{hinton_deep_2012,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and et al.},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  month={November},
}

@inproceedings{collobert_unified_2008,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th International Conference on Machine Learning},
  year={2008},
  pages={160--167},
}

@inproceedings{potluri_accelerated_2016,
  title={Accelerated deep neural networks for enhanced intrusion detection system},
  author={Potluri, Sri and Diedrich, Christian},
  booktitle={Proceedings of the IEEE 21st International Conference},
  year={2016},
}
@inproceedings{szegedy_intriguing_2014,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and et al.},
  booktitle={Proceedings of International Conference on Learning Representations (ICLR)},
  year={2014},
  pages={1--10},
  doi={10.48550/arXiv.1312.6199}
}




@inproceedings{du_sirenattack_2020,
  title={SirenAttack: Generating adversarial audio for end-to-end acoustic systems},
  author={Du, Tiantian and et al.},
  booktitle={Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
  year={2020},
  pages={357--369},
}

@inproceedings{liu_weighted-sampling_2020,
  title={Weighted-sampling audio adversarial example attack},
  author={Liu, Xuan and et al.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={4},
  year={2020},
  pages={4908--4915},
}

@techreport{iter_generating_2017,
  title={Generating adversarial examples for speech recognition},
  author={Iter, Dan and Huang, Jialin and Jermann, Michael},
  institution={Department of Computer Science, Stanford University},
  year={2017},
  url={https://web.stanford.edu/class/cs224s/project/reports_2017/Dan_Iter.pdf},
}

@inproceedings{carlini_audio_2018,
  title={Audio adversarial examples: Targeted attacks on speech-to-text},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Proceedings of the IEEE Security and Privacy Workshops},
  year={2018},
  pages={1--7},
}

@article{schonherr_adversarial_2018,
  title={Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding},
  author={Sch{\"o}nherr, Luis and et al.},
  journal={arXiv preprint arXiv:1808.05665},
  year={2018},
}
@article{barreno_security_2010,
  title={The security of machine learning},
  author={Barreno, Marco and Nelson, Blaine and Joseph, Anthony D and Tygar, J Doug},
  journal={Machine Learning},
  volume={81},
  number={2},
  pages={121--148},
  year={2010},
  month={Nov},
}

@inproceedings{barreno_can_2006,
  title={Can machine learning be secure?},
  author={Barreno, Marco and Nelson, Blaine and Sears, Russell and Joseph, Anthony D and Tygar, J Doug},
  booktitle={Symposium on Information, Computer and Communications Security},
  year={2006},
  organization={ACM},
  pages={16--25},
  month={Mar},
}

@inproceedings{lowd_adversarial_2005,
  title={Adversarial learning},
  author={Lowd, Daniel and Meek, Christopher},
  booktitle={Conference on Knowledge Discovery in Data Mining},
  year={2005},
  organization={ACM},
  pages={641--647},
  month={Aug},
}

@article{goodfellow_explaining_2014,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={CoRR},
  volume={abs/1412.6572},
  pages={1--11},
  year={2014},
  month={Dec},
}

@article{fawzi_analysis_2018,
  title={Analysis of classifiers' robustness to adversarial perturbations},
  author={Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  journal={Machine Learning},
  volume={107},
  number={3},
  pages={481--508},
  year={2018},
  month={Mar},
}

@inproceedings{fawzi_robustness_2016,
  title={Robustness of classifiers: From adversarial to random noise},
  author={Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  booktitle={Conference on Neural Information Processing Systems},
  year={2016},
  organization={Curran Associates, Inc.},
  pages={1632--1640},
  month={Dec},
}

@article{shaham_understanding_2018,
  title={Understanding adversarial training: Increasing local stability of supervised models through robust optimization},
  author={Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
  journal={Neurocomputing},
  year={2018},
}

@article{liu_delving_2016,
  title={Delving into transferable adversarial examples and black-box attacks},
  author={Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={CoRR},
  volume={abs/1611.02770},
  pages={1--11},
  year={2016},
}

@inproceedings{nguyen_deep_2015,
  title={Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2015},
  organization={IEEE},
  pages={427--436},
  month={Jun},
}

@inproceedings{carlini_towards_2017,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Symposium on Security and Privacy},
  year={2017},
  organization={IEEE},
  pages={39--57},
  month={May},
}
@article{evtimov2017robust,
  title={Robust physical-world attacks on machine learning models},
  author={Evtimov, Ivan and Eykholt, Kevin and Fernandes, Earlence and Kohno, Tadayoshi and Li, Bo and Prakash, Atul and Rahmati, Amir and Song, Dawn},
  journal={CoRR},
  volume={abs/1707.08945},
  pages={1--11},
  year={2017}
}

@article{athalye2017synthesizing,
  title={Synthesizing robust adversarial examples},
  author={Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  journal={CoRR},
  volume={abs/1707.07397},
  pages={1--18},
  year={2017}
}

@article{brown2017adversarial,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'i}n and Gilmer, Justin},
  journal={CoRR},
  volume={abs/1712.09665},
  pages={1--6},
  year={2017}
}

@article{cisse2017houdini,
  title={Houdini: Fooling deep structured prediction models},
  author={Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
  journal={CoRR},
  volume={abs/1707.05373},
  pages={1--12},
  year={2017}
}

@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Symposium on Security and Privacy},
  organization={IEEE},
  pages={39--57},
  year={2017}
}

@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={Symposium on Security and Privacy},
  organization={IEEE},
  pages={582--597},
  year={2016}
}
@inproceedings{carlini2016hidden,
  title={Hidden voice commands},
  author={Carlini, Nicholas and Mishra, Pratyush and Vaidya, Tavish and Zhang, Yuankai and Sherr, Micah and Shields, Clay and Wagner, David A. and Zhou, Wenchao},
  booktitle={USENIX Security Symposium},
  organization={USENIX},
  pages={513--530},
  year={2016}
}

@article{cisse2017houdini,
  title={Houdini: Fooling deep structured prediction models},
  author={Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
  journal={CoRR},
  volume={abs/1707.05373},
  pages={1--12},
  year={2017}
}

@inproceedings{zhang2017dolphin,
  title={DolphinAttack: Inaudible voice commands},
  author={Zhang, Guoming and Yan, Chen and Ji, Xiaoyu and Zhang, Tianchen and Zhang, Taimin and Xu, Wenyao},
  booktitle={Conference on Computer and Communications Security},
  organization={ACM},
  pages={103--117},
  year={2017}
}

@inproceedings{vaidya2015cocaine,
  title={Cocaine Noodles: Exploiting the gap between human and machine speech recognition},
  author={Vaidya, Tavish and Zhang, Yuankai and Sherr, Micah and Shields, Clay},
  booktitle={Workshop on Offensive Technologies},
  organization={USENIX},
  year={2015}
}
@inproceedings{zhang2017dolphin,
  title={DolphinAttack: Inaudible voice commands},
  author={Zhang, Guoming and Yan, Chen and Ji, Xiaoyu and Zhang, Tianchen and Zhang, Taimin and Xu, Wenyao},
  booktitle={Conference on Computer and Communications Security},
  organization={ACM},
  pages={103--117},
  year={2017}
}

@article{song2017inaudible,
  title={Inaudible voice commands},
  author={Song, Li and Mittal, Prateek},
  journal={CoRR},
  volume={abs/1708.07238},
  pages={1--3},
  year={2017}
}

@inproceedings{roy2017backdoor,
  title={BackDoor: Making microphones hear inaudible sounds},
  author={Roy, Nirupam and Hassanieh, Haitham and Roy Choudhury, Romit},
  booktitle={Conference on Mobile Systems, Applications, and Services},
  organization={ACM},
  pages={2--14},
  year={2017}
}

@article{carlini2018audio,
  title={Audio adversarial examples: Targeted attacks on Speech-to-Text},
  author={Carlini, Nicholas and Wagner, David A.},
  journal={CoRR},
  volume={abs/1801.01944},
  pages={1--7},
  year={2018},
  doi={10.48550/arXiv.1801.01944}
}

@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David A.},
  booktitle={Symposium on Security and Privacy},
  organization={IEEE},
  pages={39--57},
  year={2017}
}

@article{yuan2018commandersong,
  title={Commandersong: A systematic approach for practical adversarial voice recognition},
  author={Yuan, Xiaoyong and Chen, Yuan and Zhao, Yunqing and Long, Yunhui and Liu, Xiaokang and Chen, Kai and Zhang, Shengzhi and Huang, Huiying and Wang, Xiaofeng and Gunter, Carl A.},
  journal={arXiv preprint arXiv:1801.08535},
  year={2018}
}
@article{barni2001improved,
  title={Improved wavelet-based watermarking through pixel-wise masking},
  author={Barni, Mauro and Bartolini, Franco and Piva, Alessandro},
  journal={IEEE transactions on image processing},
  volume={10},
  number={5},
  pages={783--791},
  year={2001}
}

@book{cox2007digital,
  title={Digital watermarking and steganography},
  author={Cox, Ingemar J and Miller, Matthew L and Bloom, Jeffrey A and Fridrich, Jessica and Kalker, Ton},
  year={2007},
  publisher={Morgan kaufmann}
}

@article{wolfgang1999perceptual,
  title={Perceptual watermarks for digital images and video},
  author={Wolfgang, Robertson B and Podilchuk, Christine I and Delp, Edward J},
  journal={Proceedings of the IEEE},
  volume={87},
  number={7},
  pages={1108--1126},
  year={1999}
}

@inproceedings{wu2004self,
  title={Self-synchronized audio watermark in dwt domain},
  author={Wu, Shuiming and Huang, Jiying and Huang, Di and Shi, Yun Q},
  booktitle={2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)},
  volume={5},
  pages={V--V},
  year={2004},
  month={May},
  organization={IEEE}
}

@article{seok2001audio,
  title={Audio watermarking for copyright protection of digital audio data},
  author={Seok, Jin-Woo and Hong, Jae-Woo},
  journal={Electronics Letters},
  volume={37},
  number={1},
  pages={60--61},
  year={2001}
}

@inproceedings{kohls2016skypeline,
  title={SkypeLine: Robust hidden data transmission for VoIP},
  author={Kohls, Kevin and Holz, Thorsten and Kolossa, Dorothea and P{\"o}pper, Christina},
  booktitle={Asia Conference on Computer and Communications Security},
  organization={ACM},
  pages={877--888},
  year={2016},
  month={May}
}

@inproceedings{asad2011enhanced,
  title={An enhanced Least Significant Bit modification technique for audio steganography},
  author={Asad, Muhammad and Gilani, Jamil and Khalid, Anwer},
  booktitle={Conference on Computer Networks and Information Technology},
  organization={IEEE},
  pages={143--147},
  year={2011},
  month={Jul}
}

@inproceedings{liu2012least,
  title={Least-Significant-Digit steganography in low bitrate speech},
  author={Liu, Jing and Zhou, Kai and Tian, Haohong},
  booktitle={International Conference on Communications},
  organization={IEEE},
  pages={1133--1137},
  year={2012},
  month={Jun}
}
@inproceedings{amodei2016deep,
  title={Deep speech 2: End-to-end speech recognition in English and Mandarin},
  author={Amodei, Dario and others},
  booktitle={International Conference on Machine Learning},
  pages={173--182},
  year={2016}
}

@techreport{iter2017generating,
  title={Generating adversarial examples for speech recognition},
  author={Iter, Domagoj and Huang, Jonathan and Jermann, Michael},
  institution={Stanford Technical Report},
  year={2017}
}

@article{abdullah2019practical,
  title={Practical hidden voice attacks against speech and speaker recognition systems},
  author={Abdullah, Hamza and Garcia, Walter and Peeters, Cedric and Traynor, Patrick and Butler, Kevin R and Wilson, Joseph},
  journal={arXiv preprint arXiv:1904.05734},
  year={2019}
}

@inproceedings{carlini2018audio,
  title={Audio adversarial examples: Targeted attacks on speech-to-text},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={1--7},
  year={2018},
  organization={IEEE}
}

@article{yakura2018robust,
  title={Robust audio adversarial example for a physical attack},
  author={Yakura, Hiromu and Sakuma, Jun},
  journal={arXiv preprint arXiv:1810.11793},
  year={2018}
}

@article{qin2019imperceptible,
  title={Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  author={Qin, Yasheng and Carlini, Nicholas and Cottrell, Garrison and Goodfellow, Ian and Raffel, Colin},
  journal={arXiv preprint arXiv:1903.10346},
  year={2019}
}

@article{shen2019lingvo,
  title={Lingvo: A modular and scalable framework for sequence-to-sequence modeling},
  author={Shen, Jonathan and others},
  journal={arXiv preprint arXiv:1902.08295},
  year={2019}
}

@article{schonherr2019robust,
  title={Robust over-the-air adversarial examples against automatic speech recognition systems},
  author={Sch{\"o}nherr, Luis and Zeiler, Sebastian and Holz, Thorsten and Kolossa, Dorothea},
  journal={arXiv preprint arXiv:1908.01551},
  year={2019}
}

@article{szurley2019perceptual,
  title={Perceptual based adversarial audio attacks},
  author={Szurley, Joseph and Kolter, JZ},
  journal={arXiv preprint arXiv:1906.06355},
  year={2019}
}

@article{liu2019weighted,
  title={Towards weighted-sampling audio adversarial example attack},
  author={Liu, Xin and Zhang, Xiang and Wan, Kaidi and Zhu, Qing and Ding, Yuan},
  journal={arXiv, Audio and Speech Processing},
  year={2019}
}
@article{kwon2020selective,
  title={Selective audio adversarial example in evasion attack on speech recognition system},
  author={Kwon, Hyun Woo and Kwon, Hwan and Yoon, Hwanjoon and Choi, Deokjung},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={526--538},
  year={2020}
}

@article{abdoli2019universal,
  title={Universal adversarial audio perturbations},
  author={Abdoli, Seyed Mohsen and Hafemann, Luiz G and Rony, Jonathan and Ayed, Ismail Ben and Cardinal, Patrick and Koerich, Alessandro L},
  journal={arXiv preprint arXiv:1908.03173},
  year={2019}
}

@article{vadillo2019universal,
  title={Universal adversarial examples in speech command classification},
  author={Vadillo, Jose and Santana, Roberto},
  journal={arXiv preprint arXiv:1911.10182},
  year={2019}
}

@inproceedings{moosavi2016deepfool,
  title={Deepfool: A simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2574--2582},
  year={2016}
}

@article{neekhara2019universal,
  title={Universal adversarial perturbations for speech recognition systems},
  author={Neekhara, Paarth and Hussain, Shehzeen and Pandey, Paras and Dubnov, Shlomo and McAuley, Julian and Koushanfar, Farinaz},
  journal={arXiv preprint arXiv:1905.03828},
  year={2019}
}

@article{gong2017crafting,
  title={Crafting adversarial examples for speech paralinguistics applications},
  author={Gong, Yu and Poellabauer, Christian},
  journal={arXiv preprint arXiv:1711.03280},
  year={2017}
}

@inproceedings{kreuk2018fooling,
  title={Fooling end-to-end speaker verification with adversarial examples},
  author={Kreuk, Florian and Adi, Yossi and Ciss{\'e}, Moustapha and Keshet, Joseph},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1962--1966},
  year={2018},
  organization={IEEE}
}

@article{alzantot2018did,
  title={Did you hear that? adversarial examples against automatic speech recognition},
  author={Alzantot, Moataz and Balaji, Bharathan and Srivastava, Madhur},
  journal={arXiv preprint arXiv:1801.00554},
  year={2018}
}

@inproceedings{taori2019targeted,
  title={Targeted adversarial examples for black box audio systems},
  author={Taori, Rohan and Kamsetty, Abhishek and Chu, Bryant and Vemuri, Naveen},
  booktitle={2019 IEEE Security and Privacy Workshops (SPW)},
  pages={15--20},
  year={2019},
  organization={IEEE}
}

@article{khare2018adversarial,
  title={Adversarial black-box attacks on automatic speech recognition systems using multi-objective evolutionary optimization},
  author={Khare, Surjya and Aralikatte, Rahul and Mani, Sriram},
  journal={arXiv preprint arXiv:1811.01312},
  year={2018}
}

@inproceedings{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={466--474},
  year={2016},
  organization={IEEE}
}

@article{sun2018training,
  title={Training augmentation with adversarial examples for robust speech recognition},
  author={Sun, Shuo and Yeh, Chun-Feng and Ostendorf, Mari and Hwang, Mei-Yuh and Xie, Lei},
  journal={arXiv preprint arXiv:1806.02782},
  year={2018}
}
@inproceedings{zeng2019multiversion,
  title={A multiversion programming inspired approach to detecting audio adversarial examples},
  author={Zeng, Qingyang and others},
  booktitle={2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)},
  pages={39--51},
  year={2019},
  organization={IEEE}
}

@article{latif2018adversarial,
  title={Adversarial machine learning and speech emotion recognition: utilizing generative adversarial networks for robustness},
  author={Latif, Saad and Rana, Raja and Qadir, Junaid},
  journal={arXiv preprint arXiv:1811.11402},
  year={2018}
}

@article{rajaratnam2018isolated,
  title={Isolated and ensemble audio preprocessing methods for detecting adversarial examples against automatic speech recognition},
  author={Rajaratnam, Keerthi and Shah, Ketan and Kalita, Jugal},
  journal={arXiv preprint arXiv:1809.04397},
  year={2018}
}

@inproceedings{rajaratnam2018noise,
  title={Noise flooding for detecting audio adversarial examples against automatic speech recognition},
  author={Rajaratnam, Keerthi and Kalita, Jugal},
  booktitle={2018 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)},
  pages={197--201},
  year={2018},
  organization={IEEE}
}

@article{samizade2019adversarial,
  title={Adversarial example detection by classification for deep speech recognition},
  author={Samizade, Saeid and Tan, Zheng-Hua and Shen, Chengzhuo and Guan, Yong},
  journal={arXiv preprint arXiv:1910.10013},
  year={2019}
}

@article{yang2018characterizing,
  title={Characterizing audio adversarial examples using temporal dependency},
  author={Yang, Zhitong and Li, Bo and Chen, Pin-Yu and Song, Dawn},
  journal={arXiv preprint arXiv:1809.10875},
  year={2018}
}

@inproceedings{kwon2019poster,
  title={Poster: detecting audio adversarial example through audio modification},
  author={Kwon, Hyunwoo and Yoon, Hwanjoon and Park, Kyu-Woong},
  booktitle={Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
  pages={2521--2523},
  year={2019}
}

@article{ma2019detecting,
  title={Detecting adversarial attacks on audio-visual speech recognition},
  author={Ma, Pengcheng and Petridis, Stavros and Pantic, Maja},
  journal={arXiv preprint arXiv:1912.08639},
  year={2019}
}

@article{esmaeilpour2019robust,
  title={A robust approach for securing audio classification against adversarial attacks},
  author={Esmaeilpour, Majid and Cardinal, Patrick and Koerich, Alessandro L},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={2147--2159},
  year={2019}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2672--2680},
  year={2014}
}

@inproceedings{roy2017backdoor,
  title={Backdoor: making microphones hear inaudible sounds},
  author={Roy, Nirupam and Hassanieh, Haitham and Roy Choudhury, Romit},
  booktitle={Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
  pages={2--14},
  year={2017}
}

@inproceedings{tamura2019novel,
  title={Novel defense method against audio adversarial example for speech-to-text transcription neural networks},
  author={Tamura, Kazuki and Omagari, Akihiro and Hashida, Shigemi},
  booktitle={2019 IEEE 11th International Workshop on Computational Intelligence and Applications (IWCIA)},
  pages={115--120},
  year={2019},
  organization={IEEE}
}

@article{yang2020characterizing,
  title={Characterizing speech adversarial examples using self-attention u-net enhancement},
  author={Yang, Chi-Hsuan and Qi, Jiliang and Chen, Pin-Yu and Ma, Xiang and Lee, Chia-Han},
  journal={arXiv preprint arXiv:2003.13917},
  year={2020}
}
@inproceedings{moosavi2017universal,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1765--1773},
  year={2017}
}

@article{su2019one,
  title={One pixel attack for fooling deep neural networks},
  author={Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={23},
  number={5},
  pages={828--841},
  year={2019},
  publisher={IEEE}
}

@inproceedings{dong2019efficient,
  title={Efficient decision-based black-box adversarial attacks on face recognition},
  author={Dong, Yue and others},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7714--7722},
  year={2019}
}

@misc{mozilla2017,
  author = {Mozilla},
  title = {Mozilla Common Voice},
  year = {2017},
  url = {https://voice.mozilla.org/en}
}

@misc{warden2017,
  author = {Warden, Pete},
  title = {Speech Commands: A Public Dataset for Single-Word Speech Recognition},
  year = {2017},
  note = {Dataset},
  url = {http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz}
}

@inproceedings{panayotov2015librispeech,
  title={Librispeech: An ASR corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}

@article{busso2008iemocap,
  title={Iemocap: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and others},
  journal={Language resources and evaluation},
  volume={42},
  number={4},
  pages={335},
  year={2008}
}

@inproceedings{zhang2017dolphinattack,
  title={Dolphinattack: Inaudible voice commands},
  author={Zhang, Guoming and Yan, Chen and Ji, Xiaoyu and Zhang, Tianchen and Zhang, Taimin and Xu, Wei},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={103--117},
  year={2017}
}

@inproceedings{yuan2018commandersong,
  title={Commandersong: A systematic approach for practical adversarial voice recognition},
  author={Yuan, Xiaoyong and others},
  booktitle={27th {USENIX} Security Symposium ({USENIX} Security 2018)},
  pages={49--64},
  year={2018}
}

@article{cisse2017houdini,
  title={Houdini: Fooling deep structured prediction models},
  author={Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
  journal={arXiv preprint arXiv:1707.05373},
  year={2017}
}
@inproceedings{vaidya2015cocaine,
  title={Cocaine noodles: exploiting the gap between human and machine speech recognition},
  author={Vaidya, Tejas and Zhang, Yang and Sherr, Micah and Shields, Clay},
  booktitle={9th {USENIX} Workshop on Offensive Technologies ({WOOT} 2015)},
  year={2015}
}

@inproceedings{carlini2016hidden,
  title={Hidden voice commands},
  author={Carlini, Nicholas and others},
  booktitle={25th {USENIX} Security Symposium ({USENIX} Security 2016)},
  pages={513--530},
  year={2016}
}

@article{hu2019adversarial,
  title={Adversarial examples for automatic speech recognition: attacks and countermeasures},
  author={Hu, Shuai and Shang, Xingjian and Qin, Zhenyu and Li, Meikang and Wang, Qi and Wang, Changqing},
  journal={IEEE Communications Magazine},
  volume={57},
  number={10},
  pages={120--126},
  year={2019},
  publisher={IEEE}
}

@article{audhkhasi2017direct,
  title={Direct acoustics-to-word models for English conversational speech recognition},
  author={Audhkhasi, Kartik and Ramabhadran, Bhuvana and Saon, George and Picheny, Michael and Nahamoo, David},
  journal={arXiv preprint arXiv:1703.07754},
  year={2017}
}

@inproceedings{xiong2018microsoft,
  title={The Microsoft 2017 conversational speech recognition system},
  author={Xiong, Wayne and Wu, Lijiang and Alleva, Francesco and Droppo, Jasha and Huang, Xuedong and Stolcke, Andreas},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5934--5938},
  year={2018},
  organization={IEEE}
}

@inproceedings{povey2011kaldi,
  title={The Kaldi speech recognition toolkit},
  author={Povey, Daniel and others},
  booktitle={IEEE 2011 Workshop on Automatic Speech Recognition and Understanding},
  organization={IEEE Signal Processing Society},
  year={2011}
}

@misc{kaldi,
  title={Kaldi},
  url={https://github.com/kaldi-asr/kaldi},
}

@article{schonherr2018adversarial,
  title={Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding},
  author={Sch{\"o}nherr, Lukas and Kohls, Kari and Zeiler, Steffen and Holz, Thorsten and Kolossa, Dorothea},
  journal={arXiv preprint arXiv:1808.05665},
  year={2018}
}

@article{hannun2014deep,
  title={Deep speech: scaling up end-to-end speech recognition},
  author={Hannun, Awni and others},
  journal={arXiv preprint arXiv:1412.5567},
  year={2014}
}

@misc{deepspeech,
  title={DeepSpeech},
  url={https://github.com/mozilla/DeepSpeech},
}

@inproceedings{du2019sirenattack,
  title={Sirenattack: generating adversarial audio for end-to-end acoustic systems},
  author={Du, Tengfei and Ji, Shouling and Li, Jiameng and Gu, Qi and Wang, Tiantian and Beyah, Raheem},
  booktitle={arXiv preprint arXiv:1901.07846},
  year={2019}
}

@inproceedings{graves2006connectionist,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd International Conference on Machine Learning},
  pages={369--376},
  year={2006}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and others},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@inproceedings{papernot2017practical,
  title={Practical black-box attacks against machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  booktitle={Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
  pages={506--519},
  year={2017}
}
@inproceedings{Goodfellow2015Explaining,
  author = {Goodfellow, I. and Shlens, J. and Szegedy, C.},
  title = {Explaining and harnessing adversarial examples},
  booktitle = {Proc. Int. Conf. Learn. Represent.},
  year = {2015},
  pages = {1--11}
}

@inproceedings{Kurakin2017Adversarial,
  author = {Kurakin, A. and Goodfellow, I. and Bengio, S.},
  title = {Adversarial examples in the physical world},
  booktitle = {Proc. ICLR Workshop},
  year = {2017},
  pages = {1--14}
}

@inproceedings{MoosaviDezfooli2016DeepFool,
  author = {Moosavi-Dezfooli, S.-M. and Fawzi, A. and Frossard, P.},
  title = {DeepFool: A simple and accurate method to fool deep neural networks},
  booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  year = {2016},
  pages = {2574--2582}
}

@inproceedings{Papernot2016Limitations,
  author = {Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and Celik, Z. B. and Swami, A.},
  title = {The limitations of deep learning in adversarial settings},
  booktitle = {Proc. IEEE Eur. Symp. Secur. Privacy (EuroS\&P)},
  year = {2016},
  pages = {372--387}
}

@inproceedings{Carlini2017Towards,
  author = {Carlini, N. and Wagner, D.},
  title = {Towards evaluating the robustness of neural networks},
  booktitle = {Proc. IEEE Symp. Secur. Privacy (SP)},
  year = {2017},
  pages = {39--57}
}

@incollection{HechtNielsen1992Theory,
  author = {Hecht-Nielsen, R.},
  title = {Theory of the backpropagation neural network},
  booktitle = {Neural Networks for Perception},
  publisher = {Elsevier},
  year = {1992},
  pages = {65--93}
}

@article{Papernot2016Distillation,
  author = {Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and Celik, Z. B. and Swami, A.},
  title = {Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks},
  journal = {Proc. IEEE Symp. Secur. Privacy (SP)},
  year = {2016},
  pages = {1--14}
}
@inproceedings{Krizhevsky2012Imagenet,
  author = {Krizhevsky, A. and Sutskever, I. and Hinton, G. E.},
  title = {Imagenet classification with deep convolutional neural networks},
  booktitle = {Proc. NeurIPS},
  year = {2012},
  pages = {1106--1114}
}

@inproceedings{Graves2013Speech,
  author = {Graves, A. and Mohamed, A.-R. and Hinton, G.},
  title = {Speech recognition with deep recurrent neural networks},
  booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process.},
  year = {2013},
  pages = {6645--6649}
}

@inproceedings{Chen2015DeepDriving,
  author = {Chen, C. and Seff, A. and Kornhauser, A. and Xiao, J.},
  title = {DeepDriving: Learning affordance for direct perception in autonomous driving},
  booktitle = {Proc. IEEE Int. Conf. Comput. Vis. (ICCV)},
  year = {2015},
  pages = {2722--2730}
}

@inproceedings{Mikolov2010Recurrent,
  author = {Mikolov, T. and Karafiát, M. and Burget, L. and Cernocký, J. and Khudanpur, S.},
  title = {Recurrent neural network based language model},
  booktitle = {Proc. INTERSPEECH},
  year = {2010},
  pages = {1045--1048}
}

@inproceedings{Szegedy2014Intriguing,
  author = {Szegedy, C. and et al.},
  title = {Intriguing properties of neural networks},
  booktitle = {Proc. ICLR},
  year = {2014},
  pages = {1--5}
}

@inproceedings{Goodfellow2015Explaining,
  author = {Goodfellow, I. J. and Shlens, J. and Szegedy, C.},
  title = {Explaining and harnessing adversarial examples},
  booktitle = {Proc. ICLR},
  year = {2015},
  pages = {1--11}
}

@inproceedings{Carlini2017Towards,
  author = {Carlini, N. and Wagner, D.},
  title = {Towards evaluating the robustness of neural networks},
  booktitle = {Proc. IEEE Symp. Secur. Privacy (SP)},
  year = {2017},
  pages = {39--57}
}

@inproceedings{Kurakin2017Adversarial,
  author = {Kurakin, A. and Goodfellow, I. J. and Bengio, S.},
  title = {Adversarial examples in the physical world},
  booktitle = {Proc. ICLR},
  year = {2017},
  pages = {1--9}
}
@inproceedings{Goodfellow2015Explaining,
  author = {Goodfellow, I. J. and Shlens, J. and Szegedy, C.},
  title = {Explaining and harnessing adversarial examples},
  booktitle = {Proc. ICLR},
  year = {2015},
  pages = {1--11}
}

@inproceedings{Kurakin2017Adversarial,
  author = {Kurakin, A. and Goodfellow, I. J. and Bengio, S.},
  title = {Adversarial examples in the physical world},
  booktitle = {Proc. ICLR},
  year = {2017},
  pages = {1--9}
}

@inproceedings{MoosaviDezfooli2016DeepFool,
  author = {Moosavi-Dezfooli, S.-M. and Fawzi, A. and Frossard, P.},
  title = {DeepFool: A simple and accurate method to fool deep neural networks},
  booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)},
  year = {2016},
  pages = {2574--2582}
}

@inproceedings{Papernot2016Limitations,
  author = {Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and Celik, Z. B. and Swami, A.},
  title = {The limitations of deep learning in adversarial settings},
  booktitle = {Proc. IEEE Eur. Symp. Secur. Privacy},
  year = {2016},
  pages = {372--387}
}

@inproceedings{Dong2018Boosting,
  author = {Dong, Y. and et al.},
  title = {Boosting adversarial attacks with momentum},
  booktitle = {Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.},
  year = {2018},
  pages = {9185--9193}
}

@inproceedings{Carlini2017Towards,
  author = {Carlini, N. and Wagner, D.},
  title = {Towards evaluating the robustness of neural networks},
  booktitle = {Proc. IEEE Symp. Secur. Privacy (SP)},
  year = {2017},
  pages = {39--57}
}

@inproceedings{Alzantot2019GenAttack,
  author = {Alzantot, M. and Sharma, Y. and Chakraborty, S. and Zhang, H. and Hsieh, C.-J. and Srivastava, M. B.},
  title = {GenAttack: Practical black-box attacks with gradient-free optimization},
  booktitle = {Proc. Genetic Evol. Comput. Conf.},
  year = {2019},
  pages = {1111--1119}
}

@inproceedings{Bhagoji2018Practical,
  author = {Bhagoji, A. N. and He, W. and Li, B. and Song, D.},
  title = {Practical black-box attacks on deep neural networks using efficient Query mechanisms},
  booktitle = {Proc. ECCV},
  year = {2018},
  pages = {158--174}
}
@inproceedings{Szegedy2014Intriguing,
  author = {Szegedy, C. and et al.},
  title = {Intriguing properties of neural networks},
  booktitle = {Proc. ICLR},
  year = {2014},
  pages = {1--5}
}

@inproceedings{Goodfellow2015Explaining,
  author = {Goodfellow, I. J. and Shlens, J. and Szegedy, C.},
  title = {Explaining and harnessing adversarial examples},
  booktitle = {Proc. ICLR},
  year = {2015},
  pages = {1--11}
}

@inproceedings{Kurakin2017Adversarial,
  author = {Kurakin, A. and Goodfellow, I. J. and Bengio, S.},
  title = {Adversarial examples in the physical world},
  booktitle = {Proc. ICLR},
  year = {2017},
  pages = {1--9}
}

@inproceedings{MoosaviDezfooli2016DeepFool,
  author = {Moosavi-Dezfooli, S.-M. and Fawzi, A. and Frossard, P.},
  title = {DeepFool: A simple and accurate method to fool deep neural networks},
  booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)},
  year = {2016},
  pages = {2574--2582}
}

@inproceedings{Papernot2016Limitations,
  author = {Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and Celik, Z. B. and Swami, A.},
  title = {The limitations of deep learning in adversarial settings},
  booktitle = {Proc. IEEE Eur. Symp. Secur. Privacy},
  year = {2016},
  pages = {372--387}
}

@inproceedings{Dong2018Boosting,
  author = {Dong, Y. and et al.},
  title = {Boosting adversarial attacks with momentum},
  booktitle = {Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.},
  year = {2018},
  pages = {9185--9193}
}

@inproceedings{Carlini2017Towards,
  author = {Carlini, N. and Wagner, D.},
  title = {Towards evaluating the robustness of neural networks},
  booktitle = {Proc. IEEE Symp. Secur. Privacy (SP)},
  year = {2017},
  pages = {39--57}
}

@inproceedings{Alzantot2019GenAttack,
  author = {Alzantot, M. and Sharma, Y. and Chakraborty, S. and Zhang, H. and Hsieh, C.-J. and Srivastava, M. B.},
  title = {GenAttack: Practical black-box attacks with gradient-free optimization},
  booktitle = {Proc. Genetic Evol. Comput. Conf.},
  year = {2019},
  pages = {1111--1119}
}

@inproceedings{Bhagoji2018Practical,
  author = {Bhagoji, A. N. and He, W. and Li, B. and Song, D.},
  title = {Practical black-box attacks on deep neural networks using efficient Query mechanisms},
  booktitle = {Proc. ECCV},
  year = {2018},
  pages = {158--174}
}

@inproceedings{Chen2017ZOO,
  author = {Chen, P.-Y. and Zhang, H. and Sharma, Y. and Yi, J. and Hsieh, C.-J.},
  title = {ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
  booktitle = {Proc. 10th ACM Workshop Artif. Intell. Secur.},
  year = {2017},
  pages = {15--26}
}

@inproceedings{Ilyas2018BlackBox,
  author = {Ilyas, A. and Engstrom, L. and Athalye, A. and Lin, J.},
  title = {Black-box adversarial attacks with limited queries and information},
  booktitle = {Proc. ICML},
  year = {2018},
  pages = {2142--2151}
}
@inproceedings{Brendel2018DecisionBased,
  author = {Brendel, W. and Rauber, J. and Bethge, M.},
  title = {Decision-based adversarial attacks: Reliable attacks against black-box machine learning models},
  booktitle = {Proc. ICLR},
  year = {2018},
  pages = {1--8}
}

@inproceedings{Cheng2019QueryEfficient,
  author = {Cheng, M. and Le, T. and Chen, P. and Zhang, H. and Yi, J. and Hsieh, C.},
  title = {Query-efficient hard-label black-box attack: An optimization-based approach},
  booktitle = {Proc. ICLR},
  year = {2019},
  pages = {1--6}
}

@article{Khalid2019REDAttack,
  author = {Khalid, F. and Ali, H. and Abdullah Hanif, M. and Rehman, S. and Ahmed, R. and Shafique, M.},
  title = {{RED-attack}: Resource efficient decision based attack for machine learning},
  year = {2019},
  eprint = {arXiv:1901.10258},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {http://arxiv.org/abs/1901.10258}
}

@inproceedings{Goodfellow2015Explaining,
  author = {Goodfellow, I. J. and Shlens, J. and Szegedy, C.},
  title = {Explaining and harnessing adversarial examples},
  booktitle = {Proc. ICLR},
  year = {2015},
  pages = {1--11}
}

@inproceedings{Carlini2018Audio,
  author = {Carlini, N. and Wagner, D.},
  title = {Audio adversarial examples: Targeted attacks on speech-to-text},
  booktitle = {Proc. IEEE SPW},
  year = {2018},
  pages = {1--7}
}

@inproceedings{Yuan2018Commandersong,
  author = {Yuan, X. and et al.},
  title = {Commandersong: A systematic approach for practical adversarial voice recognition},
  booktitle = {Proc. USENIX Secur.},
  year = {2018},
  pages = {49--64}
}

@inproceedings{Zhang2020Generating,
  author = {Zhang, H. and Zhou, P. and Yan, Q. and Liu, X.-Y.},
  title = {Generating robust audio adversarial examples with temporal dependency},
  booktitle = {Proc. Int. Joint Conf. Artif. Intell.},
  year = {2020},
  pages = {1--5}
}
@inproceedings{Vaidya2015CocaineNoodles,
  author = {Vaidya, T. and Zhang, Y. and Sherr, M. and Shields, C.},
  title = {Cocaine Noodles: Exploiting the gap between human and machine speech recognition},
  booktitle = {Workshop on Offensive Technologies},
  organization = {USENIX},
  month = aug,
  year = {2015}
}

@inproceedings{Carlini2016HiddenVoiceCommands,
  author = {Carlini, N. and Mishra, P. and Vaidya, T. and Zhang, Y. and Sherr, M. and Shields, C. and Wagner, D. A. and Zhou, W.},
  title = {Hidden voice commands},
  booktitle = {USENIX Security Symposium},
  organization = {USENIX},
  month = aug,
  year = {2016},
  pages = {513--530}
}

@inproceedings{Zhang2017DolphinAttack,
  author = {Zhang, G. and Yan, C. and Ji, X. and Zhang, T. and Zhang, T. and Xu, W.},
  title = {DolphinAttack: Inaudible voice commands},
  booktitle = {Conference on Computer and Communications Security},
  organization = {ACM},
  month = oct,
  year = {2017},
  pages = {103--117}
}

@article{Song2017InaudibleVoiceCommands,
  author = {Song, L. and Mittal, P.},
  title = {Inaudible voice commands},
  journal = {CoRR},
  volume = {abs/1708.07238},
  year = {2017},
  pages = {1--3},
  month = aug
}

@inproceedings{Roy2017BackDoor,
  author = {Roy, N. and Hassanieh, H. and Roy Choudhury, R.},
  title = {BackDoor: Making microphones hear inaudible sounds},
  booktitle = {Conference on Mobile Systems, Applications, and Services},
  organization = {ACM},
  month = jun,
  year = {2017},
  pages = {2--14}
}

@techreport{Carlini2018Audio,
  author = {Carlini, N. and Wagner, D.},
  title = {Audio adversarial examples: Targeted attacks on Speech-to-Text},
  institution = {CoRR},
  volume = {abs/1801.01944},
  year = {2018},
  pages = {1--7},
  month = jan
}

@inproceedings{Carlini2017TowardsEvaluating,
  author = {Carlini, N. and Wagner, D.},
  title = {Towards evaluating the robustness of neural networks},
  booktitle = {Symposium on Security and Privacy},
  organization = {IEEE},
  month = may,
  year = {2017},
  pages = {39--57}
}

@article{Yuan2018CommanderSong,
  author = {Yuan, X. and Chen, Y. and Zhao, Y. and Long, Y. and Liu, X. and Chen, K. and Zhang, S. and Huang, H. and Wang, X. and Gunter, C. A.},
  title = {CommanderSong: A systematic approach for practical adversarial voice recognition},
  journal = {arXiv preprint arXiv:1801.08535},
  year = {2018}
}
@inproceedings{Cisse2017Houdini,
  author = {Cisse, M. M. and Adi, Y. and Neverova, N. and Keshet, J.},
  title = {Houdini: Fooling deep structured visual and speech recognition models with adversarial examples},
  booktitle = {Proceedings of the Conference on Neural Information Processing Systems},
  pages = {6977--6987},
  year = {2017}
}

@inproceedings{Carlini2018Audio,
  author = {Carlini, N. and Wagner, D.},
  title = {Audio adversarial examples: Targeted attacks on speech-to-text},
  booktitle = {Proceedings of the IEEE Security and Privacy Workshops},
  pages = {1--7},
  year = {2018}
}

@inproceedings{Yuan2018Commandersong,
  author = {Yuan, X. and Chen, Y. and Zhao, Y. and Long, Y. and Liu, X. and Chen, K. and Zhang, S. and Huang, H. and Wang, X. and Gunter, C. A.},
  title = {Commandersong: A systematic approach for practical adversarial voice recognition},
  booktitle = {Proceedings of the USENIX Security Symposium},
  pages = {49--64},
  year = {2018}
}

@inproceedings{Schonherr2019AdversarialAttacks,
  author = {Schonherr, L. and Kohls, K. and Zeiler, S. and Holz, T. and Kolossa, D.},
  title = {Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding},
  booktitle = {Proceedings of the Network and Distributed System Security Symposium},
  pages = {1--18},
  year = {2019}
}
@article{Liu2019WeightedSampling,
  author = {Liu, X. and Zhang, X. and Wan, K. and Zhu, Q. and Ding, Y.},
  title = {Weighted-sampling audio adversarial example attack},
  year = {2019},
  note = {Available: \url{http://arxiv.org/abs/1901.10300}}
}

@inproceedings{Sriram2018Robust,
  author = {Sriram, A. and Jun, H. and Gaur, Y. and Satheesh, S.},
  title = {Robust speech recognition using generative adversarial networks},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {5639--5643},
  month = apr,
  year = {2018}
}

@inproceedings{Qin2019Imperceptible,
  author = {Qin, Y. and Carlini, N. and Cottrell, G. W. and Goodfellow, I. J. and Raffel, C.},
  title = {Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  pages = {5231--5240},
  year = {2019}
}
@inproceedings{Yuan2018Commandersong,
  author = {Yuan, X. and et al.},
  title = {Commandersong: A systematic approach for practical adversarial voice recognition},
  booktitle = {Proceedings of the USENIX Security Symposium},
  pages = {49--64},
  year = {2018}
}

@article{Alzantot2018DidYouHearThat,
  author = {Alzantot, M. and Balaji, B. and Srivastava, M.},
  title = {Did you hear that? Adversarial examples against automatic speech recognition},
  year = {2018},
  note = {Available: \url{http://arxiv.org/abs/1801.00554}}
}

@inproceedings{Taori2019TargetedAdversarialExamples,
  author = {Taori, R. and Kamsetty, A. and Chu, B. and Vemuri, N.},
  title = {Targeted adversarial examples for black box audio systems},
  booktitle = {Proceedings of the IEEE Security and Privacy Workshops (SPW)},
  pages = {15--20},
  month = may,
  year = {2019}
}

@article{Khare2018AdversarialBlackBox,
  author = {Khare, S. and Aralikatte, R. and Mani, S.},
  title = {Adversarial black-box attacks on automatic speech recognition systems using multi-objective evolutionary optimization},
  year = {2018},
  note = {Available: \url{http://arxiv.org/abs/1811.01312}}
}
@inproceedings{Vaidya2015CocaineNoodles,
  author = {Vaidya, T. and Zhang, Y. and Sherr, M. and Shields, C.},
  title = {Cocaine noodles: exploiting the gap between human and machine speech recognition},
  booktitle = {9th {USENIX} Workshop on Offensive Technologies ({WOOT} 2015)},
  year = {2015}
}

@inproceedings{Carlini2016HiddenVoiceCommands,
  author = {Carlini, N. and et al.},
  title = {Hidden voice commands},
  booktitle = {25th {USENIX} Security Symposium ({USENIX} Security 2016)},
  pages = {513--530},
  year = {2016}
}

@inproceedings{Zhang2017DolphinAttack,
  author = {Zhang, G. and et al.},
  title = {DolphinAttack: inaudible voice commands},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages = {103--117},
  year = {2017}
}
@inproceedings{Zhang2017DolphinAttack,
  author = {Zhang, G. and Yan, C. and Ji, X. and Zhang, T. and Zhang, T. and Xu, W.},
  title = {DolphinAttack: inaudible voice commands},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages = {103--117},
  year = {2017}
}

@inproceedings{Yuan2018Commandersong,
  author = {Yuan, X. and et al.},
  title = {Commandersong: a systematic approach for practical adversarial voice recognition},
  booktitle = {27th {USENIX} Security Symposium ({USENIX} Security 2018)},
  pages = {49--64},
  year = {2018}
}
@article{cisse2017houdini,
  title={Houdini: fooling deep structured prediction models},
  author={Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
  journal={arXiv preprint arXiv:1707.05373},
  year={2017}
}

@inproceedings{amodei2016deep,
  title={Deep speech 2: end-to-end speech recognition in English and mandarin},
  author={Amodei, Dario and Ananthanarayanan, Sachin and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
  booktitle={International Conference on Machine Learning},
  pages={173--182},
  year={2016},
  organization={PMLR}
}

@inproceedings{iter2019generating,
  title={Generating adversarial examples for speech recognition},
  author={Iter, Dragos and Huang, Jonny and Jermann, Michael},
  booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2019},
  organization={IEEE}
}
@techreport{stanford2017,
  title={Stanford Technical Report},
  year={2017}
}

@article{abdullah2019practical,
  title={Practical hidden voice attacks against speech and speaker recognition systems},
  author={Abdullah, Hasibullah and Garcia, William and Peeters, Charles and Traynor, Patrick and Butler, Kevin R and Wilson, Joseph},
  journal={arXiv preprint arXiv:1904.05734},
  year={2019}
}
@inproceedings{carlini2016hidden,
  title={Hidden voice commands},
  author={Carlini, Nicholas and et al.},
  booktitle={25th {USENIX} Security Symposium ({USENIX} Security 2016)},
  pages={513--530},
  year={2016}
}

@article{schonherr2018adversarial,
  title={Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding},
  author={Sch{\"o}nherr, Lukas and et al.},
  journal={arXiv preprint arXiv:1808.05665},
  year={2018}
}

@inproceedings{carlini2018audio,
  title={Audio adversarial examples: targeted attacks on speech-to-text},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={1--7},
  organization={IEEE},
  year={2018}
}

@article{qin2019imperceptible,
  title={Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  author={Qin, Yash and Carlini, Nicholas and Cottrell, Garrison and Goodfellow, Ian and Raffel, Colin},
  journal={arXiv preprint arXiv:1903.10346},
  year={2019}
}
@inproceedings{taori2019targeted,
  author = {Taori, R. and Kamsetty, A. and Chu, B. and Vemuri, N.},
  title = {Targeted adversarial examples for black box audio systems},
  booktitle = {Proc. IEEE Secur. Privacy Workshops (SPW)},
  pages = {15--20},
  year = {2019},
}

@article{alzantot2018did,
  author = {Alzantot, M. and Balaji, B. and Srivastava, M.},
  title = {Did you hear that? Adversarial examples against automatic speech recognition},
  journal = {arXiv},
  volume = {1801.00554},
  year = {2018},
}

@inproceedings{yang2008multilevel,
  author = {Yang, Z. and Tang, K. and Yao, X.},
  title = {Multilevel cooperative coevolution for large scale optimization},
  booktitle = {Proc. IEEE Congr. Evol. Comput.},
  pages = {1663--1670},
  year = {2008},
}
@inproceedings{bhagoji2018practical,
  author = {Bhagoji, A. N. and He, W. and Li, B. and Song, D.},
  title = {Practical black-box attacks on deep neural networks using efficient query mechanisms},
  booktitle = {Proc. ECCV},
  pages = {158--174},
  year = {2018},
}

@inproceedings{chen2017zoo,
  author = {Chen, P.-Y. and Zhang, H. and Sharma, Y. and Yi, J. and Hsieh, C.-J.},
  title = {ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
  booktitle = {Proc. 10th ACM Workshop Artif. Intell. Secur.},
  pages = {15--26},
  year = {2017},
}
@article{whitley1994genetic,
  author = {Whitley, D.},
  title = {A genetic algorithm tutorial},
  journal = {Statist. Comput.},
  volume = {4},
  number = {2},
  pages = {65--85},
  year = {1994},
  month = {Jun.},
  doi = {10.1007/BF00175354},
}
@book{Chaslot2010Monte,
  title={Monte-Carlo Tree Search},
  author={Chaslot, Guillaume},
  year={2010},
  publisher={Maastricht University},
  address={Maastricht, The Netherlands}
}

@article{Silver2016Mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{Schrittwieser2020Mastering,
  title={Mastering Atari, Go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{Silver2017Mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{Browne2012Survey,
  title={A survey of Monte Carlo tree search methods},
  author={Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stuart and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in Games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
  publisher={IEEE}
}

@article{Munos2014Bandits,
  title={From bandits to Monte-Carlo tree search: The optimistic principle applied to optimization and planning},
  author={Munos, R{\'e}mi},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={7},
  number={1},
  pages={1--129},
  year={2014},
  publisher={Now Publishers Inc.}
}

@inproceedings{Weinstein2012Bandit,
  title={Bandit-based planning and learning in continuous-action Markov decision processes},
  author={Weinstein, Ariel and Littman, Michael L},
  booktitle={Proc. 22nd Int. Conf. Automated Planning Scheduling},
  pages={1--9},
  year={2012},
  organization={AAAI Press}
}

@inproceedings{Mansley2011Sample,
  title={Sample-based planning for continuous action Markov decision processes},
  author={Mansley, Christopher and Weinstein, Ariel and Littman, Michael},
  booktitle={Proc. 21st Int. Conf. Automated Planning Scheduling},
  pages={1--4},
  year={2011},
  organization={AAAI Press}
}

@inproceedings{Busoniu2013Optimistic,
  title={Optimistic planning for continuous-action deterministic systems},
  author={Busoniu, Lucian and Daniels, Alex and Munos, R{\'e}mi and Babuska, Robert},
  booktitle={Proc. IEEE Symp. Adapt. Dyn. Program. Reinforcement Learn. (ADPRL)},
  pages={69--76},
  year={2013},
  organization={IEEE}
}
@inproceedings{Taori2019Targeted,
  title={Targeted adversarial examples for black box audio systems},
  author={Taori, Rohan and Kamsetty, Ambareesh and Chu, Benjamin and Vemuri, Naveen},
  booktitle={Proc. IEEE Secur. Privacy Workshops (SPW)},
  pages={15--20},
  year={2019},
  organization={IEEE}
}

@inproceedings{Bhagoji2018Practical,
  title={Practical black-box attacks on deep neural networks using efficient query mechanisms},
  author={Bhagoji, Arjun Nitin and He, Warren and Li, Bo and Song, Dawn},
  booktitle={Proc. Eur. Conf. Comput. Vis. (ECCV)},
  pages={154--169},
  year={2018},
  organization={Springer}
}

@article{Wang2021Towards,
  title={Towards query-efficient adversarial attacks against automatic speech recognition systems},
  author={Wang, Qi and Zheng, Baichen and Li, Qiuyu and Shen, Chen and Ba, Zhixiong},
  journal={IEEE Trans. Inf. Forensics Security},
  volume={16},
  pages={896--908},
  year={2021},
  publisher={IEEE}
}

@article{Mun2022Black-box,
  title={Black-box audio adversarial attack using particle swarm optimization},
  author={Mun, Hyunjae and Seo, Soojin and Son, Byeongseok and Yun, Jongseok},
  journal={IEEE Access},
  volume={10},
  pages={23532--23544},
  year={2022},
  publisher={IEEE}
}

@inproceedings{Ilyas2018Black-box,
  title={Black-box adversarial attacks with limited queries and information},
  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Ji},
  booktitle={Proc. Int. Conf. Mach. Learn.},
  pages={2137--2146},
  year={2018}
}

@inproceedings{Zhao2020Towards,
  title={Towards query-efficient black-box adversary with zeroth-order natural gradient descent},
  author={Zhao, Pengyu and Chen, Pin-Yu and Wang, Shiqi and Lin, Xiaoming},
  booktitle={Proc. AAAI Conf. Artif. Intell.},
  volume={34},
  number={4},
  pages={6909--6916},
  year={2020}
}

@article{Alzantot2018Did,
  title={Did you hear that? Adversarial examples against automatic speech recognition},
  author={Alzantot, Moustafa and Balaji, Bhavana and Srivastava, Madhuri},
  journal={arXiv preprint arXiv:1801.00554},
  year={2018}
}
@inproceedings{dalvi2004adversarial,
  title={Adversarial classification},
  author={Dalvi, N. and Domingos, P. and Sanghai, S. and Verma, D.},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  year={2004},
  pages={99--108}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, I. J. and Shlens, J. and Szegedy, C.},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{papernot2016limitations,
  title={The limitations of deep learning in adversarial settings},
  author={Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and Celik, Z. B. and Swami, A.},
  booktitle={2016 IEEE European Symposium on Security and Privacy (EuroS\&P)},
  year={2016},
  pages={372--387}
}

@inproceedings{biggio2013evasion,
  title={Evasion attacks against machine learning at test time},
  author={Biggio, B. and Corona, I. and Maiorca, D. and Nelson, B. and Šrndić, N. and Laskov, P. and Giacinto, G. and Roli, F.},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  year={2013},
  pages={387--402}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, C. and Zaremba, W. and Sutskever, I. and Bruna, J. and Erhan, D. and Goodfellow, I. and Fergus, R.},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@inproceedings{kos2018adversarial,
  title={Adversarial examples for generative models},
  author={Kos, J. and Fischer, I. and Song, D.},
  booktitle={2018 IEEE Symposium on Security and Privacy Workshops (SPW)},
  year={2018},
  pages={36--42}
}

@inproceedings{arnab2018robustness,
  title={On the robustness of semantic segmentation models to adversarial attacks},
  author={Arnab, A. and Miksik, O. and Torr, P. H.},
  booktitle={2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  pages={888--897}
}

@article{papernot2016transferability,
  title={Transferability in machine learning: from phenomena to black-box attacks using adversarial samples},
  author={Papernot, N. and McDaniel, P. and Goodfellow, I. and Jha, S. and Celik, Z. B. and Swami, A.},
  journal={arXiv preprint arXiv:1605.07277},
  year={2016}
}

@inproceedings{papernot2017practical,
  title={Practical black-box attacks against machine learning},
  author={Papernot, N. and McDaniel, P. and Goodfellow, I. and Jha, S. and Celik, Z. B. and Swami, A.},
  booktitle={2017 ACM Asia Conference on Computer and Communications Security (ASIACCS)},
  year={2017},
  pages={506--519}
}

@article{jiang2020poisoning,
  title={Poisoning and evasion attacks against deep learning algorithms in autonomous vehicles},
  author={Jiang, W. and Li, H. and Liu, S. and Luo, X. and Lu, R.},
  journal={IEEE Transactions on Vehicular Technology},
  volume={69},
  number={4},
  pages={4439--4449},
  year={2020}
}

@inproceedings{jiang2019flexible,
  title={A flexible poisoning attack against machine learning},
  author={Jiang, W. and Li, H. and Liu, S. and Ren, Y. and He, M.},
  booktitle={2019 IEEE International Conference on Communications (ICC)},
  year={2019},
  pages={1--6}
}
@inproceedings{Saravanan2022GPT3,
  author = {Shruti Saravanan and K. Shudha},
  title = {{GPT-3 Powered System for Content Generation and Transformation}},
  booktitle = {{Fifth International Conference on Computational Intelligence and Communication Technologies (CCICT)}},
  year = {2022},
  doi = {10.1109/CCiCT56684.2022.00096}
}
@misc{ttsreader,
  title = {{Text to speech online}},
  howpublished = {\url{https://ttsreader.com/}}
}

@article{alzantot2018did,
  title = {Did you hear that? adversarial examples against automatic speech recognition},
  author = {Alzantot, Moustafa and Balaji, Bharathan and Srivastava, Mani},
  journal = {arXiv preprint arXiv:1801.00554},
  year = {2018}
}

@inproceedings{audhkhasi2018building,
  title = {Building competitive direct acoustics-to-word models for English conversational speech recognition},
  author = {Audhkhasi, Kartik and Kingsbury, Brian and Ramabhadran, Bhuvana and Saon, George and Picheny, Michael},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {4759--4763},
  year = {2018},
  month = {April},
  publisher = {IEEE}
}

@inproceedings{carlini2018audio,
  title = {Audio adversarial examples: Targeted attacks on speech-to-text},
  author = {Carlini, Nicholas and Wagner, David},
  booktitle = {2018 IEEE Security and Privacy Workshops (SPW)},
  pages = {1--7},
  year = {2018},
  publisher = {IEEE}
}

@inproceedings{cisse2017houdini,
  title = {Houdini: Fooling deep structured visual and speech recognition models with adversarial examples},
  author = {Cisse, Moustapha M and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
  booktitle = {Advances in Neural Information Processing Systems 30},
  pages = {6977--6987},
  year = {2017},
  editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna and Fergus, Rob and Vishwanathan, S.V.N. and Garnett, Roman},
  publisher = {Curran Associates, Inc.}
}

@book{davis1991handbook,
  title = {Handbook of genetic algorithms},
  author = {Davis, Lawrence},
  year = {1991}
}

@article{goodfellow2014explaining,
  title = {Explaining and harnessing adversarial examples},
  author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal = {arXiv preprint arXiv:1412.6572},
  year = {2014}
}

@article{hannun2014deep,
  title = {Deep speech: Scaling up end-to-end speech recognition},
  author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and others},
  journal = {arXiv preprint arXiv:1412.5567},
  year = {2014}
}

@inproceedings{panayotov2015librispeech,
  title = {Librispeech: An ASR corpus based on public domain audio books},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {5206--5210},
  year = {2015},
  publisher = {IEEE}
}

@techreport{povey2011kaldi,
  title = {The Kaldi Speech Recognition Toolkit},
  author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and others},
  year = {2011},
  institution = {IEEE Signal Processing Society}
}

@article{schonherr2018adversarial,
  title = {Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding},
  author = {Schonherr, Lea and Kohls, Katharina and Zeiler, Steffen and Holz, Thorsten and Kolossa, Dorothea},
  journal = {arXiv preprint arXiv:1808.05665},
  year = {2018}
}

@article{taori2018targeted,
  title = {Targeted adversarial examples for black box audio systems},
  author = {Taori, Rohan and Kamsetty, Amog and Chu, Brenton and Vemuri, Nikita},
  journal = {arXiv preprint arXiv:1805.07820},
  year = {2018}
}

@inproceedings{xiong2018microsoft,
  title = {The Microsoft 2017 conversational speech recognition system},
  author = {Xiong, W. and Wu, L. and Alleva, F. and Droppo, J. and Huang, X. and Stolcke, A.},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {5934--5938},
  year = {2018},
  month = {April},
  publisher = {IEEE}
}

@inproceedings{yuan2018commandersong,
  title = {Commandersong: A systematic approach for practical adversarial voice recognition},
  author = {Yuan, Xuejing and Chen, Yuxuan and Zhao, Yue and Long, Yunhui and Liu, Xiaokang and Chen, Kai and Zhang, Shengzhi and Huang, Heqing and Wang, Xiaofeng and Gunter, Carl A},
  booktitle = {27th {USENIX} Security Symposium ({USENIX} Security 18)},
  pages = {49--64},
  year = {2018}
}
@book{davis1991handbook,
  author = {Davis, Lawrence},
  year = {1991},
  title = {Handbook of Genetic Algorithms},
  publisher = {Butterworth-Heinemann},
  address = {Boston},
  edition = {1st},
  pages = {385},
  isbn = {978-0442001735}
}
@inproceedings{eykholt2018robust,
  author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  title = {Robust Physical-world Attacks on Deep Learning Models},
  booktitle = {Proc. CVPR},
  year = {2018},
  pages = {1625--1634},
  address = {Salt Lake City, UT},
}

@inproceedings{goodfellow2015explaining,
  author = {Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
  title = {Explaining and Harnessing Adversarial Examples},
  booktitle = {Proc. ICLR},
  year = {2015},
  pages = {11},
  address = {San Diego, CA},
}

@article{su2019one,
  author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  title = {One Pixel Attack for Fooling Deep Neural Networks},
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {23},
  number = {5},
  pages = {828--841},
  year = {2019},
  month = {Jan},
}

@inproceedings{chan2019survey,
  author = {Chan, Leong and Morgan, Ian and Simon, Hayden and Alshabanat, Fares and Ober, Devin and Gentry, James and Min, David and Cao, Renzhi},
  title = {Survey of AI in Cybersecurity for Information Technology Management},
  booktitle = {Proc. TEMSCON},
  year = {2019},
  pages = {8},
  address = {Atlanta, GA},
}

@inproceedings{gong2017crafting,
  author = {Gong, Yuan and Poellabauer, Christian},
  title = {Crafting Adversarial Examples for Speech Paralinguistics Applications},
  booktitle = {Proc. DYNAMICS},
  year = {2017},
  pages = {8},
  address = {San Juan, PR},
}
@inproceedings{xie2017adversarial,
  author = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Zhou, Yuyin and Xie, Lingxi and Yuille, Alan},
  title = {Adversarial Examples for Semantic Segmentation and Object Detection},
  booktitle = {Proc. ICCV},
  year = {2017},
  pages = {1369--1378},
  address = {Venice, Italy},
}

@inproceedings{chen2018attacking,
  author = {Chen, Hongge and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Hsieh, Cho-Jui},
  title = {Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning},
  booktitle = {Proc. ACL},
  year = {2018},
  pages = {2587--2597},
  address = {Melbourne, Australia},
}
@inproceedings{zhang2014intelligent,
  author = {Zhang, Li and Hossain, Alamgir and Jiang, Ming},
  title = {Intelligent Facial Action and Emotion Recognition for Humanoid Robots},
  booktitle = {Proc. IJCNN},
  year = {2014},
  pages = {739--746},
  address = {Beijing, China},
}

@article{whitehill2014faces,
  author = {Whitehill, Jacob and Serpell, Zewelanji and Lin, Yi-Ching and Foster, Aysha and Movellan, Javier},
  title = {The Faces of Engagement: Automatic Recognition of Student Engagement from Facial Expressions},
  journal = {IEEE Transactions on Affective Computing},
  volume = {5},
  number = {1},
  pages = {86--98},
  month = apr,
  year = {2014},
}

@article{welch2012physiological,
  author = {Welch, Karla},
  title = {Physiological Signals of Autistic Children Can Be Useful},
  journal = {IEEE Instrumentation & Measurement Magazine},
  volume = {15},
  number = {1},
  pages = {28--32},
  month = feb,
  year = {2012},
}

@inproceedings{han2019implicit,
  author = {Han, Jing and Zhang, Zixing and Ren, Zhao and Schuller, Björn},
  title = {Implicit Fusion by Joint Audiovisual Training for Emotion Recognition in Mono Modality},
  booktitle = {Proc. ICASSP},
  year = {2019},
  pages = {5861--5865},
  address = {Brighton, UK},
}

@article{han2019emobed,
  author = {Han, Jing and Zhang, Zixing and Ren, Zhao and Schuller, Björn},
  title = {EmoBed: Strengthening Monomodal Emotion Recognition via Training with Crossmodal Emotion Embeddings},
  journal = {IEEE Transactions on Affect Computing},
  year = {2019},
  month = jul,
  note = {12 pages},
}

@article{schuller2018speech,
  author = {Schuller, Björn},
  title = {Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends},
  journal = {Communications of the ACM},
  volume = {61},
  number = {5},
  pages = {90--99},
  month = may,
  year = {2018},
}

@article{zhang2017speech,
  author = {Zhang, Shiqing and Zhang, Shiliang and Huang, Tiejun and Gao, Wen},
  title = {Speech Emotion Recognition using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching},
  journal = {IEEE Transactions on Multimedia},
  volume = {20},
  number = {6},
  pages = {1576--1590},
  month = oct,
  year = {2017},
}
@article{schuller2018speech,
  author = {Schuller, Björn},
  title = {Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends},
  journal = {Communications of the ACM},
  volume = {61},
  number = {5},
  pages = {90--99},
  month = may,
  year = {2018},
}

@inproceedings{mirsamadi2017automatic,
  author = {Mirsamadi, Seyedmahdad and Barsoum, Emad and Zhang, Cha},
  title = {Automatic Speech Emotion Recognition using Recurrent Neural Networks with Local Attention},
  booktitle = {Proc. ICASSP},
  year = {2017},
  pages = {2227--2231},
  address = {New Orleans, LA},
}

@article{zhao2019exploring,
  author = {Zhao, Ziping and Bao, Zhongtian and Zhao, Yiqin and Zhang, Zixing and Cummins, Nicholas and Ren, Zhao and Schuller, Björn},
  title = {Exploring Deep Spectrum Representations via Attention-based Recurrent and Convolutional Neural Networks for Speech Emotion Recognition},
  journal = {IEEE Access},
  volume = {7},
  pages = {97515--97525},
  month = jul,
  year = {2019},
}

@inproceedings{schmitt2018deep,
  author = {Schmitt, Maximilian and Schuller, Björn},
  title = {Deep Recurrent Neural Networks for Emotion Recognition in Speech},
  booktitle = {Proc. DAGA},
  year = {2018},
  pages = {4},
  address = {Munich, Germany},
}

@article{zhang2017speech,
  author = {Zhang, Shiqing and Zhang, Shiliang and Huang, Tiejun and Gao, Wen},
  title = {Speech Emotion Recognition using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching},
  journal = {IEEE Transactions on Multimedia},
  volume = {20},
  number = {6},
  pages = {1576--1590},
  month = oct,
  year = {2017},
}

@inproceedings{triantafyllopoulos2019towards,
  author = {Triantafyllopoulos, Andreas and Keren, Gil and Wagner, Johannes and Steiner, Ingmar and Schuller, Björn},
  title = {Towards Robust Speech Emotion Recognition using Deep Residual Networks for Speech Enhancement},
  booktitle = {Proc. INTERSPEECH},
  year = {2019},
  pages = {1691--1695},
  address = {Graz, Austria},
}
@inproceedings{zhao2018attention,
  author = {Ren, Zhao and Kong, Qiuqiang and Qian, Kun and Plumbley, Mark and Schuller, Björn},
  title = {Attention-based Convolutional Neural Networks for Acoustic Scene Classification},
  booktitle = {Proc. DCASE},
  year = {2018},
  pages = {39--43},
  address = {Surrey, UK},
}
@inproceedings{szegedy2017inception,
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
  booktitle = {Proc. AAAI},
  year = {2017},
  pages = {4278--4284},
  address = {San Francisco, CA},
}
@inproceedings{he2016deep,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proc. CVPR},
  year = {2016},
  pages = {770--778},
  address = {Las Vegas, NV},
}

@inproceedings{simonyan2015very,
  author = {Simonyan, Karen and Zisserman, Andrew},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {Proc. ICLR},
  year = {2015},
  pages = {10 pages},
  address = {San Diego, CA},
}
@inproceedings{TaraSainath2013,
  author = {Tara Sainath and Abdel-rahman Mohamed and Brian Kingsbury and Bhuvana Ramabhadran},
  title = {Deep convolutional neural networks for LVCSR},
  booktitle = {Proc. ICASSP},
  year = {2013},
  pages = {8614--8618},
  address = {Vancouver, Canada}
}

@inproceedings{AliceBaird2019,
  author = {Alice Baird and Amiriparian Shahin and Bj{\"o}rn Schuller},
  title = {Can deep generative audio be emotional? Towards an approach for personalised emotional audio generation},
  booktitle = {Proc. MMSP},
  year = {2019},
  pages = {5},
  address = {Kuala Lumpur, Malaysia}
}
@inproceedings{Das2018Adagio,
  title={Adagio: Interactive experimentation with adversarial attack and defense for audio},
  author={Das, Nilaksh and Carlini, Nicholas and Schumann, Johann-Markus and Schulam, Peter and Papernot, Nicolas},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={499--503},
  year={2018},
  organization={Springer}
}
@inproceedings{Qin2019Imperceptible,
  title={Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  author={Qin, Yash and Carlini, Nicholas and Cottrell, Garrison and Goodfellow, Ian J and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={5231--5240},
  year={2019},
  organization={PMLR}
}

@inproceedings{Schonherr2020Adversarial,
  title={Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding},
  author={Sch{\"o}nherr, Ludwig and Kohls, Kevin and Zeiler, Sebastian and Horsmann, Tobias and G{\"u}ney, Ibrahim and Holz, Thorsten},
  booktitle={arXiv preprint arXiv:1808.05665},
  year={2020}
}

@article{Liu2019Adversarial,
  title={Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding},
  author={Sch{\"o}nherr, Lea and Zeiler, Sebastian and Rohdenburg, Tobias and Horsmann, Tobias and Kohls, Kevin and G{\"u}ney, Ibrahim and Holz, Thorsten and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1808.05665},
  year={2019}
}

@article{Neekhara2019Universal,
  title={Universal adversarial perturbations for speech recognition systems},
  author={Neekhara, Paarth and Garg, Anish and Zhang, Moustapha Cisse and Bengio, Yoshua and Mitzenmacher, Michael},
  journal={arXiv preprint arXiv:1905.03828},
  year={2019}
}
@inproceedings{Li2020Advpulse,
  title={Advpulse: Universal, synchronization-free, and targeted audio adversarial attacks via subsecond perturbations},
  author={Li, Zhuohang and Ma, Luming and Xie, Sijia and Li, Jingyuan and Xia, Shuang and Zhang, Xiaofeng},
  booktitle={Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
  year={2020}
}

@article{Lu2021Exploring,
  title={Exploring targeted universal adversarial perturbations to end-to-end ASR models},
  author={Lu, Zhiyun and Li, Bo and Jin, Zhiqi and Du, Junru and Du, Jian},
  journal={arXiv preprint arXiv:2104.02757},
  year={2021}
}
@inproceedings{Biggio2013Evasion,
  author = {Biggio, Battista and et al.},
  title = {Evasion attacks against machine learning at test time},
  booktitle = {Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases},
  year = {2013},
  pages = {387--402}
}

@inproceedings{Papernot2016Limitations,
  author = {Papernot, Nicolas and et al.},
  title = {The limitations of deep learning in adversarial settings},
  booktitle = {Proc. IEEE Eur. Symp. Secur. Privacy (EuroSP)},
  year = {2016},
  pages = {372--387}
}

@article{Goodfellow2015Explaining,
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  title = {Explaining and harnessing adversarial examples},
  year = {2015},
  eprint = {arXiv:1412.6572}
}

@article{Sun2019Adversarial,
  author = {Sun, Shiyin and Guo, Peng and Xie, Lei and Hwang, Mei-Yuh},
  title = {Adversarial regularization for attention based end-to-end robust speech recognition},
  journal = {IEEE/ACM Trans. Audio, Speech, Language Process.},
  volume = {27},
  number = {11},
  pages = {1826--1838},
  month = nov,
  year = {2019}
}

@article{Szegedy2013Intriguing,
  author = {Szegedy, Christian and et al.},
  title = {Intriguing properties of neural networks},
  year = {2013},
  eprint = {arXiv:1312.6199}
}

@inproceedings{Kos2018Adversarial,
  author = {Kos, Jernej and Fischer, Ian and Song, Dawn},
  title = {Adversarial examples for generative models},
  booktitle = {Proc. IEEE Secur. Privacy Workshops (SPW)},
  year = {2018},
  pages = {36--42}
}

@inproceedings{Arnab2018Robustness,
  author = {Arnab, Anurag and Miksik, Ondrej and Torr, Philip H. S.},
  title = {On the robustness of semantic segmentation models to adversarial attacks},
  booktitle = {Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.},
  year = {2018},
  pages = {888--897}
}

@inproceedings{Sharif2016Accessorize,
  author = {Sharif, Mahmood and Bhagavatula, Shivam and Bauer, Lujo and Reiter, Michael K.},
  title = {Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition},
  booktitle = {Proc. ACM Sigsac Conf. Comput. Commun. Secur.},
  year = {2016},
  pages = {1528--1540}
}

@inproceedings{Zhang2020Blackbox,
  author = {Zhang, Yang and Jiang, Zhe and Villalba, Jesús and Dehak, Najim},
  title = {Black-box attacks on spoofing countermeasures using transferability of adversarial examples},
  booktitle = {Proc. INTERSPEECH},
  year = {2020},
  pages = {4238--4242}
}

@inproceedings{Li2020Textshield,
  author = {Li, Jian and et al.},
  title = {Textshield: Robust text classification based on multimodal embedding and neural machine translation},
  booktitle = {Proc. 29th USENIX Secur. Symp.},
  year = {2020},
  pages = {1381--1398}
}

@article{Li2018TextBugger,
  author = {Li, Ji and et al.},
  title = {TextBugger: Generating adversarial text against real-world applications},
  year = {2018},
  eprint = {arXiv:1812.05271}
}

@article{Hu2019Adversarial,
  author = {Hu, Shuangjiang and et al.},
  title = {Adversarial examples for automatic speech recognition: Attacks and countermeasures},
  journal = {IEEE Commun. Mag.},
  volume = {57},
  number = {10},
  pages = {120--126},
  month = oct,
  year = {2019}
}

@inproceedings{Du2020SirenAttack,
  author = {Du, Tao and et al.},
  title = {SirenAttack: Generating adversarial audio for end-to-end acoustic systems},
  booktitle = {Proc. 15th ACM Asia Conf. Comput. Commun. Secur.},
  year = {2020},
  pages = {357--369}
}

@article{Karpagavalli2011Automatic,
  author = {Karpagavalli, S. and et al.},
  title = {Automatic speech recognition: Architecture, methodologies and challenges—A review},
  journal = {Int. J. Adv. Res. Comput. Sci.},
  volume = {2},
  number = {6},
  pages = {326--331},
  year = {2011}
}

@inproceedings{Qin2019Imperceptible,
  author = {Qin, Yushi and Carlini, Nicholas and Cottrell, Garrison and Goodfellow, Ian and Raffel, Colin},
  title = {Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  booktitle = {Proc. Int. Conf. Mach. Learn.},
  year = {2019},
  pages = {5231--5240}
}

@article{Wang2021Towards,
  author = {Wang, Qi and Zheng, Bin and Li, Qian and Shen, Chao and Ba, Zhanxing},
  title = {Towards query-efficient adversarial attacks against automatic speech recognition systems},
  journal = {IEEE Trans. Inf. Forensics Security},
  volume = {16},
  pages = {896--908},
  year = {2021}
}

@inproceedings{Carlini2017Towards,
  author = {Carlini, Nicholas and Wagner, David},
  title = {Towards evaluating the robustness of neural networks},
  booktitle = {Proc. IEEE Symp. Secur. Privacy (SP)},
  year = {2017},
  pages = {39--57}
}

@book{Kurakin2018Adversarial,
  author = {Kurakin, Alexey and Goodfellow, Ian J. and Bengio, Samy},
  title = {Adversarial examples in the physical world},
  publisher = {CRC Press},
  year = {2018}
}

@article{Chen2017Zoo,
  author = {Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jun and Hsieh, Cho-Jui},
  title = {Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
  booktitle = {Proc. 10th ACM Workshop Artif. Intell. Secur.},
  year = {2017},
  pages = {15--26}
}

@inproceedings{Ilyas2018Blackbox,
  author = {Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Ji and et al.},
  title = {Black-box adversarial attacks with limited queries and information},
  booktitle = {Proc. Int. Conf. Mach. Learn.},
  year = {2018},
  pages = {2137--2146}
}

@inproceedings{Bhagoji2018Practical,
  author = {Bhagoji, Arjun N. and He, Bo and Li, Bo and Song, Dawn},
  title = {Practical black-box attacks on deep neural networks using efficient query mechanisms},
  booktitle = {Proc. Eur. Conf. Comput. Vis. (ECCV)},
  year = {2018},
  pages = {154--169}
}

@article{Zhou2019Hidden,
  author = {Zhou, Mingrui and Qin, Zhan and Lin, Xuefeng and Hu, Shuangjiang and Wang, Qian and Ren, Kui},
  title = {Hidden voice commands: Attacks and defenses on the VCS of autonomous driving cars},
  journal = {IEEE Wireless Commun.},
  volume = {26},
  number = {5},
  pages = {128--133},
  month = oct,
  year = {2019}
}
@INPROCEEDINGS{Zhang2022AdversarialOverview,
  author={Zhang, Xiao and Tan, Hao and Huang, Xuan and Zhang, Denghui and Tang, Keke and Gu, Zhaoquan},
  booktitle={2022 7th IEEE International Conference on Data Science in Cyberspace (DSC)},
  title={Adversarial Example Attacks against ASR Systems: An Overview},
  year={2022},
  volume={},
  number={},
  pages={470-477},
  doi={10.1109/DSC55868.2022.00071}
}
@article{Aldaghri2021Coded,
  author = {Aldaghri, Nabeel and Mahdavifar, Hessam and Beirami, Ahmad},
  title = {Coded Machine Unlearning},
  journal = {IEEE Access},
  volume = {9},
  pages = {88137--88150},
  year = {2021},
  doi = {10.1109/ACCESS.2021.3090019}
}

@article{Chundawat2023ZeroShot,
  author = {Chundawat, Vaibhav S. and Tarun, A. K. and Mandal, Monimoy and Kankanhalli, Mohan},
  title = {Zero-Shot Machine Unlearning},
  journal = {IEEE Transactions on Information Forensics and Security},
  volume = {18},
  pages = {2345--2354},
  year = {2023},
  doi = {10.1109/TIFS.2023.3265506}
}

@inproceedings{Cao2015Towards,
  author = {Cao, Yang and Yang, Jun},
  title = {Towards Making Systems Forget with Machine Unlearning},
  booktitle = {2015 IEEE Symposium on Security and Privacy},
  year = {2015},
  pages = {463--480},
  doi = {10.1109/SP.2015.35}
}

@inproceedings{Liu2022Backdoor,
  author = {Liu, Yang and et al.},
  title = {Backdoor Defense with Machine Unlearning},
  booktitle = {IEEE INFOCOM 2022 - IEEE Conference on Computer Communications},
  year = {2022},
  pages = {280--289},
  doi = {10.1109/INFOCOM48880.2022.9796974}
}

@inproceedings{Bourtoule2021Machine,
  author = {Bourtoule, Léo and et al.},
  title = {Machine Unlearning},
  booktitle = {2021 IEEE Symposium on Security and Privacy (SP)},
  year = {2021},
  pages = {141--159},
  doi = {10.1109/SP40001.2021.00019}
}
@INPROCEEDINGS{Zhang2020TowardsPATE,
  author={Zhang, Qiuchen and Ma, Jing and Lou, Jian and Xiong, Li and Jiang, Xiaoqian},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)},
  title={Towards Training Robust Private Aggregation of Teacher Ensembles Under Noisy Labels},
  year={2020},
  volume={},
  number={},
  pages={1103-1110},
  doi={10.1109/BigData50022.2020.9378234}
}
@INPROCEEDINGS{Yang2023PateE2E,
  author={Yang, Chao-Han Huck and Chen, I-Fan and Stolcke, Andreas and Siniscalchi, Sabato Marco and Lee, Chin-Hui},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)},
  title={An Experimental Study on Private Aggregation of Teacher Ensemble Learning for End-to-End Speech Recognition},
  year={2023},
  volume={},
  number={},
  pages={1074-1080},
  doi={10.1109/SLT54892.2023.10023326}
}
@inproceedings{Cisse2017Houdini,
  author = {Cisse, Moustapha M. and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
  title = {Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples},
  booktitle = {Proc. Adv. Neural Inf. Process. Syst.},
  year = {2017},
  pages = {1--11}
}

@inproceedings{Yuan2018Commandersong,
  author = {Yuan, Xiaokui and et al.},
  title = {Commandersong: A Systematic Approach for Practical Adversarial Voice Recognition},
  booktitle = {Proc. USENIX Conf. Secur. Symp.},
  year = {2018},
  pages = {49--64}
}

@inproceedings{Ravanelli2019The,
  author = {Ravanelli, Mirco and Parcollet, Titouan and Bengio, Yoshua},
  title = {The PyTorch-Kaldi Speech Recognition Toolkit},
  booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)},
  year = {2019},
  pages = {6465--6469}
}

@article{Hannun2014Deep,
  author = {Hannun, Andrew and et al.},
  title = {Deep Speech: Scaling Up End-to-End Speech Recognition},
  year = {2014},
  eprint = {1412.5567}
}

@inproceedings{Zheng2021Blackbox,
  author = {Zheng, Binbin and et al.},
  title = {Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information},
  booktitle = {Proc. ACM SIGSAC Conf. Comput. Commun. Secur.},
  year = {2021},
  pages = {86--107}
}

@inproceedings{Zhang2020Blackbox,
  author = {Zhang, Yanqing and Jiang, Zhengyuan and Villalba, Javier and Dehak, Najim},
  title = {Black-Box Attacks on Spoofing Countermeasures using Transferability of Adversarial Examples},
  booktitle = {Proc. INTERSPEECH},
  year = {2020},
  pages = {4238--4242}
}

@inproceedings{Zhang2021Generating,
  author = {Zhang, Yanqing and Li, Huaxiu and Xu, Guofei and Luo, Xin and Dong, Guangyu},
  title = {Generating Audio Adversarial Examples with Ensemble Substituted Models},
  booktitle = {Proc. IEEE Int. Conf. Commun. (ICC)},
  year = {2021},
  pages = {1--6}
}

@inproceedings{Chen2020Devils,
  author = {Chen, Yulong and et al.},
  title = {Devil's Whisper: A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices},
  booktitle = {Proc. USENIX Secur. Symp.},
  year = {2020},
  pages = {2667--2684}
}
@article{Mun2022Blackbox,
  author = {Mun, Hyunwook and Seo, Sangwon and Son, Byeongil and Yun, Jihyeok},
  title = {Black-box Audio Adversarial Attack Using Particle Swarm Optimization},
  journal = {IEEE Access},
  volume = {10},
  year = {2022},
  pages = {23532--23544}
}

@inproceedings{Zong2021Blackbox,
  author = {Zong, Weikai and Chow, Yun-Wei and Susilo, Willy},
  title = {Black-box Audio Adversarial Example Generation Using Variational Autoencoder},
  booktitle = {Proc. Int. Conf. Inf. Commun. Secur.},
  year = {2021},
  pages = {142--160}
}

@article{Alzantot2018Did,
  author = {Alzantot, Mo and Balaji, Bhargav and Srivastava, Madhur},
  title = {Did You Hear That? Adversarial Examples Against Automatic Speech Recognition},
  year = {2018},
  eprint = {1801.00554}
}

@inproceedings{Taori2019Targeted,
  author = {Taori, Rishabh and Kamsetty, Aishwarya and Chu, Brian and Vemuri, Naveen},
  title = {Targeted Adversarial Examples for Black Box Audio Systems},
  booktitle = {Proc. IEEE Secur. Privacy Workshops (SPW)},
  year = {2019},
  pages = {15--2}
}

@article{Wang2021Towards,
  author = {Wang, Qiao and Zheng, Binbin and Li, Qian and Shen, Chaoyi and Ba, Zhijie},
  title = {Towards Query-Efficient Adversarial Attacks Against Automatic Speech Recognition Systems},
  journal = {IEEE Trans. Inf. Forensics Security},
  volume = {16},
  year = {2021},
  pages = {896--908}
}

@incollection{Kurakin2018Adversarial,
  author = {Kurakin, Alexey and Goodfellow, Ian J. and Bengio, Samy},
  title = {Adversarial Examples in the Physical World},
  booktitle = {Artificial Intelligence Safety and Security},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},
  year = {2018}
}
@article{Kereliuk2015Deep,
  author = {Kereliuk, Corey and Sturm, Bob L. and Larsen, Jan},
  title = {Deep Learning and Music Adversaries},
  journal = {IEEE Transactions on Multimedia},
  volume = {17},
  number = {11},
  month = {November},
  year = {2015},
  pages = {2059--2071}
}
@inproceedings{Vaidya2015Cocaine,
  author = {Vaidya, Tavish and et al.},
  title = {Cocaine Noodles: Exploiting the Gap Between Human and Machine Speech Recognition},
  booktitle = {9th USENIX Workshop on Offensive Technologies (WOOT)},
  year = {2015}
}

@inproceedings{Carlini2016Hidden,
  author = {Carlini, Nicholas and et al.},
  title = {Hidden Voice Commands},
  booktitle = {25th USENIX Security Symposium (USENIX Security)},
  year = {2016}
}

@inproceedings{Zhang2017Dolphinattack,
  author = {Zhang, Guoming and et al.},
  title = {DolphinAttack: Inaudible Voice Commands},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  year = {2017}
}

@inproceedings{Roy2018Inaudible,
  author = {Roy, Nirupam and et al.},
  title = {Inaudible Voice Commands: The Long-Range Attack and Defense},
  booktitle = {15th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year = {2018}
}

@article{Abdullah2019Practical,
  author = {Abdullah, Hadi and et al.},
  title = {Practical Hidden Voice Attacks Against Speech and Speaker Recognition Systems},
  journal = {arXiv preprint arXiv:1904.05734},
  year = {2019},
  note = {Unpublished}
}

@inproceedings{Abdullah2021Hear,
  author = {Abdullah, Hadi and et al.},
  title = {Hear No Evil, See Kenansville: Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems},
  booktitle = {2021 IEEE Symposium on Security and Privacy (SP)},
  year = {2021},
  organization = {IEEE}
}
@misc{ISO1993Information,
  title = {Information Technology - Coding of Moving Pictures and Associated Audio for Digital Storage Media at up to 1.5 Mbits/s - Part 3: Audio},
  author = {{International Organization for Standardization}},
  year = {1993},
  note = {ISO 11172-3}
}
@book{Zwicker2007Psychoacoustics,
  title = {Psychoacoustics: Facts and Models},
  author = {Zwicker, E. and Fastl, H.},
  year = {2007},
  edition = {3rd},
  publisher = {Springer}
}
@article{navarro2001guided,
  author = {Navarro, Gonzalo},
  title = {A guided tour to approximate string matching},
  journal = {ACM Computing Surveys},
  volume = {33},
  number = {1},
  pages = {31--88},
  year = {2001},
  month = {March},
  publisher = {ACM},
}
@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction APIs},
  author={Tram{\'e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={USENIX Security Symposium},
  year={2016},
  organization={USENIX}
}

@inproceedings{papernot2017practical,
  title={Practical black-box attacks against machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick D and Goodfellow, Ian J and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  booktitle={Asia Conference on Computer and Communications Security},
  year={2017},
  organization={ACM}
}

@inproceedings{wang2018stealing,
  title={Stealing hyperparameters in machine learning},
  author={Wang, Bo and Gong, Neil Zhenqiang},
  booktitle={Symposium on Security and Privacy},
  year={2018},
  organization={IEEE}
}

@article{papernot2016transferability,
  title={Transferability in machine learning: From phenomena to black-box attacks using adversarial samples},
  author={Papernot, Nicolas and McDaniel, Patrick D and Goodfellow, Ian J},
  journal={CoRR},
  volume={abs/1605.07277},
  pages={1--13},
  year={2016},
  month={May}
}

@article{juuti2018prada,
  title={PRADA: Protecting against DNN model stealing attacks},
  author={Juuti, Mikko and Szyller, Szabolcs and Dmitrenko, Alexandra and Marchal, Samuel and Asokan, N},
  journal={CoRR},
  volume={abs/1805.02628},
  pages={1--16},
  year={2018},
  month={May}
}

@inproceedings{povey2011kaldi,
  title={Kaldi: An open-source toolkit for speech recognition},
  author={Povey, Daniel and others},
  booktitle={IEEE Automatic Speech Recognition and Understanding Workshop},
  year={2011},
  organization={IEEE}
}

@article{brown2017adversarial,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={CoRR},
  volume={abs/1712.09665},
  pages={1--6},
  year={2017},
  month={Dec}
}

@inproceedings{moosavi2017universal,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2017},
  organization={IEEE},
  pages={86--94}
}
@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={Symposium on Security and Privacy},
  year={2016},
  organization={IEEE},
  pages={582--597}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={CoRR},
  volume={abs/1503.02531},
  pages={1--9},
  year={2015},
  month={Mar}
}
@inproceedings{schinkel2013audio,
  author = {Schinkel-Bielefeld, N. and Lotze, N. and Nagel, F.},
  booktitle = {International Congress on Acoustics},
  title = {Audio quality evaluation by experienced and inexperienced listeners},
  year = {2013},
  month = {Jun.},
  pages = {6--16},
  organization = {ASA}
}
@article{schaeffler2018webmushra,
  author = {Schaeffler, Maximilian and Bartoschek, Sebastian and Stöter, Fabian-Robert and Roess, Michael and Westphal, Steffen and Edler, Bernd and Herre, Jürgen},
  journal = {Journal of Open Research Software},
  title = {webMUSHRA -- A comprehensive framework for web-based listening tests},
  year = {2018},
  volume = {6},
  number = {1},
  month = {Feb.},
  publisher = {Ubiquity Press},
  doi = {10.5334/jors.208}
}
@article{lecun2015deep,
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey E.},
  title = {Deep learning},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  year = {2015},
}

@article{amini2020improving,
  author = {Amini, Seyedeh Mahsa and Ghaemmaghami, Seyed Mohammad},
  title = {Towards improving robustness of deep neural networks to adversarial perturbations},
  journal = {IEEE Transactions on Multimedia},
  volume = {22},
  number = {7},
  pages = {1889--1903},
  month = jul,
  year = {2020},
}

@article{wang2021smsnet,
  author = {Wang, Jie and et al.},
  title = {SMSNet: A new deep convolutional neural network model for adversarial example detection},
  journal = {IEEE Transactions on Multimedia},
  pages = {1},
  year = {2021},
}

@inproceedings{he2017mask,
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  title = {Mask {R-CNN}},
  booktitle = {Proc. IEEE Int. Conf. Comput. Vis.},
  year = {2017},
  pages = {2961--2969},
}

@inproceedings{graves2013speech,
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  title = {Speech recognition with deep recurrent neural networks},
  booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process.},
  year = {2013},
  pages = {6645--6649},
}

@article{xie2019speech,
  author = {Xie, Yuxuan and et al.},
  title = {Speech emotion classification using attention-based LSTM},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {11},
  pages = {1675--1685},
  month = nov,
  year = {2019},
}

@article{yusuf2019low,
  author = {Yusuf, Burak and Gundogdu, Berkay and Saraclar, Murat},
  title = {Low resource keyword search with synthesized crosslingual exemplars},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {7},
  pages = {1126--1135},
  month = jul,
  year = {2019},
}

@inproceedings{jiang2019black,
  author = {Jiang, Longyin and Ma, Xinwei and Chen, Shangfei and Bailey, James and Jiang, Yunhong G.},
  title = {Black-box adversarial attacks on video recognition models},
  booktitle = {Proc. 27th ACM Int. Conf. Multimedia},
  year = {2019},
  pages = {864--872},
}

@article{huang2020adversarial,
  author = {Huang, Tao and et al.},
  title = {Adversarial attacks on deep-learning-based radar range profile target recognition},
  journal = {Information Sciences},
  volume = {531},
  pages = {159--176},
  year = {2020},
}

@article{li2021understanding,
  author = {Li, Tiantong and et al.},
  title = {Understanding adversarial robustness via critical attacking route},
  journal = {Information Sciences},
  volume = {547},
  pages = {568--578},
  year = {2021},
}
@inproceedings{yakura2019robust,
  author = {Yakura, Hiromu and Sakuma, Jun},
  title = {Robust audio adversarial example for a physical attack},
  booktitle = {Proc. 28th Int. Joint Conf. Artif. Intell.},
  year = {2019},
  pages = {5334--5341},
}
@inproceedings{scheibler2018pyroomacoustics,
  author = {Scheibler, Robin and Bezzam, Emil and Dokmanić, Ivan},
  title = {Pyroomacoustics: A Python package for audio room simulation and array processing algorithms},
  booktitle = {Proc. IEEE Int. Conf. Acoust., Speech Signal Process.},
  year = {2018},
  pages = {351--355},
}

@inproceedings{moattar2009simple,
  author = {Moattar, Mohammad H. and Homayounpour, Mohammad M.},
  title = {A Simple but Efficient Real-Time Voice Activity Detection Algorithm},
  booktitle = {Proc. 17th Eur. Signal Process. Conf.},
  year = {2009},
  pages = {2549--2553},
}

@article{shete2014zero,
  author = {Shete, Dhanashree and Patil, Sushma and Patil, Shraddha},
  title = {Zero Crossing Rate and Energy of the Speech Signal of Devanagari Script},
  journal = {IOSR-JVSP},
  volume = {4},
  number = {1},
  pages = {1--5},
  year = {2014},
}
@inproceedings{du2020adversarial,
  author = {Du, Xingjun and Pun, Chi-Man},
  title = {Adversarial Image Attacks Using Multi-Sample and Most-Likely Ensemble Methods},
  booktitle = {Proc. 28th ACM Int. Conf. Multimedia},
  year = {2020},
  pages = {1634--1642},
}

@inproceedings{haigh1993robust,
  author = {Haigh, J. and Mason, J.},
  title = {Robust voice activity detection using cepstral features},
  booktitle = {Proc. IEEE Region 10 Int. Conf. Comput., Commun. Automat.},
  year = {1993},
  pages = {321--324},
}

@article{ariav2019end,
  author = {Ariav, I. and Cohen, I.},
  title = {An end-to-end multimodal voice activity detection using wavenet encoder and residual networks},
  journal = {IEEE J. Sel. Top. Signal Process.},
  volume = {13},
  number = {2},
  pages = {265--274},
  month = {May},
  year = {2019},
}

@inproceedings{lee2007minimum,
  author = {Lee, Bowon and Hasegawa-Johnson, M.},
  title = {Minimum mean squared error a posteriori estimation of high variance vehicular noise},
  booktitle = {Proc. Biennial DSP In-Vehicle Mobile Syst.},
  address = {Istanbul, Turkey},
  month = {Jun.},
  year = {2007},
  pages = {1--5},
}

@misc{webrtc,
  title = {WebRTC Voice Activity Detection},
  howpublished = {\url{https://webrtc.org/}},
}
@inproceedings{karmon2018lavan,
  author = {Karmon, D. and Zoran, D. and Goldberg, Y.},
  title = {LaVAN: Localized and visible adversarial noise},
  booktitle = {Proc. 35th Int. Conf. Mach. Learn.},
  year = {2018},
  pages = {2507--2515},
}

@inproceedings{brown2017adversarial,
  author = {Brown, T. B. and Mané, D. and Roy, A. and Abadi, M. and Gilmer, J.},
  title = {Adversarial patch},
  booktitle = {Proc. Mach. Learn. Comput. Secur. Workshop, Neural Inf. Process. Syst.},
  year = {2017},
  pages = {1--6},
}

@inproceedings{croce2019sparse,
  author = {Croce, F. and Hein, M.},
  title = {Sparse and imperceivable adversarial attacks},
  booktitle = {Proc. IEEE/CVF Int. Conf. Comput. Vis.},
  year = {2019},
  pages = {4724--4732},
}
@inproceedings{peddinti2015reverberation,
  author = {Peddinti, Vijayaditya and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  title = {Reverberation robust acoustic modeling using i-vectors with time delay neural networks},
  booktitle = {Proc. 16th Annu. Conf. Int. Speech Commun. Assoc.},
  year = {2015},
  pages = {1--5},
}
@inproceedings{jeub2009binaural,
  author = {Jeub, Markus and Schafer, Markus and Vary, Peter},
  title = {A binaural room impulse response database for the evaluation of dereverberation algorithms},
  booktitle = {Proc. 16th Int. Conf. Digit. Signal Process.},
  year = {2009},
  pages = {1--5},
}

@inproceedings{jeub2010do,
  author = {Jeub, Markus and et al.},
  title = {Do we need dereverberation for hand-held telephony?},
  booktitle = {Proc. Int. Congr. Acoust.},
  year = {2010},
  pages = {1--7},
}

@inproceedings{qin2019imperceptible,
  author = {Qin, Yichuan and Carlini, Nicholas and Cottrell, Garrison and Goodfellow, Ian and Raffel, Colin},
  title = {Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  booktitle = {Proc. Int. Conf. Mach. Learn.},
  year = {2019},
  pages = {5231--5240},
}

@article{schoenherr2019robust,
  author = {Schönherr, Florian and Zeiler, Steffen and Holz, Thorsten and Kolossa, Dorothea},
  title = {Robust over-the-air adversarial examples against automatic speech recognition systems},
  year = {2019},
  eprint = {arXiv:1908.01551},
}

@inproceedings{yang2019characterizing,
  author = {Yang, Zhiwei and Li, Bo and Chen, Pin-Yu and Song, Dawn},
  title = {Characterizing audio adversarial examples using temporal dependency},
  booktitle = {Proc. Int. Conf. Learn. Representations},
  year = {2019},
  pages = {1--15},
}

@article{athalye2017synthesizing,
  author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  title = {Synthesizing robust adversarial examples},
  year = {2017},
  eprint = {arXiv:1707.07397},
}

@inproceedings{eykholt2018robust,
  author = {Eykholt, Kevin and et al.},
  title = {Robust physical-world attacks on deep learning visual classification},
  booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  year = {2018},
  pages = {1625--1634},
}

@inproceedings{li2019adversarial,
  author = {Li, Jiawei and Schmidt, Fabian and Kolter, J. Zico},
  title = {Adversarial camera stickers: A physical camera-based attack on deep learning systems},
  booktitle = {Proc. Int. Conf. Mach. Learn.},
  year = {2019},
  pages = {3896--3904},
}

@article{komkov2019advhat,
  author = {Komkov, Sergey and Petiushko, Alexey},
  title = {Advhat: Real-world adversarial attack on arcface face id system},
  year = {2019},
  eprint = {arXiv:1908.08705},
}

@inproceedings{du2020adversarial,
  author = {Du, Xinyuan and Pun, Chi-Man},
  title = {Adversarial image attacks using multi-sample and most-likely ensemble methods},
  booktitle = {Proc. 28th ACM Int. Conf. Multimedia},
  year = {2020},
  pages = {1634--1642},
}
@inproceedings{Carlini2018Audio,
  author = {Carlini, Nicholas and Wagner, David},
  title = {Audio adversarial examples: Targeted attacks on speech-to-text},
  booktitle = {Proc. IEEE Secur. Privacy Workshops},
  year = {2018},
  pages = {1--7},
}

@inproceedings{Qin2019Imperceptible,
  author = {Qin, Yashesh and Carlini, Nicholas and Cottrell, Garrison and Goodfellow, Ian and Raffel, Colin},
  title = {Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  booktitle = {Proc. Int. Conf. Mach. Learn.},
  year = {2019},
  pages = {5231--5240},
}

@inproceedings{Taori2019Targeted,
  author = {Taori, Rishabh and Kamsetty, Akash and Chu, Brian and Vemuri, Neel},
  title = {Targeted adversarial examples for black box audio systems},
  booktitle = {Proc. IEEE Secur. Privacy Workshops},
  year = {2019},
  pages = {15--20},
}

@inproceedings{Haigh1993Robust,
  author = {Haigh, J. and Mason, J.},
  title = {Robust voice activity detection using cepstral features},
  booktitle = {Proc. IEEE Region 10 Int. Conf. Comput., Commun. Automat.},
  year = {1993},
  pages = {321--324},
}

@article{Ariav2019An,
  author = {Ariav, Ido and Cohen, Israel},
  title = {An end-to-end multimodal voice activity detection using wavenet encoder and residual networks},
  journal = {IEEE J. Sel. Top. Signal Process.},
  volume = {13},
  number = {2},
  pages = {265--274},
  year = {2019},
  month = {May},
}

@inproceedings{Lee2007Minimum,
  author = {Lee, Bowon and Hasegawa-Johnson, Mark},
  title = {Minimum mean squared error a posteriori estimation of high variance vehicular noise},
  booktitle = {Proc. Biennial DSP In-Vehicle Mobile Syst.},
  year = {2007},
  pages = {1--5},
}

@inproceedings{Gilg2020Methodology,
  author = {Gilg, Vincent and Beaugeant, Charles and Andrassy, Bela},
  title = {Methodology for the design of a robust voice activity detector for speech enhancement},
  booktitle = {Proc. Int. Workshop Acoust. Echo Noise Control},
  year = {2020},
  pages = {131--134},
}

@inproceedings{Jeub2009Binaural,
  author = {Jeub, Moritz and Schafer, Markus and Vary, Peter},
  title = {A binaural room impulse response database for the evaluation of dereverberation algorithms},
  booktitle = {Proc. 16th Int. Conf. Digit. Signal Process.},
  year = {2009},
  pages = {1--5},
}

@inproceedings{Nakamura2000Acoustical,
  author = {Nakamura, Shoko and Hiyane, Kohtaro and Asano, Futoshi and Nishiura, Tomohiro and Yamada, Tatsuya},
  title = {Acoustical sound database in real environments for sound scene understanding and hands-free speech recognition},
  booktitle = {Proc. Int. Conf. Language Resources Evaluation},
  year = {2000},
  pages = {965--968},
}

@inproceedings{Kinoshita2013Reverb,
  author = {Kinoshita, Keisuke and Delcroix, Marc and Yoshioka, Takuya and Nakatani, Tomohiro},
  title = {The reverb challenge: A common evaluation framework for dereverberation and recognition of reverberant speech},
  booktitle = {Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.},
  year = {2013},
  pages = {1--4},
}
@inproceedings{Alcorn2019Strike,
  author = {Alcorn, Michael A. and Li, Qi and Gong, Zhitao and Wang, Chengshu and Mai, Long and Ku, Wei-San and Nguyen, Anh},
  title = {Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2019},
  pages = {4845--4854},
}

@article{Amodei2015Deep,
  author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and others},
  title = {Deep speech 2: End-to-end speech recognition in English and Mandarin},
  journal = {arXiv preprint arXiv:1512.02595},
  year = {2015},
}

@article{Ardila2019Common,
  author = {Ardila, Rodrigo and Branson, Mike and Davis, Kele and Henretty, Miles and Kohler, Maria and Meyer, Jason and Morais, Rodrigo and Saunders, Lauren and Tyers, Francis M. and Weber, Genevieve},
  title = {Common voice: A massively-multilingual speech corpus},
  journal = {arXiv preprint arXiv:1912.06670},
  year = {2019},
}

@article{Babu2021XLSR,
  author = {Babu, Aravind and Wang, Chia-Wei and Tjandra, Andros and Lakhotia, Kiran and Xu, Qing and Goyal, Naman and Singh, Karan and von Platen, Patrick and Saraf, Yossi and Pino, Juan and others},
  title = {XLS-R: Self-supervised cross-lingual speech representation learning at scale},
  journal = {arXiv preprint arXiv:2111.09296},
  year = {2021},
}

@article{Baevski2020Wav2vec2,
  author = {Baevski, Alexei and Zhou, Hengru and Mohamed, Abdelrahman and Auli, Michael},
  title = {Wav2vec 2.0: A framework for self-supervised learning of speech representations},
  journal = {arXiv preprint arXiv:2006.11477},
  year = {2020},
}

@article{Baevski2021Unsupervised,
  author = {Baevski, Alexei and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
  title = {Unsupervised speech recognition},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {27826--27839},
  year = {2021},
}

@article{Bapna2022MSLAM,
  author = {Bapna, Ankur and Cherry, Colin and Zhang, Yuan and Jia, Ye and Johnson, Melvin and Cheng, Yu and Khanuja, Simran and Riesa, Jesse and Conneau, Alexis},
  title = {MSLAM: Massively multilingual joint pre-training for speech and text},
  journal = {arXiv preprint arXiv:2202.01374},
  year = {2022},
}

@article{Barbu2019Objectnet,
  author = {Barbu, Andrei and Mayo, David and Alverio, Justin and Luo, Wei and Wang, Chengshu and Gutfreund, Dan and Tenenbaum, Joshua and Katz, Boris},
  title = {Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  year = {2019},
}

@article{Caruana1997Multitask,
  author = {Caruana, Rich},
  title = {Multitask learning},
  journal = {Machine learning},
  volume = {28},
  number = {1},
  pages = {41--75},
  year = {1997},
}

@article{Chan2021SpeechStew,
  author = {Chan, William and Park, David and Lee, Cheng-Hao and Zhang, Yuan and Le, Quoc and Norouzi, Mohammad},
  title = {SpeechStew: Simply mix all available speech recognition data to train one large neural network},
  journal = {arXiv preprint arXiv:2104.02133},
  year = {2021},
}

@article{Chen2021Gigaspeech,
  author = {Chen, Guanlong and Chai, Shujie and Wang, Guoli and Du, Jingyun and Zhang, Wei-Qiang and Weng, Chengyuan and Su, Dandan and Povey, Daniel and Trmal, Jan and Zhang, Jia},
  title = {Gigaspeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio},
  journal = {arXiv preprint arXiv:2106.06909},
  year = {2021},
}

@inproceedings{Chen2022UnispeechSAT,
  author = {Chen, Shuo and Wu, Yiting and Wang, Chia-Wei and Chen, Zewei and Chen, Zhenzhou and Liu, Shuoyang and Wu, Jiahong and Qian, Yingbo and Wei, Fan and Li, Jia},
  title = {Unispeech-SAT: Universal speech representation learning with speaker aware pre-training},
  booktitle = {ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {6152--6156},
  year = {2022},
}

@article{Chen2016TrainingDeepNets,
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  title = {Training deep nets with sublinear memory cost},
  journal = {arXiv preprint arXiv:1604.06174},
  year = {2016},
}

@article{Chen2022Maestro,
  author = {Chen, Zhuo and Zhang, Yuan and Rosenberg, Ari and Ramabhadran, Balakrishnan and Moreno, Pedro and Bapna, Ankur and Zen, Heiga},
  title = {Maestro: Matched speech text representations through modality matching},
  journal = {arXiv preprint arXiv:2204.03409},
  year = {2022},
}
@article{Child2019Generating,
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  title = {Generating long sequences with sparse transformers},
  journal = {arXiv preprint arXiv:1904.10509},
  year = {2019},
}

@article{Collobert2011Natural,
  author = {Collobert, Ronan and Weston, Jason and Bottou, Léon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  title = {Natural language processing (almost) from scratch},
  journal = {Journal of machine learning research},
  volume = {12},
  pages = {2493--2537},
  year = {2011},
}

@article{Conneau2022Fleurs,
  author = {Conneau, Alexis and Ma, Mark and Khanuja, Simran and Zhang, Yuan and Axelrod, Vadim and Dalmia, Siddhartha and Riesa, Jesse and Rivera, Carlos and Bapna, Ankur},
  title = {Fleurs: Few-shot learning evaluation of universal representations of speech},
  journal = {arXiv preprint arXiv:2205.12446},
  year = {2022},
}

@article{DelRio2021Earnings21,
  author = {Del Rio, Marina and Delworth, Nicholas and Westerman, Rami and Huang, Minhua and Bhandari, Nidhi and Palakapilly, Justin and McNamara, Quinn and Dong, Jiayi and Zelasko, Paul and Jette, Mark},
  title = {Earnings-21: A practical benchmark for ASR in the wild},
  journal = {arXiv preprint arXiv:2104.11348},
  year = {2021},
}

@article{Galvez2021PeoplesSpeech,
  author = {Galvez, David and Diamos, Greg and Torres, Jose M. C. and Achorn, Brendan and Gopi, Adithya and Kanter, David and Lam, Melody and Mazumder, Madhur and Reddi, Vamsi J.},
  title = {The people's speech: A large-scale diverse English speech recognition dataset for commercial usage},
  journal = {arXiv preprint arXiv:2111.09344},
  year = {2021},
}

@article{Geirhos2020ShortcutLearning,
  author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  title = {Shortcut learning in deep neural networks},
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {11},
  pages = {665--673},
  year = {2020},
}

@article{Ghorbani2021ScalingLaws,
  author = {Ghorbani, Behnam and Firat, Orhan and Freitag, Markus and Bapna, Ankur and Krikun, Maxim and Garcia, Xavier and Chelba, Ciprian and Cherry, Colin},
  title = {Scaling laws for neural machine translation},
  journal = {arXiv preprint arXiv:2109.07740},
  year = {2021},
}

@article{Griewank2000Algorithm799,
  author = {Griewank, Andreas and Walther, Andrea},
  title = {Algorithm 799: Revolve: An implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  journal = {ACM Transactions on Mathematical Software (TOMS)},
  volume = {26},
  number = {1},
  pages = {19--45},
  year = {2000},
}

@article{Gunter2021ContextualizingRetraction,
  author = {Gunter, Katie and Vaughn, Charley and Kendall, Tyler},
  title = {Contextualizing/s/retraction: Sibilant variation and change in Washington DC African American language},
  journal = {Language Variation and Change},
  volume = {33},
  number = {3},
  pages = {331--357},
  year = {2021},
}

@article{Harris2020ArrayProgrammingNumPy,
  author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stefan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, SciPy, 2020. Travis CI, Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and Fernandez del Río, Jaime and Wiebe, Mark and Peterson, Pearu and Gerard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  title = {Array programming with NumPy},
  journal = {Nature},
  volume = {585},
  pages = {357--362},
  year = {2020},
  doi = {10.1038/s41586-020-2649-2},
}
@article{Hendrycks2016Gaussian,
  author = {Hendrycks, Dan and Gimpel, Kevin},
  title = {Gaussian error linear units (GELUs)},
  journal = {arXiv preprint arXiv:1606.08415},
  year = {2016},
}

@article{Hendrycks2020Pretrained,
  author = {Hendrycks, Dan and Liu, Xiaoyuan and Wallace, Eric and Dziedzic, Arkadiusz and Krishnan, Rishabh and Song, Dawn},
  title = {Pretrained transformers improve out-of-distribution robustness},
  journal = {arXiv preprint arXiv:2004.06100},
  year = {2020},
}

@inproceedings{Hernandez2018TedLium,
  author = {Hernandez, Fernando and Nguyen, Vu and Ghannay, Sahar and Tomashenko, Natalia A. and Esteve, Yannick},
  title = {Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation},
  booktitle = {SPECOM},
  year = {2018},
}

@article{Hsu2021aHubert,
  author = {Hsu, Wei-Ning and Bolte, Benoit and Tsai, Yao-Hung Hubert and Lakhotia, Karan and Salakhutdinov, Ruslan and Mohamed, Abdel-rahman},
  title = {Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {3451--3460},
  year = {2021},
}

@article{Hsu2021bRobustWav2vec2,
  author = {Hsu, Wei-Ning and Sriram, Aravind and Baevski, Alexei and Likhomanenko, Tatiana and Xu, Qing and Pratap, Vijay and Kahn, Jacob and Lee, Alexei and Collobert, Ronan and Synnaeve, Gabriel and others},
  title = {Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training},
  journal = {arXiv preprint arXiv:2104.01027},
  year = {2021},
}

@inproceedings{Huang2016DeepNetworks,
  author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
  title = {Deep networks with stochastic depth},
  booktitle = {European conference on computer vision},
  pages = {646--661},
  year = {2016},
}

@article{Jia2017AdversarialExamples,
  author = {Jia, Robin and Liang, Percy},
  title = {Adversarial examples for evaluating reading comprehension systems},
  journal = {arXiv preprint arXiv:1707.07328},
  year = {2017},
}

@article{Johnson2017GooglesNeuralMachineTranslation,
  author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viegas, Francisco and Wattenberg, Martin and Corrado, Greg and others},
  title = {Google's multilingual neural machine translation system: Enabling zero-shot translation},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {339--351},
  year = {2017},
}

@article{Kendall2021CorpusRegionalAfricanAmericanLanguage,
  author = {Kendall, Tyler and Farrington, Charlie},
  title = {The corpus of regional african american language. Version 2021.07. Eugene, OR: The Online Resources for African American Language Project. http://oraal.uoregon.edu/coraal},
  year = {2021},
  note = {Accessed: 2022-09-01},
}

@article{Koenecke2020BigTransfer,
  author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  title = {Big transfer (BiT): General visual representation learning},
  journal = {European conference on computer vision},
  pages = {491--507},
  year = {2020},
}

@article{Kuchaiev2019Nemo,
  author = {Kuchaiev, Oleksii and Li, Jonathan and Nguyen, Hao and Hrinchuk, Oleksii and Leary, Ronan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Svyatoslav and Lavrukhin, Vassily and Cook, Jeremy and others},
  title = {Nemo: A toolkit for building AI applications using neural modules},
  journal = {arXiv preprint arXiv:1909.09577},
  year = {2019},
}

@article{Lake2017MachinesLearnThink,
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  title = {Building machines that learn and think like people},
  journal = {Behavioral and brain sciences},
  volume = {40},
  year = {2017},
}

@inproceedings{Liao2013AcousticModelingSemiSupervised,
  author = {Liao, Heng and McDermott, Erik and Senior, Andrew},
  title = {Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription},
  booktitle = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding},
  pages = {368--373},
  year = {2013},
}

@article{Likhomanenko2020RethinkingEvaluationASR,
  author = {Likhomanenko, Tatiana and Xu, Qing and Pratap, Vijay and Tomasello, Paul and Kahn, Jacob and Avidov, Guy and Collobert, Ronan and Synnaeve, Gabriel},
  title = {Rethinking evaluation in ASR: Are our models robust enough?},
  journal = {arXiv preprint arXiv:2010.11745},
  year = {2020},
}

@article{Loshchilov2017DecoupledWeightDecay,
  author = {Loshchilov, Ilya and Hutter, Frank},
  title = {Decoupled weight decay regularization},
  journal = {arXiv preprint arXiv:1711.05101},
  year = {2017},
}

@article{Luong2015MultiTaskSeq2Seq,
  author = {Luong, Minh-Thang and Le, Quoc V. and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
  title = {Multi-task sequence to sequence learning},
  journal = {arXiv preprint arXiv:1511.06114},
  year = {2015},
}

@inproceedings{Mahajan2018WeaklySupervisedPretraining,
  author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashutosh and Van Der Maaten, Laurens},
  title = {Exploring the limits of weakly supervised pretraining},
  booktitle = {Proceedings of the European conference on computer vision (ECCV)},
  pages = {181--196},
  year = {2018},
}

@inproceedings{Mauch2013AudioDegradationToolbox,
  author = {Mauch, Matthias and Ewert, Sebastian},
  title = {The audio degradation toolbox and its application to robustness evaluation},
  booktitle = {Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR 2013)},
  year = {2013},
  note = {accepted},
}

@article{McCann2018NaturalLanguageDecathlon,
  author = {McCann, Bryan and Keskar, Nitish S. and Xiong, Caiming and Socher, Richard},
  title = {The natural language decathlon: Multitask learning as question answering},
  journal = {arXiv preprint arXiv:1806.08730},
  year = {2018},
}

@article{Meyer2020ArtieBiasCorpus,
  author = {Meyer, John and Rauchenstein, Landon and Eisenberg, Jacob D. and Howell, Nathan},
  title = {Artie bias corpus: An open dataset for detecting demographic bias in speech applications},
  journal = {Proceedings of the 12th Language Resources and Evaluation Conference},
  pages = {6462--6468},
  year = {2020},
}

@inproceedings{Miller2020NaturalDistributionShiftQA,
  author = {Miller, John and Krauth, Karl and Recht, Benjamin and Schmidt, Ludwig},
  title = {The effect of natural distribution shift on question answering models},
  booktitle = {ICML},
  year = {2020},
}
@inproceedings{Mohamed2009DeepBeliefNetworks,
  author = {Mohamed, Abdel-rahman and Dahl, George and Hinton, Geoffrey and others},
  title = {Deep belief networks for phone recognition},
  booktitle = {NIPS workshop on deep learning for speech recognition and related applications},
  volume = {1},
  pages = {39},
  year = {2009},
}

@inproceedings{Narayanan2018DomainInvariantSpeechRecognition,
  author = {Narayanan, Aravind and Misra, Akash and Sim, Karen C. and Pundak, Guy and Tripathi, Adarsh and Elfeky, Mohamed and Haghani, Paria and Strohman, Trevor and Bacchiani, Michiel},
  title = {Toward domain-invariant speech recognition via large scale training},
  booktitle = {2018 IEEE Spoken Language Technology Workshop (SLT)},
  pages = {441--447},
  year = {2018},
}

@inproceedings{Panayotov2015Librispeech,
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  title = {Librispeech: An ASR corpus based on public domain audiobooks},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {5206--5210},
  year = {2015},
}

@misc{pandas-dev2020pandas,
  author = {pandas development team and others},
  title = {pandas-dev/pandas: Pandas},
  year = {February 2020},
  note = {URL https://doi.org/10.5281/zenodo.3509134},
}

@article{Park2019SpecAugment,
  author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  title = {SpecAugment: A simple data augmentation method for automatic speech recognition},
  journal = {arXiv preprint arXiv:1904.08779},
  year = {2019},
}

@inproceedings{Pascanu2013TrainingRecurrentNeuralNetworks,
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  title = {On the difficulty of training recurrent neural networks},
  booktitle = {International Conference on Machine Learning},
  pages = {1310--1318},
  year = {2013},
}

@article{Paszke2019Pytorch,
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  title = {PyTorch: An imperative style, high-performance deep learning library},
  journal = {Advances in Neural Information Processing Systems 32},
  pages = {8024--8035},
  year = {2019},
}

@article{Pedregosa2011ScikitLearn,
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  title = {Scikit-learn: Machine learning in Python},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  year = {2011},
}

@article{Polyak1992AccelerationStochasticApproximation,
  author = {Polyak, Boris T. and Juditsky, Anatoli B.},
  title = {Acceleration of stochastic approximation by averaging},
  journal = {SIAM Journal on Control and Optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  year = {1992},
}

@article{Pratap2020aMassivelyMultilingualASR,
  author = {Pratap, Vijay and Sriram, Aravind and Tomasello, Paul and Hannun, Awni Y. and Liptchinsky, Vitaliy and Synnaeve, Gabriel and Collobert, Ronan},
  title = {Massively multilingual ASR: 50 languages, 1 model, 1 billion parameters},
  journal = {ArXiv},
  archiveprefix = {arXiv},
  eprint = {2007.03001},
  primaryclass = {eess.AS},
  year = {2020},
}

@article{Pratap2020bMLS,
  author = {Pratap, Vijay and Xu, Qing and Sriram, Aravind and Synnaeve, Gabriel and Collobert, Ronan},
  title = {MLS: A large-scale multilingual dataset for speech research},
  journal = {arXiv preprint arXiv:2012.03411},
  year = {2020},
}

@inproceedings{Press2017OutputEmbeddingLanguageModels,
  author = {Press, Ofir and Wolf, Lior},
  title = {Using the output embedding to improve language models},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages = {157--163},
  month = {April},
  year = {2017},
  address = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/E17-2025},
}

@article{Provilkov2019BpeDropout,
  author = {Provilkov, Ivan and Emelianenko, Denis and Voita, Elena},
  title = {Bpe-dropout: Simple and effective subword regularization},
  journal = {arXiv preprint arXiv:1910.13267},
  year = {2019},
}

@article{Radford2019LanguageModelsMultitaskLearners,
  author = {Radford, Alec and Wu, Jiwei and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title = {Language models are unsupervised multitask learners},
  year = {2019},
}

@article{Radford2021TransferableVisualModels,
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Christopher and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jonathan and others},
  title = {Learning transferable visual models from natural language supervision},
  journal = {arXiv preprint arXiv:2103.00020},
  year = {2021},
}

@article{Raffel2020LimitsTransferLearningTextToTextTransformer,
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Max and Zhou, Yanqi and Li, Wei and Liu, Peter J. and others},
  title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  journal = {J. Mach. Learn. Res.},
  volume = {21},
  number = {140},
  pages = {1--67},
  year = {2020},
}

@article{Ravanelli2021SpeechBrain,
  author = {Ravanelli, Mirco and Parcollet, Titouan and Plantinga, Peter and Rouhe, Aku and Cornell, Samuele and Lugosch, Loren and Subakan, Chien-Yu and Dawalatabad, Nauman and Heba, Abdelrahman and Zhong, Jiatong and others},
  title = {SpeechBrain: A general-purpose speech toolkit},
  journal = {arXiv preprint arXiv:2106.04624},
  year = {2021},
}

@inproceedings{Recht2019ImageNetGeneralization,
  author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  title = {Do ImageNet classifiers generalize to ImageNet?},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  volume = {97},
  pages = {5389--5400},
  year = {2019},
  month = {09--15 Jun},
  url = {https://proceedings.mlr.press/v97/recht19a.html},
}

@article{Russakovsky2015ImageNetLSVRC,
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  title = {Imagenet large scale visual recognition challenge},
  journal = {International Journal of Computer Vision},
  volume = {115},
  number = {3},
  pages = {211--252},
  year = {2015},
}

@book{Schultz2006MultilingualSpeechProcessing,
  author = {Schultz, Tanja and Kirchhoff, Katrin},
  title = {Multilingual speech processing},
  year = {2006},
  publisher = {Elsevier},
}

@inproceedings{Seide2011FeatureEngineeringContextDependentDNN,
  author = {Seide, Frank and Li, Gang and Chen, Xuedong and Yu, Dong},
  title = {Feature engineering in context-dependent deep neural networks for conversational speech transcription},
  booktitle = {2011 IEEE Workshop on Automatic Speech Recognition \& Understanding},
  pages = {24--29},
  year = {2011},
}
@article{Sennrich2015NeuralMTSubword,
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  title = {Neural machine translation of rare words with subword units},
  journal = {arXiv preprint arXiv:1508.07909},
  year = {2015},
}

@misc{Speer2019ftfy,
  author = {Speer, Robyn},
  title = {ftfy},
  year = {2019},
  howpublished = {Zenodo},
  doi = {10.5281/zenodo.2591652},
  version = {5.5},
}

@inproceedings{Sutskever2014Seq2SeqLearning,
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  title = {Sequence to sequence learning with neural networks},
  booktitle = {Advances in neural information processing systems},
  volume = {27},
  year = {2014},
}

@inproceedings{Taori2020MeasuringRobustness,
  author = {Taori, Rohan and Dave, Ankur and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  title = {Measuring robustness to natural distribution shifts in image classification},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {18583--18599},
  year = {2020},
  url = {https://proceedings.neurips.cc/paper/2020/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf},
}

@inproceedings{Torralba2011UnbiasedDatasetBias,
  author = {Torralba, Antonio and Efros, Alexei A.},
  title = {Unbiased look at dataset bias},
  booktitle = {CVPR 2011},
  pages = {1521--1528},
  year = {2011},
}

@inproceedings{Toshniwal2018MultilingualSpeechRecognition,
  author = {Toshniwal, Sanjeev and Sainath, Tara N. and Weiss, Ron J. and Li, Bo and Moreno, Pedro J. and Weinstein, Ewan and Rao, Kanishka},
  title = {Multilingual speech recognition with a single end-to-end model},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {4904--4908},
  year = {2018},
}

@inproceedings{Valk2021Voxlingua107,
  author = {Valk, Jaan and Alumae, Tanel},
  title = {Voxlingua107: A dataset for spoken language recognition},
  booktitle = {2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages = {652--658},
  year = {2021},
}

@inproceedings{Vaswani2017AttentionAllYouNeed,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  title = {Attention is all you need},
  booktitle = {Advances in neural information processing systems},
  pages = {5998--6008},
  year = {2017},
}

@article{Virtanen2020SciPy,
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, Charles J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jacob and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and SciPy 1.0 Contributors},
  title = {SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python},
  journal = {Nature Methods},
  volume = {17},
  pages = {261--272},
  year = {2020},
  doi = {10.1038/s41592-019-0686-2},
}

@article{Wang2020aFairseqS2T,
  author = {Wang, Chenguang and Tang, Yinhan and Ma, Xuezhe and Wu, Alex and Okhonko, Dmitry and Pino, Juan},
  title = {fairseq s2t: Fast speech-to-text modeling with fairseq},
  journal = {arXiv preprint arXiv:2010.05171},
  year = {2020},
}

@article{Wang2020bCovost2,
  author = {Wang, Chenguang and Wu, Alex and Pino, Juan},
  title = {Covost 2 and massively multilingual speech-to-text translation},
  journal = {arXiv preprint arXiv:2007.10310},
  year = {2020},
}

@article{Wang2021Voxpopuli,
  author = {Wang, Chenguang and Riviere, Mitchell and Lee, Alexis and Wu, Alex and Talnikar, Chaitanya and Haziza, David and Williamson, Michael and Pino, Juan and Dupoux, Emmanuel},
  title = {Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation},
  journal = {arXiv preprint arXiv:2101.00390},
  year = {2021},
}

@article{Wang2020cMultitaskTrainingSpeechRecognition,
  author = {Wang, Pengcheng and Sainath, Tara N. and Weiss, Ron J.},
  title = {Multitask training with text data for end-to-end speech recognition},
  journal = {arXiv preprint arXiv:2010.14318},
  year = {2020},
}

@article{Watanabe2020Chime6Challenge,
  author = {Watanabe, Shinji and Mandel, Michael and Barker, Jon and Vincent, Emmanuel and Arora, Aakash and Chang, Xiaoyu and Khudanpur, Sanjeev and Manohar, Vimal and Povey, Daniel and Raj, Dhananjay and others},
  title = {Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings},
  journal = {arXiv preprint arXiv:2004.09249},
  year = {2020},
}

@inproceedings{Xu2021SelfTrainingPretraining,
  author = {Xu, Qiantong and Baevski, Alexei and Likhomanenko, Tatiana and Tomasello, Pierre and Conneau, Alexis and Collobert, Ronan and Synnaeve, Gabriel and Auli, Michael},
  title = {Self-training and pre-training are complementary for speech recognition},
  booktitle = {ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {3030--3034},
  year = {2021},
}

@article{Zhang2020PushingLimitsSSL,
  author = {Zhang, Yanzhang and Qin, Jingqing and Park, Daniel S. and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V. and Wu, Yun},
  title = {Pushing the limits of semi-supervised learning for automatic speech recognition},
  journal = {arXiv preprint arXiv:2010.10504},
  year = {2020},
}

@article{Zhang2021BigSSL,
  author = {Zhang, Yanzhang and Park, Daniel S. and Han, Wei and Qin, Jingqing and Gulati, Anmol and Shor, Jacob and Jansen, Andrew and Xu, Yu and Huang, Yanzhang and Wang, Shuang and others},
  title = {BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition},
  journal = {arXiv preprint arXiv:2109.13226},
  year = {2021},
}

@article{Radford2022RobustSpeechRecognition,
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  url = {https://doi.org/10.48550/arXiv.2212.04356},
  year = {2022},
  month = {Dec},
}
@misc{AmazonAlexa2019,
  title = {Amazon Alexa—A Virtual Assistant},
  howpublished = {Accessed: Jul. 16, 2019},
  url = {https://developer.amazon.com/alexa},
}

@misc{AmazonCompany2019,
  title = {Amazon.com—An American Multinational Technology Company},
  howpublished = {Accessed: Jul. 16, 2019},
  url = {https://www.amazon.com/},
}

@techreport{Smeets2006Covert,
  author = {Smeets, M. and Koot, M.},
  title = {Covert channels},
  institution = {Dept. Syst. Netw. Eng., Univ. Amsterdam, Amsterdam, The Netherlands, Res. Rep., RPI},
  year = {2006},
}
@article{Eddy1996Hidden,
  author = {Eddy, S. R.},
  title = {Hidden Markov Models},
  journal = {Current Opinion in Structural Biology},
  volume = {6},
  number = {6},
  pages = {361--365},
  year = {1996},
}

@inproceedings{Graves2006Connectionist,
  author = {Graves, A. and Fernández, S. and Gomez, F. and Schmidhuber, J.},
  title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  pages = {369--376},
  year = {2006},
}

@inproceedings{Mikolov2010Recurrent,
  author = {Mikolov, T. and Karafiát, M. and Burget, L. and Černocký, J. and Khudanpur, S.},
  title = {Recurrent Neural Network Based Language Model},
  booktitle = {Proceedings of the 11th Annual Conference of the International Speech Communication Association},
  pages = {1045--1048},
  year = {2010},
}

@misc{Hannun2014DeepSpeech,
  author = {Hannun, A. and et al.},
  title = {Deep Speech: Scaling Up End-to-End Speech Recognition},
  year = {2014},
  eprint = {arXiv:1412.5567},
  eprinttype = {arxiv},
  note = {Available: \url{https://arxiv.org/abs/1412.5567}},
}

@article{Nickolls2010The,
  author = {Nickolls, J. and Dally, W. J.},
  title = {The {GPU} Computing Era},
  journal = {IEEE Micro},
  volume = {30},
  number = {2},
  pages = {56--69},
  year = {2010},
}
@inproceedings{Szegedy2014Intriguing,
  author = {Szegedy, C. and et al.},
  title = {Intriguing properties of neural networks},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  pages = {1--10},
  year = {2014},
}

@inproceedings{Carlini2017Towards,
  author = {Carlini, N. and Wagner, D.},
  title = {Towards evaluating the robustness of neural networks},
  booktitle = {Proceedings of the IEEE Symposium on Security and Privacy (SP)},
  pages = {39--57},
  month = may,
  year = {2017},
}

@inproceedings{MoosaviDezfooli2016DeepFool,
  author = {Moosavi-Dezfooli, S.-M. and Fawzi, A. and Frossard, P.},
  title = {DeepFool: A simple and accurate method to fool deep neural networks},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {2574--2582},
  month = jun,
  year = {2016},
}

@inproceedings{Papernot2017Practical,
  author = {Papernot, N. and McDaniel, P. and Goodfellow, I. and Jha, S. and Celik, Z. B. and Swami, A.},
  title = {Practical black-box attacks against machine learning},
  booktitle = {Proceedings of the ACM Asia Conference on Computer and Communications Security},
  pages = {506--519},
  year = {2017},
}

@inproceedings{Liu2017Delving,
  author = {Liu, Y. and Chen, X. and Liu, C. and Song, D.},
  title = {Delving into transferable adversarial examples and black-box attacks},
  booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
  pages = {1--24},
  year = {2017},
}

@inproceedings{MoosaviDezfooli2017Universal,
  author = {Moosavi-Dezfooli, S.-M. and Fawzi, A. and Fawzi, O. and Frossard, P.},
  title = {Universal adversarial perturbations},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {86--94},
  month = jul,
  year = {2017},
}
@article{Hannun2014Deep,
  author = {Hannun, A. and et al.},
  title = {Deep speech: Scaling up end-to-end speech recognition},
  journal = {arXiv preprint arXiv:1412.5567},
  year = {2014},
  url = {https://arxiv.org/abs/1412.5567},
}

@inproceedings{Kingma2015Adam,
  author = {Kingma, D. P. and Ba, J.},
  title = {Adam: A method for stochastic optimization},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  pages = {1--15},
  year = {2015},
}

@inproceedings{Zhang2016DNNSpeaker,
  author = {Zhang, C. and Woodland, P. C.},
  title = {DNN speaker adaptation using parameterised sigmoid and ReLU hidden activation functions},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {5300--5304},
  month = mar,
  year = {2016},
}

@inproceedings{Abadi2016TensorFlow,
  author = {Abadi, M. and et al.},
  title = {TensorFlow: A system for large-scale machine learning},
  booktitle = {Proceedings of the Symposium on Operating Systems Design and Implementation (OSDI)},
  volume = {16},
  pages = {265--283},
  year = {2016},
}
@article{park2022selective,
  author = {Park, John and Lee, Jane and Kim, David},
  title = {Selective Audio Adversarial Example Generation for Speech Recognition},
  journal = {Speech Technology Journal},
  volume = {25},
  number = {4},
  pages = {123--137},
  year = {2022},
}
@inproceedings{szegedy2014intriguing,
  author = {Szegedy, Christian and et al.},
  title = {Intriguing properties of neural networks},
  booktitle = {Proc. Int. Conf. Learn. Represent.},
  year = {2014},
  pages = {1--10},
}

@inproceedings{liu2017delving,
  author = {Liu, Yanpei and et al.},
  title = {Delving into transferable adversarial examples and black-box attacks},
  booktitle = {Proc. 5th Int. Conf. Learn. Represent. (ICLR)},
  year = {2017},
  pages = {1--24},
}

@inproceedings{carlini2018audio,
  author = {Carlini, Nicholas and Wagner, David},
  title = {Audio adversarial examples: Targeted attacks on speech-to-text},
  booktitle = {Proc. IEEE Secur. Privacy Workshops},
  year = {2018},
  pages = {1--7},
}
@inproceedings{Olsen2016Modelling,
  author = {Olsen, S. L. and Agerkvist, F. T. and MacDonald, E. and Stegenborg-Andersen, T. and Volk, C. P.},
  title = {Modelling the perceptual components of loudspeaker distortion},
  booktitle = {Proc. 140th Int. Audio Eng. Soc. Conv.},
  year = {2016},
  pages = {1--9},
}

@patent{Garudadri2004System,
  author = {Garudadri, H.},
  title = {System and method of mu-law or a-law compression of bark amplitudes for speech recognition},
  number = {US Patent 6,694,294 B1},
  month = {Feb.},
  year = {2004},
}
@article{Gentilucci1996Visual,
  author = {Gentilucci, Maurizio and Chieffi, Sergio and Daprati, Elena and Saetti, Maria Cristina and Toni, Ivan},
  title = {Visual illusion and action},
  journal = {Neuropsychologia},
  volume = {34},
  number = {5},
  pages = {369--376},
  year = {1996},
}
@misc{AppleStoresVoiceData2013,
  title = {Apple stores your voice data for two years},
  month = {Apr.},
  year = {2013},
  url = {https://www.cultofmac.com/224577/apple-stores-your-voice-data-for-two-years/},
}

@misc{SamsungShareVoiceCommands2013,
  title = {Samsung share voice commands to third parties},
  month = {Apr.},
  year = {2013},
  url = {https://www.forbes.com/sites/kashmirhill/2013/02/20/samsungs-got-a-tv-that-listens-to-your-private-conversations/#21e8a46679f0},
}

@article{Li2018CanChinaLead,
  author = {Li, Xue-Yan and Qian, Jinyu and Wang, Xiaolan},
  title = {Can China Lead the Development of Data Trading and Sharing Markets?},
  journal = {Communications of the ACM},
  volume = {61},
  number = {11},
  pages = {50--51},
  year = {2018},
}
@article{DeLeon2012EvaluationSpeaker,
  author = {De Leon, Philip L. and Pucher, Michael and Yamagishi, Junichi and Hernaez, Inma and Saratxaga, Ion},
  title = {Evaluation of Speaker Verification Security and Detection of {HMM}-Based Synthetic Speech},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {8},
  pages = {2280--2290},
  year = {2012},
  month = {Oct},
}

@article{Wu2014VoiceConversion,
  author = {Wu, Zhizheng and Li, Haizhou},
  title = {Voice Conversion Versus Speaker Verification: An Overview},
  journal = {APSIPA Transactions on Signal and Information Processing},
  volume = {3},
  year = {2014},
  note = {Article no. e17},
}

@misc{Arik2018NeuralVoiceCloning,
  author = {Arik, Sercan O. and Chen, Jingqing and Peng, Kainan and Ping, Wei and Zhou, Yi},
  title = {Neural Voice Cloning with a Few Samples},
  year = {2018},
  eprint = {arXiv:1802.06006},
}
@misc{PhoneScam,
  title = {Phone Scam},
  year = {Jan. 2017},
  url = {https://goo.gl/T4xMxM},
}

@misc{FakedObamaSpeech,
  title = {Faked Obama Speech},
  year = {Jun. 2014},
  url = {https://goo.gl/pnR3VK},
}
@inproceedings{Li2016Graph,
  author = {Xiao-Yang Li and Chen Zhang and Tao Jung and Jun Qian and Liang Chen},
  title = {Graph-based Privacy-preserving Data Publication},
  booktitle = {Proc. 35th Annu. IEEE Int. Conf. Comput. Commun.},
  year = {2016},
  pages = {1--9},
}

@inproceedings{Jung2017Accounttrade,
  author = {Tao Jung and et al.},
  title = {Accounttrade: Accountable Protocols for Big Data Trading Against Dishonest Consumers},
  booktitle = {Proc. IEEE Conf. Comput. Commun.},
  year = {2017},
  pages = {1--9},
}

@article{Qian2017Privacy,
  author = {Jun Qian and et al.},
  title = {Privacy-preserving Selective Aggregation of Online User Behavior Data},
  journal = {IEEE Trans. Comput.},
  volume = {66},
  number = {2},
  pages = {326--338},
  year = {2017},
  month = {Feb},
}
@inproceedings{Qian2018Hidebehind,
  author = {Jun Qian and Hongyan Du and Jun Hou and Liang Chen and Tao Jung and Xiao-Yang Li},
  title = {Hidebehind: Enjoy Voice Input with Voice-print Unclonability and Anonymity},
  booktitle = {Proc. 16th ACM Conf. Embedded Netw. Sensor Syst.},
  year = {2018},
  pages = {82--94},
}

@misc{AppleVoiceData,
  title = {Apple Stores Your Voice Data for Two Years},
  month = Apr,
  year = {2013},
  url = {https://goo.gl/6hx1kh},
}

@misc{GoogleVoiceInputs,
  title = {Google Stores Your Voice Inputs},
  month = Dec,
  year = {2017},
  url = {https://goo.gl/7w5We1},
}
@inproceedings{Sundermann2003VTLN,
  author = {D. Sundermann and H. Ney},
  title = {VTLN-based Voice Conversion},
  booktitle = {Proc. 3rd IEEE Int. Symp. Signal Process. Inf. Technol.},
  year = {2003},
  pages = {556--559},
}

@inproceedings{Eide1996Vocal,
  author = {E. Eide and H. Gish},
  title = {A Parametric Approach to Vocal Tract Length Normalization},
  booktitle = {Proc. IEEE Int. Conf. Acoustics Speech Signal Process. Conf.},
  year = {1996},
  pages = {346--348},
}

@article{Cohen1995Vocal,
  author = {J. Cohen and T. Kamm and A. G. Andreou},
  title = {Vocal Tract Normalization in Speech Recognition: Compensating for Systematic Speaker Variability},
  journal = {J. Acoustical Society America},
  volume = {97},
  number = {5},
  pages = {3246--3247},
  year = {1995},
}
@inproceedings{Sundermann2003VTLN,
  author = {D. Sundermann and H. Ney},
  title = {VTLN-based Voice Conversion},
  booktitle = {Proc. 3rd IEEE Int. Symp. Signal Process. Inf. Technol.},
  year = {2003},
  pages = {556--559},
}

@inproceedings{Acero1991Robust,
  author = {A. Acero and R. M. Stern},
  title = {Robust Speech Recognition by Normalization of the Acoustic Space},
  booktitle = {Proc. Int. Conf. Acoustics Speech Signal},
  year = {1991},
  pages = {893--896},
}

@inproceedings{Valbret1992Voice,
  author = {H. Valbret and E. Moulines and J.-P. Tubach},
  title = {Voice Transformation using PSOLA Technique},
  booktitle = {Proc. IEEE Int. Conf. Acoustics Speech Signal Process.},
  year = {1992},
  pages = {145--148},
}
@inproceedings{Erlingsson2014RAPPOR,
  author = {U. Erlingsson and V. Pihur and A. Korolova},
  title = {RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response},
  booktitle = {Proc. ACM SIGSAC Conf. Comput. Commun. Security},
  year = {2014},
  pages = {1054--1067},
}

@article{Dwork2014Algorithmic,
  author = {C. Dwork and A. Roth},
  title = {The Algorithmic Foundations of Differential Privacy},
  journal = {Foundations Trends Theoretical Comput. Sci.},
  volume = {9},
  number = {3-4},
  pages = {211--407},
  year = {2014},
}
@inproceedings{Stadniczuk2013OpenSource,
  author = {D. Stadniczuk and G. Bauckmann and D. Suendermann-Oeft},
  title = {An Open-Source Octave Toolbox for {VTLN}-Based Voice Conversion},
  booktitle = {Proc. Int. Conf. German Society Comput. Linguistics Language Technol.},
  year = {2013},
}
@inproceedings{Chen2014SmallFootprint,
  author = {G. Chen and C. Parada and G. Heigold},
  title = {Small-Footprint Keyword Spotting Using Deep Neural Networks},
  booktitle = {Proc. IEEE Int. Conf. Acoustics Speech Signal Process.},
  year = {2014},
  pages = {4087--4091},
}

@inproceedings{Zhang2009Unsupervised,
  author = {Y. Zhang and J. R. Glass},
  title = {Unsupervised Spoken Keyword Spotting via Segmental {DTW} on Gaussian Posteriorgrams},
  booktitle = {Proc. IEEE Workshop Automatic Speech Recognit. Understanding},
  year = {2009},
  pages = {398--403},
}
@article{Todisco2019ASVspoof,
  author = {M. Todisco and X. Wang and V. Vestman and M. Sahidullah and H. Delgado and A. Nautsch and J. Yamagishi and N. Evans and T. Kinnunen and K. A. Lee},
  title = {{ASVspoof 2019: Future horizons in spoofed and fake audio detection}},
  journal = {arXiv preprint arXiv:1904.05441},
  year = {2019}
}

@inproceedings{Zhai2021BackDoor,
  author = {T. Zhai and Y. Li and Z. Zhang and B. Wu and Y. Jiang and S.-T. Xia},
  title = {{Back-door attack against speaker verification}},
  booktitle = {2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2021},
  pages = {2560--2564},
  organization = {IEEE}
}

@inproceedings{Kreuk2018Fooling,
  author = {F. Kreuk and Y. Adi and M. Cisse and J. Keshet},
  title = {{Fooling end-to-end speaker verification with adversarial examples}},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2018},
  pages = {1962--1966},
  organization = {IEEE}
}

@article{Verdoliva2020MediaForensics,
  author = {L. Verdoliva},
  title = {{Media forensics and deepfakes: an overview}},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {14},
  number = {5},
  year = {2020},
  pages = {910--932}
}
@inproceedings{Kinnunen2018tDCF,
  author = {T. Kinnunen and K. A. Lee and H. Delgado and N. Evans and M. Todisco and M. Sahidullah and J. Yamagishi and D. Reynolds},
  title = {{t-DCF: a detection cost function for the tandem assessment of spoofing countermeasures and automatic speaker verification}},
  booktitle = {Speaker Odyssey 2018, The Speaker and Language Recognition Workshop},
  month = jun,
  year = {2018},
  address = {Les Sables d'Olonne, France},
  url = {https://hal.archives-ouvertes.fr/hal-01880306/file/t-DCF_the_Silent_Feature_for_ASV_Spoofing_Challenge.pdf}
}

@inproceedings{Wu2021Voting,
  author = {H. Wu and Y. Zhang and Z. Wu and D. Wang and H. Lee},
  title = {{Voting for the right answer: adversarial defense for speaker verification}},
  booktitle = {Proc. INTERSPEECH 2021},
  year = {2021},
  pages = {To appear}
}

@article{Borelli2021SyntheticSpeechDetection,
  author = {C. Borelli and P. Bestagini and F. Antonacci and A. Sarti and S. Tubaro},
  title = {{Synthetic speech detection through short-term and long-term prediction traces}},
  journal = {{EURASIP Journal on Information Security}},
  number = {2},
  year = {2021},
  pages = {1--14}
}

@inproceedings{Zhang2020AdversarialSeparationNetwork,
  author = {H. Zhang and L. Wang and Y. Zhang and M. Liu and K. A. Lee and J. Wei},
  title = {{Adversarial separation network for speaker recognition}},
  booktitle = {Proc. INTERSPEECH 2020},
  year = {2020},
  pages = {951--955}
}

@inproceedings{Wang2019AdversarialRegularization,
  author = {Q. Wang and P. Guo and S. Sun and L. Xie and J. H. Hansen},
  title = {{Adversarial regularization for end-to-end robust speaker verification}},
  booktitle = {Proc. INTERSPEECH 2019},
  year = {2019},
  pages = {4010--4014}
}

@article{Wu2021AdversarialDefense,
  author = {H. Wu and X. Li and A. T. Liu and Z. Wu and H. Meng and H. Lee},
  title = {{Adversarial defense for automatic speaker verification by cascaded self-supervised learning models}},
  journal = {{arXiv preprint arXiv:2102.07047}},
  year = {2021}
}

@article{ASVspoof2019,
  author = {M. Todisco and X. Wang and V. Vestman and M. Sahidullah and H. Delgado and A. Nautsch and J. Yamagishi and N. Evans and T. Kinnunen and K. A. Lee},
  title = {{ASVspoof 2019: Future horizons in spoofed and fake audio detection}},
  journal = {{arXiv preprint arXiv:1904.05441}},
  year = {2019}
}
@inproceedings{Evans2013Spoofing,
  author = {N. Evans and T. Kinnunen and J. Yamagishi},
  title = {Spoofing and countermeasures for automatic speaker verification},
  booktitle = {Proc. {INTERSPEECH} 2013},
  pages = {925-929},
  year = {2013}
}

@inproceedings{Wu2013Vulnerability,
  author = {Z. Wu and A. Larcher and K. A. Lee and E. S. Chng and T. Kinnunen and H. Li},
  title = {Vulnerability evaluation of speaker verification under voice conversion spoofing: the effect of text constraints},
  booktitle = {Proc. {INTERSPEECH} 2013},
  pages = {950-954},
  year = {2013}
}
@inproceedings{Pop2018Forensic,
  author = {G. Pop and S. Mihalache and D. Burileanu},
  title = {Forensic speaker identification using speech quality data},
  booktitle = {2018 International Conference on Communications (COMM)},
  pages = {509-512},
  year = {2018}
}

@article{Borelli2021Synthetic,
  author = {C. Borelli and P. Bestagini and F. Antonacci and A. Sarti and S. Tubaro},
  title = {Synthetic speech detection through short-term and long-term prediction traces},
  journal = {EURASIP Journal on Information Security},
  number = {2},
  pages = {1–14},
  year = {2021}
}
@inproceedings{Kinnunen2018tDCF,
  author = {T. Kinnunen and K. A. Lee and H. Delgado and N. Evans and M. Todisco and M. Sahidullah and J. Yamagishi and D. Reynolds},
  title = {{t-DCF}: a detection cost function for the tandem assessment of spoofing countermeasures and automatic speaker verification},
  booktitle = {Speaker Odyssey 2018, The Speaker and Language Recognition Workshop},
  month = jun,
  year = {2018},
  address = {Les Sables d'Olonne, France},
  url = {https://hal.archives-ouvertes.fr/hal-01880306},
}

@article{Borelli2021Synthetic,
  author = {C. Borelli and P. Bestagini and F. Antonacci and A. Sarti and S. Tubaro},
  title = {Synthetic speech detection through short-term and long-term prediction traces},
  journal = {EURASIP Journal on Information Security},
  number = {2},
  pages = {1–14},
  year = {2021}
}

@inproceedings{Wu2021Voting,
  author = {H. Wu and Y. Zhang and Z. Wu and D. Wang and H. Lee},
  title = {Voting for the right answer: adversarial defense for speaker verification},
  booktitle = {Proc. INTERSPEECH 2021},
  year = {2021},
  pages = {In press}
}
@inproceedings{Todisco2019ASVspoof,
  author = {M. Todisco and X. Wang and V. Vestman and M. Sahidullah and H. Delgado and A. Nautsch and J. Yamagishi and N. Evans and T. Kinnunen and K. A. Lee},
  title = {{ASVspoof 2019}: Future horizons in spoofed and fake audio detection},
  booktitle = {arXiv preprint arXiv:1904.05441},
  year = {2019}
}

@article{Borelli2021Synthetic,
  author = {C. Borelli and P. Bestagini and F. Antonacci and A. Sarti and S. Tubaro},
  title = {Synthetic speech detection through short-term and long-term prediction traces},
  journal = {EURASIP Journal on Information Security},
  number = {2},
  pages = {1–14},
  year = {2021}
}
@inproceedings{Nematollahi2014Digital,
  author = {M. A. Nematollahi and M. Ranjbari and S. A. R. Al-Haddad and Shyamala Doraisamy},
  title = {Digital Speech Watermarking for Anti-Spoofing Attack in Speaker Recognition},
  booktitle = {IEEE Region 10 Symposium},
  pages = {476-479},
  year = {2014}
}

@inproceedings{Dhameliya2015Feature,
  author = {Kinnal Dhameliya and Ninad Bhatt},
  title = {Feature Extraction and Classification Techniques for Speaker Recognition: A Review},
  booktitle = {IEEE International Conference on Electrical, Electronics, Signals, Communication and Optimization (EESCO)},
  address = {Visakhapatnam},
  pages = {},
  month = {24-25 January},
  year = {2015}
}
@article{Desai2016Speaker,
  author = {Nihalkumar G. Desai and Nikunj V. Tahilramani},
  title = {Speaker Recognition System Using Watermark Technology for Anti-Spoofing Attack: A Review},
  journal = {International Journal of Innovative Research in Electrical, Electronics, Instrumentation and Control Engineering},
  volume = {4},
  number = {4},
  pages = {152-156},
  month = {April},
  year = {2016}
}

@article{Tahilramani2014Steganography,
  author = {Nikunj Tahilramani and Nikita A. Malhotra},
  title = {Steganography Approach of Weighted Speech Analysis with and without Vector Quantization using Variation in Weight Factor},
  journal = {International Journal of Current Engineering and Technology},
  volume = {3},
  number = {3},
  pages = {1334-1336},
  month = {June},
  year = {2014}
}

@article{Vimal2012Real,
  author = {K. Vimal and S. A. Kather},
  title = {Real Steganography in Non-Voice Part of the Speech},
  journal = {International Journal of Computer Applications},
  volume = {46},
  pages = {},
  month = {May},
  year = {2012}
}
@article{Gamit2015English,
  author = {Mayur R. Gamit and Kinnal Dhameliya},
  title = {English Digits Recognition using MFCC, LPC and Pearson’s Correlation},
  journal = {International Journal of Emerging Technology and Advanced Engineering},
  volume = {5},
  number = {5},
  pages = {364-367},
  month = {May},
  year = {2015}
}

@article{Vimal2012Real,
  author = {K. Vimal and S. A. Kather},
  title = {Real Steganography in Non-Voice Part of the Speech},
  journal = {International Journal of Computer Applications},
  volume = {46},
  pages = {},
  month = {May},
  year = {2012}
}
@article{Gamit2015English,
  author = {Mayur R. Gamit and Kinnal Dhameliya},
  title = {English Digits Recognition using MFCC, LPC and Pearson’s Correlation},
  journal = {International Journal of Emerging Technology and Advanced Engineering},
  volume = {5},
  number = {5},
  pages = {364-367},
  month = {May},
  year = {2015}
}

@article{Dhameliya2014Recognizing,
  author = {Kinnal Dhameliya and Nidhi Desai and Vijayendra Desai},
  title = {Recognizing voice commands for robot using {MFCC} and {DTW}},
  journal = {International Journal of Advanced Research in Computer and Communication Engineering},
  volume = {3},
  number = {5},
  pages = {},
  month = {May},
  year = {2014}
}

@article{Tiwari2010MFCC,
  author = {Vibha Tiwari},
  title = {{MFCC} and its applications in speaker recognition},
  journal = {International Journal on Emerging Technologies},
  pages = {19-22},
  month = {},
  year = {2010}
}

@article{Dhameliya2015Feature,
  author = {Kinnal Dhameliya and Ninad Bhatt},
  title = {Feature Extraction and Classification Techniques for Speaker Recognition: {A} Review},
  journal = {{IEEE} International Conference on Electrical, Electronics, Signals, Communication and Optimization ({EESCO})},
  pages = {},
  month = {January},
  year = {2015}
}

@article{Nihalkumar2016Speaker,
  author = {Nihalkumar G. Desai and Nikunj V. Tahilramani},
  title = {Speaker Recognition System Using Watermark Technology for Anti-Spoofing Attack: {A} Review},
  journal = {International Journal of Innovative Research in Electrical, Electronics, Instrumentation and Control Engineering},
  volume = {4},
  number = {4},
  pages = {152-156},
  month = {April},
  year = {2016}
}
@article{Campbell1997Speaker,
  author = {J. P. Campbell Jr},
  title = {Speaker recognition: {A} tutorial},
  journal = {Proceedings of the {IEEE}},
  volume = {85},
  number = {9},
  pages = {1437-1462},
  year = {1997}
}

@inproceedings{Reynolds2002An,
  author = {D. A. Reynolds},
  title = {An overview of automatic speaker recognition technology},
  booktitle = {Acoustics, Speech, and Signal Processing ({ICASSP}), 2002 {IEEE} International Conference on},
  pages = {IV-4072},
  year = {2002},
  organization = {IEEE}
}

@inproceedings{Farrus2008How,
  author = {M. Farrus and M. Wagner and J. Anguita and J. Hernando},
  title = {How vulnerable are prosodic features to professional imitators?},
  booktitle = {The Speaker and Language Recognition Workshop ({Odyssey} 2008)},
  address = {Stellenbosch, South Africa},
  month = {January},
  year = {2008}
}

@inproceedings{DeLeon2010Evaluation,
  author = {P. DeLeon and M. Pucher and J. Yamagishi},
  title = {Evaluation of the vulnerability of speaker verification to synthetic speech},
  booktitle = {Odyssey 2010: The Speaker and Language Recognition Workshop},
  pages = {151-158},
  year = {2010},
  address = {Brno, Czech Republic},
  month = {June}
}

@inproceedings{Wu2012A,
  author = {Zhizheng Wu and Kong Aik Lee and Eng Siong Chng and Haizhou Li},
  title = {A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case},
  booktitle = {Signal \& Information Processing Association Annual Summit and Conference ({APSIPA ASC}), 2012 Asia-Pacific},
  year = {2012},
  organization = {IEEE}
}
@article{Faundez-Zanuy2006Speaker,
  author = {Marcos Faundez-Zanuy and Martin Hagmüller and Gernot Kubin},
  title = {Speaker verification security improvement by means of speech watermarking},
  journal = {Speech Communication},
  volume = {48},
  number = {12},
  pages = {1608-1619},
  year = {2006}
}

@article{Faundez-Zanuy2007Speaker,
  author = {Marcos Faundez-Zanuy and Martin Hagmüller and Gernot Kubin},
  title = {Speaker identification security improvement by means of speech watermarking},
  journal = {Pattern Recognition},
  volume = {40},
  number = {11},
  pages = {3027-3034},
  year = {2007}
}
@inproceedings{Goodfellow2015Explaining,
  author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  title = {Explaining and harnessing adversarial examples},
  booktitle = {Proceedings of International Conference on Learning Representations (ICLR)},
  year = {2015}
}

@article{Su2019OnePixel,
  author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  title = {One pixel attack for fooling deep neural networks},
  journal = {IEEE Transactions on Evolutionary Computation (TEVC)},
  volume = {23},
  pages = {828-841},
  year = {2019}
}

@inproceedings{Athalye2018Synthesizing,
  author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  title = {Synthesizing robust adversarial examples},
  booktitle = {Proceedings of International Conference on Machine Learning (ICML)},
  pages = {284-293},
  year = {2018}
}

@inproceedings{Brendel2018DecisionBased,
  author = {Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
  title = {Decision-based adversarial attacks: Reliable attacks against black-box machine learning models},
  booktitle = {Proceedings of International Conference on Learning Representations (ICLR)},
  year = {2018}
}

@inproceedings{Alzantot2017DidYouHearThat,
  author = {Alzantot, Moitreya and Balaji, Bita and Srivastava, Madhuri},
  title = {Did you hear that? adversarial examples against automatic speech recognition},
  booktitle = {Proceedings of Conference and Workshop on Neural Information Processing Systems (NIPSW)},
  year = {2017}
}

@inproceedings{Carlini2018AudioAdversarial,
  author = {Carlini, Nicholas and Wagner, David},
  title = {Audio adversarial examples: targeted attacks on speech-to-text},
  booktitle = {Proceedings of IEEE Security and Privacy (SP) Workshops},
  pages = {1-7},
  year = {2018}
}

@inproceedings{Papernot2016Distillation,
  author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  title = {Distillation as a defense to adversarial perturbations against deep neural networks},
  booktitle = {Proceedings of IEEE Symposium on Security and Privacy (SP)},
  pages = {582-597},
  year = {2016}
}

@inproceedings{Xu2019FeatureSqueezing,
  author = {Xu, Weilin and Evans, David and Qi, Yanjun},
  title = {Feature squeezing: detecting adversarial examples in deep neural networks},
  booktitle = {Proceedings of Network and Distributed Systems Security (NDSS) Symposium},
  year = {2019}
}

@inproceedings{Yang2019CharacterizingAudio,
  author = {Yang, Ziyuan and Li, Bo and Chen, Peng and Song, Dawn},
  title = {Characterizing audio adversarial examples using temporal dependency},
  booktitle = {Proceedings of International Conference on Learning Representations (ICLR)},
  year = {2019}
}
@inproceedings{Chung2016LipReadingInTheWild,
  author = {Chung, Joon Son and Zisserman, Andrew},
  title = {Lip reading in the wild},
  booktitle = {Proceedings of Asian Conference on Computer Vision (ACCV)},
  pages = {87-103},
  year = {2016}
}

@article{Cooke2006AnAudiovisualCorpus,
  author = {Cooke, Martin and Barker, Jon and Cunningham, Stuart and Shao, Xu},
  title = {An audiovisual corpus for speech perception and automatic speech recognition},
  journal = {The Journal of the Acoustical Society of America},
  volume = {120},
  pages = {2421-2424},
  year = {2006}
}

@inproceedings{Goodfellow2015Explaining,
  author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  title = {Explaining and harnessing adversarial examples},
  booktitle = {Proceedings of International Conference on Learning Representations (ICLR)},
  year = {2015}
}

@inproceedings{Carlini2018AudioAdversarial,
  author = {Carlini, Nicholas and Wagner, David},
  title = {Audio adversarial examples: targeted attacks on speech-to-text},
  booktitle = {Proceedings of IEEE Security and Privacy (SP) Workshops},
  pages = {1-7},
  year = {2018}
}

@inproceedings{Kurakin2017AdversarialExamplesInThePhysicalWorld,
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  title = {Adversarial examples in the physical world},
  booktitle = {Proceedings of International Conference on Learning Representations (ICLR) Workshops},
  year = {2017}
}
@inproceedings{Ma2019InvestigatingTheLombardEffect,
  author = {Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  title = {Investigating the Lombard effect influence on end-to-end audio-visual speech recognition},
  booktitle = {Proceedings of Interspeech},
  pages = {4090-4094},
  year = {2019}
}

@inproceedings{Stafylakis2017CombiningResidualNetworksWithLSTMsForLipreading,
  author = {Stafylakis, Themos and Tzimiropoulos, Georgios},
  title = {Combining residual networks with LSTMs for lipreading},
  booktitle = {Proceedings of Interspeech},
  volume = {9},
  pages = {3652-3656},
  year = {2017}
}
@inproceedings{chung2016syncnet,
  author = {Chung, J. S. and Zisserman, A.},
  title = {Out of time: automated lip sync in the wild},
  booktitle = {Proceedings of Asian Conference on Computer Vision (ACCV) Workshops},
  volume = {10117},
  pages = {251-263},
  year = {2016}
}

@inproceedings{chung2019perfect,
  author = {Chung, S. and Chung, J. S. and Kang, H.},
  title = {Perfect match: Improved cross-modal embeddings for audio-visual synchronisation},
  booktitle = {Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {3965-3969},
  year = {2019}
}
@inproceedings{chung2016lip,
  title={Lip reading in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Proceedings of Asian Conference on Computer Vision (ACCV)},
  year={2016},
  pages={87--103}
}

@article{cooke2006audiovisual,
  title={An audio-visual corpus for speech perception and automatic speech recognition},
  author={Cooke, Martin and Barker, Jon and Cunningham, Stuart and Shao, Xu},
  journal={The Journal of the Acoustical Society of America},
  volume={120},
  pages={2421--2424},
  year={2006}
}

@inproceedings{vougioukas2018end,
  title={End-to-end speech-driven facial animation with temporal GANs},
  author={Vougioukas, Konstantinos and Petridis, Stavros and Pantic, Maja},
  booktitle={Proceedings of British Machine Vision Conference (BMVC)},
  year={2018}
}
@article{xiong2017toward,
  title={Toward human parity in conversational speech recognition},
  author={Xiong, Wayne and Droppo, Jasha and Huang, Xuedong and Seide, Frank and Seltzer, Michael and Stolcke, Andreas and Yu, Dong and Zweig, Geoffrey},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={25},
  number={12},
  pages={2410--2423},
  year={2017},
  publisher={IEEE}
}

@article{saon2017english,
  title={English conversational telephone speech recognition by humans and machines},
  author={Saon, George and Kurata, Gakuto and Sercu, Tom and Audhkhasi, Kartik and Thomas, Samuel and Dimitriadis, Dimitrios and Cui, Xiaodong and Ramabhadran, Bhuvana and Picheny, Michael and Lim, Lei and Roomi, Bilal and Hall, Patrick},
  journal={CoRR},
  volume={abs/1703.02136},
  year={2017},
  pages={1--7}
}

@book{rabiner1993fundamentals,
  title={Fundamentals of speech recognition},
  author={Rabiner, Lawrence R and Juang, Biing-Hwang},
  year={1993},
  publisher={Prentice-Hall, Inc.}
}

@inproceedings{diao2014your,
  title={Your voice assistant is mine: How to abuse speakers to steal information and control your phone},
  author={Diao, Wenyuan and Liu, Xiaoyong and Zhou, Zhou and Zhang, Kui},
  booktitle={Workshop on Security and Privacy in Smartphones \& Mobile Devices},
  year={2014},
  organization={ACM},
  pages={63--74}
}

@misc{moynihan2017how,
  title={How to keep Amazon Echo and Google Home from responding to your TV},
  author={Moynihan, Tim},
  year={2017},
  howpublished={\url{https://www.wired.com/2017/02/keep-amazon-echo-google-home-responding-tv/}},
  note={Accessed: October 31, 2018}
}

@inproceedings{barreno2006machine,
  title={Can machine learning be secure?},
  author={Barreno, Marco and Nelson, Blaine and Sears, Russell and Joseph, Anthony D and Tygar, J Doug},
  booktitle={Symposium on Information, Computer and Communications Security},
  year={2006},
  organization={ACM}
}

@article{barreno2010security,
  title={The security of machine learning},
  author={Barreno, Marco and Nelson, Blaine and Joseph, Anthony D and Tygar, J Doug},
  journal={Machine Learning},
  volume={81},
  number={2},
  pages={121--148},
  year={2010},
  publisher={Springer}
}

@inproceedings{lowd2005adversarial,
  title={Adversarial learning},
  author={Lowd, Daniel and Meek, Christopher},
  booktitle={Conference on Knowledge Discovery in Data Mining},
  pages={641--647},
  year={2005},
  organization={ACM}
}

@inproceedings{papernot2016limitations,
  title={The limitations of deep learning in adversarial settings},
  author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z Berkay and Swami, Ananthram},
  booktitle={European Symposium on Security and Privacy},
  pages={372--387},
  year={2016},
  organization={IEEE}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={CoRR},
  volume={abs/1412.6572},
  year={2014},
  pages={1--11}
}
@book{huang2001spoken,
  title={Spoken Language Processing},
  author={Huang, Xuedong and Acero, Alex and Hon, Hsiao-Wuen},
  year={2001},
  publisher={Prentice-Hall}
}

@book{deng2003speech,
  title={Speech Processing—A Dynamic and Optimization-Oriented Approach},
  author={Deng, Li and O'Shaughnessy, Douglas},
  year={2003},
  publisher={Marcel Dekker}
}

@article{he2013speech,
  title={Speech-centric information processing: An optimization-oriented approach},
  author={He, Xueliang and Deng, Li},
  journal={Proc. IEEE},
  volume={101},
  number={5},
  pages={1116--1135},
  year={2013},
  publisher={IEEE}
}
@article{deng2002distributed,
  title={Distributed speech processing in MIPAD's multimodal user interface},
  author={Deng, Li and Wang, Kai-Fu and Hon, Hsiao-Wuen and Acero, Alex and Huang, Xuedong},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={10},
  number={8},
  pages={605--619},
  year={2002},
  publisher={IEEE}
}

@book{acero1993acoustical,
  title={Acoustical and Environmental Robustness in Automatic Speech Recognition},
  author={Acero, Alex},
  year={1993},
  publisher={Cambridge Univ. Press}
}

@article{gong1995speech,
  title={Speech recognition in noisy environments: A survey},
  author={Gong, Yifan},
  journal={Speech Communication},
  volume={16},
  pages={261--291},
  year={1995},
  publisher={Elsevier}
}

@article{lee1998stochastic,
  title={On stochastic feature and model compensation approaches to robust speech recognition},
  author={Lee, Chin-Hui},
  journal={Speech Communication},
  volume={25},
  pages={29--47},
  year={1998},
  publisher={Elsevier}
}

@article{huo2001robust,
  title={Robust speech recognition based on adaptive classification and decision strategies},
  author={Huo, Qingqing and Lee, Chin-Hui},
  journal={Speech Communication},
  volume={34},
  number={1-2},
  pages={175--194},
  year={2001},
  publisher={Elsevier}
}

@incollection{droppo2008environmental,
  title={Environmental robustness},
  author={Droppo, Jasha and Acero, Alex},
  booktitle={Handbook of Speech Process.},
  pages={33},
  year={2008},
  publisher={Springer}
}

@incollection{deng2011front,
  title={Front-end, back-end, and hybrid techniques for noise-robust speech recognition},
  author={Deng, Li},
  booktitle={Robust Speech Recognition of Uncertain or Missing Data: Theory and Application},
  pages={67--99},
  year={2011},
  publisher={Springer}
}

@incollection{haeb2011uncertainty,
  title={Uncertainty decoding and conditional bayesian estimation},
  author={Haeb-Umbach, Reinhold},
  booktitle={Robust Speech Recognition of Uncertain or Missing Data: Theory and Application},
  pages={9--34},
  year={2011},
  publisher={Springer}
}

@incollection{gales2011model,
  title={Model-based approaches to handling uncertainty},
  author={Gales, Mark JF},
  booktitle={Robust Speech Recognition of Uncertain or Missing Data: Theory and Application},
  pages={101--125},
  year={2011},
  publisher={Springer}
}

@article{kumatani2012microphone,
  title={Microphone array processing for distant speech recognition: From close-talking microphones to far-field sensors},
  author={Kumatani, Kenichi and McDonough, John W and Raj, Bhiksha},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={127--140},
  year={2012},
  publisher={IEEE}
}

@book{kolossa2011robust,
  title={Robust speech recognition of uncertain or missing data: theory and applications},
  author={Kolossa, Dorothea and Haeb-Umbach, Reinhold},
  year={2011},
  publisher={Springer}
}

@book{virtanen2012techniques,
  title={Techniques for noise robustness in automatic speech recognition},
  author={Virtanen, Tuomas and Singh, Rohit and Raj, Bhiksha},
  year={2012},
  publisher={Wiley}
}
@article{Davis1980Comparison,
  author = {S. B. Davis and P. Mermelstein},
  title = {Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences},
  journal = {IEEE Transactions on Audio, Speech, Signal Processing},
  volume = {28},
  number = {4},
  pages = {357--366},
  year = {1980},
  month = {Aug},
}

@book{Acero1993Acoustical,
  author = {A. Acero},
  title = {Acoustical and Environmental Robustness in Automatic Speech Recognition},
  publisher = {Cambridge University Press},
  address = {Cambridge, U.K.},
  year = {1993},
}
@incollection{CohenGannot2008Spectral,
  author    = {I. Cohen and S. Gannot},
  title     = {Spectral Enhancement Methods},
  booktitle = {Handbook of Speech Process.},
  editor    = {J. Benesty and M. M. Sondhi and Y. Huang},
  publisher = {Springer},
  address   = {New York, NY, USA},
  year      = {2008},
  chapter   = {44}
}
@article{deng2004estimating,
  title={Estimating cepstrum of speech under the presence of noise using a joint prior of static and dynamic features},
  author={Deng, Li and Droppo, Jasha and Acero, Alex},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={12},
  number={3},
  pages={218--233},
  year={2004},
}

@article{deng2004enhancement,
  title={Enhancement of Log Mel power spectra of speech using a phase-sensitive model of the acoustic environment and sequential estimation of the corrupting noise},
  author={Deng, Li and Droppo, Jasha and Acero, Alex},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={12},
  number={2},
  pages={133--143},
  year={2004},
}
@inproceedings{Leutnant2009analytic,
  author = {V. Leutnant and R. Haeb-Umbach},
  title = {An analytic derivation of a phase-sensitive observation model for noise-robust speech recognition},
  booktitle = {Proc. Interspeech},
  year = {2009},
  pages = {2395--2398}
}
@inproceedings{Lippmann1987Multi,
  author    = {R. Lippmann and E. Martin and D. Paul},
  title     = {Multi-style training for robust isolated-word speech recognition},
  booktitle = {Proc. ICASSP},
  year      = {1987},
  pages     = {705--708},
}
@article{Ion2008Novel,
  author = {Ion, Vasile and Haeb-Umbach, Reinhold},
  title = {A novel uncertainty decoding rule with applications to transmission error robust speech recognition},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {16},
  number = {5},
  pages = {1047--1060},
  year = {2008},
}
@techreport{Hirsch2007,
  author = {H. G. Hirsch},
  title = {Aurora-5 experimental framework for the performance evaluation of speech recognition in case of a hands-free speech input in noisy environments},
  institution = {Niederrhein Univ. of Applied Sciences},
  year = {2007}
}

@book{Makino2007,
  editor = {S. Makino and T.-W. Lee and H. Sawada},
  title = {Blind Speech Separation},
  publisher = {Springer},
  year = {2007}
}

@book{Benesty2007,
  editor = {J. Benesty and M. M. Sondhi and Y. Huang},
  title = {Springer Handbook of Speech Processing},
  publisher = {Springer},
  year = {2007}
}

@book{Naylor2010,
  editor = {P.A. Naylor and N.D. Gaubitch},
  title = {Speech Dereverberation},
  publisher = {Springer},
  year = {2010}
}

@book{Woelfel2009,
  author = {M. Woelfel and J. McDonough},
  title = {Distant Speech Recognition},
  publisher = {Wiley},
  year = {2009}
}

@article{Yoshioka2012,
  author = {T. Yoshioka and A. Sehr and M. Delcroix and K. Kinoshita and R. Maas and T. Nakatani and W. Kellermann},
  title = {Making machines understand us in reverberant rooms: Robustness against reverberation for automatic speech recognition},
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {114-126},
  year = {2012}
}

@incollection{Haeb-Umbach2012,
  author = {R. Haeb-Umbach and A. Krueger},
  editor = {T. Virtanen and R. Singh and B. Raj},
  title = {Reverberant speech recognition},
  booktitle = {Noise Robustness in Automatic Speech Recognition},
  publisher = {Wiley},
  year = {2012}
}

@incollection{Smaragdis2012,
  author = {P. Smaragdis},
  editor = {T. Virtanen and R. Singh and B. Raj},
  title = {Extraction of speech from mixture signals},
  booktitle = {Noise Robustness in Automatic Speech Recognition},
  publisher = {Wiley},
  year = {2012}
}

@article{Furui2012,
  editor = {S. Furui and L. Deng and M. Gales and H. Ney and K. Tokuda},
  title = {Special issue on fundamental technologies in modern speech recognition},
  journal = {IEEE Signal Process. Mag.},
  volume = {29},
  number = {6},
  year = {2012}
}

@inproceedings{Zhang2004,
  author = {Z. Zhang and Z. Liu and M. Sinclair etc.},
  title = {Multi-sensory microphones for robust speech detection, enhancement, and recognition},
  booktitle = {Proc. ICASSP},
  pages = {781-784},
  year = {2004}
}

@article{Deng2003,
  author = {L. Deng and J. Droppo and A. Acero},
  title = {Recursive estimation of non-stationary noise using iterative stochastic approximation for robust speech recognition},
  journal = {IEEE T-ASLP},
  volume = {11},
  number = {6},
  pages = {568-580},
  year = {2003}
}

@article{Yoshioka2013,
  author = {T. Yoshioka and T. Nakatani},
  title = {Noise model transfer: Novel approach to robustness against non-stationary noise},
  journal = {IEEE T-ASLP},
  volume = {21},
  number = {10},
  pages = {2182-2192},
  year = {2013}
}

@article{Souden2013,
  author = {M. Souden and S. Araki and K. Kinoshita and T. Nakatani and H. Sawada},
  title = {A multichannel mmse-based framework for speech source separation and noise reduction},
  journal = {IEEE T-ASLP},
  volume = {21},
  number = {9},
  pages = {1913-1928},
  year = {2013}
}

@article{Han2013,
  author = {Chang Woo Han and Shin Jae Kang and Nam Soo Kim},
  title = {Reverberation and noise robust feature compensation based on imm},
  journal = {IEEE T-ASLP},
  volume = {21},
  number = {8},
  pages = {1598-1611},
  year = {2013}
}

@article{Mosayyebpour2013,
  author = {S. Mosayyebpour and M. Esmaeili and T.A. Gulliver},
  title = {Single-microphone early and late reverberation suppression in noisy speech},
  journal = {IEEE TASLP},
  volume = {21},
  number = {2},
  pages = {322-335},
  year = {2013}
}

@article{Demir2013,
  author = {C. Demir and M. Saraclar and A.T. Cemgil},
  title = {Single-channel speech-music separation for robust asr with mixture models},
  journal = {IEEE T-ASLP},
  volume = {21},
  number = {4},
  pages = {725-736},
  year = {2013}
}

@inproceedings{Hermansky1985,
  author = {H. Hermansky and B. A. Hanson and H. Wakita},
  title = {Perceptually based linear predictive analysis of speech},
  booktitle = {Proc. ICASSP},
  volume = {I},
  pages = {509-512},
  year = {1985}
}

@misc{MathworksBeamForming,
    title = {Acoustic Beam Forming using Microphone Array},
    author = {Math Works},
    url = {https://www.mathworks.com/help/phased/ug/acoustic-beamforming-using-a-microphone-array.html}
}
@techreport{santana2017fundamentals,
  author = {Leandro de Santana},
  title = {Fundamentals of Acoustic Beamforming},
  institution = {Department Thermal Fluid Engineering, University of Twente},
  number = {STO-EN-AVT-287, NATO-OTAN},
  year = {2017},
  month = {12},
  day = {13},
}
@article{Chakrabarty2019TimeFrequency,
  author = {Soumitro Chakrabarty and Emanüel AP Habets},
  title = {Time--frequency masking based online multi-channel speech enhancement with convolutional recurrent neural networks},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {13},
  number = {4},
  pages = {787--799},
  year = {2019},
}

@inproceedings{Erdogan2016ImprovedMVDR,
  author = {Hakan Erdogan and John R Hershey and Shinji Watanabe and Michael I Mandel and Jonathan Le Roux},
  title = {Improved mvdr beamforming using single-channel mask prediction networks},
  booktitle = {Interspeech},
  pages = {1981--1985},
  year = {2016},
}

@incollection{Habets2010MVDRBeamformer,
  author = {Emanüel AP Habets and Jacob Benesty and Sharon Gannot and Israel Cohen},
  title = {The mvdr beamformer for speech enhancement},
  booktitle = {Speech Processing in Modern Communication},
  pages = {225--254},
  publisher = {Springer},
  year = {2010},
}

@inproceedings{Heymann2016NeuralNetwork,
  author = {Jahn Heymann and Lukas Drude and Reinhold Haeb-Umbach},
  title = {Neural network based spectral mask estimation for acoustic beamforming},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {196--200},
  year = {2016},
}

@article{Liu2020Multichannel,
  author = {Chang-Le Liu and Sze-Wei Fu and You-Jin Li and Jen-Wei Huang and Hsin-Min Wang and Yu Tsao},
  title = {Multichannel speech enhancement by raw waveform mapping using fully convolutional networks},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {28},
  pages = {1888--1900},
  year = {2020},
}

@inproceedings{LawinOre2014Generalized,
  author = {Toby Christian Lawin-Ore and Sebastian Stenzel and Jürgen Freudenberger and Simon Doclo},
  title = {Generalized multichannel wiener filter for spatially distributed microphones},
  booktitle = {Speech Communication; 11. ITG Symposium},
  pages = {1--4},
  publisher = {VDE},
  year = {2014},
}

@inproceedings{Tzirakis2021MultiChannel,
  author = {Panagiotis Tzirakis and Anurag Kumar and Jacob Donley},
  title = {Multi-channel speech enhancement using graph neural networks},
  booktitle = {ICASSP 2021--2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {3415--3419},
  year = {2021},
}

@article{Thiemann2016SpeechEnhancement,
  author = {Joachim Thiemann and Menno Müller and Daniel Marquardt and Simon Doclo and Steven van de Par},
  title = {Speech enhancement for multimicrophone binaural hearing aids aiming to preserve the spatial auditory scene},
  journal = {EURASIP Journal on Advances in Signal Processing},
  volume = {2016},
  number = {1},
  pages = {1--11},
  year = {2016},
}

@inproceedings{Xiao2017TimeFrequency,
  author = {Xiong Xiao and Shengkui Zhao and Douglas L Jones and Eng Siong Chng and Haizhou Li},
  title = {On time-frequency mask estimation for mvdr beamforming with application in robust speech recognition},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {3246--3250},
  year = {2017},
}

@inproceedings{Zhang2021ADLMVDR,
  author = {Zhuohuang Zhang and Yong Xu and Meng Yu and Shi-Xiong Zhang and Lianwu Chen and Dong Yu},
  title = {Adl-mvdr: All deep learning mvdr beamformer for target speech separation},
  booktitle = {ICASSP 2021--2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {6089--6093},
  year = {2021},
}

@inproceedings{Hermansky1985Perceptually,
  author={H. Hermansky and B. A. Hanson and H. Wakita},
  title={Perceptually based linear predictive analysis of speech},
  booktitle={Proc. ICASSP},
  year={1985},
  volume={I},
  pages={509-512}
}

@article{Hermansky1990Perceptual,
  author={H. Hermansky},
  title={Perceptual linear predictive (PLP) analysis of speech},
  journal={JASA},
  volume={87},
  number={4},
  pages={1738-1752},
  year={1990}
}

@inproceedings{Hanson1993Subband,
  author={B. A. Hanson and T. H. Applehaum},
  title={Subband or cepstral domain filtering for recognition of Lombard and channel-distorted speech},
  booktitle={Proc. ICASSP},
  year={1993},
  volume={II},
  pages={79-82}
}

@inproceedings{Hermansky1991Compensation,
  author={H. Hermansky and N. Morgan and A. Bayya and P. Kohn},
  title={Compensation for the effect of communication channel in auditory-like analysis of speech (RASTA-PLP)},
  booktitle={Proceedings of European Conference on Speech Technology},
  year={1991},
  pages={1367-1370}
}

@article{Hermansky1994RASTA,
  author={H. Hermansky and N. Morgan},
  title={RASTA processing of speech},
  journal={IEEE T-SAP},
  volume={2},
  number={4},
  pages={578-589},
  year={1994}
}

@inproceedings{Avendano1996Data-based,
  author={C. Avendano and S. van Vuuren and H. Hermansky},
  title={Data-based RASTA-like filter design for channel normalization in ASR},
  booktitle={Proc. ICSLP},
  year={1996},
  pages={2087-2090}
}

@article{Kim1999Auditory,
  author={D. S. Kim and S. Y. Lee and R. M. Kil},
  title={Auditory processing of speech signals for robust speech recognition in real-world noisy environments},
  journal={IEEE T-SAP},
  volume={7},
  number={1},
  pages={55-69},
  year={1999}
}

@article{Ali2002Robust,
  author={A. M. A. Ali and J. Van der Spiegel and P. Mueller},
  title={Robust auditory-based speech processing using the average localized synchrony detection},
  journal={IEEE T-SAP},
  volume={10},
  number={5},
  pages={279-292},
  year={2002}
}

@article{Yapanel2008New,
  author={U. H. Yapanel and J. H. L. Hansen},
  title={A new perceptually motivated MVDR-based acoustic front-end (PMVDR) for robust automatic speech recognition},
  journal={Speech Communication},
  volume={50},
  number={2},
  pages={142-152},
  year={2008}
}

@inproceedings{Kim2010Feature,
  author={C. Kim and R. M. Stern},
  title={Feature extraction for robust speech recognition based on maximizing the sharpness of the power distribution and on power flooring},
  booktitle={Proc. ICASSP},
  year={2010},
  pages={4574-4577}
}

@article{Muller2011Contextual,
  author={F. Muller and A. Mertins},
  title={Contextual invariant-integration features for improved speaker-independent speech recognition},
  journal={Speech Communication},
  volume={53},
  number={6},
  pages={830-841},
  year={2011}
}

@inproceedings{Moritz2011Amplitude,
  author={N. Moritz and J. Anemuller and B. Kollmeier},
  title={Amplitude modulation spectrogram based features for robust speech recognition in noisy and reverberant environments},
  booktitle={Proc. ICASSP},
  year={2011},
  pages={5492-5495}
}

@article{Shao2010Computational,
  author={Y. Shao and S. Srinivasan and Z. Jin and D. Wang},
  title={A computational auditory scene analysis system for speech segregation and robust speech recognition},
  journal={Computer Speech \& Language},
  volume={24},
  number={1},
  pages={77-93},
  year={2010}
}

@article{Fazel2012Sparse,
  author={A. Fazel and S Chakrabartty},
  title={Sparse auditory reproducing kernel (SPARK) features for noise-robust speech recognition},
  journal={IEEE T-ASLP},
  volume={20},
  number={4},
  pages={1362-1371},
  year={2012}
}

@inproceedings{Moritz2013Noise,
  author={N. Moritz and M. Schadler and K. Adiloglu and B. Meyer and T. Jurgens and T. Germann and B. Kollmeier and S. Doclo and S. Goetze},
  title={Noise robust distant automatic speech recognition utilizing nmf based source separation and auditory feature extraction},
  booktitle={the 2nd CHiME workshop on machine listening and multisource environments},
  year={2013}
}

@article{Stern2012Features,
  author={R. Stern and N. Morgan},
  title={Features based on auditory physiology and perception},
  journal={Techniques for Noise Robustness in Automatic Speech Recognition},
  year={2012}
}

@article{Chiu2012Learning,
  author={Y. H. Chiu and B. Raj and R. M. Stern},
  title={Learning-based auditory encoding for robust speech recognition},
  journal={IEEE T-ASLP},
  volume={20},
  number={3},
  pages={900-914},
  year={2012}
}
@book{Bourlard1994Connectionist,
  author={H. Bourlard and N. Morgan},
  title={Connectionist speech recognition - A Hybrid approach},
  publisher={Kluwer Academic Press},
  year={1994}
}

@inproceedings{Hermansky2000Tandem,
  author={H. Hermansky and D. P. W. Ellis and S. Sharma},
  title={Tandem connectionist feature extraction for conventional HMM systems},
  booktitle={Proc. ICASSP},
  year={2000},
  volume={3},
  pages={1635-1638}
}

@inproceedings{Hermansky1998TRAP,
  author={H. Hermansky and S. Sharma},
  title={TRAPs - classifiers of temporal patterns},
  booktitle={Proc. ICSLP},
  year={1998}
}

@inproceedings{Jain2002Distributed,
  author={P. Jain and H. Hermansky and B. Kingsbury},
  title={Distributed speech recognition using noise-robust MFCC and TRAPS-estimated manner features},
  booktitle={Proc. Interspeech},
  year={2002}
}

@inproceedings{Chen2004Learning,
  author={B. Chen and Q. Zhu and N. Morgan},
  title={Learning long-term temporal features in LVCSR using neural networks},
  booktitle={Proc. Interspeech},
  year={2004}
}

@inproceedings{Grezl2007Probabilistic,
  author={F. Grezl and M. Karafi ´ at and S. Kont ´ ar and J. Cernock ´ y},
  title={Probabilistic and bottle-neck features for LVCSR of meetings},
  booktitle={Proc. ICASSP},
  year={2007},
  volume={IV},
  pages={757-760}
}

@article{Kumar1998Heteroscedastic,
  author={N. Kumar and A. G. Andreou},
  title={Heteroscedastic discriminant analysis and reduced rank HMMs for improved speech recognition},
  journal={Speech Communication},
  year={1998},
  volume={26},
  number={4},
  pages={283-297}
}

@inproceedings{Tuske2012Context,
  author={Z. Tuske and R. Schl ¨ uter and N. Hermann and M. Sundermeyer},
  title={Context-dependent MLPs for LVCSR: Tandem, hybrid or both?},
  booktitle={Proc. Interspeech},
  year={2012},
  pages={18-21}
}

@inproceedings{Yu2010Roles,
  author={D. Yu and L. Deng and G. Dahl},
  title={Roles of pretraining and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition},
  booktitle={Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning},
  year={2010}
}

@article{Sainath2013Optimization,
  author={T. N. Sainath and B. Kingsbury and H. Soltau and B. Ramabhadran},
  title={Optimization techniques to improve training speed of deep neural networks for large speech tasks},
  journal={IEEE T-ASLP},
  year={2013},
  volume={21},
  number={11},
  pages={2267-2276}
}

@article{Dahl2012Context,
  author={G. E. Dahl and D. Yu and L. Deng and A. Acero},
  title={Context-dependent pretrained deep neural networks for large-vocabulary speech recognition},
  journal={IEEE T-ASLP},
  year={2012},
  volume={20},
  number={1},
  pages={30-42}
}

@article{Hinton2012Deep,
  author={G. Hinton and L. Deng and D. Yu and G. Dahl and et al.},
  title={Deep neural networks for acoustic modeling in speech recognition},
  journal={IEEE Sig. Proc. Mag.},
  year={2012},
  volume={29},
  number={6},
  pages={82-97}
}

@article{Mohamed2012Acoustic,
  author={A. Mohamed and G. E. Dahl and G. Hinton},
  title={Acoustic modeling using deep belief networks},
  journal={Audio, Speech, and Language Processing, IEEE Transactions on},
  year={2012},
  volume={20},
  number={1},
  pages={14-22}
}

@inproceedings{Sainath2011Making,
  author={T. N. Sainath and B. Kingsbury and B. Ramabhadran and P. Fousek and P. Novak},
  title={Making deep belief networks effective for large vocabulary continuous speech recognition},
  booktitle={Proc. ASRU},
  year={2011},
  pages={30-35}
}

@inproceedings{Seltzer2013Investigation,
  author={M. L. Seltzer and D. Yu and Y. Wang},
  title={An investigation of deep neural networks for noise robust speech recognition},
  booktitle={Proc. ICASSP},
  year={2013},
  pages={7398-7402}
}

@inproceedings{Huang2012Improving,
  author={J. Li and D. Yu and J. T. Huang and Y. Gong},
  title={Improving wideband speech recognition using mixed-bandwidth training data in CD-DNN-HMM},
  booktitle={Proc. IEEE SLT},
  year={2012},
  pages={131-136}
}

@inproceedings{Vinyals2012Revisiting,
  author={O. Vinyals and S. V. Ravuri and D. Povey},
  title={Revisiting recurrent neural networks for robust ASR},
  booktitle={Proc. ICASSP},
  year={2012},
  pages={4085-4088}
}

@inproceedings{Maas2012Recurrent,
  author={A. L. Maas and Q. V. Le and T. M. O’Neil and O. Vinyals and P. Nguyen and A. Y. Ng},
  title={Recurrent neural networks for noise reduction in robust ASR},
  booktitle={Proc. Interspeech},
  year={2012},
  pages={22-25}
}
@inproceedings{Vikki1998Recursive,
title={A recursive feature vector normalization approach for robust speech recognition in noise},
author={Vikki, Olli and Bye, Dirk and Laurila, Kari},
booktitle={Proceedings of ICASSP},
pages={733--736},
year={1998},
organization={IEEE}
}
@article{chen2007mva,
title={Mva processing of speech features},
author={Chen, C. P. and Bilmes, J.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={15},
number={1},
pages={257--270},
year={2007},
publisher={IEEE}
}
@article{suk1999cepstrum,
title={Cepstrum third-order normalization method for noisy speech recognition},
author={Suk, Y. H. and Choi, S. H. and Lee, H. S.},
journal={Electronics Letters},
volume={35},
number={7},
pages={527--528},
year={1999},
publisher={IET}
}
@article{hsu2009higher,
title={Higher order cepstral moment normalization for improved robust speech recognition},
author={Hsu, C. W. and Lee, L. S.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={17},
number={2},
pages={205--220},
year={2009},
publisher={IEEE}
}
@inproceedings{xiao2011feature,
title={Feature normalization using structured full transforms for robust speech recognition},
author={Xiao, X. and Li, J. and Siong, C. E. and Li, H.},
booktitle={Proceedings of Interspeech},
pages={693--696},
year={2011},
organization={ISCA}
}
@inproceedings{Molau2003Feature,
title={Feature space normalization in adverse acoustic conditions},
author={Molau, Sirko and Hilger, Frank and Ney, Hermann},
booktitle={Proceedings of ICASSP},
volume={1},
pages={656--659},
year={2003},
organization={IEEE}
}
@inproceedings{Dharanipragada2000Nonlinear,
title={A nonlinear unsupervised adaptation technique for speech recognition},
author={Dharanipragada, S. and Padmanabhan, M.},
booktitle={Proceedings of ICASSP},
pages={556--559},
year={2000},
organization={IEEE}
}
@article{Torre2005Histogram,
title={Histogram equalization of speech representation for robust speech recognition},
author={de la Torre, A. and Peinado, A. M. and Segura, J. C. and Perez-Cordoba, J. L. and Benitez, M. C. and Rubio, A. J.},
journal={IEEE Transactions on Speech and Audio Processing},
volume={13},
number={3},
pages={355--366},
year={2005},
publisher={IEEE}
}
@article{Hilger2006Quantile,
title={Quantile based histogram equalization for noise robust large vocabulary speech recognition},
author={Hilger, Frank and Ney, Hermann},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={14},
number={3},
pages={845--854},
year={2006},
publisher={IEEE}
}
@inproceedings{Molau2001Histogram,
title={Histogram based normalization in the acoustic feature space},
author={Molau, Sirko and Pitz, Markus and Ney, Hermann},
booktitle={Proceedings of ASRU},
pages={21--24},
year={2001},
organization={IEEE}
}
@article{Segura2004Cepstral,
title={Cepstral domain segmental nonlinear feature transformations for robust speech recognition},
author={Segura, J. C. and Benitez, C. and Torre, A. and Rubio, A. J. and Ramirez, J.},
journal={IEEE Signal Processing Letters},
volume={11},
number={5},
pages={517--520},
year={2004},
publisher={IEEE}
}
@inproceedings{Dharanipragada2000Nonlinear,
title={A nonlinear unsupervised adaptation technique for speech recognition},
author={Dharanipragada, S. and Padmanabhan, M.},
booktitle={Proceedings of ICASSP},
pages={556--559},
year={2000},
organization={IEEE}
}
@article{Torre2005Histogram,
title={Histogram equalization of speech representation for robust speech recognition},
author={de la Torre, A. and Peinado, A. M. and Segura, J. C. and Perez-Cordoba, J. L. and Benitez, M. C. and Rubio, A. J.},
journal={IEEE Transactions on Speech and Audio Processing},
volume={13},
number={3},
pages={355--366},
year={2005},
publisher={IEEE}
}
@article{Hilger2006Quantile,
title={Quantile based histogram equalization for noise robust large vocabulary speech recognition},
author={Hilger, Frank and Ney, Hermann},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={14},
number={3},
pages={845--854},
year={2006},
publisher={IEEE}
}
@inproceedings{Molau2001Histogram,
title={Histogram based normalization in the acoustic feature space},
author={Molau, Sirko and Pitz, Markus and Ney, Hermann},
booktitle={Proceedings of ASRU},
pages={21--24},
year={2001},
organization={IEEE}
}
@article{Lin2006Exploiting,
title={Exploiting polynomial-fit histogram equalization and temporal average for robust speech recognition},
author={Lin, Shih-Hau and Yeh, Yau-De and Chen, Berlin},
journal={Proceedings of ICSLP},
pages={2522--2525},
year={2006}
}
@inproceedings{Liu2004Double,
title={Double Gaussian based feature normalization for robust speech},
author={Liu, B. and Dai, L. and Li, J. and Wang, R. H.},
booktitle={International Symposium on Chinese Spoken Language Processing},
pages={705--708},
year={2004},
organization={IEEE}
}
@article{Garcia2006Parametric,
title={Parametric nonlinear feature equalization for robust speech recognition},
author={Garc{'\i}a, Luis and Segura, Juan C and Ramrez, Joaqu{'\i}n and de la Torre, Abel and Bentez, Carmen},
journal={IEEE Signal Processing Letters},
volume={14},
number={5},
pages={401--404},
year={2006},
publisher={IEEE}
}
@article{Suh2007Probabilistic,
title={Probabilistic class histogram equalization for robust speech recognition},
author={Suh, Youngjoo and Ji, Mingbo and Kim, Hanseok},
journal={IEEE Signal Processing Letters},
volume={14},
number={4},
pages={287--290},
year={2007},
publisher={IEEE}
}
@article{Garcia2012Class-based,
title={Class-based parametric approximation to histogram equalization for ASR},
author={Garc{'\i}a, Luis and Ortuzar, Carlos B and de la Torre, Abel and Segura, Juan C},
journal={IEEE Signal Processing Letters},
volume={19},
number={7},
pages={415--418},
year={2012},
publisher={IEEE}
}
@inproceedings{Xiao2013Attribute-based,
title={Attribute-based histogram equalization (HEQ) and its adaptation for robust speech recognition},
author={Xiao, Xiang and Chng, Eng Siong and Li, Haizhou},
booktitle={Proceedings of Interspeech},
pages={876--880},
year={2013}
}
@inproceedings{Tsai2004A,
title={A new feature extraction front-end for robust speech recognition using progressive histogram equalization and multieigenvector temporal filtering},
author={Tsai, Shang-Nan and Lee, Lin-Shan},
booktitle={Proceedings of ICSLP},
pages={165--168},
year={2004},
organization={IEEE}
}

@article{Boll1979Suppression,
  title={Suppression of acoustic noise in speech using spectral subtraction},
  author={Boll, Steven F.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume={27},
  number={2},
  pages={113--120},
  year={1979},
  publisher={IEEE}
}

@inproceedings{Taghia2011An,
  title={An evaluation of noise power spectral density estimation algorithms in adverse acoustic environments},
  author={Taghia, J. and Taghia, J. and Mohammadiha, N. and Sang, J. and Bouse, V. and Martin, R.},
  booktitle={Proceedings of ICASSP},
  pages={4640--4643},
  year={2011},
  organization={IEEE}
}

@inproceedings{Berouti1979Enhancement,
  title={Enhancement of speech corrupted by additive noise},
  author={Berouti, M. and Schwartz, R. and Makhoul, J.},
  booktitle={Proceedings of ICASSP},
  pages={208--211},
  year={1979},
  organization={IEEE}
}

@article{Lim1979Enhancement,
  title={Enhancement and bandwidth compression of noisy speech},
  author={Lim, Jae S. and Oppenheim, Alan V.},
  journal={Proceedings of the IEEE},
  volume={67},
  number={12},
  pages={1586--1604},
  year={1979},
  publisher={IEEE}
}

@book{Quatieri2001Discrete-Time,
  title={Discrete-Time Speech Signal Processing: Principles and Practice},
  author={Quatieri, Thomas F.},
  year={2001},
  publisher={Prentice Hall}
}

@misc{ETSI2002Speech,
  title={Speech processing, transmission and quality aspects (STQ); distributed speech recognition; advanced front-end feature extraction algorithm; compression algorithms},
  year={2002},
  organization={ETSI}
}

@inproceedings{Macho2002Evaluation,
  title={Evaluation of a noise-robust DSR front-end on Aurora databases},
  author={Macho, D. and Mauuary, L. and Noe, B. and Cheng, Y. M. and Ealey, D. and Jouvet, D. and Kelleher, H. and Pearce, D. and Saadoun, F.},
  booktitle={Proceedings of ICSLP},
  pages={17--20},
  year={2002}
}

@inproceedings{Cheng2001SNR-dependent,
  title={SNR-dependent waveform processing for robust speech recognition},
  author={Cheng, Y. M. and Macho, D.},
  booktitle={Proceedings of ICASSP},
  volume={1},
  pages={305--308},
  year={2001},
  organization={IEEE}
}

@inproceedings{Mauuary1998Blind,
  title={Blind equalization in the cepstral domain for robust telephone based speech recognition},
  author={Mauuary, L.},
  booktitle={EUSPICO},
  volume={1},
  pages={359--363},
  year={1998}
}
@misc{ETSI2002Speech,
  title={Speech processing, transmission and quality aspects (STQ); distributed speech recognition; advanced front-end feature extraction algorithm; compression algorithms},
  year={2002},
  organization={ETSI}
}

@inproceedings{Macho2002Evaluation,
  title={Evaluation of a noise-robust DSR front-end on Aurora databases},
  author={Macho, D. and Mauuary, L. and Noe, B. and Cheng, Y. M. and Ealey, D. and Jouvet, D. and Kelleher, H. and Pearce, D. and Saadoun, F.},
  booktitle={Proceedings of ICSLP},
  pages={17--20},
  year={2002}
}

@inproceedings{Cheng2001SNR-dependent,
  title={SNR-dependent waveform processing for robust speech recognition},
  author={Cheng, Y. M. and Macho, D.},
  booktitle={Proceedings of ICASSP},
  volume={1},
  pages={305--308},
  year={2001},
  organization={IEEE}
}

@inproceedings{Mauuary1998Blind,
  title={Blind equalization in the cepstral domain for robust telephone based speech recognition},
  author={Mauuary, L.},
  booktitle={EUSPICO},
  volume={1},
  pages={359--363},
  year={1998}
}

@inproceedings{Agarwal1999Two-stage,
  title={Two-stage mel-warped wiener filter for robust speech recognition},
  author={Agarwal, A. and Cheng, Y. M.},
  booktitle={Proceedings of ASRU},
  pages={67--70},
  year={1999},
  organization={IEEE}
}

@inproceedings{Noe2001Noise,
  title={Noise reduction for noise robust feature extraction for distributed speech recognition},
  author={Noe, B. and Sienel, J. and Jouvet, D. and Mauuary, L. and Boves, L. and De Veth, J. and de Wet, F.},
  booktitle={Proceedings of Eurospeech},
  year={2001}
}

@inproceedings{Li2004A,
  title={A complexity reduction of ETSI advanced front-end for DSR},
  author={Li, J. and Liu, B. and Wang, R. H. and Dai, L.},
  booktitle={Proceedings of ICASSP},
  volume={1},
  pages={61--64},
  year={2004},
  organization={IEEE}
}
@inproceedings{Padmanabhan2000Lattice-based,
  title={Lattice-based unsupervised MLLR for speaker adaptation},
  author={Padmanabhan, M. and Saon, G. and Zweig, G.},
  booktitle={Proc. ISCA ITRW ASR},
  pages={128--131},
  year={2000}
}

@article{Shinoda2001A,
  title={A structural Bayes approach to speaker adaptation},
  author={Shinoda, K. and Lee, C. H.},
  journal={IEEE T-SAP},
  volume={9},
  number={3},
  pages={276--287},
  year={2001},
  organization={IEEE}
}

@article{Siohan2001Joint,
  title={Joint maximum a posteriori adaptation of transformation and HMM parameters},
  author={Siohan, O. and Chesta, C. and Lee, C. H.},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={9},
  number={4},
  pages={417--428},
  year={2001}
}

@article{Siohan2002Structural,
  title={Structural maximum a posteriori linear regression for fast HMM adaptation},
  author={Siohan, O. and Myrvoll, T. A. and Lee, C. H.},
  journal={Computer, Speech and Language},
  volume={16},
  number={1},
  pages={5--24},
  year={2002}
}

@article{Leggetter1995Maximum,
  title={Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models},
  author={Leggetter, C. and Woodland, P.},
  journal={Computer, Speech and Language},
  volume={9},
  number={2},
  pages={171--185},
  year={1995}
}

@article{Gales1998Maximum,
  title={Maximum likelihood linear transformations for HMM-based speech recognition},
  author={Gales, M. J. F.},
  journal={Computer, Speech and Language},
  volume={12},
  pages={75--98},
  year={1998}
}

@inproceedings{Wu2002Supervised,
  title={Supervised adaptation of MCE-trained CDHMMs using minimum classification error linear regression},
  author={Wu, J. and Huo, Q.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={605--608},
  year={2002},
  organization={IEEE}
}

@inproceedings{He2003Minimum,
  title={Minimum classification error linear regression for acoustic model adaptation of continuous density HMMs},
  author={He, X. and Chou, W.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={556--559},
  year={2003},
  organization={IEEE}
}

@article{Yu2009Unsupervised,
  title={Unsupervised adaptation with discriminative mapping transforms},
  author={Yu, K. and Gales, M. J. F. and Woodland, P. C.},
  journal={IEEE T-ASLP},
  volume={17},
  number={4},
  pages={714--723},
  year={2009}
}

@inproceedings{Wang2004MPE-based,
  title={MPE-based discriminative linear transform for speaker adaptation},
  author={Wang, L. and Woodland, P. C.},
  booktitle={Proc. ICASSP},
  pages={321--324},
  year={2004},
  organization={IEEE}
}

@article{Dempster1977Maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  journal={Journal of the Royal Statistical Society},
  volume={39},
  number={1},
  pages={1--38},
  year={1977}
}
@article{Rahim1996Signal,
  title={Signal bias removal by maximum likelihood estimation for robust telephone speech recognition},
  author={Rahim, M. G. and Juang, B. H.},
  journal={IEEE T-SAP},
  volume={4},
  number={1},
  pages={19--30},
  year={1996},
  organization={IEEE}
}

@article{Gales1998Maximum,
  title={Maximum likelihood linear transformations for HMMbased speech recognition},
  author={Gales, M. J. F.},
  journal={Computer, Speech and Language},
  volume={12},
  pages={75--98},
  year={1998}
}

@article{Gales1996Mean,
  title={Mean and variance adaptation within the MLLR framework},
  author={Gales, M. J. F. and Woodland, P. C.},
  journal={Computer, Speech and Language},
  volume={10},
  pages={249--264},
  year={1996}
}

@inproceedings{Saon2001Robust,
  title={Robust digit recognition in noisy environments: the IBM Aurora 2 system},
  author={Saon, G. and Huerta, J. M. and Jan, E. E.},
  booktitle={Proc. Interspeech},
  pages={629--632},
  year={2001}
}

@inproceedings{Saon2001Linear,
  title={Linear feature space projections for speaker adaptation},
  author={Saon, G. and Zweig, G. and Padmanabhan, M.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={325--328},
  year={2001},
  organization={IEEE}
}

@article{Cui2005Noise,
  title={Noise robust speech recognition using feature compensation based on polynomial regression of utterance SNR},
  author={Cui, X. and Alwan, A.},
  journal={IEEE T-SAP},
  volume={13},
  number={6},
  pages={1161--1172},
  year={2005}
}

@inproceedings{Li2007High-performance,
  title={High-performance HMM adaptation with joint compensation of additive and convolutive distortions via vector Taylor series},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  booktitle={Proc. ASRU},
  pages={65--70},
  year={2007}
}

@article{Li2009Unified,
  title={A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  journal={Computer, Speech and Language},
  volume={23},
  number={3},
  pages={389--405},
  year={2009}
}

@phdthesis{Gales1995Model-based,
  title={Model-based techniques for noise robust speech recognition},
  author={Gales, M. J. F.},
  school={University of Cambridge},
  year={1995}
}

@phdthesis{Moreno1996Speech,
  title={Speech recognition in noisy environments},
  author={Moreno, P. J.},
  school={Carnegie Mellon University},
  year={1996}
}
@inproceedings{Acero1990Environmental,
  title={Environmental robustness in automatic speech recognition},
  author={Acero, A. and Stern, R.},
  booktitle={Proc. ICASSP},
  volume={2},
  pages={849--852},
  year={1990},
  organization={IEEE}
}

@article{acero1993acoustical,
  title={Acoustical and environmental robustness in speech recognition},
  author={Acero, A. and Stern, R. M. and Wang, W.},
  journal={IEEE T-SAP},
  volume={1},
  number={4},
  pages={430--442},
  year={1993},
  organization={IEEE}
}

@inproceedings{Liu1994Environment,
  title={Environment normalization for robust speech recognition using direct cepstral comparison},
  author={Liu, F.-H. and Stern, R. M. and Acero, A. and Moreno, P.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={61--64},
  year={1994},
  organization={IEEE}
}

@inproceedings{Liu2004Double,
  title={Double Gaussian based feature normalization for robust speech},
  author={Liu, B. and Dai, L. and Li, J. and Wang, R. H.},
  booktitle={International Symposium on Chinese Spoken Language Processing},
  pages={705--708},
  year={2004},
  organization={ISCA}
}

@inproceedings{Deng2000Large,
  title={Large vocabulary speech recognition under adverse acoustic environment},
  author={Deng, L. and Acero, A. and Plumpe, M. and Huang, X.},
  booktitle={Proc. ICSLP},
  volume={3},
  pages={806--809},
  year={2000},
  organization={IEEE}
}

@article{Deng2003Recursive,
  title={Recursive estimation of nonstationary noise using iterative stochastic approximation for robust speech recognition},
  author={Deng, L. and Droppo, J. and Acero, A.},
  journal={IEEE T-ASLP},
  volume={11},
  number={6},
  pages={568--580},
  year={2003},
  organization={IEEE}
}

@inproceedings{Deng2001Highperformance,
  title={High-performance robust speech recognition using stereo training data},
  author={Deng, L. and Acero, A. and Jiang, L. and Droppo, J. and Huang, X. D.},
  booktitle={Proc. ICASSP},
  pages={301--304},
  year={2001},
  organization={IEEE}
}
@inproceedings{Povey2005fMPE,
  title={fMPE: Discriminatively trained features for speech recognition},
  author={Povey, D. and Kingsbury, B. and Mangu, L. and Saon, G. and Soltau, H. and Zweig, G.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={961--964},
  year={2005},
  organization={IEEE}
}

@article{Deng2005Analysis,
  title={Analysis and comparison of two speech feature extraction/compensation algorithms},
  author={Deng, L. and Wu, J. and Droppo, J. and Acero, A.},
  journal={IEEE Signal Processing Letters},
  volume={12},
  number={6},
  pages={477--480},
  year={2005},
  organization={IEEE}
}

@inproceedings{Bahl1997Maximum,
  title={Maximum mutual information estimation of hidden Markov model parameters for speech recognition},
  author={Bahl, L. R. and Brown, P. F. and De Souza, P. V. and Mercer, R. L.},
  booktitle={Proc. ICASSP},
  volume={11},
  pages={49--52},
  year={1997},
  organization={IEEE}
}

@inproceedings{Droppo2005Maximum,
  title={Maximum mutual information SPLICE transform for seen and unseen conditions},
  author={Droppo, J. and Acero, A.},
  booktitle={Proc. Interspeech},
  pages={989--992},
  year={2005},
  organization={ISCA}
}

@inproceedings{Droppo2001Evaluation,
  title={Evaluation of the SPLICE algorithm on the Aurora2 database},
  author={Droppo, J. and Deng, L. and Acero, A.},
  booktitle={Proc. Eurospeech},
  pages={217--220},
  year={2001},
  organization={ISCA}
}

@inproceedings{Cui2008MMSE,
  title={MMSE-based stereo feature stochastic mapping for noise robust speech recognition},
  author={Cui, X. and Afify, M. and Gao, Y.},
  booktitle={Proc. ICASSP},
  pages={4077--4080},
  year={2008},
  organization={IEEE}
}

@inproceedings{Afify2007Stereo,
  title={Stereo-based stochastic mapping for robust speech recognition},
  author={Afify, M. and Cui, X. and Gao, Y.},
  booktitle={Proc. ICASSP},
  volume={4},
  pages={377--380},
  year={2007},
  organization={IEEE}
}

@article{Afify2009Stereo,
  title={Stereo-based stochastic mapping for robust speech recognition},
  author={Afify, M. and Cui, X. and Gao, Y.},
  journal={IEEE T-ASLP},
  volume={17},
  number={7},
  pages={1325--1334},
  year={2009},
  organization={IEEE}
}

@inproceedings{Cui2008Nbest,
  title={N-best based stochastic mapping on stereo HMM for noise robust speech recognition},
  author={Cui, X. and Afify, M. and Gao, Y.},
  booktitle={Proc. Interspeech},
  pages={1261--1264},
  year={2008},
  organization={ISCA}
}

@inproceedings{Cui2009Stereo,
  title={Stereo-based stochastic mapping with discriminative training for noise robust speech recognition},
  author={Cui, X. and Afify, M. and Gao, Y.},
  booktitle={Proc. ICASSP},
  pages={3933--3936},
  year={2009},
  organization={IEEE}
}

@inproceedings{Du2010HMMbased,
  title={HMM-based pseudo-clean speech synthesis for splice algorithm},
  author={Du, J. and Hu, Y. and Dai, L. R. and Wang, R. H.},
  booktitle={Proc. ICASSP},
  pages={4570--4573},
  year={2010},
  organization={IEEE}
}

@inproceedings{Tokuda2000Speech,
  title={Speech parameter generation algorithms for HMM-based speech synthesis},
  author={Tokuda, K. and Yoshimura, T. and Masuko, T. and Kobayashi, T. and Kitamura, T.},
  booktitle={Proc. ICASSP},
  volume={3},
  pages={1315--1318},
  year={2000},
  organization={IEEE}
}

@inproceedings{Maas2012Recurrent,
  title={Recurrent neural networks for noise reduction in robust ASR},
  author={Maas, A. L. and Le, Q. V. and O’Neil, T. M. and Vinyals, O. and Nguyen, P. and Ng, A. Y.},
  booktitle={Proc. Interspeech},
  pages={22--25},
  year={2012},
  organization={ISCA}
}

@inproceedings{Wollmer2013Feature,
  title={Feature enhancement by bidirectional lstm networks for conversational speech recognition in highly non-stationary noise},
  author={Wollmer, M. and Zhang, Z. and Weninger, F. and Schuller, B. and Rigoll, G.},
  booktitle={Proc. ICASSP},
  pages={6822--6826},
  year={2013},
  organization={IEEE}
}
@article{Gales2000Cluster,
  title={Cluster adaptive training of hidden Markov models},
  author={Gales, M. J. F.},
  journal={IEEE T-SAP},
  volume={8},
  number={4},
  pages={417--428},
  year={2000},
  organization={IEEE}
}

@article{Kuhn2000Rapid,
  title={Rapid speaker adaptation in eigenvoice space},
  author={Kuhn, R. and Junqua, J.-C. and Nguyen, P. and Niedzielski, N.},
  journal={IEEE T-SAP},
  volume={8},
  number={6},
  pages={695--707},
  year={2000},
  organization={IEEE}
}

@inproceedings{Chen2000Fast,
  title={Fast speaker adaptation using eigenspace-based maximum likelihood linear regression},
  author={Chen, K. T. and Liau, W. W. and Wang, H. M. and Lee, L. S.},
  booktitle={Proc. ICSLP},
  pages={742--745},
  year={2000},
  organization={IEEE}
}

@inproceedings{Wang2001Rapid,
  title={Rapid speaker adaptation using a priori knowledge by eigenspace analysis of MLLR parameters},
  author={Wang, N.J.-C. and Lee, S.S.-M. and Seide, F. and Lee, L. S.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={345--348},
  year={2001},
  organization={IEEE}
}

@article{Tsao2009Ensemble,
  title={An ensemble speaker and speaking environment modeling approach to robust speech recognition},
  author={Tsao, Y. and Lee, C. H.},
  journal={IEEE T-SAP},
  volume={17},
  number={5},
  pages={1025--1037},
  year={2009},
  organization={IEEE}
}

@article{Tibshirani1996Regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, R.},
  journal={J. Royal. Statist. Soc B.},
  volume={58},
  number={1},
  pages={267--288},
  year={1996}
}

@inproceedings{Xiao2012Lasso,
  title={Lasso environment model combination for robust speech recognition},
  author={Xiao, X. and Li, J. and Chng, E. S. and Li, H.},
  booktitle={Proc. ICASSP},
  pages={4305--4308},
  year={2012},
  organization={IEEE}
}

@inproceedings{Cui2009Improving,
  title={Improving online incremental speaker adaptation with eigen feature space MLLR},
  author={Cui, X. and Xue, J. and Zhou, B.},
  booktitle={Proc. ASRU},
  pages={136--140},
  year={2009},
  organization={IEEE}
}
@inproceedings{Demuynck2011Integrating,
  title={Integrating meta-information into exemplar-based speech recognition with segmental conditional random fields},
  author={Demuynck, K. and Seppi, D. and Van Compernolle, D. and Nguyen, P. and Zweig, G.},
  booktitle={Proc. ICASSP},
  pages={5048--5051},
  year={2011},
  organization={IEEE}
}

@article{Sainath2011Exemplar,
  title={Exemplar-based sparse representation features: from TIMIT to LVCSR},
  author={Sainath, T. N. and Ramabhadran, B. and Picheny, M. and Nahamoo, D. and Kanevsky, D.},
  journal={IEEE T-ASLP},
  volume={19},
  number={8},
  pages={2598--2613},
  year={2011}
}

@inproceedings{Gemmeke2010Noise,
  title={Noise robust exemplar-based connected digit recognition},
  author={Gemmeke, J. F. and Virtanen, T.},
  booktitle={Proc. ICASSP},
  pages={4546--4549},
  year={2010},
  organization={IEEE}
}

@inproceedings{Raj2010Non-negative,
  title={Non-negative matrix factorization based compensation of music for automatic speech recognition},
  author={Raj, B. and Virtanen, T. and Chaudhuri, S. and Singh, R.},
  booktitle={Proc. Interspeech},
  pages={717--720},
  year={2010}
}

@article{Gemmeke2011Exemplar,
  title={Exemplar-based sparse representations for noise robust automatic speech recognition},
  author={Gemmeke, J. F. and Virtanen, T. and Hurmalainen, A.},
  journal={IEEE T-ASLP},
  volume={19},
  number={7},
  pages={2067--2080},
  year={2011}
}

@inproceedings{Lee2000Algorithms,
  title={Algorithms for non-negative matrix factorization},
  author={Lee, D. D. and Seung, H. S.},
  booktitle={Proc. NIPS},
  pages={556--562},
  year={2000}
}

@article{Schmidt2007Linear,
  title={Linear regression on sparse features for single-channel speech separation},
  author={Schmidt, M. N. and Olsson, R. K.},
  journal={IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
  pages={26--29},
  year={2007}
}

@article{Virtanen2007Monaural,
  title={Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria},
  author={Virtanen, T.},
  journal={IEEE T-ASLP},
  volume={15},
  number={3},
  pages={1066--1074},
  year={2007}
}

@article{Mohammadiha2013Supervised,
  title={Supervised and unsupervised speech enhancement using nonnegative matrix factorization},
  author={Mohammadiha, N. and Smaragdis, P. and Leijon, A.},
  journal={IEEE T-ASLP},
  volume={21},
  number={10},
  pages={2140--2151},
  year={2013}
}

@inproceedings{Gemmeke2012Advances,
  title={Advances in noise robust digit recognition using hybrid exemplar-based techniques},
  author={Gemmeke, J. F. and Van hamme, H.},
  booktitle={Proc. Interspeech},
  pages={2134--2137},
  year={2012}
}

@inproceedings{Weninger2012Non-negative,
  title={Non-negative matrix factorization for highly noise-robust ASR: to enhance or to recognize?},
  author={Weninger, F. and Wollmer, M. and Geiger, J. and Schuller, B. and Gemmeke, J. and Hurmalainen, ¨ A. and Virtanen, T. and Rigoll, G.},
  booktitle={Proc. ICASSP},
  pages={4681--4684},
  year={2012}
}

@inproceedings{Gemmeke2009Using,
  title={Using sparse representations for exemplar based continuous digit recognition},
  author={Gemmeke, J. F. and Bosch, L. ten and Boves, L. and Cranen, B.},
  booktitle={Proc. EUSIPCO},
  pages={1755--1759},
  year={2009}
}

@article{Smaragdis2007Convolutive,
  title={Convolutive speech bases and their application to supervised speech separation},
  author={Smaragdis, P.},
  journal={IEEE T-ASLP},
  volume={15},
  number={1},
  pages={1--12},
  year={2007}
}

@inproceedings{Wilson2008speech,
  title={speech denoising using nonnegative matrix factorization with priors},
  author={Wilson, K. W. and Raj, B. and Smaragdis, P. and Divakaran, A.},
  booktitle={Proc. ICASSP},
  pages={4029--4032},
  year={2008}
}

@inproceedings{Grais2013Discriminative,
  title={Discriminative nonnegative dictionary learning using cross-coherence penalties for single channel source separation},
  author={Grais, E. M. and Erdogan, H.},
  booktitle={Proc. Interspeech},
  pages={808--812},
  year={2013}
}
@article{Cui2007A,
  title={A study of variable-parameter Gaussian mixture hidden Markov modeling for noisy speech recognition},
  author={Cui, X. and Gong, Y.},
  journal={IEEE T-ASLP},
  volume={15},
  number={4},
  pages={1366--1376},
  year={2007}
}

@article{Yu2009ANovel,
  title={A novel framework and training algorithm for variable-parameter hidden Markov models},
  author={Yu, D. and Deng, L. and Gong, Y. and Acero, A.},
  journal={IEEE T-ASLP},
  volume={17},
  number={7},
  pages={1348--1360},
  year={2009}
}

@inproceedings{Yu2008Parameter,
  title={Parameter clustering and sharing in variable-parameter HMMs for noise robust speech recognition},
  author={Yu, D. and Deng, L. and Gong, Y. and Acero, A.},
  booktitle={Proc. Interspeech},
  pages={1253--1256},
  year={2008}
}

@inproceedings{Yu2008Discriminative,
  title={Discriminative training of variable-parameter HMMs for noise robust speech recognition},
  author={Yu, D. and Deng, L. and Gong, Y. and Acero, A.},
  booktitle={Proc. Interspeech},
  pages={285--288},
  year={2008}
}

@inproceedings{Cheng2011Generalized,
  title={Generalized variable parameter HMMs for noise robust speech recognition},
  author={Cheng, N. and Liu, X. and Wang, L.},
  booktitle={Proc. Interspeech},
  pages={481--484},
  year={2011}
}

@inproceedings{Li2013Feature,
  title={Feature space generalized variable parameter HMMs for noise robust recognition},
  author={Li, Y. and Liu, X. and Wang, L.},
  booktitle={Proc. Interspeech},
  pages={2968--2972},
  year={2013}
}
@phdthesis{Gales1995Modelbased,
  title={Model-based techniques for noise robust speech recognition},
  author={Gales, M. J. F.},
  year={1995},
  school={University of Cambridge}
}

@inproceedings{Moreno2007Highperformance,
  title={High-performance HMM adaptation with joint compensation of additive and convolutive distortions via vector Taylor series},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  booktitle={Proc. ASRU},
  pages={65--70},
  year={2007}
}

@article{Li2009AUnified,
  title={A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  journal={Computer, Speech and Language},
  volume={23},
  number={3},
  pages={389--405},
  year={2009}
}

@phdthesis{Moreno1996Speech,
  title={Speech recognition in noisy environments},
  author={Moreno, P. J.},
  year={1996},
  school={Carnegie Mellon University}
}

@article{Gong2005AMethod,
  title={A method of joint compensation of additive and convolutive distortions for speaker-independent speech recognition},
  author={Gong, Y.},
  journal={IEEE T-SAP},
  volume={13},
  number={5},
  pages={975--983},
  year={2005}
}

@phdthesis{Stouten2006Robust,
  title={Robust Automatic Speech Recognition in Time-varying Environments},
  author={Stouten, V.},
  year={2006},
  school={K. U. Leuven}
}

@inproceedings{Li2007Highperformance,
  title={High-performance HMM adaptation with joint compensation of additive and convolutive distortions via vector Taylor series},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  booktitle={Proc. ASRU},
  pages={65--70},
  year={2007}
}

@article{Sagayama1997Jacobian,
  title={Jacobian approach to fast acoustic model adaptation},
  author={Sagayama, S. and Yamaguchi, Y. and Takahashi, S. and Takahashi, J.},
  journal={IEEE T-SAP},
  volume={2},
  pages={835--838},
  year={1997}
}

@article{Li2007Highperformance,
  title={High-performance HMM adaptation with joint compensation of additive and convolutive distortions via vector Taylor series},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  journal={IEEE T-SAP},
  volume={13},
  number={5},
  pages={975--983},
  year={2005}
}

@article{Gong2005AMethod,
  title={A method of joint compensation of additive and convolutive distortions for speaker-independent speech recognition},
  author={Gong, Y.},
  journal={IEEE T-SAP},
  volume={13},
  number={5},
  pages={975--983},
  year={2005}
}

@article{Zhao2000Frequencydomain,
  title={Frequency-domain maximum likelihood estimation for automatic speech recognition in additive and convolutive noises},
  author={Zhao, Y.},
  journal={IEEE T-SAP},
  volume={8},
  number={3},
  pages={255--266},
  year={2000}
}
@techreport{Liao2006Joint,
  title={Joint uncertainty decoding for robust large vocabulary speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  year={2006},
  institution={University of Cambridge},
  number={CUED/TR552}
}

@article{Zhao2012Nonlinear,
  title={Nonlinear compensation using the Gauss-Newton method for noise-robust speech recognition},
  author={Zhao, Y. and Juang, B. H.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={20},
  number={8},
  pages={2191--2206},
  year={2012}
}

@article{Kim1998Speech,
  title={Speech recognition in noisy environments using first-order vector Taylor series},
  author={Kim, D. Y. and Un, C. K. and Kim, N. S.},
  journal={Speech Communication},
  volume={24},
  number={1},
  pages={39--49},
  year={1998}
}

@inproceedings{Li2007Highperformance,
  title={High-performance HMM adaptation with joint compensation of additive and convolutive distortions via vector Taylor series},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  booktitle={Proc. ASRU},
  pages={65--70},
  year={2007}
}

@article{Li2009Unified,
  title={A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  journal={Computer, Speech and Language},
  volume={23},
  number={3},
  pages={389--405},
  year={2009}
}

@inproceedings{Zhao2010Comparative,
  title={A comparative study of noise estimation algorithms for VTS-based robust speech recognition},
  author={Zhao, Y. and Juang, B. H.},
  booktitle={Proc. Interspeech},
  pages={2090--2093},
  year={2010}
}

@inproceedings{Stouten2003Robust,
  title={Robust speech recognition using model-based feature enhancement},
  author={Stouten, V. and Van Hamme, H. and Demuynck, K. and Wambacq, P.},
  booktitle={Proc. Eurospeech},
  pages={17--20},
  year={2003}
}

@inproceedings{Droppo2003Comparison,
  title={A comparison of three non-linear observation models for noisy speech features},
  author={Droppo, J. and Deng, L. and Acero, A.},
  booktitle={Proc. Eurospeech},
  pages={681--684},
  year={2003}
}

@inproceedings{Li2011Towards,
  title={Towards high-accuracy low-cost noisy robust speech recognition exploiting structured mode},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y.},
  booktitle={ICML Workshop on Learning Architectures, Representations, and Optimization for Speech and Visual Information Processing},
  year={2011}
}

@inproceedings{Li2012Improvements,
  title={Improvements to VTS feature enhancement},
  author={Li, J. and Seltzer, M. L. and Gong, Y.},
  booktitle={Proc. ICASSP},
  pages={4677--4680},
  year={2012}
}

@article{Kalinli2010Noise,
  title={Noise adaptive training for robust automatic speech recognition},
  author={Kalinli, O. and Seltzer, M. L. and Droppo, J. and Acero, A.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={18},
  number={8},
  pages={1889--1901},
  year={2010}
}

@inproceedings{Li2012Efficient,
  title={Efficient VTS adaptation using Jacobian approximation},
  author={Li, J. and Seltzer, M. L. and Gong, Y.},
  booktitle={Proc. Interspeech},
  pages={1906--1909},
  year={2012}
}
@article{deng2004enhancement,
  title={Enhancement of Reverberant Speech Recognition Using Deep Neural Networks with Multi-Task Learning},
  author={Deng, L. and Yu, D. and Wu, X. and Seide, F. and Seltzer, M. and Li, J. and Gong, Y. and Zweig, G. and Chen, M. and Zhai, S. and others},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={22},
  number={5},
  pages={1053--1064},
  year={2014},
  publisher={IEEE}
}

@inproceedings{Kim1998Statistical,
  title={Statistical linear approximation for environment compensation},
  author={Kim, N. S.},
  booktitle={IEEE Signal Processing Letters},
  volume={5},
  number={1},
  pages={8--10},
  year={1998},
  publisher={IEEE}
}

@inproceedings{Stouten2005Effect,
  title={Effect of phase-sensitive environment model and higher order VTS on noisy speech feature enhancement},
  author={Stouten, V. and Van Hamme, H. and Wambacq, P.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={433--436},
  year={2005},
  publisher={IEEE}
}

@article{Du2011Feature,
  title={A feature compensation approach using high-order vector Taylor series approximation of an explicit distortion model for noisy speech recognition},
  author={Du, J. and Huo, Q.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={8},
  pages={2285--2293},
  year={2011},
  publisher={IEEE}
}

@inproceedings{Du2008Speech,
  title={A speech enhancement approach using piecewise linear approximation of an explicit model of environmental distortions},
  author={Du, J. and Huo, Q.},
  booktitle={Proc. Interspeech},
  pages={569--572},
  year={2008}
}

@inproceedings{Kalgaonkar2009Noise,
  title={Noise robust model adaptation using linear spline interpolation},
  author={Kalgaonkar, K. and Seltzer, M. L. and Acero, A.},
  booktitle={Proc. ASRU},
  pages={199--204},
  year={2009}
}

@inproceedings{Li2008HMM,
  title={HMM adaptation using a phase-sensitive acoustic distortion model for environment-robust speech recognition},
  author={Li, J. and Deng, L. and Yu, D. and Gong, Y. and Acero, A.},
  booktitle={Proc. ICASSP},
  pages={4069--4072},
  year={2008}
}

@article{Gales2008Discriminative,
  title={Discriminative classifiers and generative kernels for noise robust speech recognition},
  author={Gales, M. J. F. and Flego, F.},
  journal={Tech. Rep. CUED/TR605, University of Cambridge},
  year={2008}
}

@article{Lu2013Noise,
  title={Noise adaptive training for subspace Gaussian mixture models},
  author={Lu, L. and Ghoshal, A. and Renals, S.},
  journal={Proc. Interspeech},
  pages={3492--3496},
  year={2013}
}

@inproceedings{Frey2001ALGONQUIN,
  title={ALGONQUIN: iterating Laplace’s method to remove multiple types of acoustic distortion for robust speech recognition},
  author={Frey, B. and Deng, L. and Acero, A. and Kristjansson, T.},
  booktitle={Proc. Interspeech},
  pages={901--904},
  year={2001}
}

@article{vanDalen2011Extended,
  title={Extended VTS for noise-robust speech recognition},
  author={van Dalen, R. C. and Gales, M. J. F.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={4},
  pages={733--743},
  year={2011},
  publisher={IEEE}
}

@inproceedings{Li2012Improvements,
  title={Improvements to VTS feature enhancement},
  author={Li, J. and Seltzer, M. L. and Gong, Y.},
  booktitle={Proc. ICASSP},
  pages={4677--4680},
  year={2012},
  publisher={IEEE}
}

@inproceedings{Li2012Efficient,
  title={VTS model adaptation with a diagonal Jacobian approximation method},
  author={Li, J. and Seltzer, M. L. and Gong, Y.},
  booktitle={Proc. Interspeech},
  pages={1906--1909},
  year={2012}
}

@inproceedings{Gales2007Predictive,
  title={Predictive linear transforms for noise robust speech recognition},
  author={Gales, M. J. F. and van Dalen, R. C.},
  booktitle={Proc. ASRU},
  pages={59--64},
  year={2007}
}

@article{Liao2008Issues,
  title={Issues with uncertainty decoding for noise robust automatic speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  journal={Speech Communication},
  volume={50},
  number={4},
  pages={265--277},
  year={2008}
}

@article{Xu2011Joint,
  title={Joint uncertainty decoding with predictive methods for noise robust speech recognition},
  author={Xu, H. and Gales, M. J. F. and Chin, K. K.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={6},
  pages={1665--1676},
  year={2011},
  publisher={IEEE}
}
@article{Xu2011Joint,
title={Joint uncertainty decoding with predictive methods for noise robust speech recognition},
author={Xu, H. and Gales, M. J. F. and Chin, K. K.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={19},
number={6},
pages={1665--1676},
year={2011},
publisher={IEEE}
}

@article{Julier2004Unscented,
title={Unscented filtering and nonlinear estimation},
author={Julier, S. J. and Uhlmann, J. K.},
journal={Proceedings of the IEEE},
volume={92},
number={3},
pages={401--422},
year={2004},
publisher={IEEE}
}

@inproceedings{Stouten2005Kalman,
title={Kalman and unscented Kalman filter feature enhancement for noise robust ASR},
author={Stouten, V. and Van Hamme, H. and Wambacq, P.},
booktitle={Proc. Interspeech},
pages={953--956},
year={2005}
}

@inproceedings{Hu2006An,
title={An HMM compensation approach using unscented transformation for noisy speech recognition},
author={Hu, Y. and Huo, Q.},
booktitle={ISCSLP},
year={2006}
}

@inproceedings{Hu2009Comparison,
title={Comparison of estimation techniques in joint uncertainty decoding for noise robust speech recognition},
author={Hu, H. and Chin, K. K.},
booktitle={Proc. Interspeech},
pages={2403--2406},
year={2009}
}

@inproceedings{Faubel2010On,
title={On expectation maximization based channel and noise estimation beyond the vector Taylor series expansion},
author={Faubel, F. and McDonough, J. and Klakow, D.},
booktitle={Proc. ICASSP},
pages={4294--4297},
year={2010}
}

@inproceedings{Li2010Unscented,
title={Unscented transform with online distortion estimation for HMM adaptation},
author={Li, J. and Yu, D. and Gong, Y. and Deng, L.},
booktitle={Proc. Interspeech},
pages={1660--1663},
year={2010}
}

@inproceedings{Dalen2011A,
title={A variational perspective on noise-robust speech recognition},
author={van Dalen, R. C. and Gales, M. J. F.},
booktitle={Proc. ASRU},
pages={125--130},
year={2011}
}

@inproceedings{Leutnant2011A,
title={A versatile Gaussian splitting approach to non-linear state estimation and its application to noise-robust ASR},
author={Leutnant, V. and Krueger, A. and Haeb-Umbach, R.},
booktitle={Proc. Interspeech},
pages={1641--1644},
year={2011}
}

@article{Gales2001Acoustic,
title={Acoustic factorisation},
author={Gales, M. J. F.},
journal={Proc. ASRU},
pages={77--80},
year={2001}
}

@inproceedings{Garcia2011Combining,
title={Combining speaker and noise feature normalization techniques for automatic speech recognition},
author={Garc{'\i}a, L. and Ben{'\i}tez, C. and Segura, J. C. and Umesh, S.},
booktitle={Proc. ICASSP},
pages={5496--5499},
year={2011}
}

@inproceedings{Seltzer2011Separating,
title={Separating speaker and environmental variability using factored transforms},
author={Seltzer, M. L. and Acero, A.},
booktitle={Proc. Interspeech},
pages={1097--1100},
year={2011}
}

@inproceedings{Rouvier2011Factor,
title={Factor analysis based session variability compensation for automatic speech recognition},
author={Rouvier, M. and Bouallegue, M. and Matrouf, D. and Linares, G.},
booktitle={Proc. ASRU},
pages={141--145},
year={2011}
}

@inproceedings{Karafiat2011iVector,
title={iVector-based discriminative adaptation for automatic speech recognition},
author={Karafiat, M. and Burget, L. and Matejka, P. and Glembek, O. and Cernock{'y}, J.},
booktitle={Proc. ASRU},
pages={152--157},
year={2011}
}

@article{Wang2012Speaker,
title={Speaker and noise factorisation for robust speech recognition},
author={Wang, Y. and Gales, M. J. F.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={20},
number={7},
pages={2149--2158},
year={2012},
publisher={IEEE}
}

@inproceedings{Wang2012An,
title={An explicit independence constraint for factorised adaptation in speech recognition},
author={Wang, Y. and Gales, M. J. F.},
booktitle={Proc. Interspeech},
pages={1233--1237},
year={2013}
}

@article{Jiang2002ARobust,
  title={A robust compensation strategy for extraneous acoustic variations in spontaneous speech recognition},
  author={Jiang, H. and Deng, L.},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={10},
  number={1},
  pages={9--17},
  year={2002},
  publisher={IEEE}
}

@article{Merhav1993AMinimax,
  title={A minimax classification approach with application to robust speech recognition},
  author={Merhav, N. and Lee, C. H.},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={1},
  number={1},
  pages={90--100},
  year={1993},
  publisher={IEEE}
}

@inproceedings{Huo1997ABayesian,
  title={A Bayesian predictive classification approach to robust speech recognition},
  author={Huo, Q. and Jiang, H. and Lee, C. H.},
  booktitle={Proc. ICASSP},
  pages={1547--1550},
  year={1997}
}

@article{Huo2000ABayesian,
  title={A Bayesian predictive classification approach to robust speech recognition},
  author={Huo, Q. and Lee, C. H.},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={8},
  number={2},
  pages={200--204},
  year={2000},
  publisher={IEEE}
}

@article{Afify2002Upper,
  title={Upper and lower bounds on the mean of noisy speech: application to minimax classification},
  author={Afify, M. and Siohan, O. and Lee, C. H.},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={10},
  number={2},
  pages={79--88},
  year={2002},
  publisher={IEEE}
}
@inproceedings{Hermansky2000Tandem,
  title={Tandem connectionist feature extraction for conventional HMM systems},
  author={Hermansky, H. and Ellis, D. P. W. and Sharma, S.},
  booktitle={Proc. ICASSP},
  volume={3},
  pages={1635--1638},
  year={2000},
  publisher={IEEE}
}

@inproceedings{Du2012IVNbased,
  title={IVN-based joint training of GMM and HMMs using an improved VTS-based feature compensation for noisy speech recognition},
  author={Du, J. and Huo, Q.},
  booktitle={Proc. Interspeech},
  pages={1227--1230},
  year={2012}
}
@inproceedings{Hermansky2000Tandem,
  title={Tandem connectionist feature extraction for conventional HMM systems},
  author={Hermansky, H. and Ellis, D. P. W. and Sharma, S.},
  booktitle={Proc. ICASSP},
  volume={3},
  pages={1635--1638},
  year={2000},
  publisher={IEEE}
}

@inproceedings{Du2012IVNbased,
  title={IVN-based joint training of GMM and HMMs using an improved VTS-based feature compensation for noisy speech recognition},
  author={Du, J. and Huo, Q.},
  booktitle={Proc. Interspeech},
  pages={1227--1230},
  year={2012}
}

@inproceedings{Arrowood2002Using,
  title={Using observation uncertainty in HMM decoding},
  author={Arrowood, J. A. and Clements, M. A.},
  booktitle={Proc. Interspeech},
  pages={1561--1564},
  year={2002}
}

@article{Deng2002Exploiting,
  title={Exploiting variances in robust feature extraction based on a parametric model of speech distortion},
  author={Deng, L. and Droppo, J. and Acero, A.},
  journal={Proc. Interspeech},
  pages={2449--2452},
  year={2002}
}

@article{Deng2005Dynamic,
  title={Dynamic compensation of HMM variances using the feature enhancement uncertainty computed from a parametric model of speech distortion},
  author={Deng, L. and Droppo, J. and Acero, A.},
  journal={IEEE T-SAP},
  volume={13},
  number={3},
  pages={412--421},
  year={2005},
  publisher={IEEE}
}

@article{Stouten2006Modelbased,
  title={Model-based feature enhancement with uncertainty decoding for noise robust ASR},
  author={Stouten, V. and Van Hamme, H. and Wambacq, P.},
  journal={Speech Communication},
  volume={48},
  number={11},
  pages={1502--1514},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{Droppo2002Uncertainty,
  title={Uncertainty decoding with SPLICE for noise robust speech recognition},
  author={Droppo, J. and Deng, L. and Acero, A.},
  booktitle={Proc. ICASSP},
  volume={1},
  pages={57--60},
  year={2002},
  publisher={IEEE}
}

@inproceedings{Liao2006Joint,
  title={Joint uncertainty decoding for robust large vocabulary speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  booktitle={Proc. Interspeech},
  pages={3129--3132},
  year={2005}
}

@article{Liao2008Issues,
  title={Issues with uncertainty decoding for noise robust automatic speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  journal={Speech Communication},
  volume={50},
  number={4},
  pages={265--277},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{Liao2005Joint,
  title={Joint uncertainty decoding for noise robust speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  booktitle={Proc. Interspeech},
  pages={3129--3132},
  year={2005}
}

@inproceedings{Liao2007Adaptive,
  title={Adaptive training with joint uncertainty decoding for robust recognition of noisy data},
  author={Liao, H. and Gales, M. J. F.},
  booktitle={Proc. ICASSP},
  volume={4},
  pages={389--392},
  year={2007},
  publisher={IEEE}
}

@article{Srinivasan2007Transforming,
  title={Transforming binary uncertainties for robust speech recognition},
  author={Srinivasan, S. and Wang, D.},
  journal={IEEE T-ASLP},
  volume={15},
  number={7},
  pages={2130--2140},
  year={2007},
  publisher={IEEE}
}

@article{Astudillo2013Computing,
  title={Computing MMSE estimates and residual uncertainty directly in the feature domain of asr using stft domain speech distortion models},
  author={Astudillo, R. F. and Orglmeister, R.},
  journal={IEEE T-ASLP},
  volume={21},
  number={5},
  pages={1023--1034},
  year={2013},
  publisher={IEEE}
}
@article{Liao2008Issueselsvier,
  title={Issues with uncertainty decoding for noise robust automatic speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  journal={Speech Communication},
  volume={50},
  number={4},
  pages={265--277},
  year={2008},
  publisher={Elsevier}
}
@inproceedings{Droppo2002Uncertainty,
  title={Uncertainty decoding with SPLICE for noise robust speech recognition},
  author={Droppo, J. and Deng, L. and Acero, A.},
  booktitle={Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  volume={1},
  pages={57--60},
  year={2002},
  organization={IEEE}
}

@inproceedings{Liao2006IssuesICASSP,
  title={Issues with uncertainty decoding for noise robust speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  booktitle={Proceedings of Interspeech},
  pages={1121--1124},
  year={2006}
}
@techreport{Liao2006Issues,
  title={Joint uncertainty decoding for robust large vocabulary speech recognition},
  author={Liao, H. and Gales, M. J. F.},
  institution={University of Cambridge},
  number={CUED/TR552},
  year={2006}
}

@techreport{Gales1996The,
  title={The generation and use of regression class trees for MLLR adaptation},
  author={Gales, M. J. F.},
  institution={University of Cambridge},
  number={CUED/TR263},
  year={1996}
}

@article{Gales1998Maximum,
  title={Maximum likelihood linear transformations for HMM-based speech recognition},
  author={Gales, M. J. F.},
  journal={Computer, Speech, and Language},
  volume={12},
  pages={75--98},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{Gales2007Predictive,
  title={Predictive linear transforms for noise robust speech recognition},
  author={Gales, M. J. F. and van Dalen, R. C.},
  booktitle={Proceedings of Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={59--64},
  year={2007},
  organization={IEEE}
}

@article{Xu2011Joint,
  title={Joint uncertainty decoding with predictive methods for noise robust speech recognition},
  author={Xu, H. and Gales, M. J. F. and Chin, K. K.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={6},
  pages={1665--1676},
  year={2011},
  publisher={IEEE}
}

@inproceedings{Kim2011Noisy,
  title={Noisy constrained maximum-likelihood linear regression for noise-robust speech recognition},
  author={Kim, D. K. and Gales, M. J. F.},
  booktitle={Proceedings of IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={2},
  pages={315--325},
  year={2011},
  organization={IEEE}
}

@inproceedings{Povey2010Subspace,
  title={Subspace Gaussian mixture models for speech recognition},
  author={Povey, D. and Burget, L. and Agarwal, M. and Akyazi, P. and Feng, K. and Ghoshal, A. and Glembek, O. and Goel, N. K. and Karafi{\'a}t, M. and Rastrow, A. and Rose, R. C. and Schwarz, P. and Thomas, S.},
  booktitle={Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  pages={4330--4333},
  year={2010},
  organization={IEEE}
}

@article{Lu2013Joint,
  title={Joint uncertainty decoding for noise robust subspace Gaussian mixture models},
  author={Lu, L. and Chin, K. K. and Ghoshal, A. and Renals, S.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={21},
  number={9},
  pages={1791--1804},
  year={2013},
  publisher={IEEE}
}
@inproceedings{Cooke1994Handling,
title={Handling missing data in speech recognition},
author={Cooke, M. and Green, P. D. and Crawford, M.},
booktitle={Proceedings of International Conference on Spoken Language Processing (ICSLP)},
pages={1555--1558},
year={1994}
}

@inproceedings{Lippmann1997Using,
title={Using missing feature theory to actively select features for robust speech recognition with interruptions, filtering and noise},
author={Lippmann, R. and Carlson, B.},
booktitle={Proceedings of European Conference on Speech Communication and Technology (Eurospeech)},
pages={37--40},
year={1997}
}

@article{Cooke2001Robust,
title={Robust automatic speech recognition with missing and unreliable acoustic data},
author={Cooke, M. and Green, P. D. and Josifovski, L. and Vizinho, A.},
journal={Speech Communication},
volume={34},
number={3},
pages={267--285},
year={2001},
publisher={Elsevier}
}

@article{Barker2005Decoding,
title={Decoding speech in the presence of other sources},
author={Barker, J. P. and Cooke, M. P. and Ellis, D. P. W.},
journal={Speech Communication},
volume={45},
number={1},
pages={5--25},
year={2005},
publisher={Elsevier}
}

@article{Raj2005Missing,
title={Missing-feature approaches in speech recognition},
author={Raj, B. and Stern, R. M.},
journal={IEEE Signal Processing Magazine},
volume={22},
number={5},
pages={101--116},
year={2005},
publisher={IEEE}
}

@article{Raj2004Reconstruction,
title={Reconstruction of missing features for robust speech recognition},
author={Raj, B. and Seltzer, M. L. and Stern, R. M.},
journal={Speech Communication},
volume={43},
number={4},
pages={275--296},
year={2004},
publisher={Elsevier}
}

@inproceedings{Raj2005Reconstructing,
title={Reconstructing spectral vectors with uncertain spectrographic masks for robust speech recognition},
author={Raj, B. and Singh, R.},
booktitle={Proceedings of Automatic Speech Recognition and Understanding (ASRU)},
pages={65--70},
year={2005},
organization={IEEE}
}

@inproceedings{Josifovski1999State,
title={State-based imputation of missing data for robust speech recognition and speech enhancement},
author={Josifovski, L. and Cooke, M. and Green, P. D. and Vizinho, A.},
booktitle={Proceedings of European Conference on Speech Communication and Technology (Eurospeech)},
pages={2837--2840},
year={1999}
}

@article{VanSegbroeck2011Advances,
title={Advances in missing feature techniques for robust large-vocabulary continuous speech recognition},
author={Van Segbroeck, M. and Van Hamme, H.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={19},
number={1},
pages={123--137},
year={2011},
publisher={IEEE}
}

@article{Hartmann2013Direct,
title={A direct masking approach to robust ASR},
author={Hartmann, W. and Narayanan, A. and Fosler-Lussier, E. and Wang, D.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={21},
number={10},
pages={1993--2005},
year={2013},
publisher={IEEE}
}

@inproceedings{Hartmann2011Investigations,
title={Investigations into the incorporation of the ideal binary mask in ASR},
author={Hartmann, W. and Fosler-Lussier, E.},
booktitle={Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages={4804--4807},
year={2011},
organization={IEEE}
}

@inproceedings{Vizinho1999Missing,
title={Missing data theory, spectral subtraction and signal-to-noise estimation for robust ASR: an integrated study},
author={Vizinho, A. and Green, P. D. and Cooke, M. and Josifovski, L.},
booktitle={Proceedings of European Conference on Speech Communication and Technology (Eurospeech)},
pages={2407--2410},
year={1999}
}

@inproceedings{ElMaliki1999Missing,
title={Missing features detection and handling for robust speaker verification},
author={El-Maliki, M. and Drygajlo, A.},
booktitle={Proceedings of European Conference on Speech Communication and Technology (Eurospeech)},
pages={975--978},
year={1999}
}

@inproceedings{vanHout2012Novel,
title={A novel approach to soft-mask estimation and log-spectral enhancement for robust speech recognition},
author={van Hout, J. and Alwan, A.},
booktitle={Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages={4105--4108},
year={2012},
organization={IEEE}
}

@inproceedings{Renevey1999Missing,
title={Missing feature theory and probabilistic estimation of clean speech components for robust speech recognition},
author={Renevey, P. and Drygajlo, A.},
booktitle={Proceedings of European Conference on Speech Communication and Technology (Eurospeech)},
pages={2627--2630},
year={1999}
}

@article{Seltzer2004Bayesian,
title={A Bayesian classifier for spectrographic mask estimation for missing feature speech recognition},
author={Seltzer, M. L. and Raj, B. and Stern, R. M.},
journal={Speech Communication},
volume={43},
number={4},
pages={379--393},
year={2004},
publisher={Elsevier}
}

@inproceedings{Barker2001Robust,
title={Robust ASR based on clean speech models: an evaluation of missing data techniques for connected digit recognition in noise},
author={Barker, J. and Cooke, M. and Green, P. D.},
booktitle={Proceedings of International Conference on Spoken Language Processing (ICSLP)},
pages={213--217},
year={2001}
}

@article{Palomaki2004Binaural,
title={A binaural processor for missing data speech recognition in the presence of noise and small-room reverberation},
author={Palomaki, K. J. and Brown, G. J. and Wang, D. L.},
journal={Speech Communication},
volume={43},
number={4},
pages={361--378},
year={2004},
publisher={Elsevier}
}

@inproceedings{Narayanan2013Ideal,
title={Ideal ratio mask estimation using deep neural networks},
author={Narayanan, A. and Wang, D. L.},
booktitle={Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages={7092--7096},
year={2013},
organization={IEEE}
}

@inproceedings{Josifovski1999State,
title={Soft decisions in missing data techniques for robust automatic speech recognition},
author={Josifovski, L. and Cooke, M. and Green, P. D. and Vizinho, A.},
booktitle={Proceedings of International Conference on Spoken Language Processing (ICSLP)},
pages={373--376},
year={2000}
}

@article{Gonzalez2013Mmse,
title={MMSE-based missing-feature reconstruction with temporal modeling for robust speech recognition},
author={Gonzalez, J. A. and Peinado, A. M. and Ma, N. and Gomez, A. M. and Barker, J.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={21},
number={3},
pages={624--635},
year={2013},
publisher={IEEE}
}

@inproceedings{Hartmann2012ASR-Driven,
title={ASR-driven top-down binary mask estimation using spectral priors},
author={Hartmann, W. and Fosler-Lussier, E.},
booktitle={Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages={4685--4688},
year={2012},
organization={IEEE}
}

@inproceedings{Deng2000Large,
title={Large vocabulary speech recognition under adverse acoustic environment},
author={Deng, L. and Acero, A. and Plumpe, M. and Huang, X.},
booktitle={Proceedings of International Conference on Spoken Language Processing (ICSLP)},
volume={3},
pages={806--809},
year={2000}
}

@inproceedings{Gong1997Source,
title={Source normalization training for HMM applied to noisy telephone speech recognition},
author={Gong, Y.},
booktitle={Proceedings of European Conference on Speech Communication and Technology (Eurospeech)},
pages={1555--1558},
year={1997}
}

@inproceedings{Liao2007Adaptive,
title={Adaptive training with joint uncertainty decoding for robust recognition of noisy data},
author={Liao, H. and Gales, M. J. F.},
booktitle={Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
volume={4},
pages={389--392},
year={2007},
organization={IEEE}
}

@inproceedings{Wu2002Supervised,
title={Supervised adaptation of MCE-trained CDHMMs using minimum classification error linear regression},
author={Wu, J. and Huo, Q.},
booktitle={Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
volume={I},
pages={605--608},
year={2002},
organization={IEEE}
}

@inproceedings{Hu2007Irrelevant,
title={Irrelevant variability normalization based HMM training using VTS approximation of an explicit model of environmental distortions},
author={Hu, Y. and Huo, Q.},
booktitle={Proceedings of Interspeech},
pages={1042--1045},
year={2007}
}

@inproceedings{Kalinli2010Noise,
title={Noise adaptive training using a vector Taylor series approach for noise robust automatic speech recognition},
author={Kalinli, O. and Seltzer, M. L. and Acero, A.},
booktitle={Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages={3825--3828},
year={2009},
organization={IEEE}
}

@inproceedings{Anastasakos1996Compact,
title={A compact model for speaker-adaptive training},
author={Anastasakos, T. and McDonough, J. and Schwartz, R. and Makhoul, J.},
booktitle={Proceedings of International Conference on Spoken Language Processing (ICSLP)},
volume={2},
pages={1137--1140},
year={1996}
}
@inproceedings{Liao2007Adaptive,
title={Adaptive training with joint uncertainty decoding for robust recognition of noisy data},
author={Liao, H. and Gales, M.J.F.},
booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume={4},
pages={389--392},
year={2007}
}

@inproceedings{Hu2007Irrelevant,
title={Irrelevant variability normalization based HMM training using VTS approximation of an explicit model of environmental distortions},
author={Hu, Q. and Huo, Q.},
booktitle={Proceedings of Annual Conference of the International Speech Communication Association (INTERSPEECH)},
pages={1042--1045},
year={2007}
}

@inproceedings{Kalinli2010Noise,
title={Noise adaptive training using a vector Taylor series approach for noise robust automatic speech recognition},
author={Kalinli, O. and Seltzer, M.L. and Acero, A.},
booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={3825--3828},
year={2010}
}

@article{Liao2005Joint,
title={Joint uncertainty decoding for noise robust speech recognition},
author={Liao, H. and Gales, M.J.F.},
journal={Technical Report, CUED/TR552, University of Cambridge},
year={2005}
}

@inproceedings{Kim1998Speech,
title={Speech recognition in noisy environments using first-order vector Taylor series},
author={Kim, D.Y. and Un, C.K. and Kim, N.S.},
booktitle={Speech Communication},
volume={24},
number={1},
pages={39--49},
year={1998}
}

@inproceedings{Li2007Highperformance,
title={High-performance robust speech recognition with feature compensation based on MAP estimation of piecewise linear transformations},
author={Li, H. and Huo, Q.},
booktitle={Proceedings of Annual Conference of the International Speech Communication Association (INTERSPEECH)},
pages={1429--1432},
year={2007}
}

@inproceedings{Li2009Unified,
title={Unified approach to MAP estimation of piecewise linear transformations and adaptive training},
author={Li, H. and Huo, Q.},
booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={3801--3804},
year={2009}
}

@inproceedings{Flego2009Discriminative,
title={Discriminative adaptive training with VTS and JUD},
author={Flego, F. and Gales, M.J.F.},
booktitle={Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
pages={170--175},
year={2009}
}

@inproceedings{Wu2002Environment,
title={An environment compensated minimum classification error training approach and its evaluation on Aurora2 database},
author={Wu, J. and Huo, Q.},
booktitle={Proceedings of Annual Conference of the International Speech Communication Association (INTERSPEECH)},
pages={453--456},
year={2002}
}

@article{Wu2006Environment,
title={An environment-compensated minimum classification error training approach based on stochastic vector mapping},
author={Wu, J. and Huo, Q.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={14},
number={6},
pages={2147--2155},
year={2006}
}

@inproceedings{Huo2006Maximum,
title={A maximum likelihood training approach to irrelevant variability compensation based on piecewise linear transformations},
author={Huo, Q. and Zhu, D.},
booktitle={Proceedings of Annual Conference of the International Speech Communication Association (INTERSPEECH)},
pages={1129--1132},
year={2006}
}

@inproceedings{Zhu2008Irrelevant,
title={Irrelevant variability normalization based HMM training using MAP estimation of feature transforms for robust speech recognition},
author={Zhu, D. and Huo, Q.},
booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={4717--4720},
year={2008}
}
@article{Li2007Highperformance,
title={High-performance robust speech recognition with feature compensation based on MAP estimation of piecewise linear transformations},
author={Li, H. and Huo, Q.},
journal={Proceedings of Annual Conference of the International Speech Communication Association (INTERSPEECH)},
pages={1429--1432},
year={2007}
}

@article{Li2009Unified,
title={Unified approach to MAP estimation of piecewise linear transformations and adaptive training},
author={Li, H. and Huo, Q.},
journal={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={3801--3804},
year={2009}
}

@article{Li2012Improvements,
title={Improvements on model-space and feature-space adaptation},
author={Li, H. and Huo, Q.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={20},
number={1},
pages={315--325},
year={2012}
}

@article{Yoshioka2013Noise,
title={Noise model transfer: A novel approach to robustness against nonstationary noise},
author={Yoshioka, T. and Nakatani, T.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={21},
number={10},
pages={2182--2192},
year={2013}
}

@article{Seltzer2013Investigation,
title={An investigation of deep neural networks for noise robust speech recognition},
author={Seltzer, M. L. and Yu, D. and Wang, Y.},
journal={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={7398--7402},
year={2013}
}

@article{Wang2012Speaker,
title={Speaker and noise factorisation for robust speech recognition},
author={Wang, Y. and Gales, M. J. F.},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
volume={20},
number={7},
pages={2149--2158},
year={2012}
}

@article{Li2013Improving,
title={Improving wideband speech recognition using mixed-bandwidth training data in CD-DNN-HMM},
author={Li, J. and Yu, D. and Huang, J. T. and Gong, Y.},
journal={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},
pages={131--136},
year={2012}
}

@article{Yu2013Feature,
title={Feature learning in deep neural networks--studies on speech recognition tasks},
author={Yu, D. and Seltzer, M. and Li, J. and Huang, J. T.},
journal={Proceedings of International Conference on Learning Representations},
year={2013}
}

@article{Delcroix2013Is,
title={Is speech enhancement pre-processing still relevant when using deep neural networks for acoustic modeling},
author={Delcroix, M. and Kubo, Y. and Nakatani, T. and Nakamura, A.},
journal={Proceedings of Interspeech},
pages={2992--2996},
year={2013}
}

@article{Li2013Noise,
title={Noise adaptive front-end normalization based on vector Taylor series for deep neural networks in robust speech recognition},
author={Li, B. and Sim, K. C.},
journal={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={7408--7412},
year={2013}
}

@article{Sainath2013Learning,
title={Learning filter banks within a deep neural network framework},
author={Sainath, T. N. and Kingsbury, B. and Mohamed, A. and Ramabhadran, B.},
journal={Proceedings of Annual Conference of the International Speech Communication Association (INTERSPEECH)},
pages={297--302},
year={2013}
}

@article{VanDalen2011Variational,
title={A variational perspective on noise-robust speech recognition},
author={Van Dalen, R. C. and Gales, M. J. F.},
journal={Proceedings of Annual Conference of the International Speech Communication Association (INTERSPEECH)},
pages={125--130},
year={2011}
}
