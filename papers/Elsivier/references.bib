@inproceedings{farooq_improving_2019,
	title = {Improving {Large} {Vocabulary} {Urdu} {Speech} {Recognition} {System} {Using} {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html},
	doi = {10.21437/Interspeech.2019-2629},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Rauf, Sahar and Hussain, Sarmad},
	month = sep,
	year = {2019},
	pages = {2978--2982},
}

@misc{alphacephei_alpha_nodate,
	title = {Alpha {Cephei}},
	url = {https://github.com/alphacep},
	abstract = {Alpha Cephei has 37 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2022-08-16},
	journal = {GitHub},
	author = {Alphacephei},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\C9UHNTWM\\alphacep.html:text/html},
}

@misc{alphacep_vosk_2022,
	title = {Vosk {Speech} {Recognition} {Toolkit}},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-api},
	abstract = {Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C\# and Node},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-09-03T17:48:42Z},
	keywords = {android, asr, deep-learning, deep-neural-networks, deepspeech, google-speech-to-text, ios, kaldi, offline, privacy, python, raspberry-pi, speaker-identification, speaker-verification, speech-recognition, speech-to-text, speech-to-text-android, stt, voice-recognition, vosk},
}

@misc{alphacep_alphacepvosk-server_2022,
	title = {alphacep/vosk-server},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-server},
	abstract = {WebSocket, gRPC and WebRTC speech recognition server based on Vosk and Kaldi libraries},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-05-07T17:24:55Z},
	keywords = {asr, kaldi, python, speech-recognition, vosk, grpc, saas, webrtc, websocket},
}

@misc{alphacep_vosk_nodate,
	title = {{VOSK} {Offline} {Speech} {Recognition} {API}},
	url = {https://alphacephei.com/vosk/},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\H4HQA2XC\\vosk.html:text/html},
}

@misc{alphacep_vosk_nodate-1,
	title = {{VOSK} {Models}},
	url = {https://alphacephei.com/vosk/models},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\J75LJ82E\\models.html:text/html},
}

@misc{daniel_povey_kaldi_nodate,
	title = {Kaldi: {Kaldi}},
	url = {https://kaldi-asr.org/doc/},
	urldate = {2022-08-16},
	author = {Daniel Povey},
	file = {Kaldi\: Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\QYDHVT6P\\doc.html:text/html},
}

@misc{noauthor_openslrorg_nodate,
	title = {openslr.org},
	url = {https://www.openslr.org/resources.php},
	urldate = {2022-08-16},
	file = {openslr.org:C\:\\Users\\DELL\\Zotero\\storage\\EJWY9PRN\\resources.html:text/html},
}

@misc{cmu_cmu_nodate,
	title = {{CMU} {Lexicon} {Tool}},
	url = {http://www.speech.cs.cmu.edu/tools/lextool.html},
	urldate = {2022-08-16},
	author = {CMU},
	file = {CMU Lexicon Tool:C\:\\Users\\DELL\\Zotero\\storage\\BTMXKZQL\\lextool.html:text/html},
}

@misc{coqui-ai_coqui-aistt_2022,
	title = {coqui-ai/{STT}},
	copyright = {MPL-2.0},
	url = {https://github.com/coqui-ai/STT},
	abstract = {��STT - The deep learning toolkit for Speech-to-Text. Training and deploying STT models has never been so easy.},
	urldate = {2022-08-16},
	publisher = {coqui},
	author = {coqui-ai},
	month = aug,
	year = {2022},
	note = {original-date: 2021-03-04T04:54:42Z},
	keywords = {asr, deep-learning, speech-recognition, speech-to-text, stt, voice-recognition, automatic-speech-recognition, speech-recognition-api, speech-recognizer, tensorflow},
}

@misc{noauthor_speechbrain_nodate,
	title = {{SpeechBrain}: {A} {PyTorch} {Speech} {Toolkit}},
	url = {https://speechbrain.github.io/},
	urldate = {2022-08-16},
}

@misc{piero_molino_ludwig_nodate,
	title = {Ludwig - code-free deep learning toolbox},
	url = {http://ludwig.ai},
	abstract = {Ludwig is a toolbox for training and testing deep learning models without writing code},
	language = {en},
	urldate = {2022-08-16},
	author = {Piero Molino},
}

@misc{noauthor_espnet_2022,
	title = {{ESPnet}: end-to-end speech processing toolkit},
	copyright = {Apache-2.0},
	shorttitle = {{ESPnet}},
	url = {https://github.com/espnet/espnet},
	abstract = {End-to-End Speech Processing Toolkit},
	urldate = {2022-08-16},
	publisher = {ESPnet},
	month = aug,
	year = {2022},
	note = {original-date: 2017-12-13T00:45:11Z},
	keywords = {deep-learning, kaldi, speech-recognition, chainer, end-to-end, machine-translation, pytorch, speech-enhancement, speech-separation, speech-synthesis, speech-translation, voice-conversion},
}

@misc{noauthor_espnet_nodate,
	title = {{ESPnet}: end-to-end speech processing toolkit — {ESPnet} 202207 documentation},
	url = {https://espnet.github.io/espnet/},
	urldate = {2022-08-16},
}

@article{georgescu_performance_2021,
	title = {Performance vs. hardware requirements in state-of-the-art automatic speech recognition},
	volume = {2021},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00217-4},
	doi = {10.1186/s13636-021-00217-4},
	abstract = {Abstract
            The last decade brought significant advances in automatic speech recognition (ASR) thanks to the evolution of deep learning methods. ASR systems evolved from pipeline-based systems, that modeled hand-crafted speech features with probabilistic frameworks and generated phone posteriors, to end-to-end (E2E) systems, that translate the raw waveform directly into words using one deep neural network (DNN). The transcription accuracy greatly increased, leading to ASR technology being integrated into many commercial applications. However, few of the existing ASR technologies are suitable for integration in embedded applications, due to their hard constrains related to computing power and memory usage. This overview paper serves as a guided tour through the recent literature on speech recognition and compares the most popular ASR implementations. The comparison emphasizes the trade-off between ASR performance and hardware requirements, to further serve decision makers in choosing the system which fits best their embedded application. To the best of our knowledge, this is the first study to provide this kind of trade-off analysis for state-of-the-art ASR systems.},
	language = {en},
	number = {1},
	urldate = {2022-08-16},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Georgescu, Alexandru-Lucian and Pappalardo, Alessandro and Cucu, Horia and Blott, Michaela},
	month = dec,
	year = {2021},
	pages = {28},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TRA6KAJY\\Georgescu et al. - 2021 - Performance vs. hardware requirements in state-of-.pdf:application/pdf},
}

@misc{cplc_cplc_nodate,
	title = {{CPLC} – {Citizens}-{Police} {Liaison} {Committee}},
	url = {http://www.cplc.org.pk/},
	urldate = {2022-08-16},
	author = {CPLC},
}

@misc{british_broadcast_bbc_nodate,
	title = {{BBC} - {Languages} - {Urdu} - {A} {Guide} to {Urdu} - 10 facts about the {Urdu} language},
	url = {https://www.bbc.co.uk/languages/other/urdu/guide/facts.shtml},
	abstract = {Discover surprising and revealing facts about Urdu, including Urdu words used in the English language and Urdu jokes and quotes.},
	language = {en},
	urldate = {2022-08-16},
	author = {British Broadcast},
	note = {Last Modified: 2008-01-30T12:35:00Z},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4CXS8LEU\\facts.html:text/html},
}

@misc{ethnologue_urdu_nodate,
	title = {Urdu {Language} - {Ethnologue} {Report}},
	url = {https://www.ethnologue.com/language/urdu},
	abstract = {A language profile for Urdu. Get a detailed look at the language, from population to dialects and usage.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Ethnologue},
	author = {Ethnologue},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\WGWY3RRQ\\urd.html:text/html},
}

@article{ali_automatic_2015,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-16},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@phdthesis{sehar_gul_detecting_2020,
	address = {Karachi},
	title = {{DETECTING} {MALICIOUS} {ACTIVITIES} {OVER} {TELEPHONE} {NETWORK} {FOR} {URDU} {SPEAKER}},
	abstract = {Telephone is one of most important invention in the fields of communication it is because on
this invention that we are able to connect with our friends and families without hassle of travelling
and going to their places,but some people are also using it for negative purpose therefore to secure
this medium of communication is one of the most important issue of today as many malicious
activities are taking place on this channel.Humanely it is not possible to tap each and every phone
call so that one could find malicious activities that are being done.In order to find such malicious
activities we need an automatic system that can automatically detect malicious voice activities,
for that we have decided to develop an automatic speech recognition system that will detect malicious
sentences in Urdu Language from the telephonic conversation which will then be processed
further.},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Sehar Gul},
	month = jun,
	year = {2020},
}

@incollection{hutchison_speaker_2010,
	address = {Berlin, Heidelberg},
	title = {Speaker {Independent} {Urdu} {Speech} {Recognition} {Using} {HMM}},
	volume = {6177},
	isbn = {978-3-642-13880-5},
	url = {http://link.springer.com/10.1007/978-3-642-13881-2_14},
	urldate = {2022-08-16},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ashraf, Javed and Iqbal, Naveed and Khattak, Naveed Sarfraz and Zaidi, Ather Mohsin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Hopfe, Christina J. and Rezgui, Yacine and Métais, Elisabeth and Preece, Alun and Li, Haijiang},
	year = {2010},
	doi = {10.1007/978-3-642-13881-2_14},
	note = {Series Title: Lecture Notes in Computer Science
ISBN2: 978-3-642-13881-2},
	pages = {140--148},
}

@inproceedings{sarfraz_large_2010,
	address = {Islamabad, Pakistan},
	title = {Large vocabulary continuous speech recognition for {Urdu}},
	isbn = {978-1-4503-0342-2},
	url = {http://portal.acm.org/citation.cfm?doid=1943628.1943629},
	doi = {10.1145/1943628.1943629},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Frontiers} of {Information} {Technology} - {FIT} '10},
	publisher = {ACM Press},
	author = {Sarfraz, Huda and Parveen, Rahila and Hussain, Sarmad and Bokhari, Riffat and Raza, Agha Ali and Ullah, Inam and Sarfraz, Zahid and Pervez, Sophia and Mustafa, Asad and Javed, Iqra},
	year = {2010},
	pages = {1--5},
}

@inproceedings{qasim_urdu_2016,
	address = {Bali, Indonesia},
	title = {Urdu speech recognition system for district names of {Pakistan}: {Development}, challenges and solutions},
	isbn = {978-1-5090-3516-8},
	shorttitle = {Urdu speech recognition system for district names of {Pakistan}},
	url = {http://ieeexplore.ieee.org/document/7918979/},
	doi = {10.1109/ICSDA.2016.7918979},
	urldate = {2022-08-16},
	booktitle = {2016 {Conference} of {The} {Oriental} {Chapter} of {International} {Committee} for {Coordination} and {Standardization} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Qasim, Muhammad and Nawaz, Sohaib and Hussain, Sarmad and Habib, Tania},
	month = oct,
	year = {2016},
	pages = {28--32},
}

@article{aguiar_de_lima_survey_2020,
	title = {A survey on automatic speech recognition systems for {Portuguese} language and its variations},
	volume = {62},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230819302992},
	doi = {10.1016/j.csl.2019.101055},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Aguiar de Lima, Thales and Da Costa-Abreu, Márjory},
	month = jul,
	year = {2020},
	pages = {101055},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\5YG7YNP8\\Aguiar de Lima and Da Costa-Abreu - 2020 - A survey on automatic speech recognition systems f.pdf:application/pdf},
}

@inproceedings{dash_automatic_2018,
	title = {Automatic {Speech} {Recognition} with {Articulatory} {Information} and a {Unified} {Dictionary} for {Hindi}, {Marathi}, {Bengali} and {Oriya}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/dash18_interspeech.html},
	doi = {10.21437/Interspeech.2018-2122},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Dash, Debadatta and Kim, Myungjong and Teplansky, Kristin and Wang, Jun},
	month = sep,
	year = {2018},
	pages = {1046--1050},
}

@inproceedings{patil_automatic_2016,
	address = {Pune, India},
	title = {Automatic {Speech} {Recognition} of isolated words in {Hindi} language using {MFCC}},
	isbn = {978-1-5090-1338-8},
	url = {http://ieeexplore.ieee.org/document/7915008/},
	doi = {10.1109/CAST.2016.7915008},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Computing}, {Analytics} and {Security} {Trends} ({CAST})},
	publisher = {IEEE},
	author = {Patil, U. G. and Shirbahadurkar, S. D. and Paithane, A. N.},
	month = dec,
	year = {2016},
	pages = {433--438},
}

@article{a_n_mishra_robust_2011,
	title = {Robust {Features} for {Connected} {Hindi} {Digits} {Recognition}},
	volume = {4},
	number = {2},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {A. N. Mishra and Mahesh Chandra and Astik Biswas and S. N. Sharan},
	month = jun,
	year = {2011},
}

@inproceedings{aggarwal_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	abstract = {The goal of automatic speech recognition (ASR) system is to accurately and efficiently convert a speech signal into a text message independent of the device, speaker or the environment. In general the speech signal is captured and pre-processed at front-end for feature extraction and evaluated at back-end using the Gaussian mixture hidden Markov model. In this statistical approach since the evaluation of Gaussian likelihoods dominate the total computational load, the appropriate selection of Gaussian mixtures is very important depending upon the amount of training data. As the small databases are available to train the Indian languages ASR system, the higher range of Gaussian mixtures (i.e. 64 and above), normally used for European languages, cannot be applied for them. This paper reviews the statistical framework and presents an iterative procedure to select an optimum number of Gaussian mixtures that exhibits maximum accuracy in the context of Hindi speech recognition system.},
	booktitle = {International {Journal} of {Signal} {Processing}, {Image} {Processing} and {Pattern} {Recognition}},
	author = {Aggarwal, R. K. and Dave, M.},
	year = {2011},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\FVSZRDG9\\Aggarwal and Dave - 2011 - Using Gaussian Mixtures for Hindi Speech Recogniti.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\NYC9U7U3\\summary.html:text/html},
}

@inproceedings{r_k_aggarwal_and_m_dave_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	author = {R. K. Aggarwal {and} M. Dave},
	year = {2011},
}

@inproceedings{b_venkataraman_sopc-based_2006,
	title = {{SOPC}-based speech-to-text conversion},
	booktitle = {Embedded {Processor} {Design} {Contest} {Outstanding} {Designs}},
	author = {B. Venkataraman},
	year = {2006},
}

@incollection{tanveer_continuous_2019,
	address = {Singapore},
	title = {Continuous {Hindi} {Speech} {Recognition} {Using} {Kaldi} {ASR} {Based} on {Deep} {Neural} {Network}},
	volume = {748},
	isbn = {9789811309229},
	url = {http://link.springer.com/10.1007/978-981-13-0923-6_26},
	urldate = {2022-08-16},
	booktitle = {Machine {Intelligence} and {Signal} {Analysis}},
	publisher = {Springer Singapore},
	author = {Upadhyaya, Prashant and Mittal, Sanjeev Kumar and Farooq, Omar and Varshney, Yash Vardhan and Abidi, Musiur Raza},
	editor = {Tanveer, M. and Pachori, Ram Bilas},
	year = {2019},
	doi = {10.1007/978-981-13-0923-6_26},
	note = {Series Title: Advances in Intelligent Systems and Computing
ISBN2: 9789811309236},
	pages = {303--311},
}

@inproceedings{karel_vesel_sequence-discriminative_2013,
	title = {Sequence-discriminative training of deep neural networks},
	abstract = {Luk Burget},
	author = {Karel Vesel and Arnab Ghoshal and Daniel Povey},
	year = {2013},
}

@inproceedings{k_v_s_parsad_and_s_m_virk_computational_2012,
	title = {Computational evidence that {Hindi} and {Urdu} share a grammar but not the lexicon},
	booktitle = {3rd {Workshop} on {South} and {Southeast} {Asian} {NLP}},
	author = {K. V. S. Parsad {and} S. M. Virk},
	year = {2012},
}

@article{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1402.1128},
	doi = {10.48550/ARXIV.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2022-08-16},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{abdel-hamid_exploring_2013,
	title = {Exploring {Convolutional} {Neural} {Network} {Structures} and {Optimization} {Techniques} for {Speech} {Recognition}},
	url = {https://www.microsoft.com/en-us/research/publication/exploring-convolutional-neural-network-structures-and-optimization-techniques-for-speech-recognition/},
	abstract = {Recently, convolutional neural networks (CNNs) have been shown to outperform the standard fully connected deep neural networks within the hybrid deep neural network / hidden Markov model (DNN/HMM) framework on the phone recognition task. In this paper, we extend the earlier basic form of the CNN and explore it in multiple ways. We first investigate several CNN architectures, including full and limited weight sharing, convolution along frequency and time axes, and stacking of several convolution layers. We then develop a novel weighted softmax pooling layer so that the size in the pooling layer can be automatically learned. Further, we evaluate the effect of CNN pretraining, which is achieved by using a convolutional version of the RBM. We show that all CNN architectures we have investigated outperform the earlier basic form of the DNN on both the phone recognition and large vocabulary speech recognition tasks. The architecture with limited weight sharing provides additional gains over the full weight sharing architecture. The softmax pooling layer performs as well as the best CNN with the manually tuned fixed-pooling size, and has a potential for further improvement. Finally, we show that CNN pretraining produces significantly better results on a large vocabulary speech recognition task.},
	booktitle = {Interspeech 2013},
	publisher = {ISCA},
	author = {Abdel-Hamid, Ossama and Deng, Li and Yu, Dong},
	month = aug,
	year = {2013},
	note = {Edition: Interspeech 2013},
}

@misc{noauthor_gram_nodate,
	title = {{GRAM} {VAANI} {ASR} {Challenge} 2022},
	url = {https://sites.google.com/view/gramvaaniasrchallenge/home},
	abstract = {A challenge on Automatic Speech Recognition for Hindi is being organized as part of INTERSPEECH 2022 by sharing the spontaneous telephone speech recordings collected by a social technology enterprise Gram Vaani. The regional variations of Hindi together with spontaneity of speech, natural},
	language = {en},
	urldate = {2022-08-16},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P4IHK3L5\\home.html:text/html},
}

@misc{noauthor_gramvaaniorg_nodate,
	title = {gramvaani.org},
	url = {https://gramvaani.org/},
	urldate = {2022-08-16},
}

@inproceedings{latif_cross_2018,
	address = {Islamabad, Pakistan},
	title = {Cross {Lingual} {Speech} {Emotion} {Recognition}: {Urdu} vs. {Western} {Languages}},
	isbn = {978-1-5386-9355-1},
	shorttitle = {Cross {Lingual} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/8616972/},
	doi = {10.1109/FIT.2018.00023},
	urldate = {2022-08-16},
	booktitle = {2018 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	publisher = {IEEE},
	author = {Latif, Siddique and Qayyum, Adnan and Usman, Muhammad and Qadir, Junaid},
	month = dec,
	year = {2018},
	pages = {88--93},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\NLRUGBJI\\Latif et al. - 2018 - Cross Lingual Speech Emotion Recognition Urdu vs..pdf:application/pdf},
}

@article{besacier_automatic_2014,
	title = {Automatic speech recognition for under-resourced languages: {A} survey},
	volume = {56},
	issn = {01676393},
	shorttitle = {Automatic speech recognition for under-resourced languages},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988},
	doi = {10.1016/j.specom.2013.07.008},
	language = {en},
	urldate = {2022-08-16},
	journal = {Speech Communication},
	author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
	month = jan,
	year = {2014},
	pages = {85--100},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YRF6UNN8\\Besacier et al. - 2014 - Automatic speech recognition for under-resourced l.pdf:application/pdf},
}

@article{lakshmi_sri_kaldi_2020,
	title = {Kaldi recipe in {Hindi} for word level recognition and phoneme level transcription},
	volume = {171},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050920312606},
	doi = {10.1016/j.procs.2020.04.268},
	language = {en},
	urldate = {2022-08-16},
	journal = {Procedia Computer Science},
	author = {Lakshmi Sri, Karra Venkata and Srinivasan, Mayuka and Nair, Radhika Rajeev and Priya, K. Jeeva and Gupta, Deepa},
	year = {2020},
	pages = {2476--2485},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5J9VQKIM\\Lakshmi Sri et al. - 2020 - Kaldi recipe in Hindi for word level recognition a.pdf:application/pdf},
}

@misc{qureshi_urdu_2021,
	title = {Urdu {Speech} {Recognition}},
	copyright = {MIT},
	url = {https://github.com/ZoraizQ/urdu-speech-recognition},
	abstract = {Urdu Speech Recognition using Kaldi ASR, by training Triphone Acoustic GMMs using the PRUS dataset.},
	urldate = {2022-08-16},
	author = {Qureshi, Zoraiz},
	month = oct,
	year = {2021},
	note = {original-date: 2021-04-21T10:11:17Z},
	keywords = {speech-recognition, kaldi-asr, multi-speaker, prus, urdu},
}

@inproceedings{asadullah_automatic_2016,
	address = {Portsmouth},
	title = {Automatic {Urdu} {Speech} {Recognition} using {Hidden} {Markov} {Model}},
	isbn = {978-1-5090-3755-1},
	url = {https://ieeexplore.ieee.org/document/7571287/},
	doi = {10.1109/ICIVC.2016.7571287},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Image}, {Vision} and {Computing} ({ICIVC})},
	publisher = {IEEE},
	author = {{Asadullah} and Shaukat, Arslan and Ali, Hazrat and Akram, Usman},
	month = aug,
	year = {2016},
	pages = {135--139},
}

@misc{chodroff_corpus_2018,
	title = {Corpus {Phonetics} {Tutorial}},
	url = {http://arxiv.org/abs/1811.05553},
	abstract = {Corpus phonetics has become an increasingly popular method of research in linguistic analysis. With advances in speech technology and computational power, large scale processing of speech data has become a viable technique. This tutorial introduces the speech scientist and engineer to various automatic speech processing tools. These include acoustic model creation and forced alignment using the Kaldi Automatic Speech Recognition Toolkit (Povey et al., 2011), forced alignment using FAVE-align (Rosenfelder et al., 2014), the Montreal Forced Aligner (McAuliffe et al., 2017), and the Penn Phonetics Lab Forced Aligner (Yuan \& Liberman, 2008), as well as stop consonant burst alignment using AutoVOT (Keshet et al., 2014). The tutorial provides a general overview of each program, step-by-step instructions for running the program, as well as several tips and tricks.},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Chodroff, Eleanor},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05553 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3GAPGJTY\\Chodroff - 2018 - Corpus Phonetics Tutorial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5EDZSUNT\\1811.html:text/html},
}

@article{alharbi_automatic_2021,
	title = {Automatic {Speech} {Recognition}: {Systematic} {Literature} {Review}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Automatic {Speech} {Recognition}},
	doi = {10.1109/ACCESS.2021.3112535},
	abstract = {A huge amount of research has been done in the field of speech signal processing in recent years. In particular, there has been increasing interest in the automatic speech recognition (ASR) technology field. ASR began with simple systems that responded to a limited number of sounds and has evolved into sophisticated systems that respond fluently to natural language. This systematic review of automatic speech recognition is provided to help other researchers with the most significant topics published in the last six years. This research will also help in identifying recent major ASR challenges in real-world environments. In addition, it discusses current research gaps in ASR. This review covers articles available in five research databases that were completed according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) protocol. The search strategy yielded 82 conferences and articles related to the study’s scope for the period 2015–2020. The results presented in this review shed light on research trends in the area of ASR and also suggest new research directions.},
	journal = {IEEE Access},
	author = {Alharbi, Sadeen and Alrazgan, Muna and Alrashed, Alanoud and Alnomasi, Turkiayh and Almojel, Raghad and Alharbi, Rimah and Alharbi, Saja and Alturki, Sahar and Alshehri, Fatimah and Almojil, Maha},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {ASR challenges, ASR systematic review, automatic speech recognition, Automatic speech recognition, Databases, Licenses, Nails, Quality assessment, Software, Speech recognition, Systematics},
	pages = {131858--131876},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\TLVJTS37\\9536732.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\7LCUHWDI\\Alharbi et al. - 2021 - Automatic Speech Recognition Systematic Literatur.pdf:application/pdf},
}

@article{alsayadi_arabic_2021,
	title = {Arabic speech recognition using end-to-end deep learning},
	volume = {15},
	issn = {1751-9683},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12057},
	doi = {10.1049/sil2.12057},
	abstract = {Arabic automatic speech recognition (ASR) methods with diacritics have the ability to be integrated with other systems better than Arabic ASR methods without diacritics. In this work, the application of state-of-the-art end-to-end deep learning approaches is investigated to build a robust diacritised Arabic ASR. These approaches are based on the Mel-Frequency Cepstral Coefficients and the log Mel-Scale Filter Bank energies as acoustic features. To the best of our knowledge, end-to-end deep learning approach has not been used in the task of diacritised Arabic automatic speech recognition. To fill this gap, this work presents a new CTC-based ASR, CNN-LSTM, and an attention-based end-to-end approach for improving diacritisedArabic ASR. In addition, a word-based language model is employed to achieve better results. The end-to-end approaches applied in this work are based on state-of-the-art frameworks, namely ESPnet and Espresso. Training and testing of these frameworks are performed based on the Standard Arabic Single Speaker Corpus (SASSC), which contains 7 h of modern standard Arabic speech. Experimental results show that the CNN-LSTM with an attention framework outperforms conventional ASR and the Joint CTC-attention ASR framework in the task of Arabic speech recognition. The CNN-LSTM with an attention framework could achieve a word error rate better than conventional ASR and the Joint CTC-attention ASR by 5.24\% and 2.62\%, respectively.},
	language = {en},
	number = {8},
	urldate = {2022-08-16},
	journal = {IET Signal Processing},
	author = {Alsayadi, Hamzah A. and Abdelhamid, Abdelaziz A. and Hegazy, Islam and Fayed, Zaki T.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/sil2.12057},
	pages = {521--534},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8MD9EIN6\\Alsayadi et al. - 2021 - Arabic speech recognition using end-to-end deep le.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8NYWWX8A\\sil2.html:text/html},
}

@inproceedings{raza_rapid_2018,
	title = {Rapid {Collection} of {Spontaneous} {Speech} {Corpora} {Using} {Telephonic} {Community} {Forums}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/raza18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1139},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Raza, Agha Ali and Athar, Awais and Randhawa, Shan and Tariq, Zain and Saleem, Muhammad Bilal and Bin Zia, Haris and Saif, Umar and Rosenfeld, Roni},
	month = sep,
	year = {2018},
	pages = {1021--1025},
}

@inproceedings{naeem_subspace_2020,
	title = {Subspace {Gaussian} {Mixture} {Model} for {Continuous} {Urdu} {Speech} {Recognition} using {Kaldi}},
	doi = {10.1109/ICOSST51357.2020.9333026},
	abstract = {Automatic Speech Recognition Systems (ASR) have significantly improved in recent years, where deep learning is playing an important role in the development of end to end ASR's. ASR is the task of converting spoken language into computer readable text. ASRs are becoming ever more prevalent way to interact with technology, thereby significantly closing the gap in terms of how humans interact with computers, making it more natural. Urdu is an under resourced language, for which training such a system requires a huge amount of data that is not readily available. In this paper we present improvements to the architecture of a statistical automatic speech recognition system for which the components involved in a statistical ASR have been explored in great detail. We also present the results on various statistical models that are trained for Urdu language. We choose the Kaldi toolkit for training the Urdu ASR using approximately 100 hours of transcribed data. The refined Subspace Gaussian Model gives a word error rate of 9\% on the test set.},
	booktitle = {2020 14th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Naeem, Saad and Iqbal, Majid and Saqib, Muhammad and Saad, Muhammad and Raza, Muhammad Soban and Ali, Zaid and Akhtar, Naveed and Beg, Mirza Omer and Shahzad, Waseem and Arshad, Muhhamad Umair},
	month = dec,
	year = {2020},
	keywords = {Adaptation models, Analytical models, Automatic Speech Recognition, Computational modeling, Context modeling, Hidden Markov models, Hidden Markov Models, Mel-frequency Cepstrum, Probabilistic logic, Subspace Gaussian Mixture Models, Training},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\3KACECHI\\9333026.html:text/html},
}

@inproceedings{khan_multi-genre_2021,
	address = {Singapore, Singapore},
	title = {A {Multi}-{Genre} {Urdu} {Broadcast} {Speech} {Recognition} {System}},
	isbn = {978-1-66540-870-7},
	url = {https://ieeexplore.ieee.org/document/9660552/},
	doi = {10.1109/O-COCOSDA202152914.2021.9660552},
	urldate = {2022-08-16},
	booktitle = {2021 24th {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Khan, Erbaz and Rauf, Sahar and Adeeba, Farah and Hussain, Sarmad},
	month = nov,
	year = {2021},
	pages = {25--30},
}

@incollection{somogyi_automatic_2021,
	address = {Cham},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-3-030-60031-0 978-3-030-60032-7},
	url = {http://link.springer.com/10.1007/978-3-030-60032-7_5},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Application} of {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Somogyi, Zoltán},
	collaborator = {Somogyi, Zoltán},
	year = {2021},
	doi = {10.1007/978-3-030-60032-7_5},
	pages = {145--171},
}

@incollection{chapelle_automatic_2020,
	edition = {1},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4051-9473-0 978-1-4051-9843-1},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781405198431.wbeal0066.pub2},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Encyclopedia} of {Applied} {Linguistics}},
	publisher = {Wiley},
	author = {Levis, John and Suvorov, Ruslan},
	editor = {Chapelle, Carol A.},
	month = dec,
	year = {2020},
	doi = {10.1002/9781405198431.wbeal0066.pub2},
	pages = {1--8},
}

@incollection{gold_brief_2011,
	address = {Hoboken, NJ, USA},
	title = {Brief {History} of {Automatic} {Speech} {Recognition}},
	isbn = {978-1-118-14288-2 978-0-470-19536-9},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118142882.ch4},
	urldate = {2022-08-16},
	booktitle = {Speech and {Audio} {Signal} {Processing}},
	publisher = {John Wiley \& Sons, Inc.},
	collaborator = {Gold, Ben and Morgan, Nelson and Ellis, Dan},
	month = oct,
	year = {2011},
	doi = {10.1002/9781118142882.ch4},
	pages = {40--58},
}

@misc{kincaid_brief_2018,
	title = {A {Brief} {History} of {ASR}: {Automatic} {Speech} {Recognition}},
	shorttitle = {A {Brief} {History} of {ASR}},
	url = {https://medium.com/descript/a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5},
	abstract = {This is the first post in a series on Automatic Speech Recognition, the foundational technology that makes Descript possible. We’ll be…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FHD2LASC\\a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5.html:text/html},
}

@article{suma_swamy_evolution_2013,
	title = {Evolution of {Speech} {Recognition} – {A} {Brief} {History} of {Technology} {Development}},
	volume = {60},
	journal = {Elixir Adv. Engg. Info.},
	author = {Suma Swamy and Ramakrishnan, Kollengode},
	month = jul,
	year = {2013},
}

@article{smit_advances_2021,
	title = {Advances in subword-based {HMM}-{DNN} speech recognition across languages},
	volume = {66},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300917},
	doi = {10.1016/j.csl.2020.101158},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Smit, Peter and Virpioja, Sami and Kurimo, Mikko},
	month = mar,
	year = {2021},
	pages = {101158},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC6HIS7B\\Smit et al. - 2021 - Advances in subword-based HMM-DNN speech recogniti.pdf:application/pdf},
}

@misc{kincaid_state_2018,
	title = {The {State} of {Automatic} {Speech} {Recognition}: {Q}\&{A} with {Kaldi}’s {Dan} {Povey}},
	shorttitle = {The {State} of {Automatic} {Speech} {Recognition}},
	url = {https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85},
	abstract = {This article continues our series on Automatic Speech Recognition, including our recent piece on the History of ASR.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AI8J5D9U\\the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85.html:text/html},
}

@misc{blog_machine_2022,
	title = {Machine {Learning} {Models} {Are} {Only} as {Good} as the {Data} {They} {Are} {Trained} {On}},
	url = {https://deepchecks.com/machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on/},
	abstract = {Learn about the importance of data validation in machine learning, look into the various tools for different sets of data validation techniques \& procedures.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Deepchecks},
	author = {Blog, Deepchecks Community},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5T5L9UMP\\machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on.html:text/html},
}

@misc{noauthor_machine_2018,
	title = {“{A} machine learning model is only as good as the data it is fed”},
	url = {https://devm.io/machine-learning/apache-spark-machine-learning-interview-143122},
	abstract = {Apache Spark 2.3 was released earlier this year; it marked a major milestone for Structured Streaming but there are a lot of other interesting features that deserve your attention. We talked with Reynold Xin, co-founder and Chief Architect at Databricks about the Databricks Runtime and other enhancements introduced in Apache Spark 2.3.},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {devmio - expand your knowledge},
	month = jun,
	year = {2018},
	note = {Section: Artikel},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HS46J6C4\\apache-spark-machine-learning-interview-143122.html:text/html},
}

@misc{brownlee_why_2020,
	title = {Why {Data} {Preparation} {Is} {So} {Important} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/data-preparation-is-important/},
	abstract = {On a predictive modeling project, machine learning algorithms learn a mapping from input variables to a target variable. The most common form of predictive modeling project involves so-called structured data or tabular data. This is data as it looks in a spreadsheet or a matrix, with rows of examples and columns of features for each […]},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HGMCR7CK\\data-preparation-is-important.html:text/html},
}

@book{ilyas_data_2019,
	address = {New York, NY},
	title = {Data {Cleaning}},
	isbn = {978-1-4503-7153-7},
	language = {English},
	publisher = {ACM Books},
	author = {Ilyas, Ihab F. and Chu, Xu},
	month = jun,
	year = {2019},
}

@misc{khan_introduction_2021,
	title = {An {Introduction} to {Classification} {Using} {Mislabeled} {Data}},
	url = {https://towardsdatascience.com/an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5},
	abstract = {The performance of any classifier, or for that matter any machine learning task, depends crucially on the quality of the available data…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Medium},
	author = {Khan, Shihab Shahriar},
	month = mar,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SY93L99I\\an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5.html:text/html},
}

@article{cao_joint_2019,
	title = {Joint {Prostate} {Cancer} {Detection} and {Gleason} {Score} {Prediction} in mp-{MRI} via {FocalNet}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8653866/},
	doi = {10.1109/TMI.2019.2901928},
	number = {11},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Cao, Ruiming and Mohammadian Bajgiran, Amirhossein and Afshari Mirak, Sohrab and Shakeri, Sepideh and Zhong, Xinran and Enzmann, Dieter and Raman, Steven and Sung, Kyunghyun},
	month = nov,
	year = {2019},
	pages = {2496--2506},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WK7STA45\\Cao et al. - 2019 - Joint Prostate Cancer Detection and Gleason Score .pdf:application/pdf},
}

@article{fan_impact_2021,
	title = {The {Impact} of {Mislabeled} {Changes} by {SZZ} on {Just}-in-{Time} {Defect} {Prediction}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2019.2929761},
	abstract = {Just-in-Time (JIT) defect prediction-a technique which aims to predict bugs at change level-has been paid more attention. JIT defect prediction leverages the SZZ approach to identify bug-introducing changes. Recently, researchers found that the performance of SZZ (including its variants) is impacted by a large amount of noise. SZZ may considerably mislabel changes that are used to train a JIT defect prediction model, and thus impact the prediction accuracy. In this paper, we investigate the impact of the mislabeled changes by different SZZ variants on the performance and interpretation of JIT defect prediction models. We analyze four SZZ variants (i.e., B-SZZ, AG-SZZ, MA-SZZ, and RA-SZZ) that are proposed by prior studies. We build the prediction models using the labeled data by these four SZZ variants. Among the four SZZ variants, RA-SZZ is least likely to generate mislabeled changes, and we construct the testing set by using RA-SZZ. All of the four prediction models are then evaluated on the same testing set. We choose the prediction model built on the labeled data by RA-SZZ as the baseline model, and we compare the performance and metric importance of the models trained using the labeled data by the other three SZZ variants with the baseline model. Through a large-scale empirical study on a total of 126,526 changes from ten Apache open source projects, we find that in terms of various performance measures (AUC, F1-score, G-mean and Recall@20\%), the mislabeled changes by B-SZZ and MA-SZZ are not likely to cause a considerable performance reduction, while the mislabeled changes by AG-SZZ cause a statistically significant performance reduction with an average difference of 1-5 percent. When considering developers' inspection effort (measured by LOC) in practice, the changes mislabeled B-SZZ and AG-SZZ lead to 9-10 and 1-15 percent more wasted inspection effort, respectively. And the mislabeled changes by B-SZZ lead to significantly more wasted effort. The mislabeled changes by MA-SZZ do not cause considerably more wasted effort. We also find that the top-most important metric for identifying bug-introducing changes (i.e., number of files modified in a change) is robust to the mislabeling noise generated by SZZ. But the second- and third-most important metrics are more likely to be impacted by the mislabeling noise, unless random forest is used as the underlying classifier.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fan, Yuanrui and Xia, Xin and da Costa, Daniel Alencar and Lo, David and Hassan, Ahmed E. and Li, Shanping},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Analytical models, Computer bugs, Data models, Inspection, Just-in-time defect prediction, Measurement, mining software repositories, noisy data, Predictive models, SZZ, Testing},
	pages = {1559--1586},
}

@inproceedings{van_rooyen_learning_2015,
	title = {Learning with {Symmetric} {Label} {Noise}: {The} {Importance} of {Being} {Unhinged}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {van Rooyen, Brendan and Menon, Aditya and Williamson, Robert C},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{natarajan_learning_2013,
	title = {Learning with {Noisy} {Labels}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@article{frenay_classification_2014,
	title = {Classification in the {Presence} of {Label} {Noise}: {A} {Survey}},
	volume = {25},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Classification in the {Presence} of {Label} {Noise}},
	url = {http://ieeexplore.ieee.org/document/6685834/},
	doi = {10.1109/TNNLS.2013.2292894},
	number = {5},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Frenay, Benoit and Verleysen, Michel},
	month = may,
	year = {2014},
	pages = {845--869},
}

@misc{zhang_uberi_speechrecognition_nodate,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-08-16},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\9RSTEWBT\\SpeechRecognition.html:text/html},
}

@inproceedings{panayotov_librispeech_2015,
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	shorttitle = {Librispeech},
	doi = {10.1109/ICASSP.2015.7178964},
	abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {Bioinformatics, Blogs, Corpus, Electronic publishing, Genomics, Information services, LibriVox, Resource description framework, Speech Recognition},
	pages = {5206--5210},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\2AK7VZ3R\\7178964.html:text/html},
}

@article{backstrom_introduction_2022,
	title = {Introduction to {Speech} {Processing}: 2nd {Edition}},
	copyright = {Open Access},
	shorttitle = {Introduction to {Speech} {Processing}},
	url = {https://zenodo.org/record/6821775},
	doi = {10.5281/ZENODO.6821775},
	abstract = {This release is primarily about migrating all content to jupyter-books and git. The published version is now hosted at https://speechprocessingbook.aalto.fi. In addition to github, the release has long-term storage location at Zenodo, which also assigns a DOI to the release. We have some entirely new sections, such as Forensic speaker recognition. There are also plenty of small improvements everywhere.},
	language = {en},
	urldate = {2022-08-16},
	author = {Bäckström, Tom and Räsänen, Okko and Zewoudie, Abraham and Zarazaga, Pablo Pérez and Koivusalo, Liisa and Das, Sneha and Mellado, Esteban Gómez and Mariem Bouafif Mansali and Ramos, Daniel},
	month = jul,
	year = {2022},
	note = {Publisher: Zenodo
Version Number: v2},
	keywords = {speech processing},
}

@book{yu_automatic_2015,
	address = {London},
	series = {Signals and {Communication} {Technology}},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4471-5778-6},
	url = {http://link.springer.com/10.1007/978-1-4471-5779-3},
	urldate = {2022-08-16},
	publisher = {Springer London},
	author = {Yu, Dong and Deng, Li},
	year = {2015},
	note = {ISBN2: 978-1-4471-5779-3},
}

@article{park_review_2021,
	title = {A {Review} of {Speaker} {Diarization}: {Recent} {Advances} with {Deep} {Learning}},
	volume = {72},
	copyright = {arXiv.org perpetual, non-exclusive license},
	issn = {101317},
	shorttitle = {A {Review} of {Speaker} {Diarization}},
	url = {https://arxiv.org/abs/2101.09624},
	doi = {10.48550/ARXIV.2101.09624},
	abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
	urldate = {2022-08-17},
	journal = {Computer Speech \& Language},
	author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
This article is a preprint version of the article published in Computer Speech \& Language, Volume 72, March 2022, 101317},
}

@misc{juang_automatic_nodate,
	title = {Automatic {Speech} {Recognition} – {A} {Brief} {History} of the {Technology} {Development} {Abstract}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis [1, 2], the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. H. and Rabiner, Lawrence R.},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\3YU9F3ID\\Juang and Rabiner - Automatic Speech Recognition – A Brief History of .pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\I34GUPHY\\summary.html:text/html},
}

@incollection{maglogiannis__2020,
	address = {Cham},
	title = {Α {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4 978-3-030-49161-1},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\K3RKXDZZ\\Filippidou and Moussiades - 2020 - Α Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@inproceedings{morris_wer_2004,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{anusuya_speech_2010,
	title = {Speech {Recognition} by {Machine}, {A} {Review}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1001.2267},
	doi = {10.48550/ARXIV.1001.2267},
	abstract = {This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.},
	urldate = {2022-08-17},
	author = {Anusuya, M. A. and Katti, S. K.},
	year = {2010},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
25 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS December 2009, ISSN 1947 5500, http://sites.google.com/site/ijcsis/},
}

@article{upton_speech_1984,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages — this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-17},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@article{davis_automatic_1952,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@inproceedings{maja_popovic_hjerson_2011,
	title = {Hjerson: {An} {Open} {Source} {Tool} for {Automatic} {Error} {Classification} of {Machine} {Translation} {Output}},
	author = {Maja Popovic},
	year = {2011},
}

@article{errattahi_automatic_2018,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-17},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YWBD74CJ\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@inproceedings{povey_daniel_and_ghoshal_arnab_and_boulianne_gilles_and_burget_lukas_and_glembek_ondrej_and_goel_nagendra_and_hannemann_mirko_and_motlicek_petr_and_qian_yanmin_and_schwarz_petr__and_others_kaldi_2011,
	title = {The {Kaldi} speech recognition toolkit},
	booktitle = {Workshop on automatic speech recognition and understanding},
	author = {Povey, Daniel {and} Ghoshal, Arnab {and} Boulianne, Gilles {and} Burget, Lukas {and} Glembek, Ondrej {and} Goel, Nagendra {and} Hannemann, Mirko {and} Motlicek, Petr {and} Qian, Yanmin {and} Schwarz, Petr  {and} others},
	year = {2011},
}

@misc{sutherland_short_2017,
	title = {A short history of speech recognition},
	url = {https://medium.com/@sutherlandjamie/a-short-history-of-speech-recognition-9b8e78ad086a},
	abstract = {There has been more progress in speech recognition technology in the last 3 years than in the first 30 years. Computing power and…},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Sutherland, Jamie},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\LAELJJKS\\a-short-history-of-speech-recognition-9b8e78ad086a.html:text/html},
}

@inproceedings{xiong_microsoft_2017,
	address = {New Orleans, LA},
	title = {The microsoft 2016 conversational speech recognition system},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953159/},
	doi = {10.1109/ICASSP.2017.7953159},
	urldate = {2022-08-17},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Xiong, W. and Droppo, J. and Huang, X. and Seide, F. and Seltzer, M. and Stolcke, A. and Yu, D. and Zweig, G.},
	month = mar,
	year = {2017},
	pages = {5255--5259},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\ZVTCMYVF\\Xiong et al. - 2017 - The microsoft 2016 conversational speech recogniti.pdf:application/pdf},
}

@misc{qi_benchmarking_2021,
	title = {Benchmarking {Commercial} {Intent} {Detection} {Services} with {Practice}-{Driven} {Evaluations}},
	url = {http://arxiv.org/abs/2012.03929},
	abstract = {Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users' text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the intent detection models can see a different distribution of test data when being deployed in the real world, leading to poor accuracy. Finally, a practical intent detection model must be computationally efficient in both training and single query inference so that it can be used continuously and re-trained frequently. We benchmark intent detection methods on a variety of datasets. Our results show that Watson Assistant's intent detection model outperforms other commercial solutions and is comparable to large pretrained language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ.},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Qi, Haode and Pan, Lin and Sood, Atin and Shah, Abhishek and Kunc, Ladislav and Yu, Mo and Potdar, Saloni},
	month = jun,
	year = {2021},
	note = {arXiv:2012.03929 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at NAACL2021 Industry Track},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\XMUN9XIG\\Qi et al. - 2021 - Benchmarking Commercial Intent Detection Services .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5JX39X9L\\2012.html:text/html},
}

@inproceedings{arora_hint3_2020,
	address = {Online},
	title = {{HINT3}: {Raising} the bar for {Intent} {Detection} in the {Wild}},
	shorttitle = {{HINT3}},
	url = {https://www.aclweb.org/anthology/2020.insights-1.16},
	doi = {10.18653/v1/2020.insights-1.16},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Proceedings of the {First} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Arora, Gaurav and Jain, Chirag and Chaturvedi, Manas and Modi, Krupal},
	year = {2020},
	pages = {100--105},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\SYABT9YP\\Arora et al. - 2020 - HINT3 Raising the bar for Intent Detection in the.pdf:application/pdf},
}

@misc{patel_data-centric_2021,
	title = {Data-{Centric} {Approach} vs {Model}-{Centric} {Approach} in {Machine} {Learning}},
	url = {https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning},
	abstract = {Code and data are the foundations of the AI system. Both of these components play an important role in the development of a robust model but which one should you focus on more? In this article, we’ll go through the data-centric vs model-centric approaches, and see which one is better, we would also talk about […]},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {neptune.ai},
	author = {Patel, Harshil},
	month = dec,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ZT7NNEUA\\data-centric-vs-model-centric-machine-learning.html:text/html},
}

@misc{radecic_data-centric_2022,
	title = {Data-centric vs. {Model}-centric {AI}? {The} {Answer} is {Clear}},
	shorttitle = {Data-centric vs. {Model}-centric {AI}?},
	url = {https://towardsdatascience.com/data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67},
	abstract = {There’s something wrong with the current approach to AI. But there’s a solution.},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Radečić, Dario},
	month = mar,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\X4DU4GPV\\data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67.html:text/html},
}

@misc{noauthor_chat_nodate,
	title = {A {Chat} with {Andrew} on {MLOps}: {From} {Model}-centric to {Data}-centric {AI} - {YouTube}},
	url = {https://www.youtube.com/watch?v=06-AZXmwHjo&t=69s},
	urldate = {2022-08-17},
}

@misc{noauthor_data-centric_nodate,
	title = {Data-centric {Machine} {Learning}: {Making} customized {ML} solutions production-ready},
	shorttitle = {Data-centric {Machine} {Learning}},
	url = {https://dida.do/blog/data-centric-machine-learning},
	abstract = {In this article, we will see why many ML Projects do not make it into production, introduce the concepts of model- and data-centric ML, and give examples how we at dida improve projects by applying data-centric techniques.},
	language = {en},
	urldate = {2022-08-17},
	journal = {dida Machine Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\F2569RZH\\data-centric-machine-learning.html:text/html},
}

@misc{noauthor_significance_nodate,
	title = {The {Significance} of {Data}-centric {AI}},
	url = {https://www.kdnuggets.com/the-significance-of-data-centric-ai.html/},
	abstract = {How a systematic way of maintaining data quality can do wonders to your model performance.},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {KDnuggets},
	note = {Section: KDnuggets Originals},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SGUDCHX6\\significance-data-centric-ai.html:text/html},
}

@misc{deeplearningai_data-centric_2021,
	title = {Data-centric {AI}: {Real} {World} {Approaches}},
	shorttitle = {Data-centric {AI}},
	url = {https://www.youtube.com/watch?v=Yqj7Kyjznh4},
	urldate = {2022-08-17},
	author = {{DeepLearningAI}},
	month = aug,
	year = {2021},
}

@article{creutz_morph-based_2007,
	title = {Morph-based speech recognition and modeling of out-of-vocabulary words across languages},
	volume = {5},
	issn = {1550-4875, 1550-4883},
	url = {https://dl.acm.org/doi/10.1145/1322391.1322394},
	doi = {10.1145/1322391.1322394},
	abstract = {We explore the use of morph-based language models in large-vocabulary continuous-speech recognition systems across four so-called morphologically rich languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. The morphs are subword units discovered in an unsupervised, data-driven way using the
              Morfessor
              algorithm. By estimating
              n
              -gram language models over sequences of morphs instead of words, the quality of the language model is improved through better vocabulary coverage and reduced data sparsity. Standard word models suffer from high out-of-vocabulary (OOV) rates, whereas the morph models can recognize previously unseen word forms by concatenating morphs. It is shown that the morph models do perform fairly well on OOVs without compromising the recognition accuracy on in-vocabulary words. The Arabic experiment constitutes the only exception since here the standard word model outperforms the morph model. Differences in the datasets and the amount of data are discussed as a plausible explanation.},
	language = {en},
	number = {1},
	urldate = {2022-08-18},
	journal = {ACM Transactions on Speech and Language Processing},
	author = {Creutz, Mathias and Hirsimäki, Teemu and Kurimo, Mikko and Puurula, Antti and Pylkkönen, Janne and Siivola, Vesa and Varjokallio, Matti and Arisoy, Ebru and Saraçlar, Murat and Stolcke, Andreas},
	month = dec,
	year = {2007},
	pages = {1--29},
}

@misc{noauthor_mfcc_nodate,
	title = {{MFCC} vs {FBANK} for chain models ?},
	url = {https://groups.google.com/g/kaldi-help/c/_7hB74HKhC4},
	urldate = {2022-08-18},
	file = {MFCC vs FBANK for chain models ?:C\:\\Users\\DELL\\Zotero\\storage\\5FKKZNEQ\\_7hB74HKhC4.html:text/html},
}

@book{reithaug_orchestrating_2002,
	title = {Orchestrating {Success} in {Reading}},
	isbn = {978-0-9694974-4-8},
	url = {https://books.google.com.pk/books?id=\_YGZMQAACAAJ},
	publisher = {Stirling Head Enterprises},
	author = {Reithaug, D.},
	year = {2002},
}

@article{zia_pronouncur_2018,
	title = {{PronouncUR}: {An} {Urdu} {Pronunciation} {Lexicon} {Generator}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{PronouncUR}},
	url = {https://arxiv.org/abs/1801.00409},
	doi = {10.48550/ARXIV.1801.00409},
	abstract = {State-of-the-art speech recognition systems rely heavily on three basic components: an acoustic model, a pronunciation lexicon and a language model. To build these components, a researcher needs linguistic as well as technical expertise, which is a barrier in low-resource domains. Techniques to construct these three components without having expert domain knowledge are in great demand. Urdu, despite having millions of speakers all over the world, is a low-resource language in terms of standard publically available linguistic resources. In this paper, we present a grapheme-to-phoneme conversion tool for Urdu that generates a pronunciation lexicon in a form suitable for use with speech recognition systems from a list of Urdu words. The tool predicts the pronunciation of words using a LSTM-based model trained on a handcrafted expert lexicon of around 39,000 words and shows an accuracy of 64\% upon internal evaluation. For external evaluation on a speech recognition task, we obtain a word error rate comparable to one achieved using a fully handcrafted expert lexicon.},
	urldate = {2022-08-18},
	author = {Zia, Haris Bin and Raza, Agha Ali and Athar, Awais},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
5 pages, LREC 2018},
}

@book{jurafsky_speech_2009,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-187321-6},
	shorttitle = {Speech and language processing},
	publisher = {Pearson Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
	note = {OCLC: 213375806},
	keywords = {Automatic speech recognition, Computational linguistics},
}

@article{likhomanenko_rethinking_2020,
	title = {Rethinking {Evaluation} in {ASR}: {Are} {Our} {Models} {Robust} {Enough}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Rethinking {Evaluation} in {ASR}},
	url = {https://arxiv.org/abs/2010.11745},
	doi = {10.48550/ARXIV.2010.11745},
	abstract = {Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets - in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. We show that, in general, reverberative and additive noise augmentation improves generalization performance across domains. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world noisy data. Finally, we show that training a single acoustic model on the most widely-used datasets - combined - reaches competitive performance on both research and real-world benchmarks.},
	urldate = {2022-08-19},
	author = {Likhomanenko, Tatiana and Xu, Qiantong and Pratap, Vineel and Tomasello, Paden and Kahn, Jacob and Avidov, Gilad and Collobert, Ronan and Synnaeve, Gabriel},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD), 68T07, 68T10, I.2.6; I.5.4},
}

@incollection{hutchison_illustrated_2013,
	address = {Berlin, Heidelberg},
	title = {An {Illustrated} {Methodology} for {Evaluating} {ASR} {Systems}},
	volume = {7836},
	isbn = {978-3-642-37424-1},
	url = {http://link.springer.com/10.1007/978-3-642-37425-8_3},
	urldate = {2022-08-19},
	booktitle = {Adaptive {Multimedia} {Retrieval}. {Large}-{Scale} {Multimedia} {Retrieval} and {Evaluation}},
	publisher = {Springer Berlin Heidelberg},
	author = {González, María and Moreno, Julián and Martínez, José Luis and Martínez, Paloma},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Detyniecki, Marcin and García-Serrano, Ana and Nürnberger, Andreas and Stober, Sebastian},
	year = {2013},
	doi = {10.1007/978-3-642-37425-8_3},
	note = {Series Title: Lecture Notes in Computer Science
ISBN2: 978-3-642-37425-8},
	pages = {33--42},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\XVRNLJ7F\\González et al. - 2013 - An Illustrated Methodology for Evaluating ASR Syst.pdf:application/pdf},
}

@phdthesis{zhang_strategies_2019,
	type = {Thesis},
	title = {Strategies for {Handling} {Out}-of-{Vocabulary} {Words} in {Automatic} {Speech} {Recognition}},
	url = {https://jscholarship.library.jhu.edu/handle/1774.2/62275},
	abstract = {Nowadays, most ASR (automatic speech recognition) systems deployed in  industry are closed-vocabulary systems, meaning we have a limited vocabulary of words the system can recognize, and where pronunciations are provided to the system. Words out of this vocabulary are called out-of-vocabulary (OOV) words, for which either pronunciations or both spellings and pronunciations are not known to the system. The basic motivations of developing strategies to handle OOV words are: First, in the training phase, missing or wrong pronunciations of words in training data results in poor acoustic models. Second, in the test phase, words out of the vocabulary cannot be recognized at all, and mis-recognition of OOV words may affect recognition performance of its in-vocabulary neighbors as well. Therefore, this dissertation is dedicated to exploring strategies of handling OOV words in closed-vocabulary ASR. 

First, we investigate dealing with OOV words in ASR training data, by introducing an acoustic-data driven pronunciation learning framework using a likelihood-reduction based criterion for selecting pronunciation candidates from multiple sources, i.e. standard grapheme-to-phoneme algorithms (G2P) and phonetic decoding, in a greedy fashion. This framework effectively expands a small hand-crafted pronunciation lexicon to cover OOV words, for which the learned pronunciations have higher quality than approaches using G2P alone or using other baseline pruning criteria. Furthermore, applying the proposed framework to generate alternative pronunciations for in-vocabulary (IV) words improves both recognition performance on relevant words and overall acoustic model performance.

Second, we investigate dealing with OOV words in ASR test data, i.e. OOV detection and recovery. We first conduct a comparative study of a hybrid lexical model (HLM) approach for OOV detection, and several baseline approaches, with the conclusion that the HLM approach outperforms others in both OOV detection and first pass OOV recovery performance. Next, we introduce a grammar-decoding framework for efficient second pass OOV recovery, showing that with properly designed schemes of estimating OOV unigram probabilities, the framework significantly improves OOV recovery and overall decoding performance compared to first pass decoding.

Finally we propose an open-vocabulary word-level recurrent neural network language model (RNNLM) re-scoring framework, making it possible to re-score lattices containing recovered OOVs using a single word-level RNNLM, that was ignorant of OOVs when it was trained. Above all, the whole OOV recovery pipeline shows the potential of a highly efficient open-vocabulary word-level ASR decoding framework, tightly integrated into a standard WFST decoding pipeline.},
	language = {en\_US},
	urldate = {2022-08-19},
	school = {Johns Hopkins University},
	author = {Zhang, Xiaohui},
	month = oct,
	year = {2019},
	note = {Accepted: 2020-02-06T04:08:11Z},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\6E6BV6R6\\Zhang - 2019 - Strategies for Handling Out-of-Vocabulary Words in.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\QLFGJMPH\\62275.html:text/html},
}

@article{zhang_hello_2017,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@article{morgan_continuous_1995,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {1558-0792},
	doi = {10.1109/79.382443},
	abstract = {The authors focus on a tutorial description of the hybrid HMM/ANN method. The approach has been applied to large vocabulary continuous speech recognition, and variants are in use by many researchers, The method provides a mechanism for incorporating a range of sources of evidence without strong assumptions about their joint statistics, and may have applicability to much more complex systems that can incorporate deep acoustic and linguistic context. The method is inherently discriminant and conservative of parameters. Despite these potential advantages, the hybrid method has focused on implementing fairly simple systems, which do surprisingly well on large continuous speech recognition tasks, Researchers are only beginning to explore the use of more complex structures with this paradigm. In particular, they are just beginning to look at the connectionist inference of language models (including phonology) from data, which may be required in order to take advantage of locally discriminant probabilities rather than simply translating to likelihoods. Finally, the authors' current intuition is that more advanced versions of the hybrid method can greatly benefit from a perceptual perspective.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Speech recognition, Hidden Markov models, Statistics, Vocabulary},
	pages = {24--42},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\SIWUUI5M\\382443.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\99KFFZGH\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{nautsch_gdpr_2019,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\V8ZYWF7E\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@misc{noauthor_spsc_nodate,
	title = {{SPSC} {\textbar} {ISCA} {SIG}-{SPSC}},
	url = {https://www.spsc-sig.org/},
	urldate = {2022-08-19},
}

@misc{noauthor_mozilla_nodate,
	title = {Mozilla {Common} {Voice}},
	url = {https://commonvoice.mozilla.org/},
	language = {en},
	urldate = {2022-08-19},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2ZVA6IM8\\datasets.html:text/html},
}

@article{schertz_acoustic_2020,
	title = {Acoustic cues in production and perception of the four-way stop laryngeal contrast in {Hindi} and {Urdu}},
	volume = {81},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009544702030070X},
	doi = {10.1016/j.wocn.2020.100979},
	language = {en},
	urldate = {2022-08-19},
	journal = {Journal of Phonetics},
	author = {Schertz, Jessamyn and Khan, Sarah},
	month = jul,
	year = {2020},
	pages = {100979},
}

@incollection{dua_urdu_2006,
	title = {Urdu},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022446},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02244-6},
	pages = {269--275},
}

@incollection{dua_hindustani_2006,
	title = {Hindustani},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022203},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02220-3},
	pages = {309--312},
}

@incollection{annamalai_india_2006,
	title = {India: {Language} {Situation}},
	isbn = {978-0-08-044854-1},
	shorttitle = {India},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542046113},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Annamalai, E.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/04611-3},
	pages = {610--613},
}

@misc{noauthor_morfessor_nodate,
	title = {Morfessor 2.0 documentation — {Morfessor} 2.0.4 documentation},
	url = {https://morfessor.readthedocs.io/en/latest/},
	urldate = {2022-08-19},
}

@misc{noauthor_morpho_nodate,
	title = {Morpho project},
	url = {http://morpho.aalto.fi/projects/morpho/},
	urldate = {2022-08-19},
	file = {Morpho project:C\:\\Users\\DELL\\Zotero\\storage\\435PUDQA\\morpho.html:text/html},
}

@inproceedings{farooq_enhancing_2020,
	title = {Enhancing {Large} {Vocabulary} {Continuous} {Speech} {Recognition} {System} for {Urdu}-{English} {Conversational} {Code}-{Switched} {Speech}},
	doi = {10.1109/O-COCOSDA50338.2020.9295036},
	abstract = {This paper presents first step towards Large Vocabulary Continuous Speech Recognition (LVCSR) system for Urdu-English code-switched conversational speech. Urdu is the national language and lingua franca of Pakistan, with 100 million speakers worldwide. English, on the other hand, is official language of Pakistan and commonly mixed with Urdu in daily communication. Urdu, being under-resourced language, have no substantial Urdu-English code-switched corpus in hand to develop speech recognition system. In this research, readily available spontaneous Urdu speech corpus (25 hours) is revised to use it for enhancement of read speech Urdu LVCSR to recognize code-switched speech. This data set is split into 20 hours of train and 5 hours of test set. 10 hours of Urdu BroadCast (BC) data are collected and annotated in a semi-supervised way to enhance the system further. For acoustic modeling, state-of-the-art DNN-HMM modeling technique is used without any prior GMM-HMM training and alignments. Various techniques to improve language model using monolingual data are investigated. The overall percent Word Error Rate (WER) is reduced from 40.71\% to 26.95\% on test set.},
	booktitle = {2020 23rd {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Hussain, Sarmad and Rauf, Sahar and Khalid, Maryam},
	month = nov,
	year = {2020},
	note = {ISSN: 2472-7695},
	keywords = {Speech recognition, Data models, Vocabulary, Acoustics, Speech coding, Speech enhancement, Switches, under-resourced language, Urdu speech recognition, Urdu-English code-switching},
	pages = {155--159},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\88SVPLUM\\9295036.html:text/html},
}

@inproceedings{watanabe_-line_2009,
	address = {Taipei, Taiwan},
	title = {On-line adaptation and {Bayesian} detection of environmental changes based on a macroscopic time evolution system},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4960598/},
	doi = {10.1109/ICASSP.2009.4960598},
	urldate = {2022-08-19},
	booktitle = {2009 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Watanabe, Shinji and Nakamura, Atsushi},
	month = apr,
	year = {2009},
	pages = {4373--4376},
}

@article{davis_automatic_1952-1,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@article{s_review_2016,
	title = {A {Review} on {Automatic} {Speech} {Recognition} {Architecture} and {Approaches}},
	volume = {9},
	issn = {20054254, 20054254},
	url = {http://article.nadiapub.com/IJSIP/vol9_no4/34.pdf},
	doi = {10.14257/ijsip.2016.9.4.34},
	number = {4},
	urldate = {2022-08-19},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {S, Karpagavalli and E, Chandra},
	month = apr,
	year = {2016},
	pages = {393--404},
}

@incollection{maglogiannis__2020-1,
	address = {Cham},
	title = {Α {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology
ISBN2: 978-3-030-49161-1},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\79KCL3R7\\Filippidou and Moussiades - 2020 - Α Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@misc{amodei_deep_2015,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	shorttitle = {Deep {Speech} 2},
	url = {http://arxiv.org/abs/1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	month = dec,
	year = {2015},
	note = {arXiv:1512.02595 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\T5Y77XAY\\Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5HBNU4V7\\1512.html:text/html},
}

@article{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech}},
	url = {https://arxiv.org/abs/1412.5567},
	doi = {10.48550/ARXIV.1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	urldate = {2022-08-19},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{xiong_microsoft_2018,
	title = {The {Microsoft} 2017 {Conversational} {Speech} {Recognition} {System}},
	url = {https://www.microsoft.com/en-us/research/publication/conference-paper-microsoft-2017-conversational-speech-recognition-system/},
	abstract = {We describe the latest version of Microsoft's conversational speech recognition system for the Switchboard and CallHome domains. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby acoustic model posteriors are first combined at the senone/frame level,followed by a word-level voting via confusion networks. We also added another language model rescoring step following the confusion network combination. The resulting system yields a 5.1\% word error rate on the NIST 2000 Switchboard test set, and 9.8\% on the CallHome subset.},
	booktitle = {Proc. {IEEE} {ICASSP}},
	publisher = {IEEE},
	author = {Xiong, Wayne and Wu, Lingfeng and Droppo, Jasha and Huang, Xuedong and Stolcke, Andreas},
	month = apr,
	year = {2018},
	pages = {5934--5938},
}

@article{juang_automatic_2005,
	title = {Automatic {Speech} {Recognition} - {A} {Brief} {History} of the {Technology} {Development}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (1, 2), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. and Rabiner, Lawrence},
	month = jan,
	year = {2005},
}

@incollection{juang_speech_2006,
	address = {Oxford},
	title = {Speech {Recognition}, {Automatic}: {History}},
	isbn = {978-0-08-044854-1},
	shorttitle = {Speech {Recognition}, {Automatic}},
	url = {https://www.sciencedirect.com/science/article/pii/B0080448542009068},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (Dudley, 1939; Dudley et al., 1939), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface. Examples are automatic call processing in the telephone network and query-based information systems that provide updated travel information, stock price quotations, weather reports, etc. In this article we review some major highlights in the research and development of automatic speech recognition during the last few decades to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Juang, B. -H. and Rabiner, L. R.},
	editor = {Brown, Keith},
	month = jan,
	year = {2006},
	doi = {10.1016/B0-08-044854-2/00906-8},
	keywords = {Speech recognition, acoustic modeling, automatic transcription, dialog systems, finite state network, hidden Markov models, keyword spotting, language modeling, neural networks, office automation, pattern recognition, spectral analysis, speech understanding, statistical modeling, time normalization},
	pages = {806--819},
	file = {ScienceDirect Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VWPXBBVS\\B0080448542009068.html:text/html},
}

@book{brown_encyclopedia_2006,
	address = {Amsterdam},
	edition = {2nd ed},
	title = {Encyclopedia of language \& linguistics},
	isbn = {978-0-08-044854-1},
	language = {eng},
	publisher = {Elsevier},
	author = {Brown, E. K. and Anderson, Anne},
	year = {2006},
}

@article{upton_speech_1984-1,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages — this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-19},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@inproceedings{morris_wer_2004-1,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{zhang_hello_2017-1,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@inproceedings{wu_deep_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Deep neural networks employing {Multi}-{Task} {Learning} and stacked bottleneck features for speech synthesis},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178814/},
	doi = {10.1109/ICASSP.2015.7178814},
	urldate = {2022-08-19},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Zhizheng and Valentini-Botinhao, Cassia and Watts, Oliver and King, Simon},
	month = apr,
	year = {2015},
	pages = {4460--4464},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\GFDVBE7Z\\Wu et al. - 2015 - Deep neural networks employing Multi-Task Learning.pdf:application/pdf},
}

@article{pohjalainen_feature_2015,
	title = {Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits},
	volume = {29},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230813001113},
	doi = {10.1016/j.csl.2013.11.004},
	language = {en},
	number = {1},
	urldate = {2022-08-19},
	journal = {Computer Speech \& Language},
	author = {Pohjalainen, Jouni and Räsänen, Okko and Kadioglu, Serdar},
	month = jan,
	year = {2015},
	pages = {145--171},
}

@inproceedings{eyben_opensmile_2010,
	address = {Firenze, Italy},
	title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
	isbn = {978-1-60558-933-6},
	shorttitle = {Opensmile},
	url = {http://dl.acm.org/citation.cfm?doid=1873951.1874246},
	doi = {10.1145/1873951.1874246},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the international conference on {Multimedia} - {MM} '10},
	publisher = {ACM Press},
	author = {Eyben, Florian and Wöllmer, Martin and Schuller, Björn},
	year = {2010},
	pages = {1459},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4F8ACZKL\\Eyben et al. - 2010 - Opensmile the munich versatile and fast open-sour.pdf:application/pdf},
}

@article{chung_unsupervised_2019,
	title = {An {Unsupervised} {Autoregressive} {Model} for {Speech} {Representation} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1904.03240},
	doi = {10.48550/ARXIV.1904.03240},
	abstract = {This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.},
	urldate = {2022-08-19},
	author = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Accepted to Interspeech 2019. Code available at: https://github.com/iamyuanchung/Autoregressive-Predictive-Coding},
}

@inproceedings{boser_training_1992,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {A training algorithm for optimal margin classifiers},
	isbn = {978-0-89791-497-0},
	url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
	doi = {10.1145/130385.130401},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the fifth annual workshop on {Computational} learning theory  - {COLT} '92},
	publisher = {ACM Press},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	year = {1992},
	pages = {144--152},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\29LCGTE4\\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf:application/pdf},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {wav2vec 2.0},
	url = {https://arxiv.org/abs/2006.11477},
	doi = {10.48550/ARXIV.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2022-08-19},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@article{soldz_big_1999,
	title = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}: {A} 45-{Year} {Longitudinal} {Study}},
	volume = {33},
	issn = {00926566},
	shorttitle = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656699922432},
	doi = {10.1006/jrpe.1999.2243},
	language = {en},
	number = {2},
	urldate = {2022-08-19},
	journal = {Journal of Research in Personality},
	author = {Soldz, Stephen and Vaillant, George E.},
	month = jun,
	year = {1999},
	pages = {208--232},
}

@inproceedings{zhou_security_2010,
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	abstract = {Cloud Computing is becoming a well-known buzzword nowadays. Many companies, such as Amazon, Google, Microsoft and so on, accelerate their paces in developing Cloud Computing systems and enhancing their services to provide for a larger amount of users. However, security and privacy issues present a strong barrier for users to adapt into Cloud Computing systems. In this paper, we investigate several Cloud Computing system providers about their concerns on security and privacy issues. We find those concerns are not adequate and more should be added in terms of five aspects (i.e., availability, confidentiality, data integrity, control, audit) for security. Moreover, released acts on privacy are out of date to protect users' private information in the new environment (i.e., Cloud Computing system environment) since they are no longer applicable to the new relationship between users and providers, which contains three parties (i.e., Cloud service user, Cloud service provider/Cloud user, Cloud provider). Multi located data storage and services (i.e., applications) in the Cloud make privacy issues even worse. Hence, adapting released acts for new scenarios in the Cloud, it will result in more users to step into Cloud. We claim that the prosperity in Cloud Computing literature is to be coming after those security and privacy issues having be resolved.},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = dec,
	year = {2010},
	note = {Journal Abbreviation: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
Publication Title: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
DOI:},
	pages = {112},
}

@inproceedings{zhou_security_2010-1,
	address = {Beijing, China},
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	isbn = {978-1-4244-8125-5},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	url = {http://ieeexplore.ieee.org/document/5663489/},
	doi = {10.1109/SKG.2010.19},
	urldate = {2022-08-19},
	booktitle = {2010 {Sixth} {International} {Conference} on {Semantics}, {Knowledge} and {Grids}},
	publisher = {IEEE},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = nov,
	year = {2010},
	pages = {105--112},
}

@inproceedings{karimov_cloud_2021,
	title = {Cloud {Computing} {Security} {Challenges} and {Solutions}},
	doi = {10.1109/ICISCT52966.2021.9670220},
	abstract = {in this paper focuses on development of cloud computing, privacy security issues have become increasingly prominent, which is of concern to industry and academia. We review the research progress on privacy security issues from the perspective of several privacy security protection technologies in cloud computing. First, we introduce some privacy security risks of cloud computing and propose a comprehensive privacy security protection framework.},
	booktitle = {2021 {International} {Conference} on {Information} {Science} and {Communications} {Technologies} ({ICISCT})},
	author = {Karimov, Abdukodir and Olimov, Iskandar and Berdiyev, Khusniddin and Tojiakbarova, Umida and Tursunov, Otabek},
	month = nov,
	year = {2021},
	keywords = {Computational modeling, Access control, Attribute-based encryption, Cloud computing, Communications technology, Industries, Information science, Privacy, Privacy security},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\6B73HGPD\\9670220.html:text/html},
}

@incollection{takabi_introduction_2019,
	title = {Introduction to the {Cloud} and {Fundamental} {Security} and {Privacy} {Issues} of the {Cloud}},
	isbn = {978-1-119-05340-8},
	url = {https://ieeexplore.ieee.org/document/9821151},
	abstract = {Cloud Computing is the most important solution to extend Information Technology's (IT) capabilities. However, Cloud is still vulnerable to a variety of threats and attacks that affects the growth of cloud computing in recent years. Therefore, the security concerns should be considered to improve the assurance of required security for the cloud customers. The cloud security consists of different aspects such as: infrastructure, information, and identity. In this chapter, we provide an introduction to the Cloud and its fundamental security and privacy issues, investigate security issues in different cloud services delivery models and introduce Cloud security standards.},
	urldate = {2022-08-19},
	booktitle = {Security, {Privacy}, and {Digital} {Forensics} in the {Cloud}},
	publisher = {Wiley},
	author = {Takabi, Hassan and GhasemiGol, Mohammad},
	year = {2019},
	doi = {10.1002/9781119053385.ch1},
	note = {Conference Name: Security, Privacy, and Digital Forensics in the Cloud},
	keywords = {Computational modeling, Cloud computing, Privacy, Operating systems, Organizations, Security, Software as a service},
	pages = {1--22},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\NNG85SBV\\9821151.html:text/html},
}

@inproceedings{kumar_systematic_2020,
	title = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}: {Data} {Integrity}, {Confidentiality} and {Availability}},
	shorttitle = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}},
	doi = {10.1109/GUCON48875.2020.9231255},
	abstract = {The cloud computing plays the prominent role in many organizations and researchers were focus on securing the cloud computing. The privacy preserving is the major challenge that grows exponentially with increases in user. In this paper, the depth survey is conducted on the recent methodologies of the cloud storage security related with the cloud computing. The overview of the cloud computing and security issues is analyzed in this paper. The key security requirements such as data integrity, availability and confidentiality. Security issues in the recent methodologies of cloud security is analyzed. The challenges in the cloud security is analyzed and possible future scope of the method is discussed. The paper involves in analyzing the state-of-art method to investigate the advantages and limitations.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Computing}, {Power} and {Communication} {Technologies} ({GUCON})},
	author = {Kumar, Rajeev and Bhatia, M P S},
	month = oct,
	year = {2020},
	keywords = {Systematics, Privacy, Organizations, Cloud Computing, Cloud computing security, Cloud Storage Security, Conferences, Confidentiality, Data integrity, Data Integrity, Data transfer, Privacy Preserving},
	pages = {334--337},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\MT8C247U\\9231255.html:text/html},
}

@inproceedings{godfrey_switchboard_1992,
	address = {San Francisco, CA, USA},
	title = {{SWITCHBOARD}: telephone speech corpus for research and development},
	isbn = {978-0-7803-0532-8},
	shorttitle = {{SWITCHBOARD}},
	url = {http://ieeexplore.ieee.org/document/225858/},
	doi = {10.1109/ICASSP.1992.225858},
	urldate = {2022-08-20},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
	year = {1992},
	pages = {517--520 vol.1},
}

@article{zhang_hello_2017-2,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-20},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@inproceedings{lin_self-attentive_2020,
	title = {Self-{Attentive} {Similarity} {Measurement} {Strategies} in {Speaker} {Diarization}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1908},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Lin, Qingjian and Hou, Yu and Li, Ming},
	month = oct,
	year = {2020},
	pages = {284--288},
}

@article{schuller_interspeech_2013,
	title = {The interspeech 2013 computational paralinguistics challenge: {Social} signals, conflict, emotion, autism},
	shorttitle = {The interspeech 2013 computational paralinguistics challenge},
	journal = {Proceedings of Interspeech},
	author = {Schuller, Björn and Steidl, S. and Batliner, Anton and Vinciarelli, Alessandro and Scherer, K. and Ringeval, Fabien and Chetouani, Mohamed and Weninger, F. and Eyben, Florian and Marchi, Erik and Mortillaro, Marcello and Salamin, H. and Polychroniou, Anna and Valente, F. and Kim, S.},
	month = jan,
	year = {2013},
	pages = {148--152},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IJ9QFIG3\\Schuller et al. - 2013 - The interspeech 2013 computational paralinguistics.pdf:application/pdf},
}

@inproceedings{misra_spectral_2004,
	address = {Montreal, Que., Canada},
	title = {Spectral entropy based feature for robust {ASR}},
	volume = {1},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1325955/},
	doi = {10.1109/ICASSP.2004.1325955},
	urldate = {2022-08-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Misra, H. and Ikbal, S. and Bourlard, H. and Hermansky, H.},
	year = {2004},
	pages = {I--193--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\YLKP828K\\Misra et al. - 2004 - Spectral entropy based feature for robust ASR.pdf:application/pdf},
}

@article{hunt_spectral_2000,
	title = {Spectral {Signal} {Processing} for {ASR}},
	abstract = {The paper begins by discussing the difficulties in obtaining repeatable results in speech recognition. Theoretical arguments are presented for and against copying human auditory properties in automatic speech recognition. The "standard" acoustic analysis for automatic speech recognition, consisting of melscale cepstrum coefficients and their temporal derivatives, is described. Some variations and extensions of the standard analysis --- PLP, cepstrum correlation methods, LDA, and variants on log power --- are then discussed. These techniques pass the test of having been found useful at multiple sites, especially with noisy speech. The extent to which auditory properties can account for the advantage found for particular techniques is considered. It is concluded that the advantages do not in fact stem from auditory properties, and that there is so far little or no evidence that the study of the human auditory system has contributed to advances in automatic speech recognition. Contributio...},
	author = {Hunt, Melvyn},
	month = aug,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\Z9XRFNJG\\Hunt - 2000 - Spectral Signal Processing for ASR.pdf:application/pdf},
}

@article{biswas_speaker_2021,
	title = {Speaker recognition: an enhanced approach to identify singer voice using neural network},
	volume = {24},
	issn = {1381-2416, 1572-8110},
	shorttitle = {Speaker recognition},
	url = {http://link.springer.com/10.1007/s10772-020-09698-8},
	doi = {10.1007/s10772-020-09698-8},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {International Journal of Speech Technology},
	author = {Biswas, Sharmila and Solanki, Sandeep Singh},
	month = mar,
	year = {2021},
	pages = {9--21},
}

@incollection{penn_computational_2012,
	title = {Computational {Linguistics}},
	isbn = {978-0-444-51747-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444517470500056},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Philosophy of {Linguistics}},
	publisher = {Elsevier},
	author = {Penn, Gerald},
	year = {2012},
	doi = {10.1016/B978-0-444-51747-0.50005-6},
	pages = {143--173},
}

@inproceedings{brill_improved_2000,
	address = {Hong Kong},
	title = {An improved error model for noisy channel spelling correction},
	url = {http://portal.acm.org/citation.cfm?doid=1075218.1075255},
	doi = {10.3115/1075218.1075255},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Proceedings of the 38th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '00},
	publisher = {Association for Computational Linguistics},
	author = {Brill, Eric and Moore, Robert C.},
	year = {2000},
	pages = {286--293},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YP8NG3XW\\Brill and Moore - 2000 - An improved error model for noisy channel spelling.pdf:application/pdf},
}

@misc{noauthor_type_2017,
	title = {Type less, talk more},
	url = {https://blog.google/products/search/type-less-talk-more/},
	abstract = {We’re bringing voice typing (aka talking to your phone instead of typing) to 30 new languages and locales around the world, covering more than a billion people.},
	language = {en-us},
	urldate = {2022-08-20},
	journal = {Google},
	month = aug,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L9T2I37L\\type-less-talk-more.html:text/html},
}

@article{noauthor_amazon_2019,
	title = {Amazon {Workers} {Are} {Listening} to {What} {You} {Tell} {Alexa}},
	url = {https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio},
	abstract = {A global team reviews audio clips in an effort to help the voice-activated assistant respond to commands.},
	language = {en},
	urldate = {2022-08-20},
	journal = {Bloomberg.com},
	month = apr,
	year = {2019},
	keywords = {Software, Privacy, ALPHABET INC-CL A, AMAZON.COM INC, APPLE INC, Boston, business, Costa Rica, India, markets, Policy, Romania, technology},
}

@misc{noauthor_amazon_nodate,
	title = {Amazon {Sends} 1,700 {Alexa} {Voice} {Recordings} to a {Random} {Person}},
	url = {https://threatpost.com/amazon-1700-alexa-voice-recordings/140201/},
	abstract = {The intimate recordings paint a detailed picture of a man's life.},
	language = {en},
	urldate = {2022-08-20},
}

@article{wolfson_amazons_2018,
	chapter = {Technology},
	title = {Amazon's {Alexa} recorded private conversation and sent it to random contact},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2018/may/24/amazon-alexa-recorded-conversation},
	abstract = {The company, which has insisted its Echo devices aren’t always recording, has confirmed the audio was sent},
	language = {en-GB},
	urldate = {2022-08-20},
	journal = {The Guardian},
	author = {Wolfson, Sam},
	month = may,
	year = {2018},
	keywords = {Privacy, Amazon, Amazon Alexa, Internet, Surveillance, Technology, US news},
}

@article{stupp_fraudsters_2019,
	chapter = {WSJ Pro},
	title = {Fraudsters {Used} {AI} to {Mimic} {CEO}’s {Voice} in {Unusual} {Cybercrime} {Case}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402},
	abstract = {Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of funds in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.},
	language = {en-US},
	urldate = {2022-08-20},
	journal = {Wall Street Journal},
	author = {Stupp, Catherine},
	month = aug,
	year = {2019},
	keywords = {artificial intelligence, Artificial Intelligence/Machine Learning, Bobby Filar, business in europe, Business in Europe, business in the u.k., Business in the U.K., c\&e executive news filter, C\&E Executive News Filter, c\&e industry news filter, C\&E Industry News Filter, computer science, Computer Science, content types, Content Types, corporate, corporate crime, Corporate Crime/Legal Action, Corporate/Industrial News, crime, Crime/Legal Action, cybercrime, Cybercrime/Hacking, Euler Hermes Group, factiva filters, Factiva Filters, financial services, Financial Services, fraud, Fraud, general news, hacking, humanities, industrial news, insurance, Insurance, Irakli Beridze, legal action, machine learning, management, Management, non-life insurance, Non-life Insurance, Philipp Amann, political, Political/General News, PRO, Rüdiger Kirsch, sciences, Sciences/Humanities, senior level management, Senior Level Management, trade credit insurance, Trade Credit Insurance, WSJ-PRO-CYBER, WSJ-PRO-WSJ.com},
}

@book{petronio_boundaries_2002,
	address = {Albany},
	series = {{SUNY} series in communication studies},
	title = {Boundaries of privacy: dialectics of disclosure},
	isbn = {978-0-7914-5515-9},
	shorttitle = {Boundaries of privacy},
	publisher = {State University of New York Press},
	author = {Petronio, Sandra Sporbert},
	year = {2002},
	note = {ISBN2: 978-0-7914-5516-6},
	keywords = {Privacy, Interpersonal communication, Secrecy, Self-disclosure},
}

@article{xie_how_2009,
	title = {How to repair customer trust after negative publicity: {The} roles of competence, integrity, benevolence, and forgiveness},
	volume = {26},
	issn = {07426046, 15206793},
	shorttitle = {How to repair customer trust after negative publicity},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mar.20289},
	doi = {10.1002/mar.20289},
	language = {en},
	number = {7},
	urldate = {2022-08-20},
	journal = {Psychology and Marketing},
	author = {Xie, Yi and Peng, Siqing},
	month = jul,
	year = {2009},
	pages = {572--589},
}

@article{shi_edge_2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	url = {http://ieeexplore.ieee.org/document/7488250/},
	doi = {10.1109/JIOT.2016.2579198},
	number = {5},
	urldate = {2022-08-20},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	month = oct,
	year = {2016},
	pages = {637--646},
}

@misc{noauthor_mydata_2015,
	type = {Muut julkaisut},
	title = {{MyData} – {A} {Nordic} {Model} for human-centered personal data management and processing},
	copyright = {This publication is copyrighted. You may download, display and print it for Your own personal use. Commercial use is prohibited.},
	url = {https://julkaisut.valtioneuvosto.fi/handle/10024/78439},
	abstract = {This white paper presents a framework, principles, and a model for a human-centric approach to the managing and processing of personal information. The approach – defined as MyData – is based on the right of individuals to access the data collected about them. The core idea is that individuals should be in control of their own data. The MyData approach aims at strengthening digital human rights while opening new opportunities for businesses to develop innovative personal data based services built on mutual trust.},
	language = {en},
	urldate = {2022-08-20},
	year = {2015},
	note = {Accepted: 2016-11-11T10:03:41Z
ISBN: 9789522434555
Publisher: liikenne- ja viestintäministeriö},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\UBFJDIUP\\2015 - MyData – A Nordic Model for human-centered persona.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KEV9QSG2\\78439.html:text/html},
}

@inproceedings{nautsch_gdpr_2019-1,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} towards a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {http://arxiv.org/abs/1907.03458},
	doi = {10.21437/Interspeech.2019-2647},
	abstract = {Privacy preservation and the protection of speech data is in high demand, not least as a result of recent regulation, e.g. the General Data Protection Regulation (GDPR) in the EU. While there has been a period with which to prepare for its implementation, its implications for speech data is poorly understood. This assertion applies to both the legal and technology communities, and is hardly surprising since there is no universal definition of 'privacy', let alone a clear understanding of when or how the GDPR applies to the capture, storage and processing of speech data. In aiming to initiate the discussion that is needed to establish a level of harmonisation that is thus far lacking, this contribution presents some reflections of both legal and technology communities on the implications of the GDPR as regards speech data. The article outlines the need for taxonomies at the intersection of speech technology and data privacy - a discussion that is still very much in its infancy - and describes the ways to safeguards and priorities for future research. In being agnostic to any specific application, the treatment should be of interest to the speech communication community at large.},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2019},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	note = {arXiv:1907.03458 [cs, eess]},
	keywords = {Computer Science - Computers and Society, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {3695--3699},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\W7WB9QLH\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\3UUJ3JJX\\1907.html:text/html},
}

@book{european_data_protection_supervisor_edps_2019,
	address = {LU},
	series = {{EDPS} {TechDispatch}},
	title = {{EDPS} {TechDispatch}},
	url = {https://data.europa.eu/doi/10.2804/004275},
	language = {eng},
	urldate = {2022-08-20},
	publisher = {Publications Office},
	author = {{European Data Protection Supervisor}},
	year = {2019},
}

@article{konig_automatic_2015,
	title = {Automatic speech analysis for the assessment of patients with predementia and {Alzheimer}'s disease},
	volume = {1},
	issn = {2352-8729, 2352-8729},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/j.dadm.2014.11.012},
	doi = {10.1016/j.dadm.2014.11.012},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	author = {König, Alexandra and Satt, Aharon and Sorin, Alexander and Hoory, Ron and Toledo‐Ronen, Orith and Derreumaux, Alexandre and Manera, Valeria and Verhey, Frans and Aalten, Pauline and Robert, Phillipe H. and David, Renaud},
	month = mar,
	year = {2015},
	pages = {112--124},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WZVVDNX2\\König et al. - 2015 - Automatic speech analysis for the assessment of pa.pdf:application/pdf},
}

@article{konig_automatic_2015-1,
	title = {Automatic speech analysis for the assessment of patients with predementia and {Alzheimer}'s disease},
	volume = {1},
	issn = {2352-8729, 2352-8729},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/j.dadm.2014.11.012},
	doi = {10.1016/j.dadm.2014.11.012},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	author = {König, Alexandra and Satt, Aharon and Sorin, Alexander and Hoory, Ron and Toledo‐Ronen, Orith and Derreumaux, Alexandre and Manera, Valeria and Verhey, Frans and Aalten, Pauline and Robert, Phillipe H. and David, Renaud},
	month = mar,
	year = {2015},
	pages = {112--124},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\PGX9R2SR\\König et al. - 2015 - Automatic speech analysis for the assessment of pa.pdf:application/pdf},
}

@incollection{gutwirth_seven_2013,
	address = {Dordrecht},
	title = {Seven {Types} of {Privacy}},
	isbn = {978-94-007-5184-2 978-94-007-5170-5},
	url = {http://link.springer.com/10.1007/978-94-007-5170-5_1},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {European {Data} {Protection}: {Coming} of {Age}},
	publisher = {Springer Netherlands},
	author = {Finn, Rachel L. and Wright, David and Friedewald, Michael},
	editor = {Gutwirth, Serge and Leenes, Ronald and de Hert, Paul and Poullet, Yves},
	year = {2013},
	doi = {10.1007/978-94-007-5170-5_1},
	pages = {3--32},
}

@article{chen_no_2003,
	title = {[{No} title found]},
	volume = {4},
	issn = {1385951X},
	url = {http://link.springer.com/10.1023/A:1022962631249},
	doi = {10.1023/A:1022962631249},
	number = {2/3},
	urldate = {2022-08-20},
	journal = {Information Technology and Management},
	author = {Chen, Sandy C. and Dhillon, Gurpreet S.},
	year = {2003},
	pages = {303--318},
}

@misc{reuter_guide_2015,
	title = {A {Guide} to {Fully} {Homomorphic} {Encryption}},
	url = {https://eprint.iacr.org/2015/1192},
	abstract = {Fully homomorphic encryption (FHE) has been dubbed the holy grail of cryptography, an elusive goal which could solve the IT world's problems of security and trust. Research in the area exploded after 2009 when Craig Gentry showed that FHE can be realised in principle. Since that time considerable progress has been made in finding more practical and more efficient solutions. Whilst research quickly developed, terminology and concepts became diverse and confusing so that today it can be difficult to understand what the achievements of different works actually are. The purpose of this paper is to address three fundamental questions: What is FHE? What can FHE be used for? What is the state of FHE today? As well as surveying the field, we clarify different terminology in use and prove connections between different FHE notions.Updated the acknowledgements.},
	urldate = {2022-08-20},
	author = {Reuter, Colin Boyd, Christopher Carr, Kristian Gjøsteen, Angela Jäschke, Christian A., Frederik Armknecht and Strand, Martin},
	year = {2015},
	note = {Report Number: 1192},
	keywords = {Fully Homomorphic Encryption},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PE6U2WJV\\Reuter and Strand - 2015 - A Guide to Fully Homomorphic Encryption.pdf:application/pdf},
}

@book{jessica_gasiorek_message_2018,
	title = {Message {Processing}: {The} {Science} of {Creating} {Understanding}},
	copyright = {Creative Commons Attribution 4.0 International License},
	url = {http://pressbooks-dev.oer.hawaii.edu/messageprocessing/},
	publisher = {UH Mānoa Outreach College},
	author = {Jessica Gasiorek and R. Kelly Aune},
	year = {2018},
	note = {https://pressbooks.oer.hawaii.edu/messageprocessing/\#:{\textasciitilde}:text=Book\%20Title\%3A\%20Message\%20Processing\%3A\%20The\%20Science\%20of\%20Creating\%20Understanding\&text=Book\%20Description\%3A\%20The\%20text\%20provides,on\%20how\%20people\%20create\%20understanding.},
}

@misc{noauthor_kunstliche_nodate,
	title = {Künstliche {Intelligenz} kommt in {Unternehmen} allmählich voran {\textbar} {Bitkom} e.{V}.},
	url = {https://www.bitkom.org/Presse/Presseinformation/Kuenstliche-Intelligenz-kommt-in-Unternehmen-allmaehlich-voran},
	abstract = {Zwei Drittel halten KI für die wichtigste Zukunftstechnologie Bislang nutzen 8 Prozent KI-Anwendungen, jedes vierte Unternehmen will investieren Bitkom-Präsident Berg: „KI braucht noch mehr Schwung“},
	language = {de},
	urldate = {2022-08-21},
}

@misc{noauthor_scaling_nodate,
	title = {Scaling {AI}: {From} {Experimental} to {Exponential}},
	shorttitle = {Scaling {AI}},
	url = {https://www.accenture.com/us-en/insights/artificial-intelligence/ai-investments},
	abstract = {Most businesses deploy pilot \#AI programs, but they struggle when it comes to scaling it. A new Accenture report explains the 3 critical factors for scaling AI.},
	language = {en},
	urldate = {2022-08-21},
}

@article{ardila_end--end_2019,
	title = {End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-019-0447-x},
	doi = {10.1038/s41591-019-0447-x},
	language = {en},
	number = {6},
	urldate = {2022-08-21},
	journal = {Nature Medicine},
	author = {Ardila, Diego and Kiraly, Atilla P. and Bharadwaj, Sujeeth and Choi, Bokyung and Reicher, Joshua J. and Peng, Lily and Tse, Daniel and Etemadi, Mozziyar and Ye, Wenxing and Corrado, Greg and Naidich, David P. and Shetty, Shravya},
	month = jun,
	year = {2019},
	pages = {954--961},
}

@article{fukushima_neocognitron_1988,
	title = {Neocognitron: {A} hierarchical neural network capable of visual pattern recognition},
	volume = {1},
	issn = {08936080},
	shorttitle = {Neocognitron},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
	doi = {10.1016/0893-6080(88)90014-7},
	language = {en},
	number = {2},
	urldate = {2022-08-21},
	journal = {Neural Networks},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {1988},
	pages = {119--130},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2022-08-21},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@inproceedings{ghahremani_acoustic_2016,
	title = {Acoustic {Modelling} from the {Signal} {Domain} {Using} {CNNs}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/ghahremani16_interspeech.html},
	doi = {10.21437/Interspeech.2016-1495},
	language = {en},
	urldate = {2022-08-21},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Ghahremani, Pegah and Manohar, Vimal and Povey, Daniel and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {3434--3438},
}

@misc{hou_audio-visual_2018,
	title = {Audio-{Visual} {Speech} {Enhancement} {Using} {Multimodal} {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.10893},
	abstract = {Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus only on addressing audio information. In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. We also propose a multi-task learning framework for reconstructing audio and visual signals at the output layer. Precisely speaking, the proposed AVDCNN model is structured as an audio-visual encoder-decoder network, in which audio and visual data are first processed using individual CNNs, and then fused into a joint network to generate enhanced speech (the primary task) and reconstructed images (the secondary task) at the output layer. The model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five instrumental criteria. Results show that the AVDCNN model yields a notably superior performance compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process. In addition, the AVDCNN model also outperforms an existing audio-visual SE model, confirming its capability of effectively combining audio and visual information in SE.},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Hou, Jen-Cheng and Wang, Syu-Siang and Lai, Ying-Hui and Tsao, Yu and Chang, Hsiu-Wen and Wang, Hsin-Min},
	month = jan,
	year = {2018},
	note = {arXiv:1703.10893 [cs, stat]},
	keywords = {Computer Science - Multimedia, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: To appear in IEEE Transactions on Emerging Topics in Computational Intelligence. Some audio samples can be reached in this link: https://sites.google.com/view/avse2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\RZW9TSJU\\Hou et al. - 2018 - Audio-Visual Speech Enhancement Using Multimodal D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\W8FSCKT6\\1703.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2022-08-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{errattahi_automatic_2018-1,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-21},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\HJ993EDS\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@article{morgan_continuous_1995-1,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {10535888},
	url = {http://ieeexplore.ieee.org/document/382443/},
	doi = {10.1109/79.382443},
	number = {3},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	pages = {24--42},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\8AZVI7RK\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{li_hybrid_2013,
	address = {Geneva, Switzerland},
	title = {Hybrid {Deep} {Neural} {Network}--{Hidden} {Markov} {Model} ({DNN}-{HMM}) {Based} {Speech} {Emotion} {Recognition}},
	isbn = {978-0-7695-5048-0},
	url = {http://ieeexplore.ieee.org/document/6681449/},
	doi = {10.1109/ACII.2013.58},
	urldate = {2022-08-21},
	booktitle = {2013 {Humaine} {Association} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction}},
	publisher = {IEEE},
	author = {Li, Longfei and Zhao, Yong and Jiang, Dongmei and Zhang, Yanning and Wang, Fengna and Gonzalez, Isabel and Valentin, Enescu and Sahli, Hichem},
	month = sep,
	year = {2013},
	pages = {312--317},
}

@article{ochiai_speaker_2016,
	title = {Speaker {Adaptive} {Training} {Localizing} {Speaker} {Modules} in {DNN} for {Hybrid} {DNN}-{HMM} {Speech} {Recognizers}},
	volume = {E99.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E99.D/10/E99.D_2016SLP0010/_article},
	doi = {10.1587/transinf.2016SLP0010},
	language = {en},
	number = {10},
	urldate = {2022-08-21},
	journal = {IEICE Transactions on Information and Systems},
	author = {Ochiai, Tsubasa and Matsuda, Shigeki and Watanabe, Hideyuki and Lu, Xugang and Hori, Chiori and Kawai, Hisashi and Katagiri, Shigeru},
	year = {2016},
	pages = {2431--2443},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\8EY8WZF7\\Ochiai et al. - 2016 - Speaker Adaptive Training Localizing Speaker Modul.pdf:application/pdf},
}

@book{muller_fundamentals_2021,
	address = {Cham, Switzerland},
	edition = {Second edition},
	title = {Fundamentals of music processing: using {Python} and {Jupyter} notebooks},
	isbn = {978-3-030-69807-2},
	shorttitle = {Fundamentals of music processing},
	language = {eng},
	publisher = {Springer},
	author = {Müller, Meinard},
	year = {2021},
}

@inproceedings{seide_feature_2011,
	address = {Waikoloa, HI, USA},
	title = {Feature engineering in {Context}-{Dependent} {Deep} {Neural} {Networks} for conversational speech transcription},
	isbn = {978-1-4673-0367-5},
	url = {http://ieeexplore.ieee.org/document/6163899/},
	doi = {10.1109/ASRU.2011.6163899},
	urldate = {2022-08-21},
	booktitle = {2011 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} \& {Understanding}},
	publisher = {IEEE},
	author = {Seide, Frank and Li, Gang and Chen, Xie and Yu, Dong},
	month = dec,
	year = {2011},
	note = {ISBN2: 978-1-4673-0365-1 
ISBN3: 978-1-4673-0366-8},
	pages = {24--29},
}

@article{dua_developing_2022,
	title = {Developing a {Speech} {Recognition} {System} for {Recognizing} {Tonal} {Speech} {Signals} {Using} a {Convolutional} {Neural} {Network}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/12/6223},
	doi = {10.3390/app12126223},
	abstract = {Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15\% accuracy rate and a 10.56\% WER for continuous and extensive vocabulary sentences of speech signals with different tones.},
	language = {en},
	number = {12},
	urldate = {2022-08-21},
	journal = {Applied Sciences},
	author = {Dua, Sakshi and Kumar, Sethuraman Sambath and Albagory, Yasser and Ramalingam, Rajakumar and Dumka, Ankur and Singh, Rajesh and Rashid, Mamoon and Gehlot, Anita and Alshamrani, Sultan S. and AlGhamdi, Ahmed Saeed},
	month = jun,
	year = {2022},
	pages = {6223},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\97ILRS3G\\Dua et al. - 2022 - Developing a Speech Recognition System for Recogni.pdf:application/pdf},
}

@misc{garofolo_john_s_timit_1993,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}ul{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.phn" rel="nofollow"{\textgreater}phonemes{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.txt" rel="nofollow"{\textgreater}transcripts{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wav" rel="nofollow"{\textgreater}audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wrd" rel="nofollow"{\textgreater}word list{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}/ul{\textgreater}{\textless}/br{\textgreater} 
Portions © 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	year = {1993},
	doi = {10.35111/17GK-BN40},
	note = {Artwork Size: 715776 KB
Pages: 715776 KB
Type: dataset},
}

@article{bell_adaptation_2020,
	title = {Adaptation algorithms for neural network-based speech recognition: {An} overview},
	volume = {2},
	shorttitle = {Adaptation algorithms for neural network-based speech recognition},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {33--66},
}

@article{bell_adaptation_2021,
	title = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}: {An} {Overview}},
	volume = {2},
	issn = {2644-1322},
	shorttitle = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9296327/},
	doi = {10.1109/OJSP.2020.3045349},
	urldate = {2022-08-21},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
	year = {2021},
	pages = {33--66},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\X557RF2Q\\Bell et al. - 2021 - Adaptation Algorithms for Neural Network-Based Spe.pdf:application/pdf},
}

@misc{garofolo_john_s_csr-i_2007,
	title = {{CSR}-{I} ({WSJ0}) {Complete}},
	url = {https://catalog.ldc.upenn.edu/LDC93S6A},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}LDC93S6A - Complete CSR-I corpus {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6B" rel="nofollow"{\textgreater}LDC93S6B{\textless}/a{\textgreater} - CSR-I Sennheiser speech {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6C" rel="nofollow"{\textgreater}LDC93S6C{\textless}/a{\textgreater} - CSR-I other speech{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}During 1991, the DARPA Spoken Language Program initiated efforts to build a new corpus to support research on large-vocabulary Continuous Speech Recognition (CSR) systems.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The first two CSR Corpora consist primarily of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news text and are thus often known as WSJ0 and WSJ1. (Later sections of the CSR set of corpora, however, will consist of read texts from other sources of North American business news and eventually from other news domains).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The texts to be read were selected to fall within either a 5,000-word or a 20,000-word subset of the WSJ text corpus. (See the documentation for details). Some spontaneous dictation is included in addition to the read speech. The dictation portion was collected using journalists who dictated hypothetical news articles.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Two microphones are used throughout: a close-talking Sennheiser HMD414 and a secondary microphone, which may vary. The corpora are thus offered in three configurations: the speech from the Sennheiser, the speech from the other microphone and the speech from both; all three sets include all transcriptions, tests, documentation, etc.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}In general, transcriptions of the speech, test data from ARPA evaluations, scores achieved by various speech recognition systems and software used in scoring are included on separate discs from the waveform data.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Please listen to this {\textless}a href="desc/addenda/LDC93S6A.wav"{\textgreater}audio sample{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}/br{\textgreater} 
Portions © 1987-1989 Dow Jones \& Company, Inc., © 1992, 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Graff, David and Paul, Doug and Pallett, David},
	month = may,
	year = {2007},
	doi = {10.35111/EWKM-CG47},
	note = {Artwork Size: 9542041 KB
Pages: 9542041 KB
Type: dataset},
}

@article{li_deng_mnist_2012,
	title = {The {MNIST} {Database} of {Handwritten} {Digit} {Images} for {Machine} {Learning} {Research} [{Best} of the {Web}]},
	volume = {29},
	issn = {1053-5888},
	url = {http://ieeexplore.ieee.org/document/6296535/},
	doi = {10.1109/MSP.2012.2211477},
	number = {6},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {{Li Deng}},
	month = nov,
	year = {2012},
	pages = {141--142},
}

@inproceedings{deng_imagenet_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	urldate = {2022-08-21},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
}

@misc{noauthor_center_nodate,
	title = {Center for {Language} {Engineering}},
	url = {https://cle.org.pk/},
	urldate = {2022-08-22},
	file = {Center for Language Engineering:C\:\\Users\\DELL\\Zotero\\storage\\LV3V3DMG\\cle.org.pk.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub}: {Where} the world builds software},
	shorttitle = {{GitHub}},
	url = {https://github.com/},
	abstract = {GitHub is where over 83 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositories, review code like a pro, track bugs and feat...},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {citation-265913669.bib:C\:\\Users\\DELL\\Zotero\\storage\\YSPPY2BA\\citation-265913669.bib:application/x-bibtex;citation.bib:C\:\\Users\\DELL\\Zotero\\storage\\6H4SBI5D\\citation.bib:application/x-bibtex},
}

@misc{noauthor_kaggle_nodate,
	title = {Kaggle: {Your} {Machine} {Learning} and {Data} {Science} {Community}},
	shorttitle = {Kaggle},
	url = {https://www.kaggle.com/},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	language = {en},
	urldate = {2022-08-22},
}

@article{verma_i-vectors_2015,
	title = {i-{Vectors} in speech processing applications: a survey},
	volume = {18},
	issn = {1381-2416, 1572-8110},
	shorttitle = {i-{Vectors} in speech processing applications},
	url = {http://link.springer.com/10.1007/s10772-015-9295-3},
	doi = {10.1007/s10772-015-9295-3},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {International Journal of Speech Technology},
	author = {Verma, Pulkit and Das, Pradip K.},
	month = dec,
	year = {2015},
	pages = {529--546},
}

@article{verma_i-vectors_2015-1,
	title = {i-{Vectors} in speech processing applications: a survey},
	volume = {18},
	issn = {1381-2416, 1572-8110},
	shorttitle = {i-{Vectors} in speech processing applications},
	url = {http://link.springer.com/10.1007/s10772-015-9295-3},
	doi = {10.1007/s10772-015-9295-3},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {International Journal of Speech Technology},
	author = {Verma, Pulkit and Das, Pradip K.},
	month = dec,
	year = {2015},
	pages = {529--546},
}

@article{joshi_modified_2016,
	title = {Modified {Mean} and {Variance} {Normalization}: {Transforming} to {Utterance}-{Specific} {Estimates}},
	volume = {35},
	issn = {0278-081X, 1531-5878},
	shorttitle = {Modified {Mean} and {Variance} {Normalization}},
	url = {http://link.springer.com/10.1007/s00034-015-0129-y},
	doi = {10.1007/s00034-015-0129-y},
	language = {en},
	number = {5},
	urldate = {2022-08-22},
	journal = {Circuits, Systems, and Signal Processing},
	author = {Joshi, Vikas and Prasad, N. Vishnu and Umesh, S.},
	month = may,
	year = {2016},
	pages = {1593--1609},
}

@book{institute_of_electrical_and_electronics_engineers_2013_2013,
	address = {Piscataway, NJ},
	title = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013): {Olomouc}, {Czech} {Republic}, 8 - 12 {December} 2013},
	isbn = {978-1-4799-2756-2 978-1-4799-2757-9},
	shorttitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013)},
	language = {eng},
	publisher = {IEEE},
	editor = {Institute of Electrical {and} Electronics Engineers},
	year = {2013},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\K3IX7I3E\\Institute of Electrical and Electronics Engineers - 2013 - 2013 IEEE Workshop on Automatic Speech Recognition.pdf:application/pdf},
}

@misc{noauthor_urdu_nodate,
	title = {Urdu {Speech} {Dataset}},
	url = {https://www.kaggle.com/datasets/hazrat/urdu-speech-dataset},
	abstract = {2,500 Urdu audio samples},
	language = {en},
	urldate = {2022-08-22},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\G6KKGYW6\\urdu-speech-dataset.html:text/html},
}

@misc{noauthor_gramvaani_hindi_asrkaldiasr_nodate,
	title = {gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	url = {https://github.com/anish9208/gramvaani_hindi_asr},
	abstract = {This repo contains the baseline model recipes and pre-trained model for GramVanni hindi ASR challenge  - gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\MXCR2LTK\\asr.html:text/html},
}

@inproceedings{peinl_open_2020,
	address = {Deggendorf, Germany},
	title = {Open {Source} {Speech} {Recognition} on {Edge} {Devices}},
	isbn = {978-1-72816-759-6 978-1-72816-760-2},
	url = {https://ieeexplore.ieee.org/document/9208978/},
	doi = {10.1109/ACIT49673.2020.9208978},
	urldate = {2022-08-22},
	booktitle = {2020 10th {International} {Conference} on {Advanced} {Computer} {Information} {Technologies} ({ACIT})},
	publisher = {IEEE},
	author = {Peinl, Rene and Rizk, Basem and Szabad, Robert},
	month = sep,
	year = {2020},
	pages = {441--445},
}

@inproceedings{christian_gaida_comparing_2014,
	title = {Comparing {Open}-{Source} {Speech} {Recognition} {Toolkits}},
	author = {Christian Gaida and P. Lange and Rico Petrick and Patrick Proba and Ahmed Malatawy and David Suendermann-Oeft},
	year = {2014},
}

@misc{noauthor_tdnn_nodate,
	title = {{TDNN} --{\textgreater} {CNN}},
	url = {https://groups.google.com/g/kaldi-help/c/jsg1Oo4bNGQ/m/uwvFw5PtBwAJ},
	urldate = {2022-08-22},
	file = {TDNN --> CNN:C\:\\Users\\DELL\\Zotero\\storage\\X6FFGDN4\\uwvFw5PtBwAJ.html:text/html},
}

@inproceedings{kreyssig_improved_2018,
	address = {Calgary, AB},
	title = {Improved {Tdnns} {Using} {Deep} {Kernels} and {Frequency} {Dependent} {Grid}-{RNNS}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462523/},
	doi = {10.1109/ICASSP.2018.8462523},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kreyssig, F. L. and Zhang, C. and Woodland, P. C.},
	month = apr,
	year = {2018},
	pages = {4864--4868},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\MDGVZMYB\\Kreyssig et al. - 2018 - Improved Tdnns Using Deep Kernels and Frequency De.pdf:application/pdf},
}

@inproceedings{biswas_semi-supervised_2019,
	title = {Semi-{Supervised} {Acoustic} {Model} {Training} for {Five}-{Lingual} {Code}-{Switched} {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html},
	doi = {10.21437/Interspeech.2019-1325},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Biswas, Astik and Yılmaz, Emre and Wet, Febe de and Westhuizen, Ewald van der and Niesler, Thomas},
	month = sep,
	year = {2019},
	pages = {3745--3749},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5DDQYLKT\\Biswas et al. - 2019 - Semi-Supervised Acoustic Model Training for Five-L.pdf:application/pdf},
}

@inproceedings{zorila_investigation_2019,
	address = {SG, Singapore},
	title = {An {Investigation} into the {Effectiveness} of {Enhancement} in {ASR} {Training} and {Test} for {Chime}-5 {Dinner} {Party} {Transcription}},
	isbn = {978-1-72810-306-8},
	url = {https://ieeexplore.ieee.org/document/9003785/},
	doi = {10.1109/ASRU46091.2019.9003785},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Zorila, Catalin and Boeddeker, Christoph and Doddipatla, Rama and Haeb-Umbach, Reinhold},
	month = dec,
	year = {2019},
	pages = {47--53},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\IE9HPYPR\\Zorila et al. - 2019 - An Investigation into the Effectiveness of Enhance.pdf:application/pdf},
}

@article{abdel-hamid_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Speech} {Recognition}},
	volume = {22},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/6857341/},
	doi = {10.1109/TASLP.2014.2339736},
	number = {10},
	urldate = {2022-08-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
	month = oct,
	year = {2014},
	pages = {1533--1545},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\MQX94C3D\\Abdel-Hamid et al. - 2014 - Convolutional Neural Networks for Speech Recogniti.pdf:application/pdf},
}

@article{eeckt_continual_2021,
	title = {Continual {Learning} for {Monolingual} {End}-to-{End} {Automatic} {Speech} {Recognition}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2112.09427},
	doi = {10.48550/ARXIV.2112.09427},
	abstract = {Adapting Automatic Speech Recognition (ASR) models to new domains results in a deterioration of performance on the original domain(s), a phenomenon called Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to new accents, dialects, topics, etc. without suffering from CF, making them unable to be continually enhanced without storing all past data. Fortunately, Continual Learning (CL) methods, which aim to enable continual adaptation while overcoming CF, can be used. In this paper, we implement an extensive number of CL methods for End-to-End ASR and test and compare their ability to extend a monolingual Hybrid CTC-Transformer model across four new tasks. We find that the best performing CL method closes the gap between the fine-tuned model (lower bound) and the model trained jointly on all tasks (upper bound) by more than 40\%, while requiring access to only 0.6\% of the original data.},
	urldate = {2022-08-22},
	author = {Eeckt, Steven Vander and Van hamme, Hugo},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering},
	annote = {Other
Accepted at EUSIPCO 2022. 5 pages, 1 figure},
}

@article{ali_automatic_2015-1,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-22},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@inproceedings{shaik_riyaz_automatic_2019,
	title = {Automatic {Speaker} {Recognition} {System} in {Urdu} using {MFCC} {\textbackslash}\& {HMM}},
	author = {Shaik Riyaz and Bathula Lakshmi Bhavani and S. Venkatrama Phani Kumar},
	year = {2019},
}

@misc{noauthor_federated_nodate,
	title = {Federated {Learning}: {Collaborative} {Machine} {Learning} without {Centralized} {Training} {Data}},
	shorttitle = {Federated {Learning}},
	url = {http://ai.googleblog.com/2017/04/federated-learning-collaborative.html},
	abstract = {Posted by Brendan McMahan and Daniel Ramage, Research Scientists Standard machine learning approaches require centralizing the training data...},
	language = {en},
	urldate = {2022-08-22},
	journal = {Google AI Blog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\CPB7S2RU\\federated-learning-collaborative.html:text/html},
}

@inproceedings{andreas_stolcke_srilm_2002,
	title = {{SRILM} -- {An} xtensible language modeling toolkit},
	url = {https://www.sri.com/platform/srilm/},
	author = {Andreas Stolcke},
	year = {2002},
}

@inproceedings{ali_complete_2014,
	address = {South Lake Tahoe, NV, USA},
	title = {A complete {KALDI} recipe for building {Arabic} speech recognition systems},
	isbn = {978-1-4799-7129-9},
	url = {http://ieeexplore.ieee.org/document/7078629/},
	doi = {10.1109/SLT.2014.7078629},
	urldate = {2022-08-23},
	booktitle = {2014 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Ali, Ahmed and Zhang, Yifan and Cardinal, Patrick and Dahak, Najim and Vogel, Stephan and Glass, James},
	month = dec,
	year = {2014},
	pages = {525--529},
}

@article{amodei_deep_2015-1,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech} 2},
	url = {https://arxiv.org/abs/1512.02595},
	doi = {10.48550/ARXIV.1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-23},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{alhanai_development_2016,
	address = {San Diego, CA},
	title = {Development of the {MIT} {ASR} system for the 2016 {Arabic} {Multi}-genre {Broadcast} {Challenge}},
	isbn = {978-1-5090-4903-5},
	url = {http://ieeexplore.ieee.org/document/7846280/},
	doi = {10.1109/SLT.2016.7846280},
	urldate = {2022-08-23},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {AlHanai, Tuka and Hsu, Wei-Ning and Glass, James},
	month = dec,
	year = {2016},
	pages = {299--304},
}

@phdthesis{meyerjosh_multi-task_2019,
	title = {Multi-{Task} and {Transfer} {Learning} in {Low}-{Resource} {Speech} {Recognition}},
	copyright = {Pro Quest LLC},
	url = {https://www.proquest.com/openview/0d416c7a0cb00a3f6069c467ad545db5/1?pq-origsite=gscholar&cbl=18750&diss=y},
	abstract = {This thesis investigates methods for Acoustic Modeling in Automatic Speech Recognition, assuming limited access to training data in the target domain. The Acoustic
Models of interest are Deep Neural Network Acoustic Models (in both the Hybrid
and End-to-End approaches), and the target domains in question are either different
languages or different speakers. Inductive bias is transfered from a source domain
during training, via Multi-Task Learning or Transfer Learning.
With regards to Multi-Task Learning, Chapter (5) presents experiments which
explicitly incorporate linguistic knowledge (i.e. phonetics and phonology) into an
auxiliary task during neural Acoustic Model training. In Chapter (6), I investigate
Multi-Task methods which do not rely on expert knowledge (linguistic or otherwise),
by re-using existing parts of the Hybrid training pipeline. In Chapter (7), new tasks
are discovered using unsupervised learning. In Chapter (8), using the “copy-paste”
Transfer Learning approach, I demonstrate that with an appropriate early-stopping
criteria, cross-lingual transfer is possible to both large and small target datasets.
The methods and intuitions which rely on linguistic knowledge are of interest to
the Speech Recognition practitioner working in low-resource domains. These same
sections may be of interest to the theoretical linguist, as a study of the relative import
of phonetic categories in classification. To the Machine Learning practitioner, I hope
to offer approaches which can be easily ported over to other classification tasks. To
the Machine Learning researcher, I hope to inspire new ideas on addressing the small
data problem.},
	language = {en},
	urldate = {2022-08-23},
	school = {University of Arizona},
	author = {Meyer,Josh},
	year = {2019},
	note = {http://jrmeyer.github.io/misc/MEYER\_dissertation\_2019.pdf
Published by Pro Quest LLC},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\IWJEWKS8\\1.html:text/html},
}

@book{international_speech_communication_association_speech_2016,
	address = {Red Hook, NY},
	title = {Speech beyond speech towards a better understanding of the most important biosignal: 16th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({INTERSPEECH} 2015): {Dresden}, {Germany}, 6-10 {September} 2015. {Volume} 5},
	volume = {5},
	isbn = {978-1-5108-1790-6},
	shorttitle = {Speech beyond speech towards a better understanding of the most important biosignal},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {International Speech Communication Association},
	year = {2016},
	note = {Meeting Name: International Speech Communication Association},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\QHEKNZCX\\International Speech Communication Association - 2016 - Speech beyond speech towards a better understandin.pdf:application/pdf},
}

@inproceedings{shaik_improvements_2015,
	title = {Improvements in {RWTH} {LVCSR} evaluation systems for {Polish}, {Portuguese}, {English}, urdu, and {Arabic}},
	url = {https://www.isca-speech.org/archive/interspeech_2015/shaik15_interspeech.html},
	doi = {10.21437/Interspeech.2015-635},
	language = {en},
	urldate = {2022-08-23},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Shaik, M. Ali Basha and Tüske, Zoltán and Tahir, M. Ali and Nußbaum-Thom, Markus and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2015},
	pages = {3154--3158},
}

@article{kumar_large-vocabulary_2004,
	title = {A large-vocabulary continuous speech recognition system for {Hindi}},
	volume = {48},
	issn = {0018-8646, 0018-8646},
	url = {http://ieeexplore.ieee.org/document/5388836/},
	doi = {10.1147/rd.485.0703},
	number = {5.6},
	urldate = {2022-08-23},
	journal = {IBM Journal of Research and Development},
	author = {Kumar, M. and Rajput, N. and Verma, A.},
	month = sep,
	year = {2004},
	pages = {703--715},
}

@article{ming_speech_2017,
	title = {Speech {Enhancement} {Based} on {Full}-{Sentence} {Correlation} and {Clean} {Speech} {Recognition}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7814226/},
	doi = {10.1109/TASLP.2017.2651406},
	number = {3},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Ming, Ji and Crookes, Danny},
	month = mar,
	year = {2017},
	pages = {531--543},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\FKM8TYUH\\Ming and Crookes - 2017 - Speech Enhancement Based on Full-Sentence Correlat.pdf:application/pdf},
}

@article{ganapathy_multivariate_2017,
	title = {Multivariate {Autoregressive} {Spectrogram} {Modeling} for {Noisy} {Speech} {Recognition}},
	volume = {24},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7973047/},
	doi = {10.1109/LSP.2017.2724561},
	number = {9},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Ganapathy, Sriram},
	month = sep,
	year = {2017},
	pages = {1373--1377},
}

@article{lee_dnn-based_2016,
	title = {{DNN}-{Based} {Feature} {Enhancement} {Using} {DOA}-{Constrained} {ICA} for {Robust} {Speech} {Recognition}},
	volume = {23},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7497454/},
	doi = {10.1109/LSP.2016.2583658},
	number = {8},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Lee, Ho-Yong and Cho, Ji-Won and Kim, Minook and Park, Hyung-Min},
	month = aug,
	year = {2016},
	pages = {1091--1095},
}

@article{lee_threshold-based_2018,
	title = {Threshold-{Based} {Noise} {Detection} and {Reduction} for {Automatic} {Speech} {Recognition} {System} in {Human}-{Robot} {Interactions}},
	volume = {18},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/18/7/2068},
	doi = {10.3390/s18072068},
	language = {en},
	number = {7},
	urldate = {2022-08-23},
	journal = {Sensors},
	author = {Lee, Sheng-Chieh and Wang, Jhing-Fa and Chen, Miao-Hia},
	month = jun,
	year = {2018},
	pages = {2068},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TB36Q4AT\\Lee et al. - 2018 - Threshold-Based Noise Detection and Reduction for .pdf:application/pdf},
}

@article{gosztolya_domain_2016,
	title = {Domain {Adaptation} of {Deep} {Neural} {Networks} for {Automatic} {Speech} {Recognition} via {Wireless} {Sensors}},
	volume = {67},
	issn = {1339-309X},
	url = {https://www.sciendo.com/article/10.1515/jee-2016-0017},
	doi = {10.1515/jee-2016-0017},
	abstract = {Abstract
            Wireless sensors are recent, portable, low-powered devices, designed to record and transmit observations of their environment such as speech. To allow portability they are designed to have a small size and weight; this, however, along with their low power consumption, usually means that they have only quite basic recording equipment (e.g. microphone) installed. Recent speech technology applications typically require several dozen hours of audio recordings (nowadays even hundreds of hours is common), which is usually not available as recorded material by such sensors. Since systems trained with studio-level utterances tend to perform suboptimally for such recordings, a sensible idea is to adapt models which were trained on existing, larger, noise-free corpora. In this study, we experimented with adapting Deep Neural Network-based acoustic models trained on noise-free speech data to perform speech recognition on utterances recorded by wireless sensors. In the end, we were able to achieve a 5\% gain in terms of relative error reduction compared to training only on the sensor-recorded, restricted utterance subset.},
	language = {en},
	number = {2},
	urldate = {2022-08-23},
	journal = {Journal of Electrical Engineering},
	author = {Gosztolya, Gábor and Grósz, Tamás},
	month = apr,
	year = {2016},
	pages = {124--130},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CSYDKLJL\\Gosztolya and Grósz - 2016 - Domain Adaptation of Deep Neural Networks for Auto.pdf:application/pdf},
}

@article{chen_progressive_2018,
	title = {Progressive {Joint} {Modeling} in {Unsupervised} {Single}-{Channel} {Overlapped} {Speech} {Recognition}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/8080252/},
	doi = {10.1109/TASLP.2017.2765834},
	number = {1},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Chen, Zhehuai and Droppo, Jasha and Li, Jinyu and Xiong, Wayne},
	month = jan,
	year = {2018},
	pages = {184--196},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U75U9AZK\\Chen et al. - 2018 - Progressive Joint Modeling in Unsupervised Single-.pdf:application/pdf},
}

@misc{dautume_episodic_2019,
	title = {Episodic {Memory} in {Lifelong} {Language} {Learning}},
	url = {http://arxiv.org/abs/1906.01076},
	abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\textasciitilde}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {d'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
	month = nov,
	year = {2019},
	note = {arXiv:1906.01076 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Proceedings of NeurIPS 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\F68HIZ99\\d'Autume et al. - 2019 - Episodic Memory in Lifelong Language Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\U9A7WITC\\1906.html:text/html},
}

@misc{chang_towards_2021,
	title = {Towards {Lifelong} {Learning} of {End}-to-end {ASR}},
	url = {http://arxiv.org/abs/2104.01616},
	abstract = {Automatic speech recognition (ASR) technologies today are primarily optimized for given datasets; thus, any changes in the application environment (e.g., acoustic conditions or topic domains) may inevitably degrade the performance. We can collect new data describing the new environment and fine-tune the system, but this naturally leads to higher error rates for the earlier datasets, referred to as catastrophic forgetting. The concept of lifelong learning (LLL) aiming to enable a machine to sequentially learn new tasks from new datasets describing the changing real world without forgetting the previously learned knowledge is thus brought to attention. This paper reports, to our knowledge, the first effort to extensively consider and analyze the use of various approaches of LLL in end-to-end (E2E) ASR, including proposing novel methods in saving data for past domains to mitigate the catastrophic forgetting problem. An overall relative reduction of 28.7\% in WER was achieved compared to the fine-tuning baseline when sequentially learning on three very different benchmark corpora. This can be the first step toward the highly desired ASR technologies capable of synchronizing with the continuously changing real world.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Chang, Heng-Jui and Lee, Hung-yi and Lee, Lin-shan},
	month = jul,
	year = {2021},
	note = {arXiv:2104.01616 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Interspeech 2021. We acknowledge the support of Salesforce Research Deep Learning Grant},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\NZP72WVC\\Chang et al. - 2021 - Towards Lifelong Learning of End-to-end ASR.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\PTMWQGE5\\2104.html:text/html},
}

@misc{yang_online_2022,
	title = {Online {Continual} {Learning} of {End}-to-{End} {Speech} {Recognition} {Models}},
	url = {http://arxiv.org/abs/2207.05071},
	abstract = {Continual Learning, also known as Lifelong Learning, aims to continually learn from new data as it becomes available. While prior research on continual learning in automatic speech recognition has focused on the adaptation of models across multiple different speech recognition tasks, in this paper we propose an experimental setting for {\textbackslash}textit\{online continual learning\} for automatic speech recognition of a single task. Specifically focusing on the case where additional training data for the same task becomes available incrementally over time, we demonstrate the effectiveness of performing incremental model updates to end-to-end speech recognition models with an online Gradient Episodic Memory (GEM) method. Moreover, we show that with online continual learning and a selective sampling strategy, we can maintain an accuracy that is similar to retraining a model from scratch while requiring significantly lower computation costs. We have also verified our method with self-supervised learning (SSL) features.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Yang, Muqiao and Lane, Ian and Watanabe, Shinji},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05071 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at InterSpeech 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D5PLRPCB\\Yang et al. - 2022 - Online Continual Learning of End-to-End Speech Rec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Q2KHNKDV\\2207.html:text/html},
}

@article{kumar_leveraging_2020,
	title = {Leveraging {Linguistic} {Context} in {Dyadic} {Interactions} to {Improve} {Automatic} {Speech} {Recognition} for {Children}},
	volume = {63},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300346},
	doi = {10.1016/j.csl.2020.101101},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Lyon, Thomas D. and Narayanan, Shrikanth},
	month = sep,
	year = {2020},
	pages = {101101},
}

@article{pironkov_hybrid-task_2020,
	title = {Hybrid-task learning for robust automatic speech recognition},
	volume = {64},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S088523082030036X},
	doi = {10.1016/j.csl.2020.101103},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Pironkov, Gueorgui and Wood, Sean UN and Dupont, Stéphane},
	month = nov,
	year = {2020},
	pages = {101103},
}

@article{wang_wavenet_2020,
	title = {{WaveNet} {With} {Cross}-{Attention} for {Audiovisual} {Speech} {Recognition}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9197622/},
	doi = {10.1109/ACCESS.2020.3024218},
	urldate = {2022-08-23},
	journal = {IEEE Access},
	author = {Wang, Hui and Gao, Fei and Zhao, Yue and Wu, Licheng},
	year = {2020},
	pages = {169160--169168},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\FRECEY55\\Wang et al. - 2020 - WaveNet With Cross-Attention for Audiovisual Speec.pdf:application/pdf},
}

@inproceedings{sarfraz_huda_speech_2016,
	address = {Khatmandu, Nepal},
	title = {Speech {Corpus} {Development} for a {Speaker} {Independent} {Spontaneous} {Urdu} {Speech} {Recognition} {System}},
	booktitle = {Proceedings of the {O}-{COCOSDA}},
	author = {Sarfraz, Huda and Hussain, Sarmad and Bokhari, Riffat and Agha, Ali and Agha Ali Raza and Inamullah and Sarfaraz, Zahid and Parvez, Sophia and Mustafa, Asad and Javed, Iqra and Parveen, Raheela},
	month = mar,
	year = {2016},
}

@misc{mozilla_deep_nodate,
	title = {Deep {Speech} {Documentation}},
	url = {https://deepspeech.readthedocs.io/en/r0.9/?badge=latest},
	urldate = {2022-08-23},
	author = {Mozilla},
}

@inproceedings{wang_application_2015,
	address = {Beijing},
	title = {The {Application} of {Data} {Mining} {Technology} for the {Judgment} of {Poisoning} {Cases}},
	isbn = {978-1-4673-7211-4},
	url = {https://ieeexplore.ieee.org/document/7518465/},
	doi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.284},
	urldate = {2022-08-24},
	booktitle = {2015 {IEEE} 12th {Intl} {Conf} on {Ubiquitous} {Intelligence} and {Computing} and 2015 {IEEE} 12th {Intl} {Conf} on {Autonomic} and {Trusted} {Computing} and 2015 {IEEE} 15th {Intl} {Conf} on {Scalable} {Computing} and {Communications} and {Its} {Associated} {Workshops} ({UIC}-{ATC}-{ScalCom})},
	publisher = {IEEE},
	author = {Wang, Jiong and Zhang, Yunfeng and Wang, Fanglin and Gao, Bin},
	month = aug,
	year = {2015},
	pages = {1567--1571},
}

@article{zhao_data_2022,
	title = {Data {Poisoning} {Attacks} and {Defenses} in {Dynamic} {Crowdsourcing} with {Online} {Data} {Quality} {Learning}},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/9640529/},
	doi = {10.1109/TMC.2021.3133365},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Zhao, Yuxi and Gong, Xiaowen and Lin, Fuhong and Chen, Xu},
	year = {2022},
	pages = {1--1},
}

@inproceedings{hu_data_2020,
	title = {Data {Poisoning} on {Deep} {Learning} {Models}},
	doi = {10.1109/CSCI51800.2020.00111},
	abstract = {Deep learning is a form of artificial intelligence (AI) that has seen rapid development and deployment in computer software as a means to implementing AI functionality with greater efficiency and ease as compared to other alternative AI solutions, with usage seen in systems varying from search and recommendation engines to autonomous vehicles. With the demand for deep learning algorithms that can perform increasingly complex tasks in a shorter time frame growing at an exponential pace, the developments in the efficiency and productivity of algorithms has far outpaced that of the security of such algorithms, drawing concerns over the many unaddressed vulnerabilities that may be exploited to compromise the integrity of these software. This study investigated the ability of poisoning attacks, a form of attack targeting the vulnerability of deep learning training data, to compromise the integrity of a deep learning model's classificational functionality. Experimentation involved the processing of training data sets with varying deep learning models and the incremental introduction of poisoned data sets to view the efficacy of a poisoning attack under multiple circumstances and correlate such with aspects of the model's design conditions. Analysis of results showed evidence of a decrease of classificational ability correlating with an increase of poison percentage in the training data sets, but the scale of which the decrease occurred varied with the specified parameters in the model design. Based on this, it was concluded that poisoning can provide varying levels of damage to deep learning classificational ability depending on the parameters utilized in the model design, and methods to countermeasure such were proposed, such as increasing epoch count, implementing mechanisms bolstering model fit, and integrating input level filtration systems.},
	booktitle = {2020 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Hu, Charles and Hu, Yen-Hung Frank},
	month = dec,
	year = {2020},
	keywords = {Software, Computational modeling, Data models, artificial intelligence, machine learning, data poisoning, deep learning, Deep learning, Software algorithms, Toxicology, Training data},
	pages = {628--632},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\EU5RHS82\\9457922.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\2MNURGVH\\Hu and Hu - 2020 - Data Poisoning on Deep Learning Models.pdf:application/pdf},
}

@inproceedings{uprety_mitigating_2021,
	title = {Mitigating {Poisoning} {Attack} in {Federated} {Learning}},
	doi = {10.1109/SSCI50451.2021.9659839},
	abstract = {Adversarial machine learning (AML) has emerged as one of the significant research areas in machine learning (ML) because models we train lack robustness and trustworthiness. Federated learning (FL) trains models over distributed devices and model parameters are shared instead of actual data in a privacy-preserving manner. Unfortunately, FL is also vulnerable to attacks including parameter/data poisoning attacks. In this paper, we first analyze the impact of the data poisoning attack on this training method with a label-flipping attack. We propose a poisoning attack mitigation technique based on the reputation of nodes' involved in the training process. The reputation score for each client is calculated using the beta probability distribution method. This is the first work to show the removal of malicious nodes with the poisoned dataset from the training environment based on the calculated reputation score. The improvement in model performance after filtering malicious nodes is validated using the benchmark MNIST dataset. At the same time, our work contributes to preventing denial of service attacks by considering a blockchain-based server network. Our results hold for two different attack settings with different proportions of poisoned data samples.},
	booktitle = {2021 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Uprety, Aashma and Rawat, Danda B.},
	month = dec,
	year = {2021},
	keywords = {Computational modeling, Training, Data models, Collaborative work, Data poisoning attack, Data privacy, Distance learning, Filtering, reputation model, secure federated learning},
	pages = {01--07},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PM3PPZV8\\Uprety and Rawat - 2021 - Mitigating Poisoning Attack in Federated Learning.pdf:application/pdf},
}


@inproceedings{seetharaman_influence_2022,
	title = {Influence {Based} {Defense} {Against} {Data} {Poisoning} {Attacks} in {Online} {Learning}},
	doi = {10.1109/COMSNETS53615.2022.9668557},
	abstract = {Data poisoning is a type of adversarial attack on training data where an attacker manipulates a fraction of data to degrade the performance of machine learning model. There are several known defensive mechanisms for handling offline attacks, however defensive measures for online learning, where data points arrive sequentially, have not garnered similar interest. In this work, we propose a defense mechanism to minimize the degradation caused by the poisoned training data on a learner's model in an online setup. Our proposed method utilizes an influence function which is a classic technique in robust statistics. Further, we supplement it with the existing data sanitization methods for filtering out some of the poisoned data points. We study the effectiveness of our defense mechanism on multiple datasets and across multiple attack strategies against an online learner.},
	booktitle = {2022 14th {International} {Conference} on {COMmunication} {Systems} \& {NETworkS} ({COMSNETS})},
	author = {Seetharaman, Sanjay and Malaviya, Shubham and Vasu, Rosni and Shukla, Manish and Lodha, Sachin},
	month = jan,
	year = {2022},
	note = {ISSN: 2155-2509},
	keywords = {Data models, Data integrity, Training data, Filtering, Adversarial Machine Learning, Data Poisoning, Degradation, Influence Function, Linear programming, Machine learning, Online Learning},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\QI6QPWYV\\Seetharaman et al. - 2022 - Influence Based Defense Against Data Poisoning Att.pdf:application/pdf},
}

@article{zhao_garbage_2021,
	title = {Garbage {In}, {Garbage} {Out}: {Poisoning} {Attacks} {Disguised} {With} {Plausible} {Mobility} in {Data} {Aggregation}},
	volume = {8},
	issn = {2327-4697, 2334-329X},
	shorttitle = {Garbage {In}, {Garbage} {Out}},
	url = {https://ieeexplore.ieee.org/document/9511094/},
	doi = {10.1109/TNSE.2021.3103919},
	number = {3},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Zhao, Ping and Jiang, Hongbo and Li, Jie and Xiao, Zhu and Liu, Daibo and Ren, Ju and Guo, Deke},
	month = jul,
	year = {2021},
	pages = {2679--2693},
}

@inproceedings{franci_influence-driven_2022,
	title = {Influence-{Driven} {Data} {Poisoning} in {Graph}-{Based} {Semi}-{Supervised} {Classifiers}},
	abstract = {Graph-based Semi-Supervised Learning (GSSL) is a practical solution to learn from a limited amount of labelled data together with a vast amount of unlabelled data. However, due to their reliance on the known labels to infer the unknown labels, these algorithms are sensitive to data quality. It is therefore essential to study the potential threats related to the labelled data, more specifically, label poisoning. In this paper, we propose a novel data poisoning method which efficiently approximates the result of label inference to identify the inputs which, if poisoned, would produce the highest number of incorrectly inferred labels. We extensively evaluate our approach on three classification problems under 24 different experimental settings each. Compared to the state of the art, our influence-driven attack produces an average increase of error rate 50\% higher, while being faster by multiple orders of magnitude. Moreover, our method can inform engineers of inputs that deserve investigation (relabelling them) before training the learning model. We show that relabelling one-third of the poisoned inputs (selected based on their influence) reduces the poisoning effect by 50\%. ACM Reference Format: Adriano Franci, Maxime Cordy, Martin Gubri, Mike Papadakis, and Yves Le Traon. 2022. Influence-Driven Data Poisoning in Graph-Based Semi-Supervised Classifiers. In 1st Conference on AI Engineering - Software Engineering for AI (CAIN’22), May 16–24, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3522664.3528606},
	booktitle = {2022 {IEEE}/{ACM} 1st {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Franci, Adriano and Cordy, Maxime and Gubri, Martin and Papadakis, Mike and Traon, Yves Le},
	month = may,
	year = {2022},
	keywords = {Training, Measurement, Data integrity, data poisoning, Machine learning, Approximation algorithms, Error analysis, Inference algorithms, semi-supervised learning, Semisupervised learning},
	pages = {77--87},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\J9JGMGKF\\Franci et al. - 2022 - Influence-Driven Data Poisoning in Graph-Based Sem.pdf:application/pdf},
}

@article{zhang_poisongan_2021,
	title = {{PoisonGAN}: {Generative} {Poisoning} {Attacks} {Against} {Federated} {Learning} in {Edge} {Computing} {Systems}},
	volume = {8},
	issn = {2327-4662, 2372-2541},
	shorttitle = {{PoisonGAN}},
	url = {https://ieeexplore.ieee.org/document/9194010/},
	doi = {10.1109/JIOT.2020.3023126},
	number = {5},
	urldate = {2022-08-24},
	journal = {IEEE Internet of Things Journal},
	author = {Zhang, Jiale and Chen, Bing and Cheng, Xiang and Binh, Huynh Thi Thanh and Yu, Shui},
	month = mar,
	year = {2021},
	pages = {3310--3322},
}

@inproceedings{doku_mitigating_2021,
	address = {Las Vegas, NV, USA},
	title = {Mitigating {Data} {Poisoning} {Attacks} {On} a {Federated} {Learning}-{Edge} {Computing} {Network}},
	isbn = {978-1-72819-794-4},
	url = {https://ieeexplore.ieee.org/document/9369581/},
	doi = {10.1109/CCNC49032.2021.9369581},
	urldate = {2022-08-24},
	booktitle = {2021 {IEEE} 18th {Annual} {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	publisher = {IEEE},
	author = {Doku, Ronald and Rawat, Danda B.},
	month = jan,
	year = {2021},
	pages = {1--6},
}

@article{wen_great_2021,
	title = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}: {Efficient} {Poisoning} {Attacks} and {Defenses} for {Linear} {Regression} {Models}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}},
	url = {https://ieeexplore.ieee.org/document/9448089/},
	doi = {10.1109/TIFS.2021.3087332},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Wen, Jialin and Zhao, Benjamin Zi Hao and Xue, Minhui and Oprea, Alina and Qian, Haifeng},
	year = {2021},
	pages = {3709--3723},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\PWEN89AG\\Wen et al. - 2021 - With Great Dispersion Comes Greater Resilience Ef.pdf:application/pdf},
}

@article{chen_-pois_2021,
	title = {De-{Pois}: {An} {Attack}-{Agnostic} {Defense} against {Data} {Poisoning} {Attacks}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {De-{Pois}},
	url = {https://ieeexplore.ieee.org/document/9431105/},
	doi = {10.1109/TIFS.2021.3080522},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Chen, Jian and Zhang, Xuxin and Zhang, Rui and Wang, Chen and Liu, Ling},
	year = {2021},
	pages = {3412--3425},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\AK6SW628\\Chen et al. - 2021 - De-Pois An Attack-Agnostic Defense against Data P.pdf:application/pdf},
}

@inproceedings{kwon_selective_2019,
	address = {Sardinia, Italy},
	title = {Selective {Poisoning} {Attack} on {Deep} {Neural} {Network} to {Induce} {Fine}-{Grained} {Recognition} {Error}},
	isbn = {978-1-72811-488-0},
	url = {https://ieeexplore.ieee.org/document/8791700/},
	doi = {10.1109/AIKE.2019.00033},
	urldate = {2022-08-24},
	booktitle = {2019 {IEEE} {Second} {International} {Conference} on {Artificial} {Intelligence} and {Knowledge} {Engineering} ({AIKE})},
	publisher = {IEEE},
	author = {Kwon, Hyun and Yoon, Hyunsoo and Park, Ki-Woong},
	month = jun,
	year = {2019},
	pages = {136--139},
}

@inproceedings{kontopoulos_countering_2018,
	address = {Athens},
	title = {Countering {Real}-{Time} {Stream} {Poisoning}: {An} {Architecture} for {Detecting} {Vessel} {Spoofing} in {Streams} of {AIS} {Data}},
	isbn = {978-1-5386-7518-2},
	shorttitle = {Countering {Real}-{Time} {Stream} {Poisoning}},
	url = {https://ieeexplore.ieee.org/document/8512006/},
	doi = {10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00139},
	urldate = {2022-08-24},
	booktitle = {2018 {IEEE} 16th {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, 16th {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, 4th {Intl} {Conf} on {Big} {Data} {Intelligence} and {Computing} and {Cyber} {Science} and {Technology} {Congress}({DASC}/{PiCom}/{DataCom}/{CyberSciTech})},
	publisher = {IEEE},
	author = {Kontopoulos, Ioannis and Spiliopoulos, Giannis and Zissis, Dimitrios and Chatzikokolakis, Konstantinos and Artikis, Alexander},
	month = aug,
	year = {2018},
	pages = {981--986},
}

@misc{noauthor_adoption_nodate,
	title = {Adoption of {AI} advances, but foundational barriers remain {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/featured-insights/artificial-intelligence/ai-adoption-advances-but-foundational-barriers-remain},
	urldate = {2022-08-24},
	file = {Adoption of AI advances, but foundational barriers remain | McKinsey:C\:\\Users\\DELL\\Zotero\\storage\\7BXXNTS6\\ai-adoption-advances-but-foundational-barriers-remain.html:text/html},
}

@misc{noauthor_exclusive_nodate,
	title = {Exclusive: {What} is data poisoning and why should we be concerned? - {International} {Security} {Journal} ({ISJ})},
	shorttitle = {Exclusive},
	url = {https://internationalsecurityjournal.com/what-is-data-poisoning/},
	abstract = {Machine learning could be one of the most disruptive technologies the world has seen in decades. Virtually every industry can benefit from these artificial},
	language = {en-GB},
	urldate = {2022-08-24},
	note = {Section: AI \& Deep Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VYNPR2KK\\what-is-data-poisoning.html:text/html},
}

@misc{vincent_twitter_2016,
	title = {Twitter taught {Microsoft}’s friendly {AI} chatbot to be a racist asshole in less than a day},
	url = {https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist},
	abstract = {It took less than 24 hours for Twitter to corrupt an innocent AI chatbot. Yesterday, Microsoft unveiled Tay — a Twitter bot that the company described as an experiment in "conversational...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Vincent, James},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5ZCPBDM9\\tay-microsoft-chatbot-racist.html:text/html},
}

@misc{kastrenakes_microsoft_2016,
	title = {Microsoft made a chatbot that tweets like a teen},
	url = {https://www.theverge.com/2016/3/23/11290200/tay-ai-chatbot-released-microsoft},
	abstract = {Microsoft is trying to create AI that can pass for a teen. Its research team launched a chatbot this morning called Tay, which is meant to test and improve Microsoft's understanding of...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Kastrenakes, Jacob},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SJ6QZ3R3\\tay-ai-chatbot-released-microsoft.html:text/html},
}

@article{steinhardt_certified_2017,
	title = {Certified {Defenses} for {Data} {Poisoning} {Attacks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03691},
	doi = {10.48550/ARXIV.1706.03691},
	abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.},
	urldate = {2022-08-24},
	author = {Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
	annote = {Other
Appeared at NIPS 2017},
}

@article{newman_ai_nodate,
	title = {{AI} {Can} {Help} {Cybersecurity}—{If} {It} {Can} {Fight} {Through} the {Hype}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/ai-machine-learning-cybersecurity/},
	abstract = {There are a ton of claims around AI and cybersecurity that don't quite add up. Here's what's really going on.},
	language = {en-US},
	urldate = {2022-08-24},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {artificial intelligence, machine learning, ai, cybersecurity},
}

@misc{ilmoi_evasion_2019,
	title = {Evasion attacks on {Machine} {Learning} (or “{Adversarial} {Examples}”)},
	url = {https://towardsdatascience.com/evasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1},
	abstract = {Your ML model is easier to fool than you think},
	language = {en},
	urldate = {2022-08-24},
	journal = {Medium},
	author = {ilmoi},
	month = jul,
	year = {2019},
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	copyright = {Creative Commons Attribution 3.0 Unported},
	url = {https://arxiv.org/abs/1312.6199},
	doi = {10.48550/ARXIV.1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2022-08-24},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV)},
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1412.6572},
	doi = {10.48550/ARXIV.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2022-08-24},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{bursztein_attacks_nodate,
	title = {Attacks against machine learning — an overview},
	url = {https://elie.net/blog/ai/attacks-against-machine-learning-an-overview/},
	abstract = {This blog post surveys the attacks techniques that target AI (Artificial Intelligence) systems and how to protect against them.},
	language = {en},
	urldate = {2022-08-24},
	journal = {Elie Bursztein's site},
	author = {Bursztein, Elie},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P5FGBPU5\\attacks-against-machine-learning-an-overview.html:text/html},
}

@article{goh_comprehensive_2015,
	title = {Comprehensive {Literature} {Review} on {Machine} {Learning} {Structures} for {Web} {Spam} {Classification}},
	volume = {70},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915032330},
	doi = {10.1016/j.procs.2015.10.069},
	language = {en},
	urldate = {2022-08-24},
	journal = {Procedia Computer Science},
	author = {Goh, Kwang Leng and Singh, Ashutosh Kumar},
	year = {2015},
	pages = {434--441},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ZZY6ZDLK\\Goh and Singh - 2015 - Comprehensive Literature Review on Machine Learnin.pdf:application/pdf},
}

@article{jo_measuring_2017,
	title = {Measuring the tendency of {CNNs} to {Learn} {Surface} {Statistical} {Regularities}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.11561},
	doi = {10.48550/ARXIV.1711.11561},
	abstract = {Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset. We are concerned with the following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities. For the SVHN and CIFAR-10 datasets, we present two Fourier filtered variants: a low frequency variant and a randomly filtered variant. Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects. Our main finding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap across the various test sets. Moreover, we observe that significantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap. Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts.},
	urldate = {2022-08-24},
	author = {Jo, Jason and Bengio, Yoshua},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	annote = {Other
Submitted},
}

@inproceedings{florian_tramer_stealing_2016,
	address = {Austin, Texas, USA},
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	isbn = {978-1-931971-32-4},
	url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer},
	urldate = {2022-08-24},
	publisher = {USENIX Association},
	author = {Florian Tramer and Fan Zhang and Ari Juels and Michael K. Reiter and Thomas Ristenpart},
	month = aug,
	year = {2016},
	pages = {601--618},
	file = {Stealing Machine Learning Models via Prediction APIs | USENIX:C\:\\Users\\DELL\\Zotero\\storage\\KYVFLJL7\\tramer.html:text/html},
}

@misc{papernot_semi-supervised_2017,
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	url = {http://arxiv.org/abs/1610.05755},
	abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
	month = mar,
	year = {2017},
	note = {arXiv:1610.05755 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted to ICLR 17 as an oral},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\B2VJHRWF\\Papernot et al. - 2017 - Semi-supervised Knowledge Transfer for Deep Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\JK48PTLF\\1610.html:text/html},
}

@misc{papernot_scalable_2018,
	title = {Scalable {Private} {Learning} with {PATE}},
	url = {http://arxiv.org/abs/1802.08908},
	abstract = {The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a "student" model the knowledge of an ensemble of "teacher" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy (\${\textbackslash}varepsilon\$ {\textless} 1.0).},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Úlfar},
	month = feb,
	year = {2018},
	note = {arXiv:1802.08908 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Published as a conference paper at ICLR 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Y9TDCBWP\\Papernot et al. - 2018 - Scalable Private Learning with PATE.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L8FAK7HV\\1802.html:text/html},
}

@article{wang_poisoning_2022,
	title = {Poisoning attacks and countermeasures in intelligent networks: {Status} quo and prospects},
	volume = {8},
	issn = {23528648},
	shorttitle = {Poisoning attacks and countermeasures in intelligent networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S235286482100050X},
	doi = {10.1016/j.dcan.2021.07.009},
	language = {en},
	number = {2},
	urldate = {2022-08-24},
	journal = {Digital Communications and Networks},
	author = {Wang, Chen and Chen, Jian and Yang, Yang and Ma, Xiaoqiang and Liu, Jiangchuan},
	month = apr,
	year = {2022},
	pages = {225--234},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5924FJWC\\Wang et al. - 2022 - Poisoning attacks and countermeasures in intellige.pdf:application/pdf},
}

@misc{noauthor_google_nodate,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/emergency-response/},
	urldate = {2022-08-24},
	file = {Google - Site Reliability Engineering:C\:\\Users\\DELL\\Zotero\\storage\\27RWN7KZ\\emergency-response.html:text/html},
}

@misc{noauthor_google_nodate-1,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/managing-incidents/},
	urldate = {2022-08-24},
}

@misc{gaudesi_channelaugment_2021,
	title = {{ChannelAugment}: {Improving} generalization of multi-channel {ASR} by training with input channel randomization},
	shorttitle = {{ChannelAugment}},
	url = {http://arxiv.org/abs/2109.11225},
	abstract = {End-to-end (E2E) multi-channel ASR systems show state-of-the-art performance in far-field ASR tasks by joint training of a multi-channel front-end along with the ASR model. The main limitation of such systems is that they are usually trained with data from a fixed array geometry, which can lead to degradation in accuracy when a different array is used in testing. This makes it challenging to deploy these systems in practice, as it is costly to retrain and deploy different models for various array configurations. To address this, we present a simple and effective data augmentation technique, which is based on randomly dropping channels in the multi-channel audio input during training, in order to improve the robustness to various array configurations at test time. We call this technique ChannelAugment, in contrast to SpecAugment (SA) which drops time and/or frequency components of a single channel input audio. We apply ChannelAugment to the Spatial Filtering (SF) and Minimum Variance Distortionless Response (MVDR) neural beamforming approaches. For SF, we observe 10.6\% WER improvement across various array configurations employing different numbers of microphones. For MVDR, we achieve a 74\% reduction in training time without causing degradation of recognition accuracy.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Gaudesi, Marco and Weninger, Felix and Sharma, Dushyant and Zhan, Puming},
	month = sep,
	year = {2021},
	note = {arXiv:2109.11225 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	annote = {Comment: To appear in ASRU 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D2RMIMH8\\Gaudesi et al. - 2021 - ChannelAugment Improving generalization of multi-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4F2SMWT4\\2109.html:text/html},
}

@article{lin_ml_2021,
	title = {{ML} {Attack} {Models}: {Adversarial} {Attacks} and {Data} {Poisoning} {Attacks}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{ML} {Attack} {Models}},
	url = {https://arxiv.org/abs/2112.02797},
	doi = {10.48550/ARXIV.2112.02797},
	abstract = {Many state-of-the-art ML models have outperformed humans in various tasks such as image classification. With such outstanding performance, ML models are widely used today. However, the existence of adversarial attacks and data poisoning attacks really questions the robustness of ML models. For instance, Engstrom et al. demonstrated that state-of-the-art image classifiers could be easily fooled by a small rotation on an arbitrary image. As ML systems are being increasingly integrated into safety and security-sensitive applications, adversarial attacks and data poisoning attacks pose a considerable threat. This chapter focuses on the two broad and important areas of ML security: adversarial attacks and data poisoning attacks.},
	urldate = {2022-08-24},
	author = {Lin, Jing and Dang, Long and Rahouti, Mohamed and Xiong, Kaiqi},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
}

@misc{noauthor_why_nodate,
	title = {Why does kaldi require mono channel audio for training instead of stereo or surround?},
	url = {https://groups.google.com/g/kaldi-help/c/92-jEzqyNb4},
	urldate = {2022-08-24},
	file = {Why does kaldi require mono channel audio for training instead of stereo or surround?:C\:\\Users\\DELL\\Zotero\\storage\\8P3Y6TTD\\92-jEzqyNb4.html:text/html},
}

@misc{noauthor_kaldi_nodate,
	title = {Kaldi: {Data} preparation},
	url = {https://kaldi-asr.org/doc/data_prep.html},
	urldate = {2022-08-24},
	file = {Kaldi\: Data preparation:C\:\\Users\\DELL\\Zotero\\storage\\QLKDHK6G\\data_prep.html:text/html},
}

@book{international_speech_communication_association_speech_2016-1,
	address = {Red Hook, NY},
	title = {Speech beyond speech towards a better understanding of the most important biosignal: 16th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({INTERSPEECH} 2015): {Dresden}, {Germany}, 6-10 {September} 2015. {Volume} 5},
	isbn = {978-1-5108-1790-6},
	shorttitle = {Speech beyond speech towards a better understanding of the most important biosignal},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {International Speech Communication Association},
	year = {2016},
	note = {Meeting Name: International Speech Communication Association},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\22CXWDL9\\International Speech Communication Association - 2016 - Speech beyond speech towards a better understandin.pdf:application/pdf},
}

@article{kocon_offensive_2021,
	title = {Offensive, aggressive, and hate speech analysis: {From} data-centric to human-centered approach},
	volume = {58},
	issn = {03064573},
	shorttitle = {Offensive, aggressive, and hate speech analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457321001333},
	doi = {10.1016/j.ipm.2021.102643},
	language = {en},
	number = {5},
	urldate = {2022-08-26},
	journal = {Information Processing \& Management},
	author = {Kocoń, Jan and Figas, Alicja and Gruza, Marcin and Puchalska, Daria and Kajdanowicz, Tomasz and Kazienko, Przemysław},
	month = sep,
	year = {2021},
	pages = {102643},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\NSSERT5Y\\Kocoń et al. - 2021 - Offensive, aggressive, and hate speech analysis F.pdf:application/pdf},
}

@inproceedings{ghahremani_investigation_2017,
	address = {Okinawa},
	title = {Investigation of transfer learning for {ASR} using {LF}-{MMI} trained neural networks},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268947/},
	doi = {10.1109/ASRU.2017.8268947},
	urldate = {2022-09-04},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Ghahremani, Pegah and Manohar, Vimal and Hadian, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = dec,
	year = {2017},
	pages = {279--286},
}

@inproceedings{wallington_learning_2021,
	title = {On the {Learning} {Dynamics} of {Semi}-{Supervised} {Training} for {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/wallington21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1777},
	language = {en},
	urldate = {2022-09-04},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Wallington, Electra and Kershenbaum, Benji and Klejch, Ondřej and Bell, Peter},
	month = aug,
	year = {2021},
	pages = {716--720},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\F2TLM7QF\\Wallington et al. - 2021 - On the Learning Dynamics of Semi-Supervised Traini.pdf:application/pdf},
}

@inproceedings{sarkar_novel_2014,
	title = {A novel boosting algorithm for improved i-vector based speaker verification in noisy environments},
	booktitle = {Proceedings of the {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {INTERSPEECH}},
	author = {Sarkar, Sourjya and Rao, K.},
	month = sep,
	year = {2014},
}

@inproceedings{sarkar_study_2012,
	title = {Study of the {Effect} of {I}-vector {Modeling} on {Short} and {Mismatch} {Utterance} {Duration} for {Speaker} {Verification}},
	booktitle = {{INTERSPEECH}},
	author = {Sarkar, Achintya Kumar and Matrouf, Driss and Bousquet, Pierre-Michel and Bonastre, Jean-François},
	year = {2012},
}

@misc{zhang_uberi_speechrecognition_nodate-1,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-09-18},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4MV996BJ\\SpeechRecognition.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What {Is} {Automatic} {Speech} {Recognition}? - {Alexa} {Skills} {Kit} {Official} {Site}},
	shorttitle = {What {Is} {Automatic} {Speech} {Recognition}?},
	url = {https://developer.amazon.com/en-US/alexa/alexa-skills-kit/asr.html},
	abstract = {Automatic speech recognition (ASR) is technology that converts spoken words into text. Explore the topic of ASR and learn about building for voice.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Amazon (Alexa)},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\EA9BYMAB\\asr.html:text/html},
}

@misc{noauthor_speech--text_nodate,
	title = {Speech-to-{Text}: {Automatic} {Speech} {Recognition}},
	shorttitle = {Speech-to-{Text}},
	url = {https://cloud.google.com/speech-to-text},
	abstract = {Accurately convert voice to text in over 125 languages and variants by applying Google’s powerful machine learning models with an easy-to-use API.},
	language = {en},
	urldate = {2022-09-19},
	journal = {Google Cloud},
}

@misc{noauthor_siri_nodate,
	title = {Siri},
	url = {https://www.apple.com/siri/},
	abstract = {Siri is an easy way to make calls, send texts, use apps, and get things done with just your voice. And Siri is the most private intelligent assistant.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Apple},
}

@misc{noauthor_cortana_nodate,
	title = {Cortana - {Your} personal productivity assistant},
	url = {https://www.microsoft.com/en-us/cortana},
	abstract = {Cortana helps you achieve more with less effort. Your personal productivity assistant helps you stay on top of what matters, follow through, and do your best work.},
	language = {en-us},
	urldate = {2022-09-19},
	journal = {Cortana - Your personal productivity assistant},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\APLZ98WM\\cortana.html:text/html},
}

@misc{noauthor_jarvis_nodate,
	title = {Jarvis {\textbar} {NVIDIA} {NGC}},
	url = {https://catalog.ngc.nvidia.com/orgs/nvidia/collections/jarvis},
	abstract = {NVIDIA Jarvis is a framework for production-grade conversational AI inference. The Jarvis Collection on NGC includes all the resources required for getting started with Jarvis.},
	language = {en},
	urldate = {2022-09-19},
	journal = {NVIDIA NGC Catalog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8U4FAMTW\\jarvis.html:text/html},
}

@misc{noauthor_sox_nodate,
	title = {{SoX} - {Sound} {eXchange} {\textbar} {HomePage}},
	url = {http://sox.sourceforge.net/},
	urldate = {2022-09-19},
	file = {SoX - Sound eXchange | HomePage:C\:\\Users\\DELL\\Zotero\\storage\\Y5XJDGHX\\sox.sourceforge.net.html:text/html},
}

@misc{raj_note_nodate,
	title = {A note on {MFCCs} and delta features},
	url = {https://desh2608.github.io/2019-07-26-delta-feats/},
	abstract = {What are MFCCs and how are they computed? Feature extraction is the first step in any automatic speech recognition (ASR) pipeline. The objective is to compute features from speech waveforms which contain relevant information about the linguistic content of the speech, and ignore information about the background noise, emotions, etc....},
	language = {en},
	urldate = {2022-09-19},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\R4WGBL8Q\\2019-07-26-delta-feats.html:text/html},
}

@inproceedings{menne_analysis_2019,
	title = {Analysis of {Deep} {Clustering} as {Preprocessing} for {Automatic} {Speech} {Recognition} of {Sparsely} {Overlapping} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html},
	doi = {10.21437/Interspeech.2019-1728},
	language = {en},
	urldate = {2022-09-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Menne, Tobias and Sklyar, Ilya and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2019},
	pages = {2638--2642},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5X2I92JR\\Menne et al. - 2019 - Analysis of Deep Clustering as Preprocessing for A.pdf:application/pdf},
}

@article{li_tenet_2019,
	title = {{TEnet}: target speaker extraction network with accumulated speaker embedding for automatic speech recognition},
	volume = {55},
	issn = {0013-5194, 1350-911X},
	shorttitle = {{TEnet}},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/el.2019.1228},
	doi = {10.1049/el.2019.1228},
	language = {en},
	number = {14},
	urldate = {2022-09-19},
	journal = {Electronics Letters},
	author = {Li, Wenjie and Zhang, Pengyuan and Yan, Yonghong},
	month = jul,
	year = {2019},
	pages = {816--819},
}

@article{van_wyk_multivaluedness_2021,
	title = {Multivaluedness in {Networks}: {Shannon}’s {Noisy}-{Channel} {Coding} {Theorem}},
	volume = {68},
	issn = {1549-7747, 1558-3791},
	shorttitle = {Multivaluedness in {Networks}},
	url = {https://ieeexplore.ieee.org/document/9410598/},
	doi = {10.1109/TCSII.2021.3074925},
	number = {10},
	urldate = {2022-09-19},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {van Wyk, Michael Antonie and Ping, Li and Chen, Guanrong},
	month = oct,
	year = {2021},
	pages = {3234--3235},
}

@inproceedings{nautsch_gdpr_2019-2,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-09-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GPBKHSGG\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@inproceedings{mohamed_understanding_2012,
	address = {Kyoto, Japan},
	title = {Understanding how {Deep} {Belief} {Networks} perform acoustic modelling},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	url = {http://ieeexplore.ieee.org/document/6288863/},
	doi = {10.1109/ICASSP.2012.6288863},
	urldate = {2022-09-19},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Mohamed, Abdel-rahman and Hinton, Geoffrey and Penn, Gerald},
	month = mar,
	year = {2012},
	pages = {4273--4276},
}

@misc{shrawankar_adverse_2013,
	title = {Adverse {Conditions} and {ASR} {Techniques} for {Robust} {Speech} {User} {Interface}},
	url = {http://arxiv.org/abs/1303.5515},
	abstract = {The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.},
	urldate = {2022-09-19},
	publisher = {arXiv},
	author = {Shrawankar, Urmila and Thakare, V. M.},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	annote = {Comment: 10 pages 2 Tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Y9HZ2V8C\\Shrawankar and Thakare - 2013 - Adverse Conditions and ASR Techniques for Robust S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AWG8ADWP\\1303.html:text/html},
}

@article{benesty_springer_2009,
	title = {Springer {Handbook} of {Speech} {Processing}},
	volume = {126},
	issn = {00014966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/126/4/10.1121/1.3203918},
	doi = {10.1121/1.3203918},
	language = {en},
	number = {4},
	urldate = {2022-09-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Benesty, Jacob and Sondhi, Mohan M. and Huang, Yiteng and Greenberg, Steven},
	year = {2009},
	pages = {2130},
}

@misc{marvel_jrvis_nodate,
	title = {J.{A}.{R}.{V}.{I}.{S}.},
	url = {https://ironman.fandom.com/wiki/J.A.R.V.I.S.},
	abstract = {Just A Rather Very Intelligent System (J.A.R.V.I.S.) was originally Tony Stark's natural-language user interface computer system, named after Edwin Jarvis, the butler who worked for Howard Stark. Over time, he was upgraded into an artificially intelligent system, tasked with running business for Stark Industries as well as security for Tony Stark's Mansion and Stark Tower. After creating the Mark II armor, Stark uploaded J.A.R.V.I.S. into all of the Iron Man Armors, as well as allowing him to in},
	language = {en},
	urldate = {2022-09-25},
	journal = {Iron Man Wiki},
	author = {Marvel, Fandom},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2AT6NTE6\\J.A.R.V.I.S..html:text/html},
}

@misc{audacity_linux_nodate,
	title = {Audacity for Linux},
	url = {https://www.audacityteam.org/download/linux/},
	abstract = {Thank you for downloading Audacity

Your download will start in 5 seconds. 
  Problems with the download? Please use this direct link


AppImage
Audacity 3.2.0 is available as an AppImage. The AppImage should run on most modern Linux distributions. To run AppImage:

Left click the link below.
Make t},
	language = {en-US},
	urldate = {2022-09-27},
	journal = {Audacity ®},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\TZSQDL3M\\linux.html:text/html},
}

@article{hussein_arabic_2022,
	title = {Arabic speech recognition by end-to-end, modular systems and human},
	volume = {71},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230821000760},
	doi = {10.1016/j.csl.2021.101272},
	language = {en},
	urldate = {2022-10-02},
	journal = {Computer Speech \& Language},
	author = {Hussein, Amir and Watanabe, Shinji and Ali, Ahmed},
	month = jan,
	year = {2022},
	pages = {101272},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\WJXMYF47\\Hussein et al. - 2022 - Arabic speech recognition by end-to-end, modular s.pdf:application/pdf},
}

@article{dahl_context-dependent_2012,
	title = {Context-{Dependent} {Pre}-{Trained} {Deep} {Neural} {Networks} for {Large}-{Vocabulary} {Speech} {Recognition}},
	volume = {20},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5740583/},
	doi = {10.1109/TASL.2011.2134090},
	number = {1},
	urldate = {2022-10-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Dahl, G. E. and {Dong Yu} and {Li Deng} and Acero, A.},
	month = jan,
	year = {2012},
	pages = {30--42},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CT4BGLDU\\Dahl et al. - 2012 - Context-Dependent Pre-Trained Deep Neural Networks.pdf:application/pdf},
}

@article{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1303.5778},
	doi = {10.48550/ARXIV.1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates {\textbackslash}emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	urldate = {2022-10-02},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},
	annote = {Other
To appear in ICASSP 2013},
}

@article{hifny_unified_2015,
	title = {Unified {Acoustic} {Modeling} using {Deep} {Conditional} {Random} {Fields}},
	issn = {20547390},
	url = {http://scholarpublishing.org/index.php/TMLAI/article/view/1124},
	doi = {10.14738/tmlai.32.1124},
	urldate = {2022-10-02},
	journal = {Transactions on Machine Learning and Artificial Intelligence},
	author = {Hifny, Yasser},
	month = apr,
	year = {2015},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\V8U7GBTW\\Hifny - 2015 - Unified Acoustic Modeling using Deep Conditional R.pdf:application/pdf},
}

@inproceedings{povey_purely_2016,
	title = {Purely {Sequence}-{Trained} {Neural} {Networks} for {ASR} {Based} on {Lattice}-{Free} {MMI}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/povey16_interspeech.html},
	doi = {10.21437/Interspeech.2016-595},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Povey, Daniel and Peddinti, Vijayaditya and Galvez, Daniel and Ghahremani, Pegah and Manohar, Vimal and Na, Xingyu and Wang, Yiming and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {2751--2755},
}

@inproceedings{vijayaditya_time_2015,
	title = {A time delay neural network architecture for efficient modeling of long temporal contexts},
	url = {https://www.semanticscholar.org/paper/A-time-delay-neural-network-architecture-for-of-Peddinti-Povey/3a79ac688f2558b2d9693e434f010e041eba0fae},
	author = {Vijayaditya and Peddinti and Sanjeev Khudanpur and Daniel Povey},
	year = {2015},
}

@article{ali_speech_2017,
	title = {Speech {Recognition} {Challenge} in the {Wild}: {Arabic} {MGB}-3},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Speech {Recognition} {Challenge} in the {Wild}},
	url = {https://arxiv.org/abs/1709.07276},
	doi = {10.48550/ARXIV.1709.07276},
	abstract = {This paper describes the Arabic MGB-3 Challenge - Arabic Speech Recognition in the Wild. Unlike last year's Arabic MGB-2 Challenge, for which the recognition task was based on more than 1,200 hours broadcast TV news recordings from Aljazeera Arabic TV programs, MGB-3 emphasises dialectal Arabic using a multi-genre collection of Egyptian YouTube videos. Seven genres were used for the data collection: comedy, cooking, family/kids, fashion, drama, sports, and science (TEDx). A total of 16 hours of videos, split evenly across the different genres, were divided into adaptation, development and evaluation data sets. The Arabic MGB-Challenge comprised two tasks: A) Speech transcription, evaluated on the MGB-3 test set, along with the 10 hour MGB-2 test set to report progress on the MGB-2 evaluation; B) Arabic dialect identification, introduced this year in order to distinguish between four major Arabic dialects - Egyptian, Levantine, North African, Gulf, as well as Modern Standard Arabic. Two hours of audio per dialect were released for development and a further two hours were used for evaluation. For dialect identification, both lexical features and i-vector bottleneck features were shared with participants in addition to the raw audio recordings. Overall, thirteen teams submitted ten systems to the challenge. We outline the approaches adopted in each system, and summarise the evaluation results.},
	urldate = {2022-10-02},
	author = {Ali, Ahmed and Vogel, Stephan and Renals, Steve},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{khurana_qcri_2016,
	address = {San Diego, CA},
	title = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition: {MGB}-2 challenge},
	isbn = {978-1-5090-4903-5},
	shorttitle = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition},
	url = {http://ieeexplore.ieee.org/document/7846279/},
	doi = {10.1109/SLT.2016.7846279},
	urldate = {2022-10-02},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Khurana, Sameer and Ali, Ahmed},
	month = dec,
	year = {2016},
	pages = {292--298},
}

@inproceedings{smit_aalto_2017,
	address = {Okinawa},
	title = {Aalto system for the 2017 {Arabic} multi-genre broadcast challenge},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268955/},
	doi = {10.1109/ASRU.2017.8268955},
	urldate = {2022-10-02},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Smit, Peter and Gangireddy, Siva Reddy and Enarvi, Seppo and Virpioja, Sami and Kurimo, Mikko},
	month = dec,
	year = {2017},
	pages = {338--345},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\YUJC9CV7\\Smit et al. - 2017 - Aalto system for the 2017 Arabic multi-genre broad.pdf:application/pdf},
}

@inproceedings{snyder_speaker_2019,
	address = {Brighton, United Kingdom},
	title = {Speaker {Recognition} for {Multi}-speaker {Conversations} {Using} {X}-vectors},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683760/},
	doi = {10.1109/ICASSP.2019.8683760},
	urldate = {2022-10-02},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and McCree, Alan and Povey, Daniel and Khudanpur, Sanjeev},
	month = may,
	year = {2019},
	pages = {5796--5800},
}

@article{dutta_performance_2021,
	title = {Performance analysis of {ASR} system in hybrid {DNN}-{HMM} framework using a {PWL} euclidean activation function},
	volume = {15},
	issn = {2095-2228, 2095-2236},
	url = {https://link.springer.com/10.1007/s11704-020-9419-z},
	doi = {10.1007/s11704-020-9419-z},
	language = {en},
	number = {4},
	urldate = {2022-10-02},
	journal = {Frontiers of Computer Science},
	author = {Dutta, Anirban and Ashishkumar, Gudmalwar and Rao, Ch V. Rama},
	month = aug,
	year = {2021},
	pages = {154705},
}

@inproceedings{georgescu_kaldi-based_2019,
	address = {Timisoara, Romania},
	title = {Kaldi-based {DNN} {Architectures} for {Speech} {Recognition} in {Romanian}},
	isbn = {978-1-72810-984-8},
	url = {https://ieeexplore.ieee.org/document/8906555/},
	doi = {10.1109/SPED.2019.8906555},
	urldate = {2022-10-04},
	booktitle = {2019 {International} {Conference} on {Speech} {Technology} and {Human}-{Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Georgescu, Alexandru-Lucian and Cucu, Horia and Burileanu, Corneliu},
	month = oct,
	year = {2019},
	pages = {1--6},
}

@incollection{ekstein_cnn-tdnn-based_2021,
	address = {Cham},
	title = {{CNN}-{TDNN}-{Based} {Architecture} for {Speech} {Recognition} {Using} {Grapheme} {Models} in {Bilingual} {Czech}-{Slovak} {Task}},
	volume = {12848},
	isbn = {978-3-030-83526-2 978-3-030-83527-9},
	url = {https://link.springer.com/10.1007/978-3-030-83527-9_45},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {Text, {Speech}, and {Dialogue}},
	publisher = {Springer International Publishing},
	author = {Psutka, Josef V. and Švec, Jan and Pražák, Aleš},
	editor = {Ekštein, Kamil and Pártl, František and Konopík, Miloslav},
	year = {2021},
	doi = {10.1007/978-3-030-83527-9_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {523--533},
}

@misc{noauthor_lattice_nodate,
	title = {On lattice free {MMI} and {Chain} models in {Kaldi}},
	url = {https://desh2608.github.io/2019-05-21-chain/},
	urldate = {2022-10-04},
	file = {On lattice free MMI and Chain models in Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\WVI67A8B\\2019-05-21-chain.html:text/html},
}

@misc{raj_experiments_nodate,
	title = {Experiments with {Subword} {Modeling}},
	url = {https://desh2608.github.io/2018-11-22-subword-segmentation/},
	abstract = {Think about tasks such as machine translation (MT), automatic speech recognition (ASR), or handwriting recognition (HWR). While these appear very distinct, on abstraction they share the same pipeline wherein given an input signal, we are required to predict some text. The difference only lies in the form of the input...},
	language = {en},
	urldate = {2022-10-04},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ID8BRFHZ\\2018-11-22-subword-segmentation.html:text/html},
}

@inproceedings{tian_consistent_2022,
	address = {Singapore, Singapore},
	title = {Consistent {Training} and {Decoding} for {End}-to-{End} {Speech} {Recognition} {Using} {Lattice}-{Free} {MMI}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9746579/},
	doi = {10.1109/ICASSP43922.2022.9746579},
	urldate = {2022-10-05},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Tian, Jinchuan and Yu, Jianwei and Weng, Chao and Zhang, Shi-Xiong and Su, Dan and Yu, Dong and Zou, Yuexian},
	month = may,
	year = {2022},
	pages = {7782--7786},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\UFAHYT8M\\Tian et al. - 2022 - Consistent Training and Decoding for End-to-End Sp.pdf:application/pdf},
}

@misc{fayek_speech_2016,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}’s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2022-10-06},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
}

@misc{wiesner_lattice_2020,
	title = {Lattice {Free} {Maximum} {Mutual} {Information} ({LF}-{MMI})},
	url = {https://m-wiesner.github.io/LF-MMI/},
	abstract = {Everything about LF-MMI},
	language = {en},
	urldate = {2022-10-08},
	journal = {Matthew Wiesner},
	author = {Wiesner, Matthew and MatthewWiesner},
	month = jan,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ELAZNE4P\\LF-MMI.html:text/html},
}

@article{liu_time_2019,
	title = {Time {Delay} {Recurrent} {Neural} {Network} for {Speech} {Recognition}},
	volume = {1229},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1229/1/012078},
	doi = {10.1088/1742-6596/1229/1/012078},
	abstract = {Abstract
            In Automatic Speech Recognition(ASR), Time Delay Neural Network (TDNN) has been proven to be an efficient network structure for its strong ability in context modeling. In addition, as a feed-forward neural architecture, it is faster to train TDNN, compared with recurrent neural networks such as Long Short-Term Memory (LSTM). However, different from recurrent neural networks, the context in TDNN is carefully designed and is limited. Although stacking Long Short-Term Memory (LSTM) together with TDNN in order to extend the context information have been proven to be useful, it is too complex and is hard to train. In this paper, we focus on directly extending the context modeling capability of TDNNs by adding recurrent connections. Several new network architectures were investigated. The results on the Switchboard show that the best model significantly outperforms the base line TDNN system and is comparable with TDNN-LSTM architecture. In addition, the training process is much simpler than that of TDNN-LSTM.},
	number = {1},
	urldate = {2022-10-12},
	journal = {Journal of Physics: Conference Series},
	author = {Liu, Boji and Zhang, Weibin and Xu, Xiangming and Chen, Dongpeng},
	month = may,
	year = {2019},
	pages = {012078},
}

@phdthesis{ritter_neural_2019,
	title = {Neural {Architecture} {Search} for {Finding} the {Best} {Time} {Delay} {Neural} {Network} {Acoustic} {Model} for {Speech} {Recognition}},
	abstract = {Time Delay Neural Network (TDNN) is a popular type of acoustic model used for speech recognition applications. Their popularity is mainly due to their faster training and decoding times, with word error rates (WER) comparable to a long-short term memory (LSTM) based acoustic model. The popularity of TDNNs picked up in 2015 when a more efficient setup was proposed, which is known as a TDNN with sub-sampling scheme. However, the 2015 proposal with sub-sampling scheme does not offer any details on the sub-sampling used. Neural Architecture Search (NAS) is a new research field that has garnered a lot of attention with successful results in computer vision research. Despite this, NAS has received little attention in speech recognition, where design architectures for acoustic models is crucial. TDNNs are provided in the Kaldi speech recognition toolkit to be used for research or deployment purposes. From the literature, we have observed that it is common for a TDNN to be used in Kaldi as is, with no additional tuning of its hyperparameters. For this reason, this project aims to investigate if the Kaldi baseline TDNN is actually the best configuration to be used for a speech recognition application. To do so, we have made use of a recently proposed algorithm which integrates reinforcement learning in its training process. Specifically, we have used Neural Architecture Search (NAS) to target and improve automatically the sub-sampling scheme of the Kaldi TDNN. For reproducibility we have based all our results on the standard Wall Street Journal database. We performed experiments by setting a RNN based TDNN architecture generator, with the added constraint that the TDNNs evaluated have the same number of frames as the one provided with Kaldi. Our results show that NAS is able to sample a better TDNN architecture than the one provided by Kaldi in less than 40,000 iterations, achieving a 0.19 WER reduction. i},
	author = {Ritter, Fabian},
	month = aug,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8Y4PMA9Y\\Ritter - 2019 - Neural Architecture Search for Finding the Best Ti.pdf:application/pdf},
}

@misc{fayek_speech_2016-1,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}’s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2022-10-13},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\GNNWTTYV\\speech-processing-for-machine-learning.html:text/html},
}

@misc{shrawankar_adverse_2013-1,
	title = {Adverse {Conditions} and {ASR} {Techniques} for {Robust} {Speech} {User} {Interface}},
	url = {http://arxiv.org/abs/1303.5515},
	abstract = {The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Shrawankar, Urmila and Thakare, V. M.},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	annote = {Comment: 10 pages 2 Tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Q3SPV23K\\Shrawankar and Thakare - 2013 - Adverse Conditions and ASR Techniques for Robust S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\82368FRG\\1303.html:text/html},
}

@article{vipperla_ageing_2010,
	title = {Ageing {Voices}: {The} {Effect} of {Changes} in {Voice} {Parameters} on {ASR} {Performance}},
	volume = {2010},
	issn = {1687-4714, 1687-4722},
	shorttitle = {Ageing {Voices}},
	url = {http://asmp.eurasipjournals.com/content/2010/1/525783},
	doi = {10.1155/2010/525783},
	language = {en},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	year = {2010},
	pages = {1--10},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\G6TNUYYM\\Vipperla et al. - 2010 - Ageing Voices The Effect of Changes in Voice Para.pdf:application/pdf},
}

@techreport{markus_forsberg_why_2003,
	title = {Why is {Speech} {Recognition} {Difficult}?},
	url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.3677},
	institution = {Chalmers University of Technology},
	author = {Markus Forsberg},
	month = feb,
	year = {2003},
}

@article{schuller_recognition_2009,
	title = {Recognition of {Noisy} {Speech}: {A} {Comparative} {Survey} of {Robust} {Model} {Architecture} and {Feature} {Enhancement}},
	volume = {2009},
	issn = {1687-4722},
	shorttitle = {Recognition of {Noisy} {Speech}},
	url = {http://asmp.eurasipjournals.com/content/2009/1/942617},
	doi = {10.1155/2009/942617},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Schuller, Björn and Wöllmer, Martin and Moosmayr, Tobias and Rigoll, Gerhard},
	year = {2009},
	pages = {942617},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9UQKPJZ2\\Schuller et al. - 2009 - Recognition of Noisy Speech A Comparative Survey .pdf:application/pdf},
}

@article{hoge_basic_2007,
	title = {Basic parameters in speech processing. {The} need for evaluation},
	volume = {32},
	copyright = {Copyright on any open access article in the Archives of Acoustics published by Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society is retained by the author(s).   Authors grant Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society a license to publish the article and identify itself as the original publisher.   Authors also grant any user the right to use the article freely as long as its integrity is maintained and its original authors, citation details and publisher are identified.           The Creative Commons Attribution License-ShareAlike 4.0 formalizes these and other terms and conditions of publishing articles.    Exceptions to copyright policy    For the articles which were previously published, before year 2019, policies that are different from the above. In all such, access to these articles is free from fees or any other access restrictions.   Permissions for the use of the texts published in that journal may be sought directly from the Editorial Office of Archives of Acoustics},
	issn = {2300-262X},
	url = {https://acoustics.ippt.pan.pl/index.php/aa/article/view/766},
	abstract = {As basic parameters in speech processing we regard pitch, duration, intensity, voice quality, signal to noise ratio, voice activity detection and strength of Lombard effect. Taking in account also adverse conditions the performance of many published algorithms to extract those parameters from the speech signal automatically is not known. A framework based on competitive evaluation is proposed to push algorithmic research and to make progress comparable.},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {Archives of Acoustics},
	author = {Höge, Harald},
	year = {2007},
	note = {Number: 1},
	pages = {67},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\9Z4BXXSK\\Höge - 2007 - Basic parameters in speech processing. The need fo.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FSTAYK66\\766.html:text/html},
}

@article{venkatagiri_speech_2002,
	title = {Speech {Recognition} {Technology} {Applications} in {Communication} {Disorders}},
	volume = {11},
	issn = {1058-0360, 1558-9110},
	url = {http://pubs.asha.org/doi/10.1044/1058-0360%282002/037%29},
	doi = {10.1044/1058-0360(2002/037)},
	abstract = {Speech recognition (SR) is the process whereby a microprocessor-based system, typically a computer with sound processing hardware and speech recognition software, responds in predictable ways to spoken commands and/or converts speech into text. This tutorial describes the types and the general uses of SR and provides an explanation of the technology behind it. The emerging applications of SR technology for dictation, articulation training, language and literacy development, environmental control, and communication augmentation are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-10-13},
	journal = {American Journal of Speech-Language Pathology},
	author = {Venkatagiri, H. S.},
	month = nov,
	year = {2002},
	pages = {323--332},
}

@techreport{noauthor_alternative_nodate,
	title = {Alternative {Control} {Technologies}: {Human} {Factors} {Issues}},
	shorttitle = {Alternative {Control} {Technologies}},
	url = {https://apps.dtic.mil/sti/citations/ADA355911},
	abstract = {With the increasing intelligence of computer systems, it is becoming more desirable to have an operator communicate with machines rather than simply operate them. In combat aircraft, this need to communicate is made quite crucial due to high temporal pressure and workload during critical phases of the flight ingress, engagement, deployment of self-defense. The HOTAS concept, with manual controls fitted on the stick and throttle, has been widely used in modern fighters such as F16, F18, EFA and Rafale. This concept allows pilots to input real time commands to the aircraft system. However, it increases the complexity of the pilot task due to inflation of real time controls, with some controls being multifunction. It is therefore desirable, in the framework of ecological interfaces, to introduce alternative input channels in order to reduce the complexity of manual control in the HOTAS concept and allow more direct and natural access to the aircraft systems. Control and display technologies are the critical enablers for these advanced interfaces. There are a variety of novel alternative control technologies that when integrated usefully with critical mission tasks can make natural use of the innate potential of human sensory and motor systems. Careful design and integration of candidate control technologies will result in human-machine interfaces which are natural, easier to learn, easier to use, and less prone to error. Significant progress is being made on using signals from the brain, muscles, voice, lip, head position, eye position and gestures for the control of computers and other devices. Judicious application of alternative control technologies has the potential to increase the bandwidth of operator-system interaction, improve the effectiveness of military systems, and realize cost savings. Alternative controls can reduce workload and improve efficiency within the cockpit, directly supporting the warfighter.},
	language = {en},
	urldate = {2022-10-13},
	note = {Section: Technical Reports},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\96Z76MGM\\ADA355911.html:text/html},
}

@book{vasilescu_cross-lingual_2011,
	title = {Cross-{Lingual} {Study} of {ASR} {Errors}: {On} the {Role} of the {Context} in {Human} {Perception} of {Near}-{Homophones}.},
	shorttitle = {Cross-{Lingual} {Study} of {ASR} {Errors}},
	author = {Vasilescu, Ioana and Yahia, Dahbia and Snoeren, Natalie and Adda-Decker, Martine and Lamel, Lori},
	month = aug,
	year = {2011},
	note = {Pages: 1952},
}

@inproceedings{povey_semi-orthogonal_2018,
	title = {Semi-{Orthogonal} {Low}-{Rank} {Matrix} {Factorization} for {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/povey18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1417},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Povey, Daniel and Cheng, Gaofeng and Wang, Yiming and Li, Ke and Xu, Hainan and Yarmohammadi, Mahsa and Khudanpur, Sanjeev},
	month = sep,
	year = {2018},
	pages = {3743--3747},
}

@inproceedings{yeh_taiwanese_2020,
	address = {Taipei, Taiwan},
	title = {Taiwanese {Speech} {Recognition} {Based} on {Hybrid} {Deep} {Neural} {Network} {Architecture}},
	url = {https://aclanthology.org/2020.rocling-1.11},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 32nd {Conference} on {Computational} {Linguistics} and {Speech} {Processing} ({ROCLING} 2020)},
	publisher = {The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)},
	author = {Yeh, Yu-Fu and Su, Bo-Hao and Ou, Yang-Yen and Wang, Jhing-Fa and Tsai, An-Chao},
	month = sep,
	year = {2020},
	pages = {102--113},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\MTSZB5PS\\Yeh et al. - 2020 - Taiwanese Speech Recognition Based on Hybrid Deep .pdf:application/pdf},
}

@article{s_review_2016-1,
	title = {A {Review} on {Automatic} {Speech} {Recognition} {Architecture} and {Approaches}},
	volume = {9},
	issn = {20054254, 20054254},
	url = {http://article.nadiapub.com/IJSIP/vol9_no4/34.pdf},
	doi = {10.14257/ijsip.2016.9.4.34},
	number = {4},
	urldate = {2022-10-18},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {S, Karpagavalli and E, Chandra},
	month = apr,
	year = {2016},
	pages = {393--404},
}
@misc{hannun_speech_nodate,
	title = {Speech {Recognition} {Is} {Not} {Solved}},
	url = {https://awni.github.io/speech-recognition/},
	abstract = {Ever since Deep Learning hit the scene in speech recognition, word error rates
have fallen dramatically. But despite articles you may have read, we still
don’t have human-level speech recognition. Speech recognizers have many failure
modes. Acknowledging these and taking steps towards solving them is critical to
progress. It’s the only way to go from ASR
which works for some people, most of the time to ASR which works for all
people, all of the time.},
	urldate = {2022-10-23},
	author = {Hannun, Awni},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HK2ZJCFX\\speech-recognition.html:text/html},
}

@misc{rev_speech_2021,
	title = {Speech {Recognition} {Challenges} and {How} to {Solve} {Them}},
	url = {https://www.rev.com/blog/speech-to-text-technology/speech-recognition-challenges-and-how-to-solve-them},
	abstract = {Cutting edge tech is always a challenge. That’s one of the reasons we love it. The breakthrough discovery, the moment when we figure out how to solve the},
	language = {en-US},
	urldate = {2022-10-27},
	journal = {Rev},
	author = {Rev},
	month = sep,
	year = {2021},
}

@phdthesis{ander_gonzalez_docasal_noisy_2018,
	title = {Noisy speech recognition using {Kaldi} and neural architectures},
	url = {http://hdl.handle.net/10810/27865},
	abstract = {Noisy Speech Recognition using Kaldi and Neural Architectures ABSTRACT The goal of an Automatic Speech Recognition (ASR) system is to transform a set of acoustic features into a sequence of words. It mainly consists of various parts: the feature extraction part which extracts information from a speech signal; the acoustic model, in charge of the conversion from speech to phonemes; and the language model that transforms the detected phonemes into the most probable sequence of words. Throughout their history, these systems were built with statistical methods, mainly Hidden Markov Models (HMM) and Gaussian Mixture Models (GMM). However, in recent years the use of neural architectures such as Deep, Convolutional and Recurrent Neural Networks (DNN, CNN and RNN), have improved the achieved results significantly. Moreover, freely available tools made ASR research develop quickly. Kaldi is one of the most known and widely used ASR systems. It includes a set of neural network packages —nnet1, nnet2 and nnet3— which can be used for implementing the acoustic model. These are fast, accurate and able to handle huge databases since they distribute the load on clusters of machines. However, Kaldi’s slow development cycle implies that new neural architectures may be introduced many years after their publications. Therefore, in this work we substitute the neural acoustic model of Kaldi by our own implementations written in TensorFlow. TensorFlow has the largest community of users and the best support among the available deep learning libraries. By substituting the Acoustic Model of Kaldi with different architectures and testing their performance on the well-known database Aurora-4, we managed to reduce Word Error Rate (WER) by 3.17 \% (baseline 15.14 \%) when using a CNN architecture. Also, focusing on just the clean subset of the Test part of the database, a further improvement has been achieved once implementing a CNN + RNN structure, from a 4.54 \% WER with the CNNs alone to a 4.13 \% with this architecture. This work is therefore believed to improve the results on obtained by one of the widely used ASR tools simply by implementing more advanced deep learning techniques, which could be executed by more powerful and dedicated external programs. For future work, a further analysis on more complex convolutional networks could lead to a better performance in this particular database and, in general, in noisy environments. Finally, further improvement of convolutional and recurrent architectures is suggested in clean and noise-free conditions, since they have been shown to obtain the best results in this specific circumstances.},
	school = {University of Crete},
	author = {Ander González Docasal},
	month = feb,
	year = {2018},
}

@inproceedings{mirzaei_errors_2015,
	title = {Errors in automatic speech recognition versus difficulties in second language listening},
	isbn = {978-1-908416-29-2},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2015.000367},
	doi = {10.14705/rpnet.2015.000367},
	urldate = {2022-10-27},
	booktitle = {Critical {CALL} – {Proceedings} of the 2015 {EUROCALL} {Conference}, {Padova}, {Italy}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Meshgi, Kourosh and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2015},
	pages = {410--415},
}

@inproceedings{noauthor_notitle_nodate,
	url = {http://nl.ijs.si/janes/wp-content/uploads/2014/09/baron2003.pdf},
}

@book{farghaly_handbook_2003,
	address = {Stanford, Calif},
	series = {{CSLI} lecture notes},
	title = {Handbook for language engineers},
	isbn = {978-1-57586-395-5 978-1-57586-396-2},
	url = {https://web.stanford.edu/group/cslipublications/cslipublications/site/1575863960.shtml},
	number = {no. 164},
	publisher = {CSLI Publications},
	author = {Farghaly, Ali Ahmed Sabry},
	year = {2003},
	keywords = {Computational linguistics, Applied linguistics},
}

@techreport{kardome_how_2022,
	title = {How {Kardome}'s {Advanced} {Speech} {Recognition} {Technology} {Improves} {Voice} {Enabled} {Devices}},
	url = {https://uploads-ssl.webflow.com/5f6ccdcd2e8b3e529677788d/6306b0cf32d2742c5a0e8db7_kardome-wake-word-detection-and-speech-recognition-study-for-consumer-voice-devices_6306ab45.pdf},
	urldate = {2022-10-27},
	author = {Kardome},
	year = {2022},
}

@misc{noauthor_solving_nodate,
	title = {Solving {Automatic} {Speech} {Recognition} {Deployment} {Challenges} {\textbar} {NVIDIA} {Technical} {Blog}},
	url = {https://developer.nvidia.com/blog/solving-automatic-speech-recognition-deployment-challenges/},
	urldate = {2022-10-29},
	file = {Solving Automatic Speech Recognition Deployment Challenges | NVIDIA Technical Blog:C\:\\Users\\DELL\\Zotero\\storage\\4ZD2CTIL\\solving-automatic-speech-recognition-deployment-challenges.html:text/html},
}

@misc{noauthor_global_nodate,
	title = {Global voice recognition market 2026},
	url = {https://www.statista.com/statistics/1133875/global-voice-recognition-market-size/},
	abstract = {The global voice recognition market size was forecast to grow from 10.7 billion U.S.},
	language = {en},
	urldate = {2022-10-29},
	journal = {Statista},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2MV2NQEG\\global-voice-recognition-market-size.html:text/html},
}

@misc{noauthor_speech_nodate,
	title = {Speech {Recognition} {Software} {Market} 2022 by {Size}, {Share}, international business analysis, {Key} firms {Profile} and {Forecast} to 2029 {\textbar} 104 {Pages} {Report}},
	url = {https://www.marketwatch.com/press-release/speech-recognition-software-market-2022-by-size-share-international-business-analysis-key-firms-profile-and-forecast-to-2029-104-pages-report-2022-09-29},
	abstract = {Sep 29, 2022 (The Expresswire) --
[Premium 104 Pages Report] Speech Recognition Software market trend that pertains to the world market for ICT Industry. The...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KKRL736U\\speech-recognition-software-market-2022-by-size-share-international-business-analysis-key-firms.html:text/html},
}

@misc{noauthor_speech_nodate-1,
	title = {Speech and {Voice} {Recognition} {Market} {Size} {And} {Opportunities} for {New} {Players}, {Forecast} from 2022 {To} 2029 with {Top} {Countries} {Data} {\textbar} 127 {Pages} {Report}},
	url = {https://www.marketwatch.com/press-release/speech-and-voice-recognition-market-size-and-opportunities-for-new-players-forecast-from-2022-to-2029-with-top-countries-data-127-pages-report-2022-10-13},
	abstract = {Oct 13, 2022 (The Expresswire) --
According to this latest study, In 2022 the growth of Speech and Voice Recognition Market is projected to reach...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\CPRRIETA\\speech-and-voice-recognition-market-size-and-opportunities-for-new-players-forecast-from-2022-t.html:text/html},
}

@misc{noauthor_speech_nodate-2,
	title = {Speech and {Voice} {Recognition} {Technology} {Market} {Size} 2022, {Demand}, {Share}, {Global} {Trend}, {Business} {Growth}, {Top} {Key} {Players} {Update}, {Business} {Statistics} and {Research} {Methodology} by {Forecast} to 2028},
	url = {https://www.marketwatch.com/press-release/speech-and-voice-recognition-technology-market-size-2022-demand-share-global-trend-business-growth-top-key-players-update-business-statistics-and-research-methodology-by-forecast-to-2028-2022-10-14},
	abstract = {Oct 14, 2022 (The Expresswire) --
"Final Report will add the analysis of the impact of Pre and Post COVID-19 on this Speech and Voice Recognition Technology...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
}

@misc{noauthor_speech_nodate-3,
	title = {Speech {Recognition} {Software} {Market} {Size}, {Share} and {Forecast} till 2028},
	url = {https://www.marketwatch.com/press-release/speech-recognition-software-market-size-share-and-forecast-till-2028-2022-09-16},
	abstract = {Sep 16, 2022 (Reportmines via Comtex) --
Pre and Post Covid is covered and Report Customization is available. It provides a detailed market research report...},
	language = {EN-US},
	urldate = {2022-10-29},
	journal = {MarketWatch},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\GF4FZHDT\\speech-recognition-software-market-size-share-and-forecast-till-2028-2022-09-16.html:text/html},
}

@misc{noauthor_pwc_2018,
	title = {{PwC}: {Lack} of trust in {AI} assistants like {Alexa} could hinder adoption},
	shorttitle = {{PwC}},
	url = {https://venturebeat.com/ai/pwc-lack-of-trust-in-ai-assistants-like-alexa-could-hinder-adoption/},
	abstract = {Register now for your free virtual pass to the Low-Code/No-Code Summit this November 9. Hear from executives from Service Now, Credit Karma, Stitch Fix, Appian, and more. Learn more. Tech leaders like Microsoft CEO Satya Nadella argue that voice and conversational AI represent a new age of computing, but a survey and report released today by […]},
	language = {en-US},
	urldate = {2022-10-29},
	journal = {VentureBeat},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KIY5FN7G\\pwc-lack-of-trust-in-ai-assistants-like-alexa-could-hinder-adoption.html:text/html},
}

@misc{noauthor_rise_2022,
	title = {The {Rise} and {Stall} of the {U}.{S}. {Smart} {Speaker} {Market} - {New} {Report}},
	url = {https://voicebot.ai/2022/03/02/the-rise-and-stall-of-the-u-s-smart-speaker-market-new-report/},
	abstract = {Smart speakers powered by Alexa, Google Assistant, and Siri represented a white-hot consumer device market in the 2016-2019 period. Surging..},
	language = {en-US},
	urldate = {2022-10-29},
	journal = {Voicebot.ai},
	month = mar,
	year = {2022},
	note = {Section: Ai},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\LNTBT9F4\\the-rise-and-stall-of-the-u-s-smart-speaker-market-new-report.html:text/html},
}

@inproceedings{k_e_mats_blomberg_automatisk_1997,
	title = {Automatisk igenk¨anning av tal},
	author = {K. E. Mats Blomberg},
	year = {1997},
}

@phdthesis{lowerre_harpy_1976,
	title = {The {Harpy} speech recognition system},
	url = {https://ui.adsabs.harvard.edu/abs/1976PhDT........81L},
	abstract = {The Harpy connected speech recognition system is the result of an attempt to understand the relative importance of various design choices of two earlier speech recognition systems developed at Carnegie-Mellon University: The Hearsay-1 system and the Dragon system. Knowledge is represented in the Hearsay-1 system as procedures and in the Dragon system as a Markov network with a-priori transition probabilities between states. Systematic performance analysis of various design choices of these two systems resulted in the HARPY system, in which knowledge is represented as a finite state transition network but without the a-priori transition probabilities. Harpy searches only a few 'best' syntactic (and acoustic) paths in parallel to determine the optimal path, and uses segmentation to effectively reduce the utterance length, thereby reducing the number of state probability updates that must be done. Several new heuristics have been added to the HARPY system to improve its performance and speed: detection of common sub-nets and collapsing them to reduce overall network size and complexity, eliminating the need for doing an acoustic match for all phonemic types at every time sample, and semi-automatic techniques for learning the lexical representations (that are needed for a steady-state system of this type) and the phonemic templates from training data, thus automatically accounting for the commonly occurring intra-word coarticulation and juncture phenomena. Inter-word phenomena are handled by the use of juncture rules which are applied at network generation time, thereby eliminating the need for repetitive and time consuming application of phonological rules during the recognition phase.},
	urldate = {2022-10-30},
	author = {Lowerre, B. T.},
	month = apr,
	year = {1976},
	note = {Publication Title: Ph.D. Thesis
ADS Bibcode: 1976PhDT........81L},
	keywords = {Speech Recognition, Communications and Radar, Dictionaries, Grammars, Heuristic Methods, Knowledge, Performance Tests},
}

@misc{noauthor_ibm_2003,
	type = {{TS200}},
	title = {{IBM} {Archives}: {IBM} {Shoebox}},
	copyright = {© Copyright IBM Corp. 2011},
	shorttitle = {{IBM} {Archives}},
	url = {//www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html},
	abstract = {IBM Archives: Exhibits: IBM special products (vol. 1): IBM Shoebox},
	language = {en-US},
	urldate = {2022-10-30},
	month = jan,
	year = {2003},
}

@misc{noauthor_audrey_2021,
	title = {Audrey, {Alexa}, {Hal}, and {More}},
	url = {https://computerhistory.org/blog/audrey-alexa-hal-and-more/},
	abstract = {Star Trek got speech recognition right. How did this science fiction fantasy of only a few decades ago come true? What's the history of speech recognition and where is it headed?},
	language = {en},
	urldate = {2022-10-30},
	journal = {CHM},
	month = jun,
	year = {2021},
	note = {Section: Curatorial Insights},
}

@article{levinson_continuously_1986,
	title = {Continuously variable duration hidden {Markov} models for automatic speech recognition},
	volume = {1},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230886800092},
	doi = {10.1016/S0885-2308(86)80009-2},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {Computer Speech \& Language},
	author = {Levinson, S.E.},
	month = mar,
	year = {1986},
	pages = {29--45},
}

@article{lee_speech_1990,
	title = {Speech recognition using hidden {Markov} models: {A} {CMU} perspective},
	volume = {9},
	issn = {01676393},
	shorttitle = {Speech recognition using hidden {Markov} models},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167639390900255},
	doi = {10.1016/0167-6393(90)90025-5},
	language = {en},
	number = {5-6},
	urldate = {2022-10-30},
	journal = {Speech Communication},
	author = {Lee, Kai-Fu and Hon, Hsiao-Wuen and Hwang, Mei-Yuh and Huang, Xuedong},
	month = dec,
	year = {1990},
	pages = {497--508},
}

@article{baker_dragon_1975,
	title = {The {DRAGON} system--{An} overview},
	volume = {23},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1162650/},
	doi = {10.1109/TASSP.1975.1162650},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Baker, J.},
	month = feb,
	year = {1975},
	pages = {24--29},
}

@misc{kikel_history_2022,
	title = {History of {Voice} {Recognition} {Technology}},
	url = {https://www.totalvoicetech.com/a-brief-history-of-voice-recognition-technology/},
	abstract = {Learn about the history of voice recognition technology. It has come a long way since the 1950's, and will continue to evolve.},
	language = {en-US},
	urldate = {2022-10-30},
	journal = {Total Voice Technologies},
	author = {Kikel, Chris},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Y55S4SD6\\a-brief-history-of-voice-recognition-technology.html:text/html},
}

@article{shneiderman_limits_2000,
	title = {The limits of speech recognition},
	volume = {43},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/348941.348990},
	doi = {10.1145/348941.348990},
	language = {en},
	number = {9},
	urldate = {2022-10-30},
	journal = {Communications of the ACM},
	author = {Shneiderman, Ben},
	month = sep,
	year = {2000},
	pages = {63--65},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC9Z8S9X\\Shneiderman - 2000 - The limits of speech recognition.pdf:application/pdf},
}

@article{humes_speech-recognition_1990,
	title = {Speech-{Recognition} {Difficulties} of the {Hearing}-{Impaired} {Elderly}: {The} {Contributions} of {Audibility}},
	volume = {33},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Speech-{Recognition} {Difficulties} of the {Hearing}-{Impaired} {Elderly}},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3304.726},
	doi = {10.1044/jshr.3304.726},
	abstract = {The role that sensorineural hearing loss plays in the speech-recognition difficulties of the hearing-impaired elderly is examined. One approach to this issue was to make between-group comparisons of performance for three groups of subjects: (a) young normal-hearing adults; (b) elderly hearing-impaired adults; and (c) young normal-hearing adults with simulated sensorineural hearing loss equivalent to that of the elderly subjects produced by a spectrally shaped masking noise. Another approach to this issue employed correlational analyses to examine the relation between audibility and speech recognition within the group of elderly hearing-impaired subjects. An additional approach was pursued in which an acoustical index incorporating adjustments for threshold elevation was used to examine the role audibility played in the speech-recognition performance of the hearing-impaired elderly. A wide range of listening conditions was sampled in this experiment. The conclusion was that the primary determiner of speech-recognition performance in the elderly hearing-impaired subjects was their threshold elevation.},
	language = {en},
	number = {4},
	urldate = {2022-10-30},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Humes, Larry E. and Roberts, Lisa},
	month = dec,
	year = {1990},
	pages = {726--735},
}

@techreport{meyer_bernd_whats_2013,
	title = {What's the difference? {Comparing} humans and machines on the {Aurora} 2 speech recognition task},
	url = {https://www.researchgate.net/publication/290246535_What's_the_difference_Comparing_humans_and_machines_on_the_Aurora_2_speech_recognition_task/references},
	abstract = {The comparison of human speech recognition (HSR) and machine performance allows to learn from the differences between HSR and automatic speech recognition (ASR) and serves as motivation for using auditory-inspired strategies in ASR. The recognition of noisy digit strings from the Aurora 2 framework is one of the most widely used tasks in the ASR community. This paper establishes a baseline with a close-to-optimal classifier, i.e., our auditory system by comparing results from 10 normal-hearing listeners to the Aurora 2 reference system using identical speech material. The baseline ASR system reaches the human performance level only when the signal-to-noise ratio is increased by 10 or 21 dB depending on the training condition. The recognition of 1-digit recordings was found to be considerably better for HSR, indicating that onset detection is an important feature neglected in standard ASR systems. Results of recent studies are considered in the light of these findings to measure how far we have come on the way to human speech recognition performance.},
	institution = {Medical Physics, Carl-von-Ossietzky Universit¨at Oldenburg, Germany},
	author = {Meyer, Bernd},
	month = jan,
	year = {2013},
	pages = {2634--2638},
}

@article{lippmann_speech_1997,
	title = {Speech recognition by machines and humans},
	volume = {22},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639397000216},
	doi = {10.1016/S0167-6393(97)00021-6},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {Speech Communication},
	author = {Lippmann, Richard P.},
	month = jul,
	year = {1997},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\D25IW8PV\\Lippmann - 1997 - Speech recognition by machines and humans.pdf:application/pdf},
}

@article{meyer_effect_2011,
	title = {Effect of speech-intrinsic variations on human and automatic recognition of spoken phonemes},
	volume = {129},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3514525},
	doi = {10.1121/1.3514525},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Meyer, Bernd T. and Brand, Thomas and Kollmeier, Birger},
	month = jan,
	year = {2011},
	pages = {388--403},
}

@inproceedings{vasilescu_ioana__and_______adda-decker_martine__and_______lamel_lori_cross-lingual_2012,
	address = {Istanbul, Turkey},
	title = {Cross-lingual studies of {ASR} errors: paradigms for perceptual evaluations},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/300_Paper.pdf},
	abstract = {It is well-known that human listeners significantly outperform machines when it comes to transcribing speech. This paper presents a progress report of the joint research in the automatic vs human speech transcription and of the perceptual experiments developed at LIMSI that aims to increase our understanding of automatic speech recognition errors. Two paradigms are described here in which human listeners are asked to transcribe speech segments containing words that are frequently misrecognized by the system. In particular, we sought to gain information about the impact of increased context to help humans disambiguate problematic lexical items, typically homophone or near-homophone words. The long-term aim of this research is to improve the modeling of ambiguous contexts so as to reduce automatic transcription errors."},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} (\{{LREC}\}'12)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Vasilescu, Ioana  {and}       Adda-Decker, Martine  {and}       Lamel, Lori},
	month = may,
	year = {2012},
	pages = {3511--3518},
}

@book{gold_speech_2011,
	address = {Hoboken, N.J},
	edition = {2nd ed},
	title = {Speech and audio signal processing: processing and perception of speech and music},
	isbn = {978-0-470-19536-9},
	shorttitle = {Speech and audio signal processing},
	publisher = {Wiley},
	author = {Gold, Bernard and Morgan, Nelson and Ellis, Dan},
	year = {2011},
	keywords = {Digital techniques, Electronic music, Signal processing, Speech processing systems},
}

@inproceedings{xu_semantic_2010,
	address = {Berkeley, CA, USA},
	title = {Semantic understanding by combining extended {CFG} parser with {HMM} model},
	isbn = {978-1-4244-7904-7},
	url = {http://ieeexplore.ieee.org/document/5700824/},
	doi = {10.1109/SLT.2010.5700824},
	urldate = {2022-10-30},
	booktitle = {2010 {IEEE} {Spoken} {Language} {Technology} {Workshop}},
	publisher = {IEEE},
	author = {Xu, Yushi and Seneff, Stephanie and Li, Alice and Polifroni, Joe},
	month = dec,
	year = {2010},
	pages = {67--72},
}

@inproceedings{ye-yi_wang_is_2003,
	address = {St Thomas, VI, USA},
	title = {Is word error rate a good indicator for spoken language understanding accuracy},
	isbn = {978-0-7803-7980-0},
	url = {http://ieeexplore.ieee.org/document/1318504/},
	doi = {10.1109/ASRU.2003.1318504},
	urldate = {2022-10-30},
	booktitle = {2003 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({IEEE} {Cat}. {No}.{03EX721})},
	publisher = {IEEE},
	author = {{Ye-Yi Wang} and Acero, A. and Chelba, C.},
	year = {2003},
	pages = {577--582},
}

@inproceedings{riccardi_stochastic_1998,
	title = {Stochastic language models for speech recognition and understanding},
	url = {https://www.isca-speech.org/archive/icslp_1998/riccardi98b_icslp.html},
	doi = {10.21437/ICSLP.1998-502},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {5th {International} {Conference} on {Spoken} {Language} {Processing} ({ICSLP} 1998)},
	publisher = {ISCA},
	author = {Riccardi, Giuseppe and Gorin, Allen L.},
	month = nov,
	year = {1998},
	pages = {paper 0111--0},
}

@inproceedings{esteve_conceptual_2003,
	title = {Conceptual decoding for spoken dialog systems},
	url = {https://www.isca-speech.org/archive/eurospeech_2003/esteve03_eurospeech.html},
	doi = {10.21437/Eurospeech.2003-260},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {8th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 2003)},
	publisher = {ISCA},
	author = {Esteve, Yannick and Raymond, Christian and Bechet, Frédéric and Mori, Renato De},
	month = sep,
	year = {2003},
	pages = {617--620},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\KSIVHEB4\\Esteve et al. - 2003 - Conceptual decoding for spoken dialog systems.pdf:application/pdf},
}

@phdthesis{muhammad_danyal_khan_automatic_2022,
	address = {Karachi, Pakistan},
	title = {Automatic {Speech} {Recognition} to {Detect} {Malicious}/{Mal}-intended {Speech}},
	url = {https://www.researchgate.net/publication/365349074_Automatic_Speech_Recognition_to_Detect_MaliciousMal-intended_Speech},
	abstract = {Speech Recognition is a growing field since last five decades and there have been many advancements which has led to its applications like Speech to Text. This has allowed a possibility of Transcription of audio files to text and much of work is available on this in English, Arabic and Cantonese Languages.  
 
However, Urdu is a low-resource language in field of ASR although it is the world's \$11{\textasciicircum}\{th\}\$ most widely spoken language, with 232 Million speakers worldwide. There are no applicable models available which can be readily deployed for Speech To Text in a noisy scenario which is why Urdu Community is devoid of all the benefits of ASR. 
 
Apart from noise problems in normal telephonic or call-center conversations in Urdu, people tend to spontaneously use words from other language since Pakistan is a multi-cultural society, which presents a code-switching problem. 
 
Hence, we proposed an implementation of Automatic Speech Recognition/ Speech to Text System in a noisy/ call center environment with less labelled training data available using Hybrid HMM-DNN in a Resource constraint environment in terms of time, budget, computation power, HR etc.
 
We were able to access to call center audio files, thanks to CPLC {\textbackslash}cite\{cplc\_cplc\_nodate\} (a semi-government Law Enforcement Agency), some of which was labelled manually. We further integrated various open source data-sets to include more variety in data-set. The data comprised of mix of noisy and clean audio as well as single utterances and long sentences (1-20 second audios). It was split into 6.5 hours and 3.5 hours of train and test data-set respectively. 
 
The Language Model was developed from the training data-set and for acoustic modelling we used HMM (Monophone and Triphone) based on which we trained a Neural Network based model using Chain CNN-TDNN, achieving up to 5.2{\textbackslash}\% WER with noisy and clean data-set as well as on single word to spontaneous speech data as well.


    {\textbackslash}textbf\{{\textbackslash}{\textbackslash} Keywords:\} {\textbackslash}textit\{{\textbackslash}{\textbackslash} Speech Recognition, ASR, Call Center, audio transcription, Urdu language, Code-switched Urdu ASR, Speech to Text, AI, Cyber Security, ASR for Resource Constrained Environment, ASR for Noisy Environment, ASR for Low Resource Languages, Under Resourced Languages\}},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Muhammad Danyal Khan},
	month = oct,
	year = {2022},
	doi = {10.13140/RG.2.2.34319.05281},
}

@inproceedings{hercigonja_comparative_2017,
	title = {Comparative {Analysis} of {Cryptographic} {Algorithms}},
	author = {Hercigonja, Zoran},
	year = {2017},
}

@article{abood_survey_2018,
	title = {A {Survey} on {Cryptography} {Algorithms}},
	volume = {8},
	doi = {10.29322/IJSRP.8.7.2018.p7978},
	journal = {International Journal of Scientific and Research Publications},
	author = {Abood, Omar and Guirguis, Shawkat},
	month = jul,
	year = {2018},
	pages = {495--516},
}

@article{coretti_constructive_2013,
	title = {A {Constructive} {Perspective} on {Key} {Encapsulation}},
	volume = {8260},
	doi = {10.1007/978-3-642-42001-6_16},
	author = {Coretti, Sandro and Maurer, Ueli and Tackmann, Björn},
	month = jan,
	year = {2013},
	note = {ISBN: 978-3-642-42000-9},
}

@inproceedings{shor_algorithms_1994,
	title = {Algorithms for quantum computation: discrete logarithms and factoring},
	doi = {10.1109/SFCS.1994.365700},
	booktitle = {Proceedings 35th {Annual} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Shor, P.W.},
	year = {1994},
	pages = {124--134},
}

@inproceedings{grover_fast_1996,
	title = {A fast quantum mechanical algorithm for database search},
	booktitle = {{STOC} '96},
	author = {Grover, Lov K.},
	year = {1996},
}

@article{harrow_quantum_2009,
	title = {Quantum {Algorithm} for {Linear} {Systems} of {Equations}},
	volume = {103},
	doi = {10.1103/PhysRevLett.103.150502},
	journal = {Physical review letters},
	author = {Harrow, Aram and Hassidim, Avinatan and Lloyd, Seth},
	month = oct,
	year = {2009},
	pages = {150502},
}

@article{micciancio_hardness_2001,
	title = {The hardness of the closest vector problem with preprocessing},
	volume = {47},
	journal = {IEEE Transactions on Information Theory},
	author = {Micciancio, Daniele},
	year = {2001},
	pages = {2001},
}

@inproceedings{ajtai_generating_1996,
	title = {Generating {Hard} {Instances} of {Lattice} {Problems} ({Extended} {Abstract})},
	booktitle = {In {Proceedings} of the {Twenty}-{Eighth} {Annual} {ACM} {Symposium} on the {Theory} of {Computing}},
	publisher = {ACM},
	author = {Ajtai, M.},
	year = {1996},
	pages = {99--108},
}

@article{regev_learning_2010,
	title = {The {Learning} with {Errors} {Problem}},
	author = {Regev, Oded},
	month = jan,
	year = {2010},
}

@misc{lyubashevsky_o_2010,
	title = {O.: {On} ideal lattices and learning with errors over rings},
	author = {Lyubashevsky, Vadim and Peikert, Chris and Regev, Oded},
	year = {2010},
}

@misc{langlois_worst-case_2012,
	title = {Worst-{Case} to {Average}-{Case} {Reductions} for {Module} {Lattices}},
	url = {https://eprint.iacr.org/2012/090},
	author = {Langlois, Adeline and Stehle, Damien},
	year = {2012},
	note = {Published: Cryptology ePrint Archive, Paper 2012/090},
	annote = {https://eprint.iacr.org/2012/090},
}

@misc{boudgoust_hardness_2022,
	title = {On the {Hardness} of {Module} {Learning} {With} {Errors} with {Short} {Distributions}},
	url = {https://eprint.iacr.org/2022/472},
	author = {Boudgoust, Katharina and Jeudy, Corentin and Roux-Langlois, Adeline and Wen, Weiqiang},
	year = {2022},
	note = {Published: Cryptology ePrint Archive, Paper 2022/472},
	annote = {https://eprint.iacr.org/2022/472},
}

@misc{bos_crystals_2017,
	title = {{CRYSTALS} – {Kyber}: a {CCA}-secure module-lattice-based {KEM}},
	url = {https://eprint.iacr.org/2017/634},
	author = {Bos, Joppe and Ducas, Léo and Kiltz, Eike and Lepoint, Tancrède and Lyubashevsky, Vadim and Schanck, John M. and Schwabe, Peter and Seiler, Gregor and Stehlé, Damien},
	year = {2017},
	doi = {10.1109/EuroSP.2018.00032},
	note = {Published: Cryptology ePrint Archive, Paper 2017/634},
	annote = {https://eprint.iacr.org/2017/634},
}

@article{schonhage_schnelle_2005,
	title = {Schnelle {Multiplikation} großer {Zahlen}},
	volume = {7},
	journal = {Computing},
	author = {Schönhage, Arnold and Strassen, Volker},
	year = {2005},
	pages = {281--292},
}

@inproceedings{avanzi_crystals-kyber_2019,
	title = {{CRYSTALS}-{Kyber} {Algorithm} {Specification} {And} {Supporting} {Documentation} ( version 2 . 0 )},
	author = {Avanzi, Roberto Maria and Bos, Joppe W. and Ducas, Léo and Kiltz, Eike and Lepoint, Tancrède and Lyubashevsky, Vadim and Schanck, John M. and Schwabe, Peter and Seiler, Gregor and Stehlé, Damien},
	year = {2019},
}

@article{lyubashevsky_nttru_2019,
	title = {{NTTRU}: {Truly} {Fast} {NTRU} {Using} {NTT}},
	volume = {2019},
	journal = {IACR Cryptol. ePrint Arch.},
	author = {Lyubashevsky, Vadim and Seiler, Gregor},
	year = {2019},
	pages = {40},
}

@article{cooley_algorithm_1965,
	title = {An algorithm for the machine calculation of complex {Fourier} series},
	volume = {19},
	journal = {Mathematics of Computation},
	author = {Cooley, James W. and Tukey, John W.},
	year = {1965},
	pages = {297--301},
}

@article{montgomery_modular_1985,
	title = {Modular multiplication without trial division},
	volume = {44},
	journal = {Mathematics of Computation},
	author = {Montgomery, Peter L.},
	year = {1985},
	pages = {519--521},
}

@inproceedings{barrett_implementing_1986,
	title = {Implementing the {Rivest} {Shamir} and {Adleman} {Public} {Key} {Encryption} {Algorithm} on a {Standard} {Digital} {Signal} {Processor}},
	booktitle = {{CRYPTO}},
	author = {Barrett, Paul},
	year = {1986},
}

@inproceedings{gentleman_fast_1966,
	title = {Fast {Fourier} {Transforms}: for fun and profit},
	booktitle = {{AFIPS} '66 ({Fall})},
	author = {Gentleman, W. Morven and Sande, Gordon},
	year = {1966},
}

@phdthesis{kannwischer_polynomial_2022,
	type = {{PhD} {Thesis}},
	title = {Polynomial {Multiplication} for {Post}-{Quantum} {Cryptography}},
	url = {https://kannwischer.eu/thesis/},
	school = {Radboud University, Max Planck Institute for Security and Privacy, and Academia Sinica},
	author = {Kannwischer, Matthias J.},
	year = {2022},
}

@misc{roy_compact_2013,
	title = {Compact {Ring}-{LWE} based {Cryptoprocessor}},
	url = {https://eprint.iacr.org/2013/866},
	author = {Roy, Sujoy Sinha and Vercauteren, Frederik and Mentens, Nele and Chen, Donald Donglong and Verbauwhede, Ingrid},
	year = {2013},
	note = {Published: Cryptology ePrint Archive, Paper 2013/866},
	annote = {https://eprint.iacr.org/2013/866},
}

@misc{poppelmann_high-performance_2015,
	title = {High-{Performance} {Ideal} {Lattice}-{Based} {Cryptography} on 8-bit {ATxmega} {Microcontrollers}},
	url = {https://eprint.iacr.org/2015/382},
	author = {Pöppelmann, Thomas and Oder, Tobias and Güneysu, Tim},
	year = {2015},
	note = {Published: Cryptology ePrint Archive, Paper 2015/382},
	annote = {https://eprint.iacr.org/2015/382},
}

@inproceedings{chu_inside_2019,
	title = {Inside the {FFT} {Black} {Box}: {Serial} and {Parallel} {Fast} {Fourier} {Transform} {Algorithms}},
	author = {Chu, Eleanor and George, Alan},
	year = {2019},
}

@misc{klemsa_fast_2021,
	title = {Fast and {Error}-{Free} {Negacyclic} {Integer} {Convolution} using {Extended} {Fourier} {Transform}},
	url = {https://eprint.iacr.org/2021/480},
	author = {Klemsa, Jakub},
	year = {2021},
	note = {Published: Cryptology ePrint Archive, Paper 2021/480},
	annote = {https://eprint.iacr.org/2021/480},
}

@book{noauthor_arm_2001,
	edition = {updated},
	title = {{ARM} {Developer} {Suite} ® {Version} 1.2 {Assembler} {Guide}},
	shorttitle = {www.arm.com},
	url = {https://developer.arm.com/documentation/dui0068/latest/},
	language = {English},
	month = nov,
	year = {2001},
}

@misc{sprenkels_kyberdilithium_2020,
	title = {The {Kyber}/{Dilithium} {NTT}},
	url = {https://dsprenkels.com/ntt.html},
	author = {Sprenkels, Daan},
	month = sep,
	year = {2020},
	note = {Publication Title: Blog Post
Type: Blog Post},
}

@misc{seiler_faster_2018,
	title = {Faster {AVX2} optimized {NTT} multiplication for {Ring}-{LWE} lattice cryptography},
	url = {https://eprint.iacr.org/2018/039},
	author = {Seiler, Gregor},
	year = {2018},
	note = {Published: Cryptology ePrint Archive, Paper 2018/039},
	annote = {https://eprint.iacr.org/2018/039},
}

@misc{noauthor_round_nodate,
	title = {Round 2 {Submissions} - {Post}-{Quantum} {Cryptography} {\textbar} {CSRC} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/Projects/post-quantum-cryptography/post-quantum-cryptography-standardization/round-2-submissions},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_round_nodate-1,
	title = {Round 1 {Submissions} - {Post}-{Quantum} {Cryptography} {\textbar} {CSRC} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/Projects/post-quantum-cryptography/post-quantum-cryptography-standardization/Round-1-Submissions},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_round_nodate-2,
	title = {Round 3 {Submissions} - {Post}-{Quantum} {Cryptography} {\textbar} {CSRC} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/Projects/post-quantum-cryptography/post-quantum-cryptography-standardization/round-3-submissions},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_announcing_nodate,
	title = {Announcing {PQC} {Candidates} to be {Standardized}, {Plus} {Fourth} {Round} {Candidates} {\textbar} {CSRC} — csrc.nist.gov},
	url = {https://csrc.nist.gov/News/2022/pqc-candidates-to-be-standardized-and-round-4},
	annote = {[Accessed 24-Sep-2022]},
}

@misc{noauthor_ibm_nodate,
	title = {{IBM} {Unveils} {New} {Roadmap} to {Practical} {Quantum} {Computing} {Era}; {Plans} to {Deliver} 4,000+ {Qubit} {System} — newsroom.ibm.com},
	url = {https://newsroom.ibm.com/2022-05-10-IBM-Unveils-New-Roadmap-to-Practical-Quantum-Computing-Era-Plans-to-Deliver-4,000-Qubit-System},
	annote = {[Accessed 24-Sep-2022]},
}

@inproceedings{padamvathi_quantum_2016,
	title = {Quantum {Cryptography} and {Quantum} {Key} {Distribution} {Protocols}: {A} {Survey}},
	doi = {10.1109/IACC.2016.109},
	booktitle = {2016 {IEEE} 6th {International} {Conference} on {Advanced} {Computing} ({IACC})},
	author = {Padamvathi, V. and Vardhan, B. Vishnu and Krishna, A.V.N.},
	year = {2016},
	pages = {556--562},
}

@book{noauthor_submission_2016,
	title = {Submission {Requirements} and {Evaluation} {Criteria} for the {Post}-{Quantum} {Cryptography} {Standardization} {Process}},
	url = {https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf},
	publisher = {National Institute of Standards and Technology},
	year = {2016},
}

@inproceedings{cheng_efficient_2005,
	title = {Efficient {FPGA} implementation of {FFT} based multipliers},
	doi = {10.1109/CCECE.2005.1557215},
	booktitle = {Canadian {Conference} on {Electrical} and {Computer} {Engineering}, 2005.},
	author = {Cheng, Lo Sing and Miri, A. and Yeap, Tet Hin},
	year = {2005},
	pages = {1300--1303},
}

@inproceedings{emeliyanenko_efficient_2009,
	address = {Berlin, Heidelberg},
	title = {Efficient {Multiplication} of {Polynomials} on {Graphics} {Hardware}},
	isbn = {978-3-642-03644-6},
	abstract = {We present the algorithm to multiply univariate polynomials with integer coefficients efficiently using the Number Theoretic transform (NTT) on Graphics Processing Units (GPU). The same approach can be used to multiply large integers encoded as polynomials. Our algorithm exploits fused multiply-add capabilities of the graphics hardware. NTT multiplications are executed in parallel for a set of distinct primes followed by reconstruction using the Chinese Remainder theorem (CRT) on the GPU. Our benchmarking experiences show the NTT multiplication performance up to 77 GMul/s. We compared our approach with CPU-based implementations of polynomial and large integer multiplication provided by NTL and GMP libraries.},
	booktitle = {Advanced {Parallel} {Processing} {Technologies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Emeliyanenko, Pavel},
	editor = {Dou, Yong and Gruber, Ralf and Joller, Josef M.},
	year = {2009},
	pages = {134--149},
}

@inproceedings{renteria-mejia_hardware_2014,
	title = {Hardware {Design} of an n-coefficient {NTT}-{Based} {Polynomial} {Multiplier}},
	doi = {10.1109/SPL.2014.7002209},
	booktitle = {2014 {IX} {Southern} {Conference} on {Programmable} {Logic} ({SPL})},
	author = {Rentería-Mejía, C. P. and Velasco-Medina, J.},
	year = {2014},
	pages = {1--5},
}

@inproceedings{ghosh_speed_2012,
	title = {A {Speed} {Area} {Optimized} {Embedded} {Co}-processor for {McEliece} {Cryptosystem}},
	doi = {10.1109/ASAP.2012.16},
	booktitle = {2012 {IEEE} 23rd {International} {Conference} on {Application}-{Specific} {Systems}, {Architectures} and {Processors}},
	author = {Ghosh, Santosh and Delvaux, Jeroen and Uhsadel, Leif and Verbauwhede, Ingrid},
	year = {2012},
	pages = {102--108},
}

@inproceedings{nguyen_high-level_2019,
	title = {A {High}-{Level} {Synthesis} {Approach} to the {Software}/{Hardware} {Codesign} of {NTT}-{Based} {Post}-{Quantum} {Cryptography} {Algorithms}},
	doi = {10.1109/ICFPT47387.2019.00070},
	booktitle = {2019 {International} {Conference} on {Field}-{Programmable} {Technology} ({ICFPT})},
	author = {Nguyen, Duc Tri and Dang, Viet B. and Gaj, Kris},
	year = {2019},
	pages = {371--374},
}

@inproceedings{farahmand_softwarehardware_2019,
	title = {Software/{Hardware} {Codesign} of the {Post} {Quantum} {Cryptography} {Algorithm} {NTRUEncrypt} {Using} {High}-{Level} {Synthesis} and {Register}-{Transfer} {Level} {Design} {Methodologies}},
	doi = {10.1109/FPL.2019.00042},
	booktitle = {2019 29th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Farahmand, Farnoud and Nguyen, Duc Tri and Dang, Viet B. and Ferozpuri, Ahmed and Gaj, Kris},
	year = {2019},
	pages = {225--231},
}

@inproceedings{alkim_newhope_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NewHope} on {ARM} {Cortex}-{M}},
	volume = {10076},
	booktitle = {Security, {Privacy}, and {Advanced} {Cryptography} {Engineering}},
	publisher = {Springer-Verlag Berlin Heidelberg},
	author = {Alkim, Erdem and Jakubeit, Philipp and Schwabe, Peter},
	editor = {Carlet, Claude and Hasan, Anwar and Saraswat, Vishal},
	year = {2016},
	pages = {332--349},
	annote = {Document ID: c7a82d41d39c535fd09ca1b032ebca1b, http://cryptojedi.org/papers/\#newhopearm},
}

@misc{bisheh-niasar_high-speed_2021,
	title = {High-{Speed} {NTT}-based {Polynomial} {Multiplication} {Accelerator} for {CRYSTALS}-{Kyber} {Post}-{Quantum} {Cryptography}},
	url = {https://eprint.iacr.org/2021/563},
	author = {Bisheh-Niasar, Mojtaba and Azarderakhsh, Reza and Mozaffari-Kermani, Mehran},
	year = {2021},
	note = {Published: Cryptology ePrint Archive, Paper 2021/563},
	annote = {https://eprint.iacr.org/2021/563},
}

@inproceedings{du_towards_2016,
	title = {Towards efficient polynomial multiplication for lattice-based cryptography},
	doi = {10.1109/ISCAS.2016.7527456},
	booktitle = {2016 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Du, Chaohui and Bai, Guoqiang},
	year = {2016},
	pages = {1178--1181},
}

@article{chung_ntt_2021,
	title = {{NTT} {Multiplication} for {NTT}-unfriendly {Rings}: {New} {Speed} {Records} for {Saber} and {NTRU} on {Cortex}-{M4} and {AVX2}},
	volume = {2021},
	url = {https://tches.iacr.org/index.php/TCHES/article/view/8791},
	doi = {10.46586/tches.v2021.i2.159-188},
	number = {2},
	journal = {IACR Transactions on Cryptographic Hardware and Embedded Systems},
	author = {Chung, Chi-Ming Marvin and Hwang, Vincent and Kannwischer, Matthias J. and Seiler, Gregor and Shih, Cheng-Jhih and Yang, Bo-Yin},
	month = feb,
	year = {2021},
	pages = {159--188},
}

@inproceedings{poppelmann_towards_2012,
	address = {Berlin, Heidelberg},
	title = {Towards {Efficient} {Arithmetic} for {Lattice}-{Based} {Cryptography} on {Reconfigurable} {Hardware}},
	isbn = {978-3-642-33481-8},
	abstract = {In recent years lattice-based cryptography has emerged as quantum secure and theoretically elegant alternative to classical cryptographic schemes (like ECC or RSA). In addition to that, lattices are a versatile tool and play an important role in the development of efficient fully or somewhat homomorphic encryption (SHE/FHE) schemes. In practice, ideal lattices defined in the polynomial ring ℤp[x]/〈xnþinspace+þinspace1〉 allow the reduction of the generally very large key sizes of lattice constructions. Another advantage of ideal lattices is that polynomial multiplication is a basic operation that has, in theory, only quasi-linear time complexity of \$\{{\textbackslash}backslashmathcal O\}(n {\textbackslash}backslashlog\{n\})\$in ℤp[x]/〈xnþinspace+þinspace1〉. However, few is known about the practical performance of the FFT in this specific application domain and whether it is really an alternative. In this work we make a first step towards efficient FFT-based arithmetic for lattice-based cryptography and show that the FFT can be implemented efficiently on reconfigurable hardware. We give instantiations of recently proposed parameter sets for homomorphic and public-key encryption. In a generic setting we are able to multiply polynomials with up to 4096 coefficients and a 17-bit prime in less than 0.5 milliseconds. For a parameter set of a SHE scheme (n=1024,p=1061093377) our implementation performs 9063 polynomial multiplications per second on a mid-range Spartan-6.},
	booktitle = {Progress in {Cryptology} – {LATINCRYPT} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Pöppelmann, Thomas and Güneysu, Tim},
	editor = {Hevia, Alejandro and Neven, Gregory},
	year = {2012},
	pages = {139--158},
}

@inproceedings{el-kady_high-level_2021,
	title = {High-{Level} {Synthesis} design approach for {Number}-{Theoretic} {Transform} {Implementations}},
	doi = {10.1109/VLSI-SoC53125.2021.9607003},
	booktitle = {2021 {IFIP}/{IEEE} 29th {International} {Conference} on {Very} {Large} {Scale} {Integration} ({VLSI}-{SoC})},
	author = {El-Kady, Alexander and Fournaris, Apostolos P. and Tsakoulis, Thanasis and Haleplidis, Evangelos and Paliouras, Vassilis},
	year = {2021},
	pages = {1--6},
}


@article{nannipieri_risc-v_2021,
	title = {A {RISC}-{V} {Post} {Quantum} {Cryptography} {Instruction} {Set} {Extension} for {Number} {Theoretic} {Transform} to {Speed}-{Up} {CRYSTALS} {Algorithms}},
	volume = {9},
	doi = {10.1109/ACCESS.2021.3126208},
	journal = {IEEE Access},
	author = {Nannipieri, Pietro and Di Matteo, Stefano and Zulberti, Luca and Albicocchi, Francesco and Saponara, Sergio and Fanucci, Luca},
	year = {2021},
	pages = {150798--150808},
}

@inproceedings{karabulut_rantt_2020,
	title = {{RANTT}: {A} {RISC}-{V} {Architecture} {Extension} for the {Number} {Theoretic} {Transform}},
	doi = {10.1109/FPL50879.2020.00016},
	booktitle = {2020 30th {International} {Conference} on {Field}-{Programmable} {Logic} and {Applications} ({FPL})},
	author = {Karabulut, Emre and Aysu, Aydin},
	year = {2020},
	pages = {26--32},
}

@misc{kannwischer_pqm4_nodate,
	title = {{PQM4}: {Post}-quantum crypto library for the {ARM} {Cortex}-{M4}},
	author = {Kannwischer, Matthias J. and Petri, Richard and Rijneveld, Joost and Schwabe, Peter and Stoffelen, Ko},
	annote = {https://github.com/mupq/pqm4},
}

@misc{alkim_newhope_2016-1,
	title = {{NewHope} on {ARM} {Cortex}-{M}},
	url = {https://eprint.iacr.org/2016/758},
	author = {Alkim, Erdem and Jakubeit, Philipp and Schwabe, Peter},
	year = {2016},
	note = {Published: Cryptology ePrint Archive, Paper 2016/758},
	annote = {https://eprint.iacr.org/2016/758},
}

@misc{botros_memory-efficient_2019,
	title = {Memory-{Efficient} {High}-{Speed} {Implementation} of {Kyber} on {Cortex}-{M4}},
	url = {https://eprint.iacr.org/2019/489},
	author = {Botros, Leon and Kannwischer, Matthias J. and Schwabe, Peter},
	year = {2019},
	note = {Published: Cryptology ePrint Archive, Paper 2019/489},
	annote = {https://eprint.iacr.org/2019/489},
}

@misc{kannwischer_pqm4_nodate-1,
	title = {{PQM4}: {Post}-quantum crypto library for the {ARM} {Cortex}-{M4}},
	author = {Kannwischer, Matthias J. and Petri, Richard and Rijneveld, Joost and Schwabe, Peter and Stoffelen, Ko},
	annote = {https://github.com/mupq/pqm4},
}

@misc{kannwischer_pqclean_nodate,
	title = {{PQClean}: {Clean}, portable, tested implementations of post-quantum cryptography},
	abstract = {Clean, portable, tested implementations of post-quantum cryptography - GitHub - PQClean/PQClean: Clean, portable tested implementations of post-quantum cryptography},
	author = {Kannwischer, Matthias J. and Rijneveld, Joost and Schwabe, Peter and Stebila, Douglas and Wiggers, Thom},
	annote = {https://github.com/PQClean/PQClean},
}

@article{venkatagiri_speech_2002-1,
	title = {Speech {Recognition} {Technology} {Applications} in {Communication} {Disorders}},
	volume = {11},
	issn = {1058-0360, 1558-9110},
	url = {http://pubs.asha.org/doi/10.1044/1058-0360%282002/037%29},
	doi = {10.1044/1058-0360(2002/037)},
	abstract = {Speech recognition (SR) is the process whereby a microprocessor-based system, typically a computer with sound processing hardware and speech recognition software, responds in predictable ways to spoken commands and/or converts speech into text. This tutorial describes the types and the general uses of SR and provides an explanation of the technology behind it. The emerging applications of SR technology for dictation, articulation training, language and literacy development, environmental control, and communication augmentation are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {American Journal of Speech-Language Pathology},
	author = {Venkatagiri, H. S.},
	month = nov,
	year = {2002},
	pages = {323--332},
}

@article{wang_overview_2019,
	title = {Overview of end-to-end speech recognition},
	volume = {1187},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1187/5/052068},
	doi = {10.1088/1742-6596/1187/5/052068},
	number = {5},
	urldate = {2022-11-05},
	journal = {Journal of Physics: Conference Series},
	author = {Wang, Song and Li, Guanyu},
	month = apr,
	year = {2019},
	pages = {052068},
}

@techreport{timothy_r_anderson_applications_1998,
	address = {Bre’tigny, France},
	title = {{APPLICATIONS} {OF} {SPEECH}-{BASED} {CONTROL}},
	author = {Timothy R. Anderson},
	month = oct,
	year = {1998},
	note = {Ohio, USA, 14-15 October 1998
Published in RTO EN-3},
}

@article{haton_problems_1994,
	title = {Problems and solutions for noisy speech recognition},
	volume = {04},
	issn = {1155-4339},
	url = {http://www.edpsciences.org/10.1051/jp4:1994592},
	doi = {10.1051/jp4:1994592},
	number = {C5},
	urldate = {2022-11-05},
	journal = {Le Journal de Physique IV},
	author = {Haton, J.-P.},
	month = may,
	year = {1994},
	pages = {C5--439--C5--448},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5UM3AEJV\\Haton - 1994 - Problems and solutions for noisy speech recognitio.pdf:application/pdf},
}

@inproceedings{garcia_lecumberri_ml_cooke_m_cutugno_f_giurgiu_m_meyer_bt_scharenborg_o_van_dommelen_w_volin_j_non-native_2008,
	address = {Brisbane, Australia},
	title = {The non-native consonant challenge for {European} languages},
	booktitle = {Proceedings of {Interspeech}},
	author = {García Lecumberri, M.L., Cooke, M., Cutugno, F., Giurgiu, M., Meyer, B.T., Scharenborg, O., van Dommelen, W., Volin, J},
	year = {2008},
	pages = {1781--1784},
}

@article{boothroyd_statistical_1968,
	title = {Statistical {Theory} of the {Speech} {Discrimination} {Score}},
	volume = {43},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1910787},
	doi = {10.1121/1.1910787},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Boothroyd, A.},
	month = feb,
	year = {1968},
	pages = {362--367},
}

@techreport{l_lamel_quaero_2010,
	address = {Limsi Orsay},
	title = {Quaero program - ctc project - progress report on task 5.1: {Speech} to text},
	url = {https://www.linkedin.com/company/quaero/},
	urldate = {2022-11-05},
	author = {L. Lamel},
	year = {2010},
	note = {http://www.quaero.org/developpement-scientifique-onglet-synthese/
Cd.ctc.5.6., Quaero Program, .},
}

@article{lippmann_speech_1997-1,
	title = {Speech recognition by machines and humans},
	volume = {22},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639397000216},
	doi = {10.1016/S0167-6393(97)00021-6},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Lippmann, Richard P.},
	month = jul,
	year = {1997},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4VM3MB94\\Lippmann - 1997 - Speech recognition by machines and humans.pdf:application/pdf},
}

@inproceedings{lawrence_r_rabiner_and_sorin_dusan_can_2005,
	title = {{CAN} {AUTOMATIC} {SPEECH} {RECOGNITION} {LEARN} {MORE} {FROM} {HUMAN} {SPEECH} {PERCEPTION}},
	author = {Lawrence R. Rabiner {and} Sorin Dusan},
	year = {2005},
}

@article{benzeghiba_automatic_2007,
	title = {Automatic speech recognition and speech variability: {A} review},
	volume = {49},
	issn = {01676393},
	shorttitle = {Automatic speech recognition and speech variability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000404},
	doi = {10.1016/j.specom.2007.02.006},
	language = {en},
	number = {10-11},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Benzeghiba, M. and De Mori, R. and Deroo, O. and Dupont, S. and Erbes, T. and Jouvet, D. and Fissore, L. and Laface, P. and Mertins, A. and Ris, C. and Rose, R. and Tyagi, V. and Wellekens, C.},
	month = oct,
	year = {2007},
	pages = {763--786},
}

@inproceedings{rk_moore_comparison_2003,
	address = {Gen`eve, Suisse},
	title = {A comparison of the data requirements of automatic speech recognition systems and human listeners},
	booktitle = {In {Proceedings} of {Eurospeech}},
	author = {R.K. Moore},
	year = {2003},
	pages = {2582--2584},
}

@inproceedings{rk_moore_and_a_cutler_constraints_2001,
	title = {Constraints on theories of humans vs. machine recognition of speech.},
	booktitle = {{SPRAAC} {Workshop} on {HSR} as {Pattern} {Classification}},
	author = {R.K. Moore {and} A. Cutler},
	year = {2001},
}

@inproceedings{n_deshmuk_rj_duncan_a_ganapathiraju_and_j_picone_benchmarking_nodate,
	title = {Benchmarking human performance for continuous speech recognition},
	booktitle = {{ICSLP} 1996.},
	author = {N. Deshmuk, R.J. Duncan, A. Ganapathiraju, {and} J. Picone.},
}

@inproceedings{n_deshmuk_rj_duncan_a_ganapathraju_and_j_picone_response_nodate,
	address = {Philadelphia, USA},
	title = {Response time as metric for comparison of speech recognition by humans and machines},
	booktitle = {{ICSLP}},
	author = {N. Deshmuk, R.J. Duncan, A. Ganapathraju, {and} J. Picone},
}

@inproceedings{lcw_pols_flexible_1997,
	title = {Flexible human speech recognition},
	booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}},
	author = {L.C.W. Pols},
	year = {1997},
}

@inproceedings{o_scharenborg_d_norris_l_ten_bosch_and_jm_mc-queen_how_2005,
	title = {How should a speech recognizer work? {Cognitive} {Science}, 29(2005):867–918.},
	author = {O. Scharenborg, D. Norris, L. ten Bosch, {and} J.M. Mc-Queen},
	year = {2005},
}

@article{scharenborg_how_2005,
	title = {How {Should} a {Speech} {Recognizer} {Work}?},
	volume = {29},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1207/s15516709cog0000_37},
	doi = {10.1207/s15516709cog0000_37},
	language = {en},
	number = {6},
	urldate = {2022-11-05},
	journal = {Cognitive Science},
	author = {Scharenborg, Odette and Norris, Dennis and ten Bosch, Louis and McQueen, James M.},
	month = nov,
	year = {2005},
	pages = {867--918},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\8MAVAN8F\\Scharenborg et al. - 2005 - How Should a Speech Recognizer Work.pdf:application/pdf},
}

@article{scharenborg_reaching_2007,
	title = {Reaching over the gap: {A} review of efforts to link human and automatic speech recognition research},
	volume = {49},
	issn = {01676393},
	shorttitle = {Reaching over the gap},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000106},
	doi = {10.1016/j.specom.2007.01.009},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Scharenborg, Odette},
	month = may,
	year = {2007},
	pages = {336--347},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\A8IJ4SXY\\Scharenborg - 2007 - Reaching over the gap A review of efforts to link.pdf:application/pdf},
}

@inproceedings{furui_robust_2003,
	title = {Robust methods in automatic speech recognition and understanding},
	url = {https://www.isca-speech.org/archive/eurospeech_2003/furui03_eurospeech.html},
	doi = {10.21437/Eurospeech.2003-575},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {8th {European} {Conference} on {Speech} {Communication} and {Technology} ({Eurospeech} 2003)},
	publisher = {ISCA},
	author = {Furui, Sadaoki},
	month = sep,
	year = {2003},
	pages = {1993--1998},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\BJUNT7HU\\Furui - 2003 - Robust methods in automatic speech recognition and.pdf:application/pdf},
}

@inproceedings{takahiro_shinozaki_and_sadaoki_furui_assessment_2003,
	title = {An assessment of automatic recognition techniques for spontaneous speech in comparison with human performance},
	url = {https://www.isca-speech.org/archive/sspr_2003/shinozaki03_sspr.html},
	booktitle = {Proc. {ISCA}/{IEEE} {Workshop} on {Spontaneous} {Speech} {Processing} and {Recognition}},
	author = {Takahiro Shinozaki {and} Sadaoki Furui},
	year = {2003},
	pages = {paper MAP15},
}

@inproceedings{a_cutler_and_t_robinson_1992_1992,
	address = {Banff, Canada.},
	title = {1992. {Response} time as metric for comparison of speech recognition by humans and machines},
	booktitle = {In {Proc}. of {ICSLP}},
	author = {A. Cutler {and} T. Robinson},
	year = {1992},
	pages = {189--192},
}

@inproceedings{da_van_leeuwen_l-g_van_den_berg_and_hjmsteeneken_human_1995,
	address = {Madrid, Spain},
	title = {Human benchmarks for speaker independent large vocabulary recognition performance},
	booktitle = {In {Proc}. of {Eurospeech}},
	author = {D.A. Van Leeuwen, L.-G. Van den Berg, {and} H.J.M.Steeneken},
	year = {1995},
	pages = {1461--1464},
}

@inproceedings{b_meyer_and_twesker_human-machine_2006,
	address = {Toulouse, France},
	title = {A human-machine comparison in speech recognition based on a logatome corpus},
	booktitle = {In {Proc}. of {Workshop} on {Speech} {Recognition} and {Intrinsic} {Variation}},
	author = {B. Meyer {and} T.Wesker},
	year = {2006},
}

@article{sroka_and_ld_braida_human_2005,
	title = {Human and machine consonant recognition.},
	journal = {Speech Communication, 45(2005)},
	author = {Sroka {and} L.D. Braida},
	year = {2005},
	pages = {401--423},
}

@inproceedings{w_shen_j_olive_and_d_jones_two_2008,
	address = {Brisbane, Australia},
	title = {Two protocols comparing human and machine phonetic discrimination performance in conversational speech},
	booktitle = {In {Proc}. of {Interspeech}},
	author = {W. Shen, J. Olive, {and} D. Jones},
	year = {2008},
}

@article{rost_michael_introducing_1994,
	title = {Introducing listening},
	author = {Rost, Michael},
	month = jan,
	year = {1994},
}

@book{rost_michael_teaching_2016,
	title = {Teaching and researching listening: {Third} edition},
	author = {Rost, Michael},
	month = jan,
	year = {2016},
}

@article{webb_vocabulary_2009,
	title = {Vocabulary {Demands} of {Television} {Programs}},
	volume = {59},
	issn = {00238333, 14679922},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9922.2009.00509.x},
	doi = {10.1111/j.1467-9922.2009.00509.x},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Language Learning},
	author = {Webb, Stuart and Rodgers, Michael P. H.},
	month = jun,
	year = {2009},
	pages = {335--366},
}

@article{webb_lexical_2009,
	title = {The {Lexical} {Coverage} of {Movies}},
	volume = {30},
	issn = {0142-6001, 1477-450X},
	url = {https://academic.oup.com/applij/article-lookup/doi/10.1093/applin/amp010},
	doi = {10.1093/applin/amp010},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Applied Linguistics},
	author = {Webb, S. and Rodgers, M. P. H.},
	month = sep,
	year = {2009},
	pages = {407--427},
}

@article{webb_stuart_using_2010,
	title = {Using glossaries to increase the lexical coverage of television programs},
	volume = {22},
	journal = {Reading in a Foreign Language},
	author = {Webb, Stuart},
	month = jan,
	year = {2010},
}

@article{goldwater_which_2010,
	title = {Which words are hard to recognize? {Prosodic}, lexical, and disfluency factors that increase speech recognition error rates},
	volume = {52},
	issn = {01676393},
	shorttitle = {Which words are hard to recognize?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639309001599},
	doi = {10.1016/j.specom.2009.10.001},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Speech Communication},
	author = {Goldwater, Sharon and Jurafsky, Dan and Manning, Christopher D.},
	month = mar,
	year = {2010},
	pages = {181--200},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\W4MWM6JM\\Goldwater et al. - 2010 - Which words are hard to recognize Prosodic, lexic.pdf:application/pdf},
}

@article{field_bricks_2008,
	title = {Bricks or {Mortar}: {Which} {Parts} of the {Input} {Does} a {Second} {Language} {Listener} {Rely} on?},
	volume = {42},
	issn = {0039-8322, 1545-7249},
	shorttitle = {Bricks or {Mortar}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.1545-7249.2008.tb00139.x},
	doi = {10.1002/j.1545-7249.2008.tb00139.x},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {TESOL Quarterly},
	author = {Field, John},
	month = sep,
	year = {2008},
	pages = {411--432},
}

@article{webb_learning_2011,
	title = {Learning {Collocations}: {Do} the {Number} of {Collocates}, {Position} of the {Node} {Word}, and {Synonymy} {Affect} {Learning}?},
	volume = {32},
	issn = {1477-450X, 0142-6001},
	shorttitle = {Learning {Collocations}},
	url = {https://academic.oup.com/applij/article-lookup/doi/10.1093/applin/amq051},
	doi = {10.1093/applin/amq051},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Applied Linguistics},
	author = {Webb, Stuart and Kagimoto, Eve},
	month = jul,
	year = {2011},
	pages = {259--276},
}

@article{webb_pre-learning_2010,
	title = {Pre-learning low-frequency vocabulary in second language television programmes},
	volume = {14},
	issn = {1362-1688, 1477-0954},
	url = {http://journals.sagepub.com/doi/10.1177/1362168810375371},
	doi = {10.1177/1362168810375371},
	abstract = {This study investigated the potential of pre-learning frequently occurring low-frequency vocabulary as a means to increase comprehension of television and incidental vocabulary learning through watching television. Eight television programmes, each representing different television genres, were analysed using the RANGE program to determine the 10 most frequent low-frequency word-families in each programme and the coverage that they represented. The results showed that coverage of the 10 most frequent low-frequency word-families ranged from 0.70\% to 3.91\%, coverage of the most frequent 3,000 to 3,999 word-families ranged from 0.22\% to 2.58\%, and coverage of the 4,000 to 4,999 word-families ranged from 0.35\% to 1.96\%. This result shows the relative value of pre-learning vocabulary in television programmes and provides a strong argument for pre-learning vocabulary. The findings also suggested that if learners knew the most frequent 3,000 word-families and pre-learned low-frequency vocabulary, comprehension and incidental vocabulary learning may increase.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Language Teaching Research},
	author = {Webb, Stuart},
	month = oct,
	year = {2010},
	pages = {501--515},
}

@article{webb_corpus_2010,
	title = {A corpus driven study of the potential for vocabulary learning through watching movies},
	volume = {15},
	issn = {1384-6655, 1569-9811},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.4.03web},
	doi = {10.1075/ijcl.15.4.03web},
	abstract = {In this corpus driven study, the scripts of 143 movies consisting of 1,267,236 running words were analyzed using the RANGE program (Heatley et al. 2002) to determine the number of encounters with low frequency words. Low frequency words were operationalized as items from Nation’s (2004) 4th to 14th 1,000-word BNC lists. The results showed that in a single movie, few words were encountered 10 or more times indicating that only a small number of words may be learned through watching one movie. However, as the number of movies analyzed increased, the number of words encountered 10 or more times increased. Twenty-three percent of the word families from Nation’s (2004) 4th 1,000-word list were encountered 10 or more times in a set of 70 movies. This indicates that if learners watch movies regularly over a long period of time, there is the potential for significant incidental learning to occur},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {International Journal of Corpus Linguistics},
	author = {Webb, Stuart},
	month = nov,
	year = {2010},
	pages = {497--519},
}

@article{webb_selecting_2011,
	title = {Selecting {Television} {Programs} for {Language} {Learning}: {Investigating} {Television} {Programs} from the {Same} {Genre}},
	volume = {11},
	shorttitle = {Selecting {Television} {Programs} for {Language} {Learning}},
	doi = {10.6018/ijes.11.1.137131},
	abstract = {The scripts of 288 television episodes were analysed to determine the extent to which vocabulary reoccurs in television programs from the same subgenres and unrelated television programs from different genres. Episodes from two programs from each of the following three subgenres of the American drama genre: medical, spy/action, and criminal forensic investigation were compared with different sets of random episodes. The results showed that although there were an equivalent number of running words in each set of episodes, the episodes from programs within the same subgenre contained fewer word families than random programs. The findings also showed that low frequency word families (4000-14,000 levels) reoccur more often in programs within the same subgenre. Together the results indicate that watching programs within the same subgenre may be an effective approach to language learning with television because it reduces the lexical demands of viewing and increases the potential for vocabulary learning. RESUMEN Los guiones de 288 episodios televisivos se analizaron para determinar el alcance de la recursividad del vocabulario en programas de televisión del mismo subgénero y en programas no relacionados de géneros diferentes. Se compararon episodios de tres subgéneros del drama americano: médico, de espías/acción y de investigación forense, con varios grupos de episodios elegidos al azar. Los resultados muestran que, aunque el número de palabras en cada grupo de episodios era equivalente, los episodios del mismo subgénero contienen menos familias de palabras que aquellos elegidos al azar. Los hallazgos mostraron que las familias de baja frecuencia (niveles de 4.000-14.000) se repiten con más frecuencia en los programas del mismo subgénero. En conjunto, los resultados indican que el visionado de programas del mismo subgénero puede ser un método efectivo para aprender el lenguaje por medio de la televisión porque reduce la demanda léxica de la proyección y aumenta el potencial de aprendizaje de vocabulario. PALABRAS CLAVE: Comprensión, Lingüística de corpus, género, aprendizaje incidental, televisión, cobertura del vocabulario, frecuencia léxica.},
	journal = {International Journal of English Studies (IJES)},
	author = {Webb, Stuart},
	month = jun,
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IBB7BUSU\\Webb - 2011 - Selecting Television Programs for Language Learnin.pdf:application/pdf},
}

@book{holmes_introduction_2013,
	edition = {0},
	title = {An {Introduction} to {Sociolinguistics}},
	isbn = {978-1-292-00506-5},
	url = {https://www.taylorfrancis.com/books/9781317860723},
	language = {en},
	urldate = {2022-11-05},
	publisher = {Routledge},
	author = {Holmes, Janet},
	month = oct,
	year = {2013},
	doi = {10.4324/9781315833057},
}

@inproceedings{mirzaei_errors_2015-1,
	title = {Errors in automatic speech recognition versus difficulties in second language listening},
	isbn = {978-1-908416-29-2},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2015.000367},
	doi = {10.14705/rpnet.2015.000367},
	urldate = {2022-11-05},
	booktitle = {Critical {CALL} – {Proceedings} of the 2015 {EUROCALL} {Conference}, {Padova}, {Italy}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Meshgi, Kourosh and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2015},
	pages = {410--415},
}

@inproceedings{mirzaei_partial_2014,
	title = {Partial and synchronized captioning: {A} new tool for second language listening development},
	isbn = {978-1-908416-20-9},
	shorttitle = {Partial and synchronized captioning},
	url = {https://research-publishing.net/manuscript?10.14705/rpnet.2014.000223},
	doi = {10.14705/rpnet.2014.000223},
	urldate = {2022-11-05},
	booktitle = {{CALL} {Design}: {Principles} and {Practice} - {Proceedings} of the 2014 {EUROCALL} {Conference}, {Groningen}, {The} {Netherlands}},
	publisher = {Research-publishing.net},
	author = {Mirzaei, Maryam Sadat and Akita, Yuya and Kawahara, Tatsuya},
	month = dec,
	year = {2014},
	pages = {230--236},
}

@inproceedings{shimogori_automatically_2010,
	address = {Copenhagen, Denmark},
	title = {Automatically generated captions: will they help non-native speakers communicate in english?},
	isbn = {978-1-4503-0108-4},
	shorttitle = {Automatically generated captions},
	url = {http://portal.acm.org/citation.cfm?doid=1841853.1841865},
	doi = {10.1145/1841853.1841865},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 3rd international conference on {Intercultural} collaboration - {ICIC} '10},
	publisher = {ACM Press},
	author = {Shimogori, Nobuhiro and Ikeda, Tomoo and Tsuboi, Sougo},
	year = {2010},
	pages = {79},
}

@inproceedings{shinozaki_error_2001,
	address = {Madonna di Campiglio, Italy},
	title = {Error analysis using decision trees in spontaneous presentation speech recognition},
	isbn = {978-0-7803-7343-3},
	url = {http://ieeexplore.ieee.org/document/1034621/},
	doi = {10.1109/ASRU.2001.1034621},
	urldate = {2022-11-05},
	booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}, 2001. {ASRU} '01.},
	publisher = {IEEE},
	author = {Shinozaki, T. and Furui, S.},
	year = {2001},
	pages = {198--201},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AYTJH6KH\\Shinozaki and Furui - 2001 - Error analysis using decision trees in spontaneous.pdf:application/pdf},
}

@article{thomson_computer_2011,
	title = {Computer {Assisted} {Pronunciation} {Training}: {Targeting} {Second} {Language} {Vowel} {Perception} {Improves} {Pronunciation}},
	volume = {28},
	issn = {07427778},
	shorttitle = {Computer {Assisted} {Pronunciation} {Training}},
	url = {http://www.equinoxpub.com/journals/index.php/CALICO/article/view/22985},
	doi = {10.11139/cj.28.3.744-765},
	number = {3},
	urldate = {2022-11-05},
	journal = {CALICO Journal},
	author = {Thomson, Ron I.},
	month = may,
	year = {2011},
	pages = {744--765},
}

@article{yuan_pauses_2021,
	title = {Pauses for {Detection} of {Alzheimer}’s {Disease}},
	volume = {2},
	issn = {2624-9898},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2020.624488/full},
	doi = {10.3389/fcomp.2020.624488},
	abstract = {Pauses, disfluencies and language problems in Alzheimer’s disease can be naturally modeled by fine-tuning Transformer-based pre-trained language models such as BERT and ERNIE. Using this method with pause-encoded transcripts, we achieved 89.6\% accuracy on the test set of the ADReSS (
              A
              lzheimer’s
              D
              ementia
              Re
              cognition through
              S
              pontaneous
              S
              peech) Challenge. The best accuracy was obtained with ERNIE, plus an encoding of pauses. Robustness is a challenge for large models and small training sets. Ensemble over many runs of BERT/ERNIE fine-tuning reduced variance and improved accuracy. We found that
              um
              was used much less frequently in Alzheimer’s speech, compared to
              uh
              . We discussed this interesting finding from linguistic and cognitive perspectives.},
	urldate = {2022-11-05},
	journal = {Frontiers in Computer Science},
	author = {Yuan, Jiahong and Cai, Xingyu and Bian, Yuchen and Ye, Zheng and Church, Kenneth},
	month = jan,
	year = {2021},
	pages = {624488},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\67RYRPW8\\Yuan et al. - 2021 - Pauses for Detection of Alzheimer’s Disease.pdf:application/pdf},
}

@article{bucks_analysis_2000,
	title = {Analysis of spontaneous, conversational speech in dementia of {Alzheimer} type: {Evaluation} of an objective technique for analysing lexical performance},
	volume = {14},
	issn = {0268-7038, 1464-5041},
	shorttitle = {Analysis of spontaneous, conversational speech in dementia of {Alzheimer} type},
	url = {http://www.tandfonline.com/doi/abs/10.1080/026870300401603},
	doi = {10.1080/026870300401603},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Aphasiology},
	author = {Bucks, R. S. and Singh, S. and Cuerden, J. M. and Wilcock, G. K.},
	month = jan,
	year = {2000},
	pages = {71--91},
}

@article{singh_evaluation_2001,
	title = {Evaluation of an objective technique for analysing temporal variables in {DAT} spontaneous speech},
	volume = {15},
	issn = {0268-7038, 1464-5041},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02687040143000041},
	doi = {10.1080/02687040143000041},
	language = {en},
	number = {6},
	urldate = {2022-11-05},
	journal = {Aphasiology},
	author = {Singh, Sameer and Bucks, Romola S. and Cuerden, Joanne M.},
	month = jun,
	year = {2001},
	pages = {571--583},
}

@article{pulido_alzheimers_2020,
	title = {Alzheimer's disease and automatic speech analysis: {A} review},
	volume = {150},
	issn = {09574174},
	shorttitle = {Alzheimer's disease and automatic speech analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420300397},
	doi = {10.1016/j.eswa.2020.113213},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Pulido, María Luisa Barragán and Hernández, Jesús Bernardino Alonso and Ballester, Miguel Ángel Ferrer and González, Carlos Manuel Travieso and Mekyska, Jiří and Smékal, Zdeněk},
	month = jul,
	year = {2020},
	pages = {113213},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4YMA25IC\\Pulido et al. - 2020 - Alzheimer's disease and automatic speech analysis.pdf:application/pdf},
}

@inproceedings{li_comparative_2021,
	address = {Toronto, ON, Canada},
	title = {A {Comparative} {Study} of {Acoustic} and {Linguistic} {Features} {Classification} for {Alzheimer}'s {Disease} {Detection}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414147/},
	doi = {10.1109/ICASSP39728.2021.9414147},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	pages = {6423--6427},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\54WJ2NZX\\Li et al. - 2021 - A Comparative Study of Acoustic and Linguistic Fea.pdf:application/pdf},
}

@inproceedings{yuan_pause-encoded_2021,
	address = {Toronto, ON, Canada},
	title = {Pause-{Encoded} {Language} {Models} for {Recognition} of {Alzheimer}’s {Disease} and {Emotion}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413548/},
	doi = {10.1109/ICASSP39728.2021.9413548},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Yuan, Jiahong and Cai, Xingyu and Church, Kenneth},
	month = jun,
	year = {2021},
	pages = {7293--7297},
}

@inproceedings{koo_exploiting_2020,
	title = {Exploiting {Multi}-{Modal} {Features} from {Pre}-{Trained} {Networks} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/koo20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3153},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Koo, Junghyun and Lee, Jie Hwan and Pyo, Jaewoo and Jo, Yujin and Lee, Kyogu},
	month = oct,
	year = {2020},
	pages = {2217--2221},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\P9SR4M7P\\Koo et al. - 2020 - Exploiting Multi-Modal Features from Pre-Trained N.pdf:application/pdf},
}

@article{szatloczki_speaking_2015,
	title = {Speaking in {Alzheimer}’s {Disease}, is {That} an {Early} {Sign}? {Importance} of {Changes} in {Language} {Abilities} in {Alzheimer}’s {Disease}},
	volume = {7},
	issn = {1663-4365},
	shorttitle = {Speaking in {Alzheimer}’s {Disease}, is {That} an {Early} {Sign}?},
	url = {http://journal.frontiersin.org/Article/10.3389/fnagi.2015.00195/abstract},
	doi = {10.3389/fnagi.2015.00195},
	urldate = {2022-11-05},
	journal = {Frontiers in Aging Neuroscience},
	author = {Szatloczki, Greta and Hoffmann, Ildiko and Vincze, Veronika and Kalman, Janos and Pakaski, Magdolna},
	month = oct,
	year = {2015},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ZFBPTM9I\\Szatloczki et al. - 2015 - Speaking in Alzheimer’s Disease, is That an Early .pdf:application/pdf},
}

@article{meilan_speech_2014,
	title = {Speech in {Alzheimer}'s {Disease}: {Can} {Temporal} and {Acoustic} {Parameters} {Discriminate} {Dementia}?},
	volume = {37},
	issn = {1420-8008, 1421-9824},
	shorttitle = {Speech in {Alzheimer}'s {Disease}},
	url = {https://www.karger.com/Article/FullText/356726},
	doi = {10.1159/000356726},
	abstract = {\textbf{\textit{Aims:}} The study explores how speech measures may be linked to language profiles in participants with Alzheimer's disease (AD) and how these profiles could distinguish AD from changes associated with normal aging. \textbf{\textit{Methods:}} We analysed simple sentences spoken by older adults with and without AD. Spectrographic analysis of temporal and acoustic characteristics was carried out using the Praat software. \textbf{\textit{Results:}} We found that measures of speech, such as variations in the percentage of voice breaks, number of periods of voice, number of voice breaks, shimmer (amplitude perturbation quotient), and noise-to-harmonics ratio, characterise people with AD with an accuracy of 84.8\%. \textbf{\textit{Discussion:}} These measures offer a sensitive method of assessing spontaneous speech output in AD, and they discriminate well between people with AD and healthy older adults. This method of evaluation is a promising tool for AD diagnosis and prognosis, and it could be used as a dependent measure in clinical trials.},
	language = {en},
	number = {5-6},
	urldate = {2022-11-05},
	journal = {Dementia and Geriatric Cognitive Disorders},
	author = {Meilán, Juan José G. and Martínez-Sánchez, Francisco and Carro, Juan and López, Dolores E. and Millian-Morell, Lymarie and Arana, José M.},
	year = {2014},
	pages = {327--334},
}

@inproceedings{meghanani_exploration_2021,
	address = {Shenzhen, China},
	title = {An {Exploration} of {Log}-{Mel} {Spectrogram} and {MFCC} {Features} for {Alzheimer}’s {Dementia} {Recognition} from {Spontaneous} {Speech}},
	isbn = {978-1-72817-066-4},
	url = {https://ieeexplore.ieee.org/document/9383491/},
	doi = {10.1109/SLT48900.2021.9383491},
	urldate = {2022-11-05},
	booktitle = {2021 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Meghanani, Amit and C. S., Anoop and Ramakrishnan, A. G.},
	month = jan,
	year = {2021},
	pages = {670--677},
}

@inproceedings{raghavendra_pappagari_jaejin_cho_laureano_moro-velazquez_et_al_using_2020,
	title = {Using state of the art speaker recognition and natural language processing technologies to detect alzheimer’s disease and assess its severity},
	author = {Raghavendra Pappagari, Jaejin Cho, Laureano Moro-Velazquez, et al.},
	year = {2020},
	pages = {2177--2181},
}

@inproceedings{pompili_anna_and_rolland_thomas_and_abad_alberto_inesc-id_2020,
	title = {The {INESC}-{ID} {Multi}-{Modal} {System} for the {ADReSS} 2020 {Challenge}},
	author = {Pompili, Anna {and} Rolland, Thomas {and} Abad, Alberto},
	month = may,
	year = {2020},
	note = {arXiv preprint arXiv:2005.14646, 2020},
}

@inproceedings{cummins_comparison_2020,
	title = {A {Comparison} of {Acoustic} and {Linguistics} {Methodologies} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/cummins20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2635},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Cummins, Nicholas and Pan, Yilin and Ren, Zhao and Fritsch, Julian and Nallanthighal, Venkata Srikanth and Christensen, Heidi and Blackburn, Daniel and Schuller, Björn W. and Magimai-Doss, Mathew and Strik, Helmer and Härmä, Aki},
	month = oct,
	year = {2020},
	pages = {2182--2186},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\B32UPSNS\\Cummins et al. - 2020 - A Comparison of Acoustic and Linguistics Methodolo.pdf:application/pdf},
}

@inproceedings{eyben_openear_2009,
	address = {Amsterdam},
	title = {{OpenEAR} — {Introducing} the munich open-source emotion and affect recognition toolkit},
	isbn = {978-1-4244-4800-5},
	url = {http://ieeexplore.ieee.org/document/5349350/},
	doi = {10.1109/ACII.2009.5349350},
	urldate = {2022-11-05},
	booktitle = {2009 3rd {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} and {Workshops}},
	publisher = {IEEE},
	author = {Eyben, Florian and Wollmer, Martin and Schuller, Bjorn},
	month = sep,
	year = {2009},
	pages = {1--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\MU67XKJM\\Eyben et al. - 2009 - OpenEAR — Introducing the munich open-source emoti.pdf:application/pdf},
}

@inproceedings{rohanian_multi-modal_2020,
	title = {Multi-{Modal} {Fusion} with {Gating} {Using} {Audio}, {Lexical} and {Disfluency} {Features} for {Alzheimer}’s {Dementia} {Recognition} from {Spontaneous} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/rohanian20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2721},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Rohanian, Morteza and Hough, Julian and Purver, Matthew},
	month = oct,
	year = {2020},
	pages = {2187--2191},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\956T2T7L\\Rohanian et al. - 2020 - Multi-Modal Fusion with Gating Using Audio, Lexica.pdf:application/pdf},
}

@inproceedings{balagopalan_impact_2020,
	address = {Online},
	title = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}: {All} {Errors} are {Equal}, but {Deletions} are {More} {Equal} than {Others}},
	shorttitle = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}},
	url = {https://www.aclweb.org/anthology/2020.wnut-1.21},
	doi = {10.18653/v1/2020.wnut-1.21},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Noisy} {User}-generated {Text} ({W}-{NUT} 2020)},
	publisher = {Association for Computational Linguistics},
	author = {Balagopalan, Aparna and Shkaruta, Ksenia and Novikova, Jekaterina},
	year = {2020},
	pages = {159--164},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\459E52FL\\Balagopalan et al. - 2020 - Impact of ASR on Alzheimer’s Disease Detection Al.pdf:application/pdf},
}

@inproceedings{liu_detecting_2021,
	address = {Toronto, ON, Canada},
	title = {Detecting {Alzheimer}’s {Disease} from {Speech} {Using} {Neural} {Networks} with {Bottleneck} {Features} and {Data} {Augmentation}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413566/},
	doi = {10.1109/ICASSP39728.2021.9413566},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Liu, Zhaoci and Guo, Zhiqiang and Ling, Zhenhua and Li, Yunxia},
	month = jun,
	year = {2021},
	pages = {7323--7327},
}

@inproceedings{toth_automatic_2015,
	title = {Automatic detection of mild cognitive impairment from spontaneous speech using {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2015/toth15_interspeech.html},
	doi = {10.21437/Interspeech.2015-568},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Tóth, László and Gosztolya, Gábor and Vincze, Veronika and Hoffmann, Ildikó and Szatlóczki, Gréta and Biró, Edit and Zsura, Fruzsina and Pákáski, Magdolna and Kálmán, János},
	month = sep,
	year = {2015},
	pages = {2694--2698},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\3FGE8JIU\\Tóth et al. - 2015 - Automatic detection of mild cognitive impairment f.pdf:application/pdf},
}

@misc{tits_asr-based_2018,
	title = {{ASR}-based {Features} for {Emotion} {Recognition}: {A} {Transfer} {Learning} {Approach}},
	shorttitle = {{ASR}-based {Features} for {Emotion} {Recognition}},
	url = {http://arxiv.org/abs/1805.09197},
	abstract = {During the last decade, the applications of signal processing have drastically improved with deep learning. However areas of affecting computing such as emotional speech synthesis or emotion recognition from spoken language remains challenging. In this paper, we investigate the use of a neural Automatic Speech Recognition (ASR) as a feature extractor for emotion recognition. We show that these features outperform the eGeMAPS feature set to predict the valence and arousal emotional dimensions, which means that the audio-to-text mapping learning by the ASR system contain information related to the emotional dimensions in spontaneous speech. We also examine the relationship between first layers (closer to speech) and last layers (closer to text) of the ASR and valence/arousal.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Tits, Noé and Haddad, Kevin El and Dutoit, Thierry},
	month = jun,
	year = {2018},
	note = {arXiv:1805.09197 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to be published in the First Workshop on Computational Modeling of Human Multimodal Language - ACL 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\RIXSMVYF\\Tits et al. - 2018 - ASR-based Features for Emotion Recognition A Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\DDGNTFFZ\\1805.html:text/html},
}

@misc{zhang_unified_2021,
	title = {Unified {Streaming} and {Non}-streaming {Two}-pass {End}-to-end {Model} for {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2012.05481},
	abstract = {In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60\% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42\% CER with 640ms latency in a streaming ASR system.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Zhang, Binbin and Wu, Di and Yao, Zhuoyuan and Wang, Xiong and Yu, Fan and Yang, Chao and Guo, Liyong and Hu, Yaguang and Xie, Lei and Lei, Xin},
	month = dec,
	year = {2021},
	note = {arXiv:2012.05481 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3YGQUSLL\\Zhang et al. - 2021 - Unified Streaming and Non-streaming Two-pass End-t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Y9XSYRY3\\2012.html:text/html},
}

@article{sun_ernie_2020,
	title = {{ERNIE} 2.0: {A} {Continual} {Pre}-{Training} {Framework} for {Language} {Understanding}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{ERNIE} 2.0},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6428},
	doi = {10.1609/aaai.v34i05.6428},
	abstract = {Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
	number = {05},
	urldate = {2022-11-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
	month = apr,
	year = {2020},
	pages = {8968--8975},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WVPE3NJM\\Sun et al. - 2020 - ERNIE 2.0 A Continual Pre-Training Framework for .pdf:application/pdf},
}

@inproceedings{schuller_interspeech_2010,
	title = {The {INTERSPEECH} 2010 paralinguistic challenge},
	url = {https://www.isca-speech.org/archive/interspeech_2010/schuller10b_interspeech.html},
	doi = {10.21437/Interspeech.2010-739},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2010},
	publisher = {ISCA},
	author = {Schuller, Björn and Steidl, Stefan and Batliner, Anton and Burkhardt, Felix and Devillers, Laurence and Müller, Christian and Narayanan, Shrikanth S.},
	month = sep,
	year = {2010},
	pages = {2794--2797},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3QAQXZVB\\Schuller et al. - 2010 - The INTERSPEECH 2010 paralinguistic challenge.pdf:application/pdf},
}

@misc{noauthor_ad2021_2022,
	title = {{AD2021}: {Alzheimer}'s {Disease} {Recognition} {Evaluation} 2021},
	copyright = {Apache-2.0},
	shorttitle = {{AD2021}},
	url = {https://github.com/THUsatlab/AD2021},
	abstract = {Alzheimer's Disease Recognition Evaluation 2021},
	urldate = {2022-11-05},
	publisher = {THUsatlab},
	month = aug,
	year = {2022},
	note = {original-date: 2021-07-08T04:10:11Z},
}

@inproceedings{gui_end--end_2022,
	address = {Singapore, Singapore},
	title = {End-to-{End} {ASR}-{Enhanced} {Neural} {Network} for {Alzheimer}’s {Disease} {Diagnosis}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9747856/},
	doi = {10.1109/ICASSP43922.2022.9747856},
	urldate = {2022-11-05},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Gui, Jiancheng and Li, Yikai and Chen, Kai and Siebert, Joanna and Chen, Qingcai},
	month = may,
	year = {2022},
	pages = {8562--8566},
}

@inproceedings{garnerin_investigating_2021,
	address = {Online},
	title = {Investigating the {Impact} of {Gender} {Representation} in {ASR} {Training} {Data}: a {Case} {Study} on {Librispeech}},
	shorttitle = {Investigating the {Impact} of {Gender} {Representation} in {ASR} {Training} {Data}},
	url = {https://aclanthology.org/2021.gebnlp-1.10},
	doi = {10.18653/v1/2021.gebnlp-1.10},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 3rd {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	year = {2021},
	pages = {86--92},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9ZB3LE4W\\Garnerin et al. - 2021 - Investigating the Impact of Gender Representation .pdf:application/pdf},
}

@inproceedings{abushariah_mohammad_and_sawalha_majdi_effects_2013,
	address = {Lancaster, UK},
	title = {The effects of speakers' gender, age, and region on overall performance of {Arabic} automatic speech recognition systems using the phonetically rich and balanced {Modern} {Standard} {Arabic} speech corpus},
	url = {http://eprints.whiterose.ac.uk/81859/},
	booktitle = {Proceedings of the 2nd {Workshop} of {Arabic} {Corpus} {Linguistics}},
	author = {Abushariah, Mohammad {and} Sawalha, Majdi},
	month = jul,
	year = {2013},
	note = {University of Leeds},
}

@inproceedings{shah_predictive_2020,
	address = {Online},
	title = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}: {A} {Conceptual} {Framework} and {Overview}},
	shorttitle = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.468},
	doi = {10.18653/v1/2020.acl-main.468},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shah, Deven Santosh and Schwartz, H. Andrew and Hovy, Dirk},
	year = {2020},
	pages = {5248--5264},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\C44TW2PL\\Shah et al. - 2020 - Predictive Biases in Natural Language Processing M.pdf:application/pdf},
}

@inproceedings{hovy_social_2016,
	address = {Berlin, Germany},
	title = {The {Social} {Impact} of {Natural} {Language} {Processing}},
	url = {http://aclweb.org/anthology/P16-2096},
	doi = {10.18653/v1/P16-2096},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Spruit, Shannon L.},
	year = {2016},
	pages = {591--598},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\Z3PQLK4Y\\Hovy and Spruit - 2016 - The Social Impact of Natural Language Processing.pdf:application/pdf},
}

@article{garg_word_2018,
	title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1720347115},
	doi = {10.1073/pnas.1720347115},
	abstract = {Significance
            Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science.
          , 
            Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.},
	language = {en},
	number = {16},
	urldate = {2022-11-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	month = apr,
	year = {2018},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\RQYPG4XA\\Garg et al. - 2018 - Word embeddings quantify 100 years of gender and e.pdf:application/pdf},
}

@book{kutuzov_diachronic_2018,
	title = {Diachronic word embeddings and semantic shifts: a survey},
	shorttitle = {Diachronic word embeddings and semantic shifts},
	abstract = {Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.},
	author = {Kutuzov, Andrei and Øvrelid, Lilja and Szymanski, Terrence and Velldal, Erik},
	month = jun,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5PQKNLMN\\Kutuzov et al. - 2018 - Diachronic word embeddings and semantic shifts a .pdf:application/pdf},
}

@inproceedings{sap_risk_2019,
	address = {Florence, Italy},
	title = {The {Risk} of {Racial} {Bias} in {Hate} {Speech} {Detection}},
	url = {https://www.aclweb.org/anthology/P19-1163},
	doi = {10.18653/v1/P19-1163},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
	year = {2019},
	pages = {1668--1678},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\85P2NR4F\\Sap et al. - 2019 - The Risk of Racial Bias in Hate Speech Detection.pdf:application/pdf},
}

@article{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	month = jul,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5YSE3CSF\\Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf},
}

@inproceedings{zhao_gender_2019,
	address = {Minneapolis, Minnesota},
	title = {Gender {Bias} in {Contextualized} {Word} {Embeddings}},
	url = {http://aclweb.org/anthology/N19-1064},
	doi = {10.18653/v1/N19-1064},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
	year = {2019},
	pages = {629--634},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\JX738BCA\\Zhao et al. - 2019 - Gender Bias in Contextualized Word Embeddings.pdf:application/pdf},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	abstract = {Machines learn what people know implicitly
            
              AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan
              et al.
              now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.
            
            
              Science
              , this issue p.
              183
              ; see also p.
              133
            
          , 
            Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.
          , 
            Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	language = {en},
	number = {6334},
	urldate = {2022-11-05},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	pages = {183--186},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\ER9DI443\\Caliskan et al. - 2017 - Semantics derived automatically from language corp.pdf:application/pdf},
}

@inproceedings{adda-decker_speech_2005,
	title = {Do speech recognizers prefer female speakers?},
	url = {https://www.isca-speech.org/archive/interspeech_2005/addadecker05_interspeech.html},
	doi = {10.21437/Interspeech.2005-699},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2005},
	publisher = {ISCA},
	author = {Adda-Decker, Martine and Lamel, Lori},
	month = sep,
	year = {2005},
	pages = {2205--2208},
}

@misc{feng_quantifying_2021,
	title = {Quantifying {Bias} in {Automatic} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2103.15122},
	abstract = {Automatic speech recognition (ASR) systems promise to deliver objective interpretation of human speech. Practice and recent evidence suggests that the state-of-the-art (SotA) ASRs struggle with the large variation in speech due to e.g., gender, age, speech impairment, race, and accents. Many factors can cause the bias of an ASR system. Our overarching goal is to uncover bias in ASR systems to work towards proactive bias mitigation in ASR. This paper is a first step towards this goal and systematically quantifies the bias of a Dutch SotA ASR system against gender, age, regional accents and non-native accents. Word error rates are compared, and an in-depth phoneme-level error analysis is conducted to understand where bias is occurring. We primarily focus on bias due to articulation differences in the dataset. Based on our findings, we suggest bias mitigation strategies for ASR development.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Feng, Siyuan and Kudina, Olya and Halpern, Bence Mark and Scharenborg, Odette},
	month = apr,
	year = {2021},
	note = {arXiv:2103.15122 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	annote = {Comment: Submitted to INTERSPEECH (IS) 2021. This preprint version differs slightly from the version submitted to IS 2021: Figure 1 is not included in IS 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\GJK7YX95\\Feng et al. - 2021 - Quantifying Bias in Automatic Speech Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\XRIQKM99\\2103.html:text/html},
}

@inproceedings{tatman_gender_2017,
	address = {Valencia, Spain},
	title = {Gender and {Dialect} {Bias} in {YouTube}'s {Automatic} {Captions}},
	url = {http://aclweb.org/anthology/W17-1606},
	doi = {10.18653/v1/W17-1606},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tatman, Rachael},
	year = {2017},
	pages = {53--59},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\QHGSA22L\\Tatman - 2017 - Gender and Dialect Bias in YouTube's Automatic Cap.pdf:application/pdf},
}

@inproceedings{tatman_effects_2017,
	title = {Effects of {Talker} {Dialect}, {Gender} \& {Race} on {Accuracy} of {Bing} {Speech} and {YouTube} {Automatic} {Captions}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/tatman17_interspeech.html},
	doi = {10.21437/Interspeech.2017-1746},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Tatman, Rachael and Kasten, Conner},
	month = aug,
	year = {2017},
	pages = {934--938},
}

@techreport{frank_wilcoxon_sk_katti_and_roberta_a_wilcox_critical_1963,
	address = {Pearl River, NY, USA},
	title = {Critical values and probability levels for the {Wilcoxon} rank sum test and the {Wilcoxon} signed rank test},
	institution = {American Cyanamid Company},
	author = {Frank Wilcoxon, SK Katti, {and} Roberta A Wilcox},
	year = {1963},
}

@inproceedings{garnerin_gender_2019,
	address = {Nice, France},
	title = {Gender {Representation} in {French} {Broadcast} {Corpora} and {Its} {Impact} on {ASR} {Performance}},
	isbn = {978-1-4503-6917-6},
	url = {http://dl.acm.org/citation.cfm?doid=3347449.3357480},
	doi = {10.1145/3347449.3357480},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {AI} for {Smart} {TV} {Content} {Production}, {Access} and {Delivery} - {AI4TV} '19},
	publisher = {ACM Press},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	year = {2019},
	pages = {3--9},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GWSDNHSG\\Garnerin et al. - 2019 - Gender Representation in French Broadcast Corpora .pdf:application/pdf},
}

@inproceedings{garnerin_gender_2020,
	address = {Marseille, France},
	title = {Gender {Representation} in {Open} {Source} {Speech} {Resources}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.813},
	abstract = {With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited/non elicited speech, low/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.},
	language = {English},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Garnerin, Mahault and Rossato, Solange and Besacier, Laurent},
	month = may,
	year = {2020},
	pages = {6599--6605},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\HW5HZXVR\\Garnerin et al. - 2020 - Gender Representation in Open Source Speech Resour.pdf:application/pdf},
}

@article{adamek_whose_2018,
	title = {Whose {Artifacts}? {Whose} {Stories}? {Public} {History} and {Representation} of {Women} at the {Canada} {Science} and {Technology} {Museum}},
	issn = {0121-1617, 1900-6152},
	shorttitle = {Whose {Artifacts}?},
	url = {https://revistas.uniandes.edu.co/doi/10.7440/histcrit68.2018.03},
	doi = {10.7440/histcrit68.2018.03},
	language = {pt},
	number = {68},
	urldate = {2022-11-05},
	journal = {Historia Crítica},
	author = {Adamek, Anna and Gann, Emily},
	month = apr,
	year = {2018},
	pages = {47--66},
}

@incollection{goos_classification_2001,
	address = {Berlin, Heidelberg},
	title = {Classification on {Data} with {Biased} {Class} {Distribution}},
	volume = {2167},
	isbn = {978-3-540-42536-6 978-3-540-44795-5},
	url = {http://link.springer.com/10.1007/3-540-44795-4_45},
	urldate = {2022-11-05},
	booktitle = {Machine {Learning}: {ECML} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Vucetic, Slobodan and Obradovic, Zoran},
	editor = {Goos, G. and Hartmanis, J. and van Leeuwen, J. and De Raedt, Luc and Flach, Peter},
	year = {2001},
	doi = {10.1007/3-540-44795-4_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {527--538},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WWEYIB8Z\\Vucetic and Obradovic - 2001 - Classification on Data with Biased Class Distribut.pdf:application/pdf},
}

@article{haibo_he_learning_2009,
	title = {Learning from {Imbalanced} {Data}},
	volume = {21},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5128907/},
	doi = {10.1109/TKDE.2008.239},
	number = {9},
	urldate = {2022-11-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {{Haibo He} and Garcia, E.A.},
	month = sep,
	year = {2009},
	pages = {1263--1284},
}

@inproceedings{solon_barocas_kate_crawford_aaron_shapiro_and_hanna_wallach_problem_2017,
	address = {Philadelphia, PA, USA},
	title = {The problem with bias: {Allocative} versus representational harms in machine learning},
	author = {Solon Barocas, Kate Crawford, Aaron Shapiro, {and} Hanna Wallach},
	year = {2017},
}

@inproceedings{noauthor_trouble_2017,
	address = {Long Beach, CA, USA},
	title = {The trouble with bias},
	url = {https://www.youtube.com/watch?v=fMym_BKWQzk},
	booktitle = {31st {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	year = {2017},
	note = {Keynote at the 31st Annual Conference on Neural Information Processing Systems},
}

@article{d_a_mahler_r_a_rosiello_and_j_loke_aging_1986,
	title = {The {Aging} {Lung}},
	volume = {2},
	number = {2},
	journal = {Clinics in Geriatric Medicine},
	author = {D. A. Mahler, R. A. Rosiello, {and} J. Loke},
	year = {1986},
	pages = {215--225},
}

@article{karam_anatomic_2013,
	title = {Anatomic and {Physiologic} {Changes} of the {Aging} {Kidney}},
	volume = {29},
	issn = {07490690},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749069013000396},
	doi = {10.1016/j.cger.2013.05.006},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Clinics in Geriatric Medicine},
	author = {Karam, Zeina and Tuazon, Jennifer},
	month = aug,
	year = {2013},
	pages = {555--564},
}

@article{tolep_comparison_1995,
	title = {Comparison of diaphragm strength between healthy adult elderly and young men.},
	volume = {152},
	issn = {1073-449X, 1535-4970},
	url = {https://www.atsjournals.org/doi/10.1164/ajrccm.152.2.7633725},
	doi = {10.1164/ajrccm.152.2.7633725},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {American Journal of Respiratory and Critical Care Medicine},
	author = {Tolep, K and Higgins, N and Muza, S and Criner, G and Kelsen, S G},
	month = aug,
	year = {1995},
	pages = {677--682},
}

@article{linville_vocal_1995,
	title = {Vocal aging:},
	volume = {3},
	issn = {1068-9508},
	shorttitle = {Vocal aging},
	url = {http://journals.lww.com/00020840-199506000-00006},
	doi = {10.1097/00020840-199506000-00006},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Current Opinion in Otolaryngology \& Head and Neck Surgery},
	author = {Linville, Sue Ellen},
	month = jun,
	year = {1995},
	pages = {183--187},
}

@article{ramig_aging_2001,
	title = {The {Aging} {Voice}: {A} {Review}, {Treatment} {Data} and {Familial} and {Genetic} {Perspectives}},
	volume = {53},
	issn = {1021-7762, 1421-9972},
	shorttitle = {The {Aging} {Voice}},
	url = {https://www.karger.com/Article/FullText/52680},
	doi = {10.1159/000052680},
	abstract = {This paper will provide a review of aspects of vocal aging within the context of general body aging and describe two data sets related to the aging voice. Data will be presented which document pre- to posttreatment improvement in select voice characteristics (sound pressure level, subglottal air pressure, thyroarytenoid laryngeal muscle activity and voice quality) following application of an intensive voice treatment program (the LSVT$^{\textrm{®}}$) to 3 individuals with aged voice. Additionally, physiological data (forced expiratory volume, visual accommodation, bone density, taste discrimination, white blood count and resting heart rate) and select perceptual (perceived age) and acoustic measures (reflecting both cycle-to-cycle and longer-term intensity and frequency stability) from 67 subjects will be reviewed from the work of Gray and colleagues to document the differential impact of the global aging process across organ systems including the aging voice.},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {Folia Phoniatrica et Logopaedica},
	author = {Ramig, Lorraine Olson and Gray, Steven and Baker, Kristin and Corbin-Lewis, Kim and Buder, Eugene and Luschei, Erich and Coon, Hillary and Smith, Marshall},
	year = {2001},
	pages = {252--265},
}

@article{paulsen_degenerative_1998,
	title = {Degenerative {Changes} in the {Human} {Cricoarytenoid} {Joint}},
	volume = {124},
	issn = {0886-4470},
	url = {http://archotol.jamanetwork.com/article.aspx?doi=10.1001/archotol.124.8.903},
	doi = {10.1001/archotol.124.8.903},
	language = {en},
	number = {8},
	urldate = {2022-11-05},
	journal = {Archives of Otolaryngology–Head \& Neck Surgery},
	author = {Paulsen, Friedrich P. and Tillmann, Bernhard N.},
	month = aug,
	year = {1998},
	pages = {903},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\VHRPKC7W\\Paulsen and Tillmann - 1998 - Degenerative Changes in the Human Cricoarytenoid J.pdf:application/pdf},
}

@article{rodeno_histochemical_1993,
	title = {Histochemical and {Morphometrical} {Ageing} {Changes} in {Human} {Vocal} {Cord} {Muscles}},
	volume = {113},
	issn = {0001-6489, 1651-2251},
	url = {http://www.tandfonline.com/doi/full/10.3109/00016489309135842},
	doi = {10.3109/00016489309135842},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Acta Oto-Laryngologica},
	author = {Rodeño, M. T. and Sánchez-fernández, J. M. and Rivera-pomar, J. M.},
	month = jan,
	year = {1993},
	pages = {445--449},
}

@article{hirano_ageing_1989,
	title = {Ageing of the {Vibratory} {Tissue} of {Human} {Vocal} {Folds}},
	volume = {107},
	issn = {0001-6489, 1651-2251},
	url = {http://www.tandfonline.com/doi/full/10.3109/00016488909127535},
	doi = {10.3109/00016488909127535},
	language = {en},
	number = {5-6},
	urldate = {2022-11-05},
	journal = {Acta Oto-Laryngologica},
	author = {Hirano, Minoru and Kurita, Shigejiro and Sakaguchi, Shinji},
	month = jan,
	year = {1989},
	pages = {428--433},
}

@article{sato_age-related_1997,
	title = {Age-{Related} {Changes} of {Elastic} {Fibers} in the {Superficial} {Layer} of the {Lamina} {Propria} of {Vocal} {Folds}},
	volume = {106},
	issn = {0003-4894, 1943-572X},
	url = {http://journals.sagepub.com/doi/10.1177/000348949710600109},
	doi = {10.1177/000348949710600109},
	abstract = {An investigation was carried out to determine the morphologic characteristics of elastic fibers in the superficial layer of the lamina propria of aged vocal folds (EFAVFs). Excised human adult vocal folds served as the material for this study. Scanning and transmission electron microscopic observations were made. The results can be summarized as follows. First, the EFAVFs were composed of amorphous substances and microfibrils. The amorphous substances increased in amount and the microfibrils became less numerous. Second, the EFAVFs ran in various directions, were branched, and formed a complicated network. The surface of the fibers was rough, and the fibers appeared to vary in size. Some EFAVFs united to form a sheet with a rough surface. Third, the EFAVFs could not be easily digested by elastase compared with those of younger adults. We conclude that the morphologic and metabolic changes of elastic fibers in the most important vibrating portion (superficial layer of the lamina propria) of the aged vocal folds contribute partially to aging of the voice.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Annals of Otology, Rhinology \& Laryngology},
	author = {Sato, Kiminori and Hirano, Minoru},
	month = jan,
	year = {1997},
	pages = {44--48},
}

@article{rother_morphometrically_2002,
	title = {Morphometrically observable aging changes in the human tongue},
	volume = {184},
	issn = {09409602},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0940960202800115},
	doi = {10.1016/S0940-9602(02)80011-5},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Annals of Anatomy - Anatomischer Anzeiger},
	author = {Rother, Paul and Wohlgemuth, Balthasar and Wolff, Werner and Rebentrost, Ines},
	month = mar,
	year = {2002},
	pages = {159--164},
}

@article{b_weinstein_biology_nodate,
	title = {The biology of aging},
	journal = {Geriatric Audiology},
	author = {B. Weinstein},
	pages = {pp. 15--40},
}

@book{weinstein_geriatric_2013,
	address = {New York},
	edition = {Second edition},
	title = {Geriatric audiology},
	isbn = {978-1-60406-174-1 978-1-60406-775-0},
	publisher = {Thieme},
	author = {Weinstein, Barbara E.},
	year = {2013},
	note = {“The biology of aging,” in Geriatric Audiology,
pp. 15–40, Georg Thieme, Stuttgart, Germany, 2000},
	keywords = {Aged, Aging, Health Services for the Aged, Hearing, Hearing Disorders, physiology},
}

@article{xue_changes_2003,
	title = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}: {A} {Pilot} {Study}},
	volume = {46},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282003/054%29},
	doi = {10.1044/1092-4388(2003/054)},
	abstract = {This investigation used a derivation of acoustic reflection (AR) technology to make cross-sectional measurements of changes due to aging in the oral and pharyngeal lumina of male and female speakers. The purpose of the study was to establish preliminary normative data for such changes and to obtain acoustic measurements of changes due to aging in the formant frequencies of selected spoken vowels and their long-term average spectra (LTAS) analysis. Thirty- eight young men and women and 38 elderly men and women were involved in the study. The oral and pharyngeal lumina of the participants were measured with AR technology, and their formant frequencies were analyzed using the Kay Elemetrics Computerized Speech Lab. The findings have delineated specific and similar patterns of aging changes in human vocal tract configurations in speakers of both genders. Namely, the oral cavity length and volume of elderly speakers increased significantly compared to their young cohorts. The total vocal tract volume of elderly speakers also showed a significant increment, whereas the total vocal tract length of elderly speakers did not differ significantly from their young cohorts. Elderly speakers of both genders also showed similar patterns of acoustic changes of speech production, that is, consistent lowering of formant frequencies (especially F1) across selected vowel productions. Although new research models are still needed to succinctly account for the speech acoustic changes of the elderly, especially for their specific patterns of human vocal tract dimensional changes, this study has innovatively applied the noninvasive and cost-effective AR technology to monitor age-related human oral and pharyngeal lumina changes that have direct consequences for speech production.},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Xue, Steve An and Hao, Grace Jianping},
	month = jun,
	year = {2003},
	pages = {689--701},
}

@article{baba_acoustic_2004,
	title = {Acoustic models of the elderly for large-vocabulary continuous speech recognition},
	volume = {87},
	issn = {8756-663X, 1520-6432},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ecjb.20101},
	doi = {10.1002/ecjb.20101},
	language = {en},
	number = {7},
	urldate = {2022-11-05},
	journal = {Electronics and Communications in Japan (Part II: Electronics)},
	author = {Baba, Akira and Yoshizawa, Shinichi and Yamada, Miichi and Lee, Akinobu and Shikano, Kiyohiro},
	month = jul,
	year = {2004},
	pages = {49--57},
}

@article{xue_changes_2003-1,
	title = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}: {A} {Pilot} {Study}},
	volume = {46},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Changes in the {Human} {Vocal} {Tract} {Due} to {Aging} and the {Acoustic} {Correlates} of {Speech} {Production}},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282003/054%29},
	doi = {10.1044/1092-4388(2003/054)},
	abstract = {This investigation used a derivation of acoustic reflection (AR) technology to make cross-sectional measurements of changes due to aging in the oral and pharyngeal lumina of male and female speakers. The purpose of the study was to establish preliminary normative data for such changes and to obtain acoustic measurements of changes due to aging in the formant frequencies of selected spoken vowels and their long-term average spectra (LTAS) analysis. Thirty- eight young men and women and 38 elderly men and women were involved in the study. The oral and pharyngeal lumina of the participants were measured with AR technology, and their formant frequencies were analyzed using the Kay Elemetrics Computerized Speech Lab. The findings have delineated specific and similar patterns of aging changes in human vocal tract configurations in speakers of both genders. Namely, the oral cavity length and volume of elderly speakers increased significantly compared to their young cohorts. The total vocal tract volume of elderly speakers also showed a significant increment, whereas the total vocal tract length of elderly speakers did not differ significantly from their young cohorts. Elderly speakers of both genders also showed similar patterns of acoustic changes of speech production, that is, consistent lowering of formant frequencies (especially F1) across selected vowel productions. Although new research models are still needed to succinctly account for the speech acoustic changes of the elderly, especially for their specific patterns of human vocal tract dimensional changes, this study has innovatively applied the noninvasive and cost-effective AR technology to monitor age-related human oral and pharyngeal lumina changes that have direct consequences for speech production.},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Xue, Steve An and Hao, Grace Jianping},
	month = jun,
	year = {2003},
	pages = {689--701},
}



@book{vipperla_longitudinal_2008,
	title = {Longitudinal study of {ASR} performance on ageing voices},
	abstract = {This paper presents the results of a longitudinal study of ASR performance on ageing voices. Experiments were conducted on the audio recordings of the proceedings of the Supreme Court Of The United States (SCOTUS). Results show that the Au- tomatic Speech Recognition (ASR) Word Error Rates (WERs) for elderly voices are significantly higher than those of adult voices. The word error rate increases gradually as the age of the elderly speakers increase. Use of maximum likelihood linear regression (MLLR) based speaker adaptation on ageing voices improves the WER though the performance is still considerably lower compared to adult voices. Speaker adaptation however reduces the increase in WER with age during old age. IndexTerms: Ageing Voices, longitudinal study, SCOTUScor- pus, MLLR},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	month = jan,
	year = {2008},
	note = {Pages: 2553},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\YEYS3FPY\\Vipperla et al. - 2008 - Longitudinal study of ASR performance on ageing vo.pdf:application/pdf},
}

@inproceedings{wilpon_study_1996,
	address = {Atlanta, GA, USA},
	title = {A study of speech recognition for children and the elderly},
	volume = {1},
	isbn = {978-0-7803-3192-1},
	url = {http://ieeexplore.ieee.org/document/541104/},
	doi = {10.1109/ICASSP.1996.541104},
	urldate = {2022-11-05},
	booktitle = {1996 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing} {Conference} {Proceedings}},
	publisher = {IEEE},
	author = {Wilpon, J.G. and Jacobsen, C.N.},
	year = {1996},
	pages = {349--352},
}

@book{moller_corpus_2008,
	title = {Corpus {Analysis} of {Spoken} {Smart}-{Home} {Interactions} with {Older} {Users}.},
	abstract = {In this paper, we present the collection and analysis of a spoken dialogue corpus obtained from interactions of older and younger users with a smart-home system. Our aim is to identify the amount and the origin of linguistic differences in the way older and younger users address the system. In addition, we investigate changes in the users' linguistic behaviour after exposure to the system. The results show that the two user groups differ in their speaking style as well as their vocabulary. In contrast to younger users, who adapt their speaking style to the expected limitations of the system, older users tend to use a speaking style that is closer to human-human communication in terms of sentence complexity and politeness. However, older users are far less easy to stereotype than younger users.},
	author = {Möller, Sebastian and Wolters, Maria},
	month = jan,
	year = {2008},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\SHJVDLVP\\Möller and Wolters - 2008 - Corpus Analysis of Spoken Smart-Home Interactions .pdf:application/pdf},
}

@article{wolters_being_2009,
	title = {Being {Old} {Doesn}’t {Mean} {Acting} {Old}: {How} {Older} {Users} {Interact} with {Spoken} {Dialog} {Systems}},
	volume = {2},
	issn = {1936-7228, 1936-7236},
	shorttitle = {Being {Old} {Doesn}’t {Mean} {Acting} {Old}},
	url = {https://dl.acm.org/doi/10.1145/1525840.1525842},
	doi = {10.1145/1525840.1525842},
	abstract = {Most studies on adapting voice interfaces to older users work top-down by comparing the interaction behavior of older and younger users. In contrast, we present a bottom-up approach. A statistical cluster analysis of 447 appointment scheduling dialogs between 50 older and younger users and 9 simulated spoken dialog systems revealed two main user groups, a “social” group and a “factual” group. “Factual” users adapted quickly to the systems and interacted efficiently with them. “Social” users, on the other hand, were more likely to treat the system like a human, and did not adapt their interaction style. While almost all “social” users were older, over a third of all older users belonged in the “factual” group. Cognitive abilities and gender did not predict group membership. We conclude that spoken dialog systems should adapt to users based on observed behavior, not on age.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {ACM Transactions on Accessible Computing},
	author = {Wolters, Maria and Georgila, Kallirroi and Moore, Johanna D. and MacPherson, Sarah E.},
	month = may,
	year = {2009},
	pages = {1--39},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\7MBYCCBB\\Wolters et al. - 2009 - Being Old Doesn’t Mean Acting Old How Older Users.pdf:application/pdf},
}

@incollection{stephanidis_speech_2009,
	address = {Berlin, Heidelberg},
	title = {Speech {Input} from {Older} {Users} in {Smart} {Environments}: {Challenges} and {Perspectives}},
	volume = {5615},
	isbn = {978-3-642-02709-3 978-3-642-02710-9},
	shorttitle = {Speech {Input} from {Older} {Users} in {Smart} {Environments}},
	url = {http://link.springer.com/10.1007/978-3-642-02710-9_14},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Intelligent} and {Ubiquitous} {Interaction} {Environments}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vipperla, Ravichander and Wolters, Maria and Georgila, Kallirroi and Renals, Steve},
	editor = {Stephanidis, Constantine},
	year = {2009},
	doi = {10.1007/978-3-642-02710-9_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {117--126},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\SHIF9M32\\Vipperla et al. - 2009 - Speech Input from Older Users in Smart Environment.pdf:application/pdf},
}

@book{mller_combining_2007,
	title = {Combining short-term cepstral and long-term pitch features for automatic recognition of speaker age.},
	abstract = {The most successful systems in previous comparative studies on speaker age recognition used short-term cepstral features modeled with Gaussian Mixture Models (GMMs) or applied multiple phone recognizers trained with the data of speakers of the respective class. Acoustic analyses, however, indic ate that certain features such as pitch extracted from a longer s pan of speech correlate clearly with the speaker age although the systems based on those features have been inferior to the be- fore mentioned approaches. In this paper, three novel systems combining short-term cepstral features and long-term features for speaker age recognition are compared to each other. A sys- tem combining GMMs using frame-based MFCCs and Support- Vector-Machines using long-term pitch performs best. The re- sults indicate that the combination of the two feature types is a promising approach, which corresponds to findings in relat ed fields like speaker recognition. Index Terms: speaker classifi- cation, age recognition, GMM, SVM},
	author = {Mller, Christian and Burkhardt, Felix},
	month = jan,
	year = {2007},
	note = {Pages: 2280},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PUAFDFIY\\Mller and Burkhardt - 2007 - Combining short-term cepstral and long-term pitch .pdf:application/pdf},
}

@book{wolters_age_2009,
	title = {Age recognition for spoken dialogue systems: do we need it?},
	shorttitle = {Age recognition for spoken dialogue systems},
	abstract = {When deciding whether to adapt relevant aspects of the system to the particular needs of older users, spoken dialogue systems often rely on automatic detection of chronological age. In this paper, we show that vocal age- ing as measured by acoustic features is an unreliable indi- cator of the need for adaptation. Simple lexical features greatly improve the prediction of both relevant aspects of cognition and interactions style. Lexical features also boost age group prediction. We suggest that adaptation should be based on observed behaviour, not on chrono- logical age, unless it is not feasible to build classifiers for relevant adaptation decisions. Index Terms :a ge recognition, pitch, keyword spotting, cognitive ageing},
	author = {Wolters, Maria and Vipperla, Ravichander and Renals, Steve},
	month = jan,
	year = {2009},
	note = {Pages: 1438},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\S7NGAXUA\\Wolters et al. - 2009 - Age recognition for spoken dialogue systems do we.pdf:application/pdf},
}

@article{hillenbrand_acoustic_1996,
	title = {Acoustic correlates of breathy vocal quality: {Dysphonic} voices and continuous speech},
	volume = {39},
	shorttitle = {Acoustic correlates of breathy vocal quality},
	abstract = {In an earlier study, we evaluated the effectiveness of several acoustic measures in predicting breathiness ratings for sustained vowels spoken by nonpathological talkers who were asked to produce nonbreathy, moderately breathy, and very breathy phonation (Hillenbrand, Cleveland, \& Erickson, 1994). The purpose of the present study was to extend these results to speakers with laryngeal pathologies and to conduct tests using connected speech in addition to sustained vowels. Breathiness ratings were obtained from a sustained vowel and a 12-word sentence spoken by 20 pathological and 5 nonpathological talkers. Acoustic measures were made of (a) signal periodicity, (b) first harmonic amplitude, and (c) spectral tilt. For the sustained vowels, a frequency domain measure of periodicity provided the most accurate predictions of perceived breathiness, accounting for 92\% of the variance in breathiness ratings. The relative amplitude of the first harmonic and two measures of spectral tilt correlated moderately with breathiness ratings. For the sentences, both signal periodicity and spectral tilt provided accurate predictions of breathiness ratings, accounting for 70\%-85\% of the variance.},
	journal = {Journal of speech and hearing research},
	author = {Hillenbrand, James and Houde, Robert},
	month = apr,
	year = {1996},
	pages = {311--21},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\445N846Q\\Hillenbrand and Houde - 1996 - Acoustic correlates of breathy vocal quality Dysp.pdf:application/pdf},
}

@article{klich_relationships_1982,
	title = {Relationships of {Vowel} {Characteristics} to {Listener} {Ratings} of {Breathiness}},
	volume = {25},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.2504.574},
	doi = {10.1044/jshr.2504.574},
	abstract = {This experiment was designed to investigate the relationships of listener ratings of breathiness to vowel duration, speaking rate, and the relative energy in three frequency ranges (100–500, 1500–2500, and 3500–4500 Hz) in vowel spectra. The effects of vowel SPL also were considered. Listeners used a seven-point equal-appearing interval scale to rate a sentence spoken by each of 10 young adult females in each of four voice qualities: normal speech, mildly breathy, severely breathy, and whisper. Significant Pearson correlations to the ratings were found only for mean SPL and the relative energy in the 100–500 and 3500–4500 Hz ranges. After the effects of mean SPL were accounted for in partial correlation and multiple regression analyses, all vowel parameters were related significantly to the mean ratings. The partial correlations for vowel duration were as high as those for the three frequency ranges. Vowel duration may be as important as spectral characteristics of vowels when breathiness is judged from samples of connected discourse.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Klich, Richard J.},
	month = dec,
	year = {1982},
	pages = {574--580},
}

@article{hillenbrand_acoustic_1994,
	title = {Acoustic {Correlates} of {Breathy} {Vocal} {Quality}},
	volume = {37},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3704.769},
	doi = {10.1044/jshr.3704.769},
	abstract = {The purpose of this study was to evaluate the effectiveness of several acoustic measures in predicting breathiness ratings. Recordings were made of eight normal men and seven normal women producing normally phonated, moderately breathy, and very breathy sustained vowels. Twenty listeners rated the degree of breathiness using a direct magnitude estimation procedure. Acoustic measures were made of: (a) signal periodicity, (b) first harmonic amplitude, and (c) spectral tilt. Periodicity measures provided the most accurate predictions of perceived breathiness, accounting for approximately 80\% of the variance in breathiness ratings. The relative amplitude of the first harmonic correlated moderately with breathiness ratings, and two measures of spectral tilt correlated weakly with perceived breathiness.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Hillenbrand, James and Cleveland, Ronald A. and Erickson, Robert L.},
	month = aug,
	year = {1994},
	pages = {769--778},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\SCZ928SH\\Hillenbrand et al. - 1994 - Acoustic Correlates of Breathy Vocal Quality.pdf:application/pdf},
}

@article{boersma_accurate_2000,
	title = {Accurate {Short}-{Term} {Analysis} {Of} {The} {Fundamental} {Frequency} {And} {The} {Harmonics}-{To}-{Noise} {Ratio} {Of} {A} {Sampled} {Sound}},
	volume = {17},
	abstract = {We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods. By definition, the best candidate for the acoustic pitch period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for p...},
	journal = {Proceedings of the Institute of Phonetic Sciences},
	author = {Boersma, Paul},
	month = jan,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\YHEXVA6R\\Boersma - 2000 - Accurate Short-Term Analysis Of The Fundamental Fr.pdf:application/pdf},
}

@article{deliyski_effects_2001,
	title = {{EFFECTS} {OF} {AGING} {ON} {SELECTED} {ACOUSTIC} {VOICE} {PARAMETERS}: {PRELIMINARY} {NORMATIVE} {DATA} {AND} {EDUCATIONAL} {IMPLICATIONS}},
	volume = {27},
	issn = {0360-1277, 1521-0472},
	shorttitle = {{EFFECTS} {OF} {AGING} {ON} {SELECTED} {ACOUSTIC} {VOICE} {PARAMETERS}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03601270151075561},
	doi = {10.1080/03601270151075561},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Educational Gerontology},
	author = {Deliyski, Dimitar, Steve An Xue},
	month = mar,
	year = {2001},
	pages = {159--168},
}

@article{vipperla_ageing_2010-1,
	title = {Ageing {Voices}: {The} {Effect} of {Changes} in {Voice} {Parameters} on {ASR} {Performance}},
	volume = {2010},
	issn = {1687-4714, 1687-4722},
	shorttitle = {Ageing {Voices}},
	url = {http://asmp.eurasipjournals.com/content/2010/1/525783},
	doi = {10.1155/2010/525783},
	language = {en},
	urldate = {2022-11-05},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	year = {2010},
	pages = {1--10},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AJZD42W5\\Vipperla et al. - 2010 - Ageing Voices The Effect of Changes in Voice Para.pdf:application/pdf},
}

@misc{noauthor_lombard_2022,
	title = {Lombard effect},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Lombard_effect&oldid=1100217835},
	abstract = {The Lombard effect or Lombard reflex is the involuntary tendency of speakers to increase their vocal effort when speaking in loud noise to enhance the audibility of their voice. This change includes not only loudness but also other acoustic features such as pitch, rate, and duration of syllables. This compensation effect maintains the auditory signal-to-noise ratio of the speaker's spoken words.
The effect links to the needs of effective communication, as there is a reduced effect when words are repeated or lists are read where communication intelligibility is not important. Since the effect is involuntary it is used as a means to detect malingering in those simulating hearing loss. Research on birds and monkeys find that the effect also occurs in the vocalizations of animals.
The effect was discovered in 1909 by Étienne Lombard, a French otolaryngologist.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Wikipedia},
	month = jul,
	year = {2022},
	note = {Page Version ID: 1100217835},
}

@article{lane_lombard_1971,
	title = {The {Lombard} {Sign} and the {Role} of {Hearing} in {Speech}},
	volume = {14},
	issn = {0022-4685},
	url = {http://pubs.asha.org/doi/10.1044/jshr.1404.677},
	doi = {10.1044/jshr.1404.677},
	abstract = {Lombard noted in 1911 that a speaker changes his voice level similarly when the ambient noise level increases, on the one hand, and when the level at which he hears his own voice (his sidetone) decreases, on the other. We can now state the form of these two functions, show that they are related to each other and to the equal-sensation function for imitating speech or noise loudness, and account for their form in terms of the underlying sensory scales and the hypothesis that the speaker tries to maintain a speech-to-noise ratio favorable for communication.
            Perturbations in the timing and spectrum of sidetone also lead the speaker to compensate for the apparent deterioration in his intelligibility. Such compensations reflect direct and indirect audience control of speech, rather than its autoregulation by sidetone. When not harassed by prying experimenters or an unfavorable acoustic environment, the speaker need no more listen to himself while speaking than he need speak to himself while listening.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech and Hearing Research},
	author = {Lane, Harlan and Tranel, Bernard},
	month = dec,
	year = {1971},
	pages = {677--709},
}

@article{lane_lombard_1971-1,
	title = {The {Lombard} {Sign} and the {Role} of {Hearing} in {Speech}},
	volume = {14},
	issn = {0022-4685},
	url = {http://pubs.asha.org/doi/10.1044/jshr.1404.677},
	doi = {10.1044/jshr.1404.677},
	abstract = {Lombard noted in 1911 that a speaker changes his voice level similarly when the ambient noise level increases, on the one hand, and when the level at which he hears his own voice (his sidetone) decreases, on the other. We can now state the form of these two functions, show that they are related to each other and to the equal-sensation function for imitating speech or noise loudness, and account for their form in terms of the underlying sensory scales and the hypothesis that the speaker tries to maintain a speech-to-noise ratio favorable for communication.
            Perturbations in the timing and spectrum of sidetone also lead the speaker to compensate for the apparent deterioration in his intelligibility. Such compensations reflect direct and indirect audience control of speech, rather than its autoregulation by sidetone. When not harassed by prying experimenters or an unfavorable acoustic environment, the speaker need no more listen to himself while speaking than he need speak to himself while listening.},
	language = {en},
	number = {4},
	urldate = {2022-11-05},
	journal = {Journal of Speech and Hearing Research},
	author = {Lane, Harlan and Tranel, Bernard},
	month = dec,
	year = {1971},
	pages = {677--709},
}

@article{derryberry_singing_2020,
	title = {Singing in a silent spring: {Birds} respond to a half-century soundscape reversion during the {COVID}-19 shutdown},
	volume = {370},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Singing in a silent spring},
	url = {https://www.science.org/doi/10.1126/science.abd5777},
	doi = {10.1126/science.abd5777},
	abstract = {Songbirds reclaim favored frequencies
            
              When severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic lockdowns were instituted across entire countries, human activities ceased in an unprecedented way. Derryberry
              et al.
              found that the reduction in traffic sound in the San Francisco Bay Area of California to levels not seen for half a century led to a shift in song frequency in white-crowned sparrows (see the Perspective by Halfwerk). This shift was especially notable because the frequency of human-produced traffic noise occurs within a range that interferes with the highest performance and most effective song. Thus, our “quiet” allowed the birds to quickly fill the most effective song space.
            
            
              Science
              , this issue p.
              575
              ; see also p.
              523
            
          , 
            Reductions in noise pollution during the pandemic shutdown allowed for more effective song production in white-crowned sparrows.
          , 
            Actions taken to control the coronavirus disease 2019 (COVID-19) pandemic have conspicuously reduced motor vehicle traffic, potentially alleviating auditory pressures on animals that rely on sound for survival and reproduction. Here, by comparing soundscapes and songs across the San Francisco Bay Area before and during the recent statewide shutdown, we evaluated whether a common songbird responsively exploited newly emptied acoustic space. We show that noise levels in urban areas were substantially lower during the shutdown, characteristic of traffic in the mid-1950s. We also show that birds responded by producing higher performance songs at lower amplitudes, effectively maximizing communication distance and salience. These findings illustrate that behavioral traits can change rapidly in response to newly favorable conditions, indicating an inherent resilience to long-standing anthropogenic pressures such as noise pollution.},
	language = {en},
	number = {6516},
	urldate = {2022-11-05},
	journal = {Science},
	author = {Derryberry, Elizabeth P. and Phillips, Jennifer N. and Derryberry, Graham E. and Blum, Michael J. and Luther, David},
	month = oct,
	year = {2020},
	pages = {575--579},
}

@article{summers_effects_1988,
	title = {Effects of noise on speech production: {Acoustic} and perceptual analyses},
	volume = {84},
	issn = {0001-4966},
	shorttitle = {Effects of noise on speech production},
	url = {http://asa.scitation.org/doi/10.1121/1.396660},
	doi = {10.1121/1.396660},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Summers, W. Van and Pisoni, David B. and Bernacki, Robert H. and Pedlow, Robert I. and Stokes, Michael A.},
	month = sep,
	year = {1988},
	pages = {917--928},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\HVCCM58H\\Summers et al. - 1988 - Effects of noise on speech production Acoustic an.pdf:application/pdf},
}

@article{summers_effects_1988-1,
	title = {Effects of noise on speech production: {Acoustic} and perceptual analyses},
	volume = {84},
	issn = {0001-4966},
	shorttitle = {Effects of noise on speech production},
	url = {http://asa.scitation.org/doi/10.1121/1.396660},
	doi = {10.1121/1.396660},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Summers, W. Van and Pisoni, David B. and Bernacki, Robert H. and Pedlow, Robert I. and Stokes, Michael A.},
	month = sep,
	year = {1988},
	pages = {917--928},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\P8ZUXZP7\\Summers et al. - 1988 - Effects of noise on speech production Acoustic an.pdf:application/pdf},
}

@article{manabe_control_1998,
	title = {Control of vocal intensity in budgerigars ( \textit{{Melopsittacus} undulatus} ): {Differential} reinforcement of vocal intensity and the {Lombard} effect},
	volume = {103},
	issn = {0001-4966},
	shorttitle = {Control of vocal intensity in budgerigars ( \textit{{Melopsittacus} undulatus} )},
	url = {http://asa.scitation.org/doi/10.1121/1.421227},
	doi = {10.1121/1.421227},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Manabe, Kazuchika and Sadr, Ehsanee I. and Dooling, Robert J.},
	month = feb,
	year = {1998},
	pages = {1190--1198},
}

@article{brumm_causes_2004,
	title = {Causes and consequences of song amplitude adjustment in a territorial bird: a case study in nightingales},
	volume = {76},
	issn = {0001-3765},
	shorttitle = {Causes and consequences of song amplitude adjustment in a territorial bird},
	url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0001-37652004000200017&lng=en&tlng=en},
	doi = {10.1590/S0001-37652004000200017},
	abstract = {Vocal amplitude, one of the crucial factors for the exchange of acoustic signals, has been neglected in studies of animal communication, but recent studies on song variation in Common Nightingales Luscinia megarhynchos have revealed new insights into its importance in the singing behavior of territorial birds. In nightingales song amplitude is not maximized per se, but is individually regulated according to the level of masking background noise. Also, birds adjust their vocal intensity according to social variables, as in male-male interactions. Moreover, during such interactions, males exploited the directionality of their songs to broadcast them in the direction of the intended receivers ensuring the most effective signal transmission. Studies of the development of this typical long-range signaling suggest that sound level is highly interrelated with overall developmental progression and learning, and thus should be viewed as an integral part of song ontogeny. I conclude that song amplitude is a dynamic feature of the avian signal system, which is individually regulated according to the ecological demands of signal transmission and the social context of communication.
          , 
            A amplitude vocal, um dos fatores cruciais para a troca de sinais acústicos, tem sido negligenciada nos estudos da comunicação animal, mas trabalhos recentes sobre a variação do canto do Rouxinol-comum Luscinia megarhynchos evidenciaram sua importância no comportamento de canto das aves territoriais. No rouxinol a amplitude do canto não é aumentada ao máximo per se, mas é regulada individualmente de acordo com o nível de ruído de fundo que mascara o sinal. As aves também ajustam sua intensidade vocal às variáveis sociais, tais como nas interações entre machos. Além disso, durante essas interações, os machos tiram proveito da direcionalidade de seus cantos para emiti-los em direção aos receptores desejados no intuito de garantir a mais eficiente transmissão do sinal. Estudos do desenvolvimento desta sinalização típica de longo alcance sugerem que o nível sonoro seja altamente relacionado com o desenvolvimento geral e a aprendizagem, e deveria portanto ser visto como parte integrante da ontogenia do canto. Concluímos que a amplitude do canto é um parâmetro dinâmico do sistema de sinalização em aves, que é regulado individualmente de acordo com as exigências ecológicas da transmissão do sinal e o contexto social da comunicação.},
	number = {2},
	urldate = {2022-11-05},
	journal = {Anais da Academia Brasileira de Ciências},
	author = {Brumm, Henrik},
	month = jun,
	year = {2004},
	pages = {289--295},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TL5VXJ8V\\Brumm - 2004 - Causes and consequences of song amplitude adjustme.pdf:application/pdf},
}

@article{sinnott_regulation_1975,
	title = {Regulation of voice amplitude by the monkey},
	volume = {58},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.380685},
	doi = {10.1121/1.380685},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Sinnott, Joan M. and Stebbins, William C. and Moody, David B.},
	month = aug,
	year = {1975},
	pages = {412--414},
}

@article{patel_influence_2008,
	title = {The {Influence} of {Linguistic} {Content} on the {Lombard} {Effect}},
	volume = {51},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282008/016%29},
	doi = {10.1044/1092-4388(2008/016)},
	abstract = {Purpose
              
                The
                Lombard effect
                describes the tendency for speakers to increase pitch, intensity, and duration in the presence of noise. It is unclear whether these modifications are uniformly applied across all words within an utterance or whether information-bearing content words are further enhanced compared with function words. In the present study, the authors investigated the influence of linguistic content on acoustic modifications made to speech in noise.
              
            
            
              Method
              
                Sixteen speaker–listener pairs engaged in an interactive cooperative game in quiet, 60 dB of multitalker noise, and 90 dB of multitalker noise. Speaker productions were analyzed to examine differences in fundamental frequency (F
                0
                ), intensity, and duration of target words in sentences across noise conditions.
              
            
            
              Results
              
                Proportional increases in F
                0,
                intensity, and duration were noted for all word types as noise increased from quiet to 60 dB. From quiet to 90 dB, content words that referred to agents, objects, and locations were disproportionately elongated compared with function words. Additionally, agents were further enhanced by increased F
                0
                .
              
            
            
              Conclusions
              
                At moderate noise levels, most word types appear to be uniformly boosted in F
                0
                , intensity, and duration. As noise increases, linguistic content shapes the extent of the Lombard effect, with F
                0
                and duration serving as primary cues for marking information-bearing word types.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Patel, Rupal and Schell, Kevin W.},
	month = feb,
	year = {2008},
	pages = {209--220},
}

@article{winkworth_speech_1997,
	title = {Speech {Breathing} and the {Lombard} {Effect}},
	volume = {40},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jslhr.4001.159},
	doi = {10.1044/jslhr.4001.159},
	abstract = {Respiratory measurements were made using linearized magnetometers placed antero-posteriorly over the rib cages and abdomens of five healthy young women. Background noise was introduced over headphones simultaneously as "babble" presented binaurally at 55 dB ("moderate noise") and 70 dB ("high noise"). Speech during oral reading and spontaneous monologue was transduced with a microphone positioned near the lips, from which a speaking intensity signal (dBA) was derived. Subjects were instructed to speak during the noise conditions, but no instruction was given to alter speaking intensity. Compared with a "no noise" condition, the speaking intensities of all the subjects increased significantly for both speech tasks in the moderate and high noise conditions, thereby replicating the well-documented Lombard effect. No consistent trend of lung volume change was observed, in contrast to the linear increases in speech intensity as the noise level increased. For the higher speech intensities during the moderate and high noise conditions both initiation and termination lung volumes either increased or decreased. These preliminary findings suggest that when speech intensity is increased following the introduction of noise via headphones rather than by specific instructions to speak more loudly, speakers employ variable lung volume strategies for intensity control.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Winkworth, Alison L. and Davis, Pamela J.},
	month = feb,
	year = {1997},
	pages = {159--169},
}

@article{vatikiotisbateson_auditory_2006,
	title = {Auditory, but perhaps not visual, processing of {Lombard} speech},
	volume = {119},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4786950},
	doi = {10.1121/1.4786950},
	language = {en},
	number = {5},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Vatikiotis‐Bateson, Eric and Chung, Victor and Lutz, Kevin and Mirante, Nicole and Otten, Jolien and Tan, Johanna},
	month = may,
	year = {2006},
	pages = {3444--3444},
}

@article{pick_inhibiting_1989,
	title = {Inhibiting the {Lombard} effect},
	volume = {85},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.397561},
	doi = {10.1121/1.397561},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Pick, Herbert L. and Siegel, Gerald M. and Fox, Paul W. and Garber, Sharon R. and Kearney, Joseph K.},
	month = feb,
	year = {1989},
	pages = {894--900},
}

@inproceedings{choudhury_comparitive_2015,
	address = {Silchar, India},
	title = {A comparitive study on classifiers to classify languages into {Tonal} and {Non}-{Tonal} {Languages}},
	isbn = {978-1-4673-6707-3 978-1-4673-6708-0},
	url = {http://ieeexplore.ieee.org/document/7377329/},
	doi = {10.1109/ISACC.2015.7377329},
	urldate = {2022-11-05},
	booktitle = {2015 {International} {Symposium} on {Advanced} {Computing} and {Communication} ({ISACC})},
	publisher = {IEEE},
	author = {Choudhury, Biplav and Choudhury, Tameem Salman},
	month = sep,
	year = {2015},
	pages = {132--135},
}

@incollection{ide_bruce_2000,
	address = {Dordrecht},
	title = {Bruce, {Pierrehumbert}, and the {Elements} of {Intonational} {Phonology}},
	volume = {14},
	isbn = {978-90-481-5562-0 978-94-015-9413-4},
	url = {http://link.springer.com/10.1007/978-94-015-9413-4_3},
	urldate = {2022-11-05},
	booktitle = {Prosody: {Theory} and {Experiment}},
	publisher = {Springer Netherlands},
	author = {Ladd, D. Robert},
	editor = {Ide, Nancy and Véronis, Jean and Horne, Merle},
	year = {2000},
	doi = {10.1007/978-94-015-9413-4_3},
	note = {Series Title: Text, Speech and Language Technology},
	pages = {37--50},
}

@article{werker_cross-language_1984,
	title = {Cross-language speech perception: {Evidence} for perceptual reorganization during the first year of life},
	volume = {7},
	issn = {01636383},
	shorttitle = {Cross-language speech perception},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0163638384800223},
	doi = {10.1016/S0163-6383(84)80022-3},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {Infant Behavior and Development},
	author = {Werker, Janet F. and Tees, Richard C.},
	month = jan,
	year = {1984},
	pages = {49--63},
}

@article{greenberg_temporal_2003,
	title = {Temporal properties of spontaneous speech—a syllable-centric perspective},
	volume = {31},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447003000603},
	doi = {10.1016/j.wocn.2003.09.005},
	language = {en},
	number = {3-4},
	urldate = {2022-11-05},
	journal = {Journal of Phonetics},
	author = {Greenberg, Steven and Carvey, Hannah and Hitchcock, Leah and Chang, Shuangyu},
	month = jul,
	year = {2003},
	pages = {465--485},
}

@incollection{van_oostendorp_stress-timed_2011,
	address = {Oxford, UK},
	title = {Stress-{Timed} \textit{vs} . {Syllable}-{Timed} {Languages}: {Stress}-timed \textit{vs} . {Syllable}-timed {Languages}},
	isbn = {978-1-4443-3526-2},
	shorttitle = {Stress-{Timed} \textit{vs} . {Syllable}-{Timed} {Languages}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781444335262.wbctp0048},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {The {Blackwell} {Companion} to {Phonology}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Nespor, Marina and Shukla, Mohinish and Mehler, Jacques},
	editor = {van Oostendorp, Marc and Ewen, Colin J. and Hume, Elizabeth and Rice, Keren},
	month = apr,
	year = {2011},
	doi = {10.1002/9781444335262.wbctp0048},
	pages = {1--13},
}

@book{pike_intonation_1963,
	series = {Linguistics},
	title = {The {Intonation} of {American} {English}},
	url = {https://books.google.com.pk/books?id=Jz3iAAAAMAAJ},
	publisher = {University of Michigan Press},
	author = {Pike, K.L.},
	year = {1963},
	lccn = {45004529},
}

@inproceedings{adda-decker_m_reconnaissance_2006,
	title = {De la reconnaissance automatique de la parole `a l’analyse linguistique de corpus oraux},
	author = {Adda-Decker, M},
	year = {2006},
}

@article{avendano_properties_1998,
	title = {On the {Properties} of {Temporal} {Processing} for {Speech} in {Adverse} {Environments}},
	url = {https://www.researchgate.net/publication/2466964_Carlos_Avendano_and_Hynek_Hermansky_On_the_Properties_of_Temporal_Processing_for_Speech_in_Adverse_Environments_On_the_Properties_of_Temporal_Processing_for_Speech_in_Adverse_Environments},
	abstract = {In this paper we report on the results that we have obtained in the application of temporal processing to speech signals. We describe what are the properties that make temporal processing an interesting and useful technique to alleviate the harmful effects that environmental factors have on speech. Though temporal processing has been used in the past, its analysis and properties have not been studied in detail. We summarize some results that we obtained in a detailed analysis, and describe a data-driven design technique to design the processing. We demonstrate a speech enhancementsystem which illustrates some properties, advantages, and short-comings of the technique. 1. Introduction The performance of speech communication systems often degrades under realistic environmental conditions. Adverse environmental factors include additive noise sources, room reverberation, and transmission channel distortions. This work discusses the processing of speech in the temporal-feature or modulati...},
	author = {Avendaño, Carlos and Hermansky, Hynek},
	month = sep,
	year = {1998},
}

@inproceedings{melot_analysis_2015,
	address = {Scottsdale, AZ, USA},
	title = {Analysis of factors affecting system performance in the {ASpIRE} challenge},
	isbn = {978-1-4799-7291-3},
	url = {http://ieeexplore.ieee.org/document/7404838/},
	doi = {10.1109/ASRU.2015.7404838},
	urldate = {2022-11-05},
	booktitle = {2015 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU})},
	publisher = {IEEE},
	author = {Melot, Jennifer and Malyska, Nicolas and Ray, Jessica and Shen, Wade},
	month = dec,
	year = {2015},
	pages = {512--517},
}

@misc{noauthor_literature_nodate,
	title = {Literature {Review} - {Speech} {Recognition} for {Noisy} {Environments}},
	url = {https://users.dcc.uchile.cl/~abassi/WWW/Voz/speech-recog.html},
	urldate = {2022-11-08},
	file = {Literature Review - Speech Recognition for Noisy Environments:C\:\\Users\\DELL\\Zotero\\storage\\66WTUREM\\speech-recog.html:text/html},
}

@article{endres_voice_1971,
	title = {Voice {Spectrograms} as a {Function} of {Age}, {Voice} {Disguise}, and {Voice} {Imitation}},
	volume = {49},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1912589},
	doi = {10.1121/1.1912589},
	language = {en},
	number = {6B},
	urldate = {2022-11-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Endres, W. and Bambach, W. and Flösser, G.},
	month = jun,
	year = {1971},
	pages = {1842--1848},
}

@incollection{muller_study_2007,
	address = {Berlin, Heidelberg},
	title = {A {Study} of {Acoustic} {Correlates} of {Speaker} {Age}},
	volume = {4441},
	isbn = {978-3-540-74121-3 978-3-540-74122-0},
	url = {http://link.springer.com/10.1007/978-3-540-74122-0_1},
	language = {en},
	urldate = {2022-11-10},
	booktitle = {Speaker {Classification} {II}},
	publisher = {Springer Berlin Heidelberg},
	author = {Schötz, Susanne and Müller, Christian},
	editor = {Müller, Christian},
	year = {2007},
	doi = {10.1007/978-3-540-74122-0_1},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {1--9},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\WBR85E2H\\Schötz and Müller - 2007 - A Study of Acoustic Correlates of Speaker Age.pdf:application/pdf},
}

@inproceedings{gillick_statistical_1989,
	address = {Glasgow, UK},
	title = {Some statistical issues in the comparison of speech recognition algorithms},
	url = {http://ieeexplore.ieee.org/document/266481/},
	doi = {10.1109/ICASSP.1989.266481},
	urldate = {2022-11-10},
	booktitle = {International {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Gillick, L. and Cox, S.J.},
	year = {1989},
	pages = {532--535},
}

@inproceedings{aleksic_improved_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Improved recognition of contact names in voice commands},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178957/},
	doi = {10.1109/ICASSP.2015.7178957},
	urldate = {2022-11-10},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Aleksic, Petar and Allauzen, Cyril and Elson, David and Kracun, Aleksandar and Casado, Diego Melendo and Moreno, Pedro J.},
	month = apr,
	year = {2015},
	pages = {5172--5175},
}

@article{isaacs_rater_2013,
	title = {Rater {Experience}, {Rating} {Scale} {Length}, and {Judgments} of {L2} {Pronunciation}: {Revisiting} {Research} {Conventions}},
	volume = {10},
	issn = {1543-4303, 1543-4311},
	shorttitle = {Rater {Experience}, {Rating} {Scale} {Length}, and {Judgments} of {L2} {Pronunciation}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/15434303.2013.769545},
	doi = {10.1080/15434303.2013.769545},
	language = {en},
	number = {2},
	urldate = {2022-11-10},
	journal = {Language Assessment Quarterly},
	author = {Isaacs, Talia and Thomson, Ron I.},
	month = apr,
	year = {2013},
	pages = {135--159},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\D4D585T7\\Isaacs and Thomson - 2013 - Rater Experience, Rating Scale Length, and Judgmen.pdf:application/pdf},
}

@article{saito_second_2016,
	title = {Second language speech production: {Investigating} linguistic correlates of comprehensibility and accentedness for learners at different ability levels},
	volume = {37},
	issn = {0142-7164, 1469-1817},
	shorttitle = {Second language speech production},
	url = {https://www.cambridge.org/core/product/identifier/S0142716414000502/type/journal_article},
	doi = {10.1017/S0142716414000502},
	abstract = {ABSTRACT
            The current project aimed to investigate the potentially different linguistic correlates of comprehensibility (i.e., ease of understanding) and accentedness (i.e., linguistic nativelikeness) in adult second language (L2) learners’ extemporaneous speech production. Timed picture descriptions from 120 beginner, intermediate, and advanced Japanese learners of English were analyzed using native speaker global judgments based on learners’ comprehensibility and accentedness, and then submitted to segmental, prosodic, temporal, lexical, and grammatical analyses. Results showed that comprehensibility was related to all linguistic domains, and accentedness was strongly tied with pronunciation (specifically segmentals) rather than lexical and grammatical domains. In particular, linguistic correlates of L2 comprehensibility and accentedness were found to vary by learners’ proficiency levels. In terms of comprehensibility, optimal rate of speech, appropriate and rich vocabulary use, and adequate and varied prosody were important for beginner to intermediate levels, whereas segmental accuracy, good prosody, and correct grammar featured strongly for intermediate to advanced levels. For accentedness, grammatical complexity was a feature of intermediate to high-level performance, whereas segmental and prosodic variables were essential to accentedness across all levels. These findings suggest that syllabi tailored to learners’ proficiency level (beginner, intermediate, or advanced) and learning goal (comprehensibility or nativelike accent) would be advantageous for the teaching of L2 speaking.},
	language = {en},
	number = {2},
	urldate = {2022-11-10},
	journal = {Applied Psycholinguistics},
	author = {Saito, Kazuya and Trofimovich, Pavel and Isaacs, Talia},
	month = mar,
	year = {2016},
	pages = {217--240},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\VPJLHZP5\\Saito et al. - 2016 - Second language speech production Investigating l.pdf:application/pdf},
}

@inproceedings{chuchual_inbound_2010,
	address = {Tokyo, Japan},
	title = {Inbound and outbound calls assignment for an efficient call center},
	isbn = {978-1-4244-6485-2},
	url = {http://ieeexplore.ieee.org/document/5530196/},
	doi = {10.1109/ICSSSM.2010.5530196},
	urldate = {2022-11-13},
	booktitle = {2010 7th {International} {Conference} on {Service} {Systems} and {Service} {Management}},
	publisher = {IEEE},
	author = {Chuchual, Panichapat and Chongpravatisakul, Narissa and Kusolmanomai, Teerasarn and Komolavanij, Somrote},
	month = jun,
	year = {2010},
	pages = {1--4},
}

@inproceedings{liston_simulation_2017,
	address = {Las Vegas, NV},
	title = {Simulation based decision support for contact centers},
	isbn = {978-1-5386-3428-8},
	url = {http://ieeexplore.ieee.org/document/8248217/},
	doi = {10.1109/WSC.2017.8248217},
	urldate = {2022-11-13},
	booktitle = {2017 {Winter} {Simulation} {Conference} ({WSC})},
	publisher = {IEEE},
	author = {Liston, Paul and Byrne, James and Keogh, Orla and Byrne, P.J. and Bourke, Joe and Jones, Karl},
	month = dec,
	year = {2017},
	pages = {4586--4587},
}

@inproceedings{nwobodo-anyadiegwu_evaluating_2018,
	address = {Singapore},
	title = {Evaluating variables that affect job satisfaction of bank customer contact centre agents in {South} {Africa}},
	isbn = {978-1-5386-5747-8 978-1-5386-5748-5},
	url = {https://ieeexplore.ieee.org/document/8387081/},
	doi = {10.1109/IEA.2018.8387081},
	urldate = {2022-11-13},
	booktitle = {2018 5th {International} {Conference} on {Industrial} {Engineering} and {Applications} ({ICIEA})},
	publisher = {IEEE},
	author = {Nwobodo-Anyadiegwu, Eveth and Mbohwa, Charles and Ndlovu, Nokukhanya},
	month = apr,
	year = {2018},
	pages = {116--121},
}

@inproceedings{liu_research_2012,
	address = {Hangzhou, China},
	title = {Research on {Forecasting} {Call} {Center} {Traffic} through {PCA} and {BP} {Artificial} {Neural} {Network}},
	isbn = {978-1-4673-2646-9},
	url = {http://ieeexplore.ieee.org/document/6407017/},
	doi = {10.1109/ISCID.2012.117},
	urldate = {2022-11-13},
	booktitle = {2012 {Fifth} {International} {Symposium} on {Computational} {Intelligence} and {Design}},
	publisher = {IEEE},
	author = {Liu, Tao and Liu, Lieli},
	month = oct,
	year = {2012},
	pages = {444--447},
}

@inproceedings{archawaporn_erlang_2013,
	address = {Nakorn Pathom, Thailand},
	title = {Erlang {C} model for evaluate incoming call uncertainty in automotive call centers},
	isbn = {978-1-4673-5324-3 978-1-4673-5322-9},
	url = {http://ieeexplore.ieee.org/document/6694762/},
	doi = {10.1109/ICSEC.2013.6694762},
	urldate = {2022-11-13},
	booktitle = {2013 {International} {Computer} {Science} and {Engineering} {Conference} ({ICSEC})},
	publisher = {IEEE},
	author = {Archawaporn, Laksamon and Wongseree, Waranyu},
	month = sep,
	year = {2013},
	pages = {109--113},
}

@inproceedings{nguyen_development_2017,
	address = {Seoul},
	title = {Development of a {Vietnamese} speech recognition system for {Viettel} call center},
	isbn = {978-1-5386-3333-5},
	url = {https://ieeexplore.ieee.org/document/8384456/},
	doi = {10.1109/ICSDA.2017.8384456},
	urldate = {2022-11-17},
	booktitle = {2017 20th {Conference} of the {Oriental} {Chapter} of the {International} {Coordinating} {Committee} on {Speech} {Databases} and {Speech} {I}/{O} {Systems} and {Assessment} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Nguyen, Quoc Bao and Do, Van Hai and Dam, Ba Quyen and Le, Minh Hung},
	month = nov,
	year = {2017},
	pages = {1--5},
}

@inproceedings{do_agentclient_2020,
	address = {Kuala Lumpur, Malaysia},
	title = {Agent/{Client} {Speech} {Identification} for {Mixed}-{Channel} {Conversation} in {Customer} {Service} {Call} {Centers}},
	isbn = {978-1-72817-689-5},
	url = {https://ieeexplore.ieee.org/document/9310469/},
	doi = {10.1109/IALP51396.2020.9310469},
	urldate = {2022-11-17},
	booktitle = {2020 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	publisher = {IEEE},
	author = {Do, Van Hai and Mai, Van Tuan},
	month = dec,
	year = {2020},
	pages = {197--200},
}

@inproceedings{min_tang_call-type_2003,
	address = {St Thomas, VI, USA},
	title = {Call-type classification and unsupervised training for the call center domain},
	isbn = {978-0-7803-7980-0},
	url = {http://ieeexplore.ieee.org/document/1318429/},
	doi = {10.1109/ASRU.2003.1318429},
	urldate = {2022-11-17},
	booktitle = {2003 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({IEEE} {Cat}. {No}.{03EX721})},
	publisher = {IEEE},
	author = {{Min Tang} and Pellom, B. and Hacioglu, K.},
	year = {2003},
	pages = {204--208},
}

@inproceedings{galanis_classification_2013,
	address = {Budapest, Hungary},
	title = {Classification of emotional speech units in call centre interactions},
	isbn = {978-1-4799-1546-0 978-1-4799-1543-9},
	url = {http://ieeexplore.ieee.org/document/6719279/},
	doi = {10.1109/CogInfoCom.2013.6719279},
	urldate = {2022-11-17},
	booktitle = {2013 {IEEE} 4th {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
	publisher = {IEEE},
	author = {Galanis, Dimitrios and Karabetsos, Sotiris and Koutsombogera, Maria and Papageorgiou, Harris and Esposito, Anna and Riviello, Maria-Teresa},
	month = dec,
	year = {2013},
	pages = {403--406},
}

@inproceedings{draman_malay_2017,
	address = {Malacca City},
	title = {Malay speech corpus of telecommunication call center preparation for {ASR}},
	isbn = {978-1-5090-4912-7},
	url = {https://ieeexplore.ieee.org/document/8074675/},
	doi = {10.1109/ICoICT.2017.8074675},
	urldate = {2022-11-17},
	booktitle = {2017 5th {International} {Conference} on {Information} and {Communication} {Technology} ({ICoIC7})},
	publisher = {IEEE},
	author = {Draman, M. and Tee, D.C. and Lambak, Z. and Yahya, M.R. and Mohd Yusoff, M.I. and Ibrahim, S.H. and Saidon, S. and Abu Haris, N. and Tan, T.P.},
	month = may,
	year = {2017},
	pages = {1--6},
}

@inproceedings{deschamps-berger_end--end_2021,
	address = {Nara, Japan},
	title = {End-to-{End} {Speech} {Emotion} {Recognition}: {Challenges} of {Real}-{Life} {Emergency} {Call} {Centers} {Data} {Recordings}},
	isbn = {978-1-66540-019-0},
	shorttitle = {End-to-{End} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9597419/},
	doi = {10.1109/ACII52823.2021.9597419},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo and Lamel, Lori and Devillers, Laurence},
	month = sep,
	year = {2021},
	pages = {1--8},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\EQSN7492\\Deschamps-Berger et al. - 2021 - End-to-End Speech Emotion Recognition Challenges .pdf:application/pdf},
}

@inproceedings{zweig_automated_2006,
	address = {Toulouse, France},
	title = {Automated {Quality} {Monitoring} in the {Call} {Center} with {ASR} and {Maximum} {Entropy}},
	volume = {1},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1660089/},
	doi = {10.1109/ICASSP.2006.1660089},
	urldate = {2022-11-17},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Zweig, G. and Siohan, O. and Saon, G. and Ramabhadran, B. and Povey, D. and Mangu, L. and Kingsbury, B.},
	year = {2006},
	pages = {I--589--I--592},
}

@inproceedings{tarjan_n-gram_2019,
	address = {Naples, Italy},
	title = {N-gram {Approximation} of {LSTM} {Recurrent} {Language} {Models} for {Single}-pass {Recognition} of {Hungarian} {Call} {Center} {Conversations}},
	isbn = {978-1-72814-793-2},
	url = {https://ieeexplore.ieee.org/document/9089959/},
	doi = {10.1109/CogInfoCom47531.2019.9089959},
	urldate = {2022-11-17},
	booktitle = {2019 10th {IEEE} {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
	publisher = {IEEE},
	author = {Tarjan, Balazs and Szaszak, Gyorgy and Fegyo, Tibor and Mihajlik, Peter},
	month = oct,
	year = {2019},
	pages = {131--136},
}

@inproceedings{tarjan_improved_2013,
	address = {Cluj-Napoca, Romania},
	title = {Improved recognition of {Hungarian} call center conversations},
	isbn = {978-1-4799-1065-6 978-1-4799-1063-2},
	url = {http://ieeexplore.ieee.org/document/6682652/},
	doi = {10.1109/SpeD.2013.6682652},
	urldate = {2022-11-17},
	booktitle = {2013 7th {Conference} on {Speech} {Technology} and {Human} - {Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Tarjan, Balazs and Sarosi, Gellert and Fegyo, Tibor and Mihajlik, Peter},
	month = oct,
	year = {2013},
	pages = {1--6},
}

@article{plaza_call_2021,
	title = {Call {Transcription} {Methodology} for {Contact} {Center} {Systems}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9508438/},
	doi = {10.1109/ACCESS.2021.3102502},
	urldate = {2022-11-17},
	journal = {IEEE Access},
	author = {Plaza, Miroslaw and Pawlik, Lukasz and Deniziak, Stanislaw},
	year = {2021},
	pages = {110975--110988},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\JM7QLB6V\\Plaza et al. - 2021 - Call Transcription Methodology for Contact Center .pdf:application/pdf},
}

@article{fan_gated_2021,
	title = {Gated {Recurrent} {Fusion} {With} {Joint} {Training} {Framework} for {Robust} {End}-to-{End} {Speech} {Recognition}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9265262/},
	doi = {10.1109/TASLP.2020.3039600},
	urldate = {2022-11-17},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Fan, Cunhang and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Liu, Bin and Wen, Zhengqi},
	year = {2021},
	pages = {198--209},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\36B4AJ37\\Fan et al. - 2021 - Gated Recurrent Fusion With Joint Training Framewo.pdf:application/pdf},
}

@inproceedings{deschamps-berger_end--end_2021-1,
	address = {Nara, Japan},
	title = {End-to-{End} {Speech} {Emotion} {Recognition}: {Challenges} of {Real}-{Life} {Emergency} {Call} {Centers} {Data} {Recordings}},
	isbn = {978-1-66540-019-0},
	shorttitle = {End-to-{End} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9597419/},
	doi = {10.1109/ACII52823.2021.9597419},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo and Lamel, Lori and Devillers, Laurence},
	month = sep,
	year = {2021},
	pages = {1--8},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\DJESTVXN\\Deschamps-Berger et al. - 2021 - End-to-End Speech Emotion Recognition Challenges .pdf:application/pdf},
}

@inproceedings{chunwijitra_improving_2021,
	address = {Chiang Mai, Thailand},
	title = {Improving automatic transcription of call center speech using data simulation},
	isbn = {978-1-66540-382-5},
	url = {https://ieeexplore.ieee.org/document/9454803/},
	doi = {10.1109/ECTI-CON51831.2021.9454803},
	urldate = {2022-11-17},
	booktitle = {2021 18th {International} {Conference} on {Electrical} {Engineering}/{Electronics}, {Computer}, {Telecommunications} and {Information} {Technology} ({ECTI}-{CON})},
	publisher = {IEEE},
	author = {Chunwijitra, Vataya and Kurpukdee, Nattapong},
	month = may,
	year = {2021},
	pages = {842--845},
}

@inproceedings{deschamps-berger_emotion_2021,
	address = {Nara, Japan},
	title = {Emotion {Recognition} {In} {Emergency} {Call} {Centers}: {The} challenge of real-life emotions},
	isbn = {978-1-66540-021-3},
	shorttitle = {Emotion {Recognition} {In} {Emergency} {Call} {Centers}},
	url = {https://ieeexplore.ieee.org/document/9666308/},
	doi = {10.1109/ACIIW52867.2021.9666308},
	urldate = {2022-11-17},
	booktitle = {2021 9th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} {Workshops} and {Demos} ({ACIIW})},
	publisher = {IEEE},
	author = {Deschamps-Berger, Theo},
	month = sep,
	year = {2021},
	pages = {1--5},
}

@inproceedings{seknedy_speech_2021,
	address = {Cairo, Egypt},
	title = {Speech {Emotion} {Recognition} {System} for {Human} {Interaction} {Applications}},
	isbn = {978-1-66544-076-9},
	url = {https://ieeexplore.ieee.org/document/9694246/},
	doi = {10.1109/ICICIS52592.2021.9694246},
	urldate = {2022-11-17},
	booktitle = {2021 {Tenth} {International} {Conference} on {Intelligent} {Computing} and {Information} {Systems} ({ICICIS})},
	publisher = {IEEE},
	author = {Seknedy, Mai El and Fawzi, Sahar},
	month = dec,
	year = {2021},
	pages = {361--368},
}

@article{bai_fast_2021,
	title = {Fast {End}-to-{End} {Speech} {Recognition} {Via} {Non}-{Autoregressive} {Models} and {Cross}-{Modal} {Knowledge} {Transferring} {From} {BERT}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9437636/},
	doi = {10.1109/TASLP.2021.3082299},
	urldate = {2022-11-17},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Wen, Zhengqi and Zhang, Shuai},
	year = {2021},
	pages = {1897--1911},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U8MZQEJI\\Bai et al. - 2021 - Fast End-to-End Speech Recognition Via Non-Autoreg.pdf:application/pdf},
}

@techreport{tits_sector_effect_1993,
	address = {Helsinki},
	title = {Effect of transmission impairments},
	url = {https://www.itu.int/ITU-T/recommendations/rec.aspx?rec=1718&lang=en},
	language = {en},
	urldate = {2022-11-17},
	institution = {International Telecommunications Union},
	author = {T.I.T.S Sector},
	month = mar,
	year = {1993},
	pages = {11},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VRNTN6WT\\rec.html:text/html},
}

@article{huerta_speech_1970,
	title = {Speech {Recognition} {From} {Gsm} {Codec} {Parameters}},
	volume = {4},
	abstract = {Speech coding affects speech recognition performance, with recognition accuracy deteriorating as the coded bit rate decreases. Virtually all systems that recognize coded speech reconstruct the speech waveform from the coded parameters, and then perform recognition (after possible noise and/or channel compensation) using conventional techniques. In this paper we compare the recognition accuracy of coded speech obtained by reconstructing the speech waveform with the speech recognition accuracy obtained when using cepstral features derived from the coding parameters. We focus our efforts on speech that has been coded using the 13-kbps full-rate GSM codec, a Regular Pulse Excited Long Term Prediction (RPE-LTP) codec. The GSM codec develops separate representations for the linear prediction (LPC) filter and the residual signal components of the coded speech. We measure the effects of quantization and coding on the accuracy with which these parameters are represented, and present two differe...},
	author = {Huerta, Juan and Stern, Richard},
	month = feb,
	year = {1970},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\AF6Y34IV\\Huerta and Stern - 1970 - Speech Recognition From Gsm Codec Parameters.pdf:application/pdf},
}

@inproceedings{besacier_effect_2001,
	address = {Cannes, France},
	title = {The effect of speech and audio compression on speech recognition performance},
	isbn = {978-0-7803-7025-8},
	url = {http://ieeexplore.ieee.org/document/962750/},
	doi = {10.1109/MMSP.2001.962750},
	urldate = {2022-11-17},
	booktitle = {2001 {IEEE} {Fourth} {Workshop} on {Multimedia} {Signal} {Processing} ({Cat}. {No}.{01TH8564})},
	publisher = {IEEE},
	author = {Besacier, L. and Bergamini, C. and Vaufreydaz, D. and Castelli, E.},
	year = {2001},
	pages = {301--306},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\CJL6XFAB\\Besacier et al. - 2001 - The effect of speech and audio compression on spee.pdf:application/pdf},
}

@incollection{sojka_robust_2018,
	address = {Cham},
	title = {Robust {Recognition} of {Conversational} {Telephone} {Speech} via {Multi}-condition {Training} and {Data} {Augmentation}},
	volume = {11107},
	isbn = {978-3-030-00793-5 978-3-030-00794-2},
	url = {http://link.springer.com/10.1007/978-3-030-00794-2_35},
	language = {en},
	urldate = {2022-11-17},
	booktitle = {Text, {Speech}, and {Dialogue}},
	publisher = {Springer International Publishing},
	author = {Málek, Jiří and Ždánský, Jindřich and Červa, Petr},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2018},
	doi = {10.1007/978-3-030-00794-2_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {324--333},
}

@inproceedings{vu_audio_2019,
	address = {Lanzhou, China},
	title = {Audio {Codec} {Simulation} based {Data} {Augmentation} for {Telephony} {Speech} {Recognition}},
	isbn = {978-1-72813-248-8},
	url = {https://ieeexplore.ieee.org/document/9023257/},
	doi = {10.1109/APSIPAASC47483.2019.9023257},
	urldate = {2022-11-17},
	booktitle = {2019 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Vu, Thi-Ly and Zeng, Zhiping and Xu, Haihua and Chng, Eng-Siong},
	month = nov,
	year = {2019},
	pages = {198--203},
}

@inproceedings{zeng_end--end_2019,
	title = {On the {End}-to-{End} {Solution} to {Mandarin}-{English} {Code}-{Switching} {Speech} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/zeng19_interspeech.html},
	doi = {10.21437/Interspeech.2019-1429},
	language = {en},
	urldate = {2022-11-17},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Zeng, Zhiping and Khassanov, Yerbolat and Pham, Van Tung and Xu, Haihua and Chng, Eng Siong and Li, Haizhou},
	month = sep,
	year = {2019},
	pages = {2165--2169},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\NGBDPXY8\\Zeng et al. - 2019 - On the End-to-End Solution to Mandarin-English Cod.pdf:application/pdf},
}

@book{neustein_advances_2010,
	address = {Boston, MA},
	title = {Advances in {Speech} {Recognition}},
	isbn = {978-1-4419-5950-8 978-1-4419-5951-5},
	url = {http://link.springer.com/10.1007/978-1-4419-5951-5},
	language = {en},
	urldate = {2022-11-17},
	publisher = {Springer US},
	editor = {Neustein, Amy},
	year = {2010},
	doi = {10.1007/978-1-4419-5951-5},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3U2RL686\\Neustein - 2010 - Advances in Speech Recognition.pdf:application/pdf},
}

@article{valizada_development_2021,
	title = {Development of {Speech} {Recognition} {Systems} in {Emergency} {Call} {Centers}},
	volume = {13},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/13/4/634},
	doi = {10.3390/sym13040634},
	abstract = {In this paper, various methodologies of acoustic and language models, as well as labeling methods for automatic speech recognition for spoken dialogues in emergency call centers were investigated and comparatively analyzed. Because of the fact that dialogue speech in call centers has specific context and noisy, emotional environments, available speech recognition systems show poor performance. Therefore, in order to accurately recognize dialogue speeches, the main modules of speech recognition systems—language models and acoustic training methodologies—as well as symmetric data labeling approaches have been investigated and analyzed. To find an effective acoustic model for dialogue data, different types of Gaussian Mixture Model/Hidden Markov Model (GMM/HMM) and Deep Neural Network/Hidden Markov Model (DNN/HMM) methodologies were trained and compared. Additionally, effective language models for dialogue systems were defined based on extrinsic and intrinsic methods. Lastly, our suggested data labeling approaches with spelling correction are compared with common labeling methods resulting in outperforming the other methods with a notable percentage. Based on the results of the experiments, we determined that DNN/HMM for an acoustic model, trigram with Kneser–Ney discounting for a language model and using spelling correction before training data for a labeling method are effective configurations for dialogue speech recognition in emergency call centers. It should be noted that this research was conducted with two different types of datasets collected from emergency calls: the Dialogue dataset (27 h), which encapsulates call agents’ speech, and the Summary dataset (53 h), which contains voiced summaries of those dialogues describing emergency cases. Even though the speech taken from the emergency call center is in the Azerbaijani language, which belongs to the Turkic group of languages, our approaches are not tightly connected to specific language features. Hence, it is anticipated that suggested approaches can be applied to the other languages of the same group.},
	language = {en},
	number = {4},
	urldate = {2022-11-17},
	journal = {Symmetry},
	author = {Valizada, Alakbar and Akhundova, Natavan and Rustamov, Samir},
	month = apr,
	year = {2021},
	pages = {634},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\RWD8YXGN\\Valizada et al. - 2021 - Development of Speech Recognition Systems in Emerg.pdf:application/pdf},
}

@article{bernstein_recognizing_2000,
	title = {Recognizing {Call}-{Center} {Speech} {Using} {Models} {Trained} {From} {Other} {Domains}},
	abstract = {In this paper, we introduce a new conversational speech task -- recognizing call-center speech -- using data collected from Dragon's own technical support line. We compare performance of models trained from conversational telephone speech (the Switchboard corpus) and models trained from predominantly read, microphone speech, and report on a series of experiments focusing on adapting the microphone speech models to the telephone channel and conversational task. We also discuss the importance of task-specific language model data. We benchmark our test set by comparing the performance of our 1998 Switchboard Evaluation system to that of our simpler call-center system. 1. INTRODUCTION In this paper we investigate what happens when we take models trained for other tasks/domains and apply them to a new task for which we have no transcribed data: recognition of telephone calls to Dragon Systems' technical support line. The goal of the study was not to produce a highly optimized multi-pass s...},
	author = {Bernstein, Erica and McAllaster, Don and Gillick, Larry and Peskin, Barbara},
	month = nov,
	year = {2000},
}

@article{nollenburg_visual_2022,
	title = {Visual {Business} {Analytics}: {Using} the {Example} of a {Call} {Center}},
	volume = {8},
	issn = {18495664, 18495419},
	shorttitle = {Visual {Business} {Analytics}},
	url = {https://researchleap.com/visual-business-analytics-using-the-example-of-a-call-center/},
	doi = {10.18775/ijmsba.1849-5664-5419.2014.84.1001},
	abstract = {In this article we will examine an approach to the analysis of semi-structured log data using the example of a call center as a subsection of a central corporate service center. In such data all events of a caller passing through the routing are stored. Consequently, it is possible to trace more precisely what the customer experiences during his call. However, this information is only available in semi-structured form. A little-known approach in Anglo-Saxon literature, the Visual Business Analytics (VBA), represents a holistic concept for achieving added value from semi-structured data. In the VBA, data is initially transformed and structured and then prepared for analysis purposes. The goal is to derive recommendations for supporting management decisions in a call center. In the further course, the development of the approaches of information representation is examined first, then the VBA is presented and applied to the log protocols in a call center. Finally, other call center applications of VBA are considered, and an outlook is given on other industries in which the use of VBA offers advantages.},
	number = {4},
	urldate = {2022-11-17},
	journal = {THE INTERNATIONAL JOURNAL OF MANAGEMENT SCIENCE AND BUSINESS ADMINISTRATION},
	author = {Nollenburg, Pascal-Philipp and Dill, Arthur},
	month = may,
	year = {2022},
	pages = {7--16},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\L9CKJ7CH\\Nollenburg and Dill - 2022 - Visual Business Analytics Using the Example of a .pdf:application/pdf},
}

@article{aygen_zetter_systematic_2021,
	title = {A {SYSTEMATIC} {REVIEW} {OF} {THESES} {ON} “{CALL} {CENTER}” {IN} {TURKEY} {BASED} {ON} {CALL} {CENTER} {COMPONENTS}: 2003-2020},
	volume = {6},
	issn = {27177386},
	shorttitle = {A {SYSTEMATIC} {REVIEW} {OF} {THESES} {ON} “{CALL} {CENTER}” {IN} {TURKEY} {BASED} {ON} {CALL} {CENTER} {COMPONENTS}},
	url = {https://www.pearsonjournal.com/DergiTamDetay.aspx?ID=354&Detay=Ozet},
	doi = {10.46872/pj.354},
	abstract = {In this study, it is aimed to systematically examine the theses on the call center. On the subject the thesis archive of the Council of Higher Education was searched using the keyword "Call Center". As a result of the search, 224 studies were reached, however, a total of 206 studies that met the inclusion criteria were reviewed. It was determined that 120 of the scanned articles were written by the Social Sciences Institute, 28 were written by Marmara University, and 180 were postgraduate theses. When the subjects of the examined studies were classi-fied in terms of call center components, it was observed that 114 postgraduate theses were related to the human factor. In addition, it was found that strategy, process and technology-related subjects were among the research topics in the call center components. As a result, other disciplines as well as social sciences should show interest equally in the subject of call center, which should be addressed by many disciplines, in terms of contributing to the development of call centers. It is thought that it is possible to improve the system by focusing on the studies that examine the "strategy" and "technology" factor, as well as the employee and customer-oriented studies investigating the "human" factor from the call center components.},
	number = {15},
	urldate = {2022-11-17},
	journal = {IEDSR Association},
	author = {Aygen Zetter, Selin and Bi̇Li̇Şli̇, Yasemin},
	month = sep,
	year = {2021},
	pages = {246--260},
}

@article{tovar_rethinking_2022,
	title = {Rethinking call centers: {From} stigma to productive experience},
	volume = {16},
	issn = {1750-8657, 1750-8649},
	shorttitle = {Rethinking call centers},
	url = {https://journal.equinoxpub.com/SS/article/view/21181},
	doi = {10.1558/sols.42282},
	abstract = {Call centers have been critiqued in academia and the media for widespread standardization. This paper argues that although this critique of working conditions is well-intended, it has led to unwanted stigmatization of not just call center work but also of call center agents. Much has been published on call centers, but the stigma this work entails and the effect this has on agents on and off the phone has been overlooked. This paper applies Goffman’s notion of stigma to data collected through long-term ethnography and interviews with over seventy call center agents in a London call center. I show how agents experience, manage, and resist stigma. The analysis reveals that agents attempt to hide where they work by adopting different accents and avoiding specific lexis associated with call center language. I conclude by suggesting potential avenues for reducing the stigma of working in a call center, e.g. shifting the dominant discussion in academia beyond debates surrounding standardization.},
	number = {1},
	urldate = {2022-11-17},
	journal = {Sociolinguistic Studies},
	author = {Tovar, Johanna},
	month = apr,
	year = {2022}
}

@book{herzog_callcenter_2017,
	address = {Wiesbaden},
	title = {Callcenter – {Analyse} und {Management}},
	isbn = {978-3-658-18308-0 978-3-658-18309-7},
	url = {http://link.springer.com/10.1007/978-3-658-18309-7},
	language = {de},
	urldate = {2022-11-17},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Herzog, Alexander},
	year = {2017},
	doi = {10.1007/978-3-658-18309-7},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\J6LI87LS\\Herzog - 2017 - Callcenter – Analyse und Management.pdf:application/pdf}
}

@incollection{glas_einzelhandel_2017,
	address = {Berlin, Heidelberg},
	title = {Einzelhandel in {Läden} – {Ein} {Auslaufmodell}?},
	isbn = {978-3-662-53331-4 978-3-662-53332-1},
	url = {http://link.springer.com/10.1007/978-3-662-53332-1_2},
	language = {de},
	urldate = {2022-11-17},
	booktitle = {Handel 4.0},
	publisher = {Springer Berlin Heidelberg},
	author = {Jahn, Manuel},
	editor = {Gläß, Rainer and Leukert, Bernd},
	year = {2017},
	doi = {10.1007/978-3-662-53332-1_2},
	pages = {25--50}
}

@misc{commerzbank_commerzbank_2021,
	title = {Commerzbank beschließt neue {Strategie} bis 2024, {Pressemitteilung}, {Frankfurt} am {Main}: {Group} {Communications} (2021).},
	url = {https://www.commerzbank.de/de/hauptnavigation/presse/pressemitteilungen/archiv1/2021/1__quartal_1/presse_archiv_detail_21_01_93834.htm},
	author = {Commerzbank},
	month = feb,
	year = {2021},
}

@incollection{fasel_digitalisierung_2016,
	address = {Wiesbaden},
	title = {Die {Digitalisierung} als {Herausforderung} für {Unternehmen}: {Status} {Quo}, {Chancen} und {Herausforderungen} im {Umfeld} {BI} \& {Big} {Data}},
	isbn = {978-3-658-11588-3 978-3-658-11589-0},
	shorttitle = {Die {Digitalisierung} als {Herausforderung} für {Unternehmen}},
	url = {http://link.springer.com/10.1007/978-3-658-11589-0_3},
	language = {de},
	urldate = {2022-11-17},
	booktitle = {Big {Data}},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Seufert, Andreas},
	editor = {Fasel, Daniel and Meier, Andreas},
	year = {2016},
	doi = {10.1007/978-3-658-11589-0_3},
	note = {Series Title: Edition HMD},
	pages = {39--57}
}

@book{kopparapu_non-linguistic_2015,
	address = {Cham},
	series = {{SpringerBriefs} in {Electrical} and {Computer} {Engineering}},
	title = {Non-{Linguistic} {Analysis} of {Call} {Center} {Conversations}},
	isbn = {978-3-319-00896-7 978-3-319-00897-4},
	url = {http://link.springer.com/10.1007/978-3-319-00897-4},
	urldate = {2022-11-17},
	publisher = {Springer International Publishing},
	author = {Kopparapu, Sunil Kumar},
	year = {2015},
	doi = {10.1007/978-3-319-00897-4},
}

@article{benzeghiba_automatic_2007-1,
	title = {Automatic speech recognition and speech variability: {A} review},
	volume = {49},
	issn = {01676393},
	shorttitle = {Automatic speech recognition and speech variability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639307000404},
	doi = {10.1016/j.specom.2007.02.006},
	language = {en},
	number = {10-11},
	urldate = {2022-11-19},
	journal = {Speech Communication},
	author = {Benzeghiba, M. and De Mori, R. and Deroo, O. and Dupont, S. and Erbes, T. and Jouvet, D. and Fissore, L. and Laface, P. and Mertins, A. and Ris, C. and Rose, R. and Tyagi, V. and Wellekens, C.},
	month = oct,
	year = {2007},
	pages = {763--786}
}

@article{chang_preliminary_2022,
	title = {A {Preliminary} {Study} of {Robust} {Speech} {Feature} {Extraction} {Based} on {Maximizing} the {Probability} of {States} in {Deep} {Acoustic} {Models}},
	volume = {5},
	issn = {2571-5577},
	url = {https://www.mdpi.com/2571-5577/5/4/71},
	doi = {10.3390/asi5040071},
	abstract = {This study proposes a novel robust speech feature extraction technique to improve speech recognition performance in noisy environments. This novel method exploits the information provided by the original acoustic model in the automatic speech recognition (ASR) system to learn a deep neural network that converts the original speech features. This deep neural network is trained to maximize the posterior accuracy of the state sequences of acoustic models with respect to the speech feature sequences. Compared with the robustness methods that retrain or adapt acoustic models, the new method has the advantages of less computation load and faster training. In the experiments conducted on the medium-vocabulary TIMIT database and task, the presented method provides lower word error rates than the unprocessed baseline and speech-enhancement-based techniques. These results indicate that the presented method is promising and worth further developing.},
	language = {en},
	number = {4},
	urldate = {2022-11-19},
	journal = {Applied System Innovation},
	author = {Chang, Li-Chia and Hung, Jeih-Weih},
	month = jul,
	year = {2022},
	pages = {71}
}

@article{sinha_acoustic-phonetic_2015,
	title = {Acoustic-{Phonetic} {Feature} {Based} {Dialect} {Identification} in {Hindi} {Speech}},
	volume = {8},
	issn = {1178-5608},
	url = {https://www.sciendo.com/article/10.21307/ijssis-2017-757},
	doi = {10.21307/ijssis-2017-757},
	abstract = {Abstract
            Every individual has some unique speaking style and this variation influences their speech characteristics. Speakers’ native dialect is one of the major factors influencing their speech characteristics that influence the performance of automatic speech recognition system (ASR). In this paper, we describe a method to identify Hindi dialects and examine the contribution of different acoustic-phonetic features for the purpose. Mel frequency cepstral coefficients (MFCC), Perceptual linear prediction coefficients (PLP) and PLP derived from Mel-scale filter bank (MF- PLP) have been extracted as spectral features from the spoken utterances. They are further used to measure the capability of Auto-associative neural networks (AANN) for capturing non-linear relation specific to information from spectral features. Prosodic features are for capturing long - range features. Based on these features efficiency of AANN is measured to model intrinsic characteristics of speech features due to dialects.},
	language = {en},
	number = {1},
	urldate = {2022-11-19},
	journal = {International Journal on Smart Sensing and Intelligent Systems},
	author = {Sinha, Shweta and Jain, Aruna and Agrawal, S. S.},
	month = jan,
	year = {2015},
	pages = {235--254} 
}

@inproceedings{koo_exploiting_2020-1,
	title = {Exploiting {Multi}-{Modal} {Features} from {Pre}-{Trained} {Networks} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/koo20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3153},
	language = {en},
	urldate = {2022-11-19},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Koo, Junghyun and Lee, Jie Hwan and Pyo, Jaewoo and Jo, Yujin and Lee, Kyogu},
	month = oct,
	year = {2020},
	pages = {2217--2221},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\9RFH3BIS\\Koo et al. - 2020 - Exploiting Multi-Modal Features from Pre-Trained N.pdf:application/pdf},
}

@inproceedings{balagopalan_impact_2020-1,
	address = {Online},
	title = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}: {All} {Errors} are {Equal}, but {Deletions} are {More} {Equal} than {Others}},
	shorttitle = {Impact of {ASR} on {Alzheimer}’s {Disease} {Detection}},
	url = {https://www.aclweb.org/anthology/2020.wnut-1.21},
	doi = {10.18653/v1/2020.wnut-1.21},
	language = {en},
	urldate = {2022-11-19},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Noisy} {User}-generated {Text} ({W}-{NUT} 2020)},
	publisher = {Association for Computational Linguistics},
	author = {Balagopalan, Aparna and Shkaruta, Ksenia and Novikova, Jekaterina},
	year = {2020},
	pages = {159--164},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\IB23EMTY\\Balagopalan et al. - 2020 - Impact of ASR on Alzheimer’s Disease Detection Al.pdf:application/pdf},
}

@misc{koo_exploiting_2021,
	title = {Exploiting {Multi}-{Modal} {Features} {From} {Pre}-trained {Networks} for {Alzheimer}'s {Dementia} {Recognition}},
	url = {http://arxiv.org/abs/2009.04070},
	abstract = {Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient's medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer's Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer's Dementia by providing acoustic and textual data. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. Our test results surpass baseline's accuracy by 18.75\%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70\%.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Koo, Junghyun and Lee, Jie Hwan and Pyo, Jaewoo and Jo, Yujin and Lee, Kyogu},
	month = mar,
	year = {2021},
	note = {arXiv:2009.04070 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Machine Learning},
	annote = {Comment: In the Proceedings of INTERSPEECH 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\QY98I8C8\\Koo et al. - 2021 - Exploiting Multi-Modal Features From Pre-trained N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\UYAE7LTN\\2009.html:text/html},
}

@inproceedings{li_comparative_2021-1,
	address = {Toronto, ON, Canada},
	title = {A {Comparative} {Study} of {Acoustic} and {Linguistic} {Features} {Classification} for {Alzheimer}'s {Disease} {Detection}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414147/},
	doi = {10.1109/ICASSP39728.2021.9414147},
	urldate = {2022-11-19},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen},
	month = jun,
	year = {2021},
	pages = {6423--6427},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\XNGARJH9\\Li et al. - 2021 - A Comparative Study of Acoustic and Linguistic Fea.pdf:application/pdf},
}

@misc{pompili_inesc-id_2020,
	title = {The {INESC}-{ID} {Multi}-{Modal} {System} for the {ADReSS} 2020 {Challenge}},
	url = {http://arxiv.org/abs/2005.14646},
	abstract = {This paper describes a multi-modal approach for the automatic detection of Alzheimer's disease proposed in the context of the INESC-ID Human Language Technology Laboratory participation in the ADReSS 2020 challenge. Our classification framework takes advantage of both acoustic and textual feature embeddings, which are extracted independently and later combined. Speech signals are encoded into acoustic features using DNN speaker embeddings extracted from pre-trained models. For textual input, contextual embedding vectors are first extracted using an English Bert model and then used either to directly compute sentence embeddings or to feed a bidirectional LSTM-RNNs with attention. Finally, an SVM classifier with linear kernel is used for the individual evaluation of the three systems. Our best system, based on the combination of linguistic and acoustic information, attained a classification accuracy of 81.25\%. Results have shown the importance of linguistic features in the classification of Alzheimer's Disease, which outperforms the acoustic ones in terms of accuracy. Early stage features fusion did not provide additional improvements, confirming that the discriminant ability conveyed by speech in this case is smooth out by linguistic data.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Pompili, Anna and Rolland, Thomas and Abad, Alberto},
	month = may,
	year = {2020},
	note = {arXiv:2005.14646 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 5 pages, 1 figure. Submitted to INTERSPEECH2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\JFYDTKI9\\Pompili et al. - 2020 - The INESC-ID Multi-Modal System for the ADReSS 202.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\MTUUKIBC\\2005.html:text/html},
}

@inproceedings{cummins_comparison_2020-1,
	title = {A {Comparison} of {Acoustic} and {Linguistics} {Methodologies} for {Alzheimer}’s {Dementia} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/cummins20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2635},
	language = {en},
	urldate = {2022-11-19},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Cummins, Nicholas and Pan, Yilin and Ren, Zhao and Fritsch, Julian and Nallanthighal, Venkata Srikanth and Christensen, Heidi and Blackburn, Daniel and Schuller, Björn W. and Magimai-Doss, Mathew and Strik, Helmer and Härmä, Aki},
	month = oct,
	year = {2020},
	pages = {2182--2186},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\G885GZTY\\Cummins et al. - 2020 - A Comparison of Acoustic and Linguistics Methodolo.pdf:application/pdf},
}

@inproceedings{yuan_pause-encoded_2021-1,
	address = {Toronto, ON, Canada},
	title = {Pause-{Encoded} {Language} {Models} for {Recognition} of {Alzheimer}’s {Disease} and {Emotion}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413548/},
	doi = {10.1109/ICASSP39728.2021.9413548},
	urldate = {2022-11-19},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Yuan, Jiahong and Cai, Xingyu and Church, Kenneth},
	month = jun,
	year = {2021},
	pages = {7293--7297},
}

@misc{noauthor_ad2021_2022-1,
	title = {{AD2021}: {Alzheimer}'s {Disease} {Recognition} {Evaluation} 2021},
	copyright = {Apache-2.0},
	shorttitle = {{AD2021}},
	url = {https://github.com/THUsatlab/AD2021},
	abstract = {Alzheimer's Disease Recognition Evaluation 2021},
	urldate = {2022-11-20},
	publisher = {THUsatlab},
	month = nov,
	year = {2022},
	note = {original-date: 2021-07-08T04:10:11Z},
}

@article{abushariah_modern_2012,
	title = {Modern standard {Arabic} speech corpus for implementing and evaluating automatic continuous speech recognition systems},
	volume = {349},
	issn = {00160032},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016003211001013},
	doi = {10.1016/j.jfranklin.2011.04.011},
	language = {en},
	number = {7},
	urldate = {2022-11-20},
	journal = {Journal of the Franklin Institute},
	author = {Abushariah, Mohammad Abd-Alrahman Mahmoud and Ainon, Raja Noor and Zainuddin, Roziati and Alqudah, Assal Ali Mustafa and Elshafei Ahmed, Moustafa and Khalifa, Othman Omran},
	month = sep,
	year = {2012},
	pages = {2215--2242},
}

@article{abushariah_arabic_2012,
	title = {Arabic {Speaker}-{Independent} {Continuous} {Automatic} {Speech} {Recognition} {Based} on a {Phonetically} {Rich} and {Balanced} {Speech} {Corpus}},
	volume = {9},
	abstract = {This paper describes and proposes an efficient and effective framework for the design and development of a speaker-independent continuous automatic Arabic speech recognition system based on a phonetically rich and balanced speech corpus. The speech corpus contains a total of 415 sentences recorded by 40 (20 male and 20 female) Arabic native speakers from 11 different Arab countries representing the three major regions (Levant, Gulf, and Africa) in the Arab world. The proposed Arabic speech recognition system is based on the Carnegie Mellon University (CMU) Sphinx tools, and the Cambridge HTK tools were also used at some testing stages. The speech engine uses 3-emitting state Hidden Markov Models (HMM) for tri-phone based acoustic models. Based on experimental analysis of about 7 hours of training speech data, the acoustic model is best using continuous observation's probability model of 16 Gaussian mixture distributions and the state distributions were tied to 500 senones. The language model contains both bi-grams and tri-grams. For similar speakers but different sentences, the system obtained a word recognition accuracy of 92.67\% and 93.88\% and a Word Error Rate (WER) of 11.27\% and 10.07\% with and without diacritical marks respectively. For different speakers with similar sentences, the system obtained a word recognition accuracy of 95.92\% and 96.29\% and a WER of 5.78\% and 5.45\% with and without diacritical marks respectively. Whereas different speakers and different sentences, the system obtained a word recognition accuracy of 89.08\% and 90.23\% and a WER of 15.59\% and 14.44\% with and without diacritical marks respectively.},
	journal = {Int. Arab J. Inf. Technol.},
	author = {Abushariah, Mohammad and Ainon, Raja and Zainuddin, Roziati and Elshafei, Moustafa and Khalifa, Othman},
	month = jan,
	year = {2012},
	pages = {84--93},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\HLDTCDBU\\Abushariah et al. - 2012 - Arabic Speaker-Independent Continuous Automatic Sp.pdf:application/pdf},
}

@inproceedings{abushariah_phonetically_2010,
	address = {Kuala Lumpur, Malaysia},
	title = {Phonetically rich and balanced speech corpus for {Arabic} speaker-independent continuous automatic speech recognition systems},
	isbn = {978-1-4244-7165-2},
	url = {http://ieeexplore.ieee.org/document/5605554/},
	doi = {10.1109/ISSPA.2010.5605554},
	urldate = {2022-11-20},
	booktitle = {10th {International} {Conference} on {Information} {Science}, {Signal} {Processing} and their {Applications} ({ISSPA} 2010)},
	publisher = {IEEE},
	author = {Abushariah, Mohammad A. M. and Ainon, Raja N. and Zainuddin, Roziati and Elshafei, Moustafa and Khalifa, Othman O.},
	month = may,
	year = {2010},
	pages = {65--68},
}

@article{park_study_1992,
	title = {A {Study} on the {Spoken} {KOrean}-{Digit} {Recognition} {Using} the {Neural} {Netwok}},
	volume = {11},
	abstract = {Taking devantage of the property that Korean digit is a mono-syllable word, we proposed a spoken Korean-digit recognition scheme using the multi-layer perceptron. The spoken Korean-digit is divided into three segments (initial sound, medial vowel, and final consonant) based on the voice starting / ending points and a peak point in the middle of vowel sound. The feature vectors such as cepstrum, reflection coefficients, cepstrum and energy are extracted from each segment. It has been shown that cepstrum, as an input vector to the neural network, gives higher recognition rate than reflection coefficients. Regression coefficients of cepstrum did not affect as much as we expected on the recognition rate. That is because, it is believed, we extracted features from the selected stationary segments of the input speech signal. With 150 ceptral coefficients obtained from each spoken digit, we achieved correct recognition rate of 97.8\%.},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Park, Hyun-Hwa and Gahang, Hae and Bae, Keun},
	month = jan,
	year = {1992},
}

@inproceedings{willett_discriminatively_2006,
	address = {Toulouse, France},
	title = {Discriminatively {Trained} {Context}-{Dependent} {Duration}-{Bigram} {Models} for {Korean} {Digit} {Recognition}},
	volume = {1},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1659948/},
	doi = {10.1109/ICASSP.2006.1659948},
	urldate = {2022-11-20},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Willett, D. and Gerl, F. and Brueckner, R.},
	year = {2006},
	pages = {I--25--I--28},
}

@article{pfitzinger_local_2000,
	title = {Local {Speech} {Rate} {Perception} {In} {German} {Speech}},
	abstract = {A model for deriving perceived local speech rate directly out of the speech signal is developed based on perception experiments. Since local speech rate modifies acoustic cues (e.g. transitions and voice-onset time [5]), phones [1, 4, 7, 19], syllables [3, 15], and even words, it is one of the most important prosodic cues. Our local speech rate estimation method is based on a linear combination of the local syllable rate and the local phone rate, since earlier investigations [11, 12] strongly suggest that neither the syllable rate nor the phone rate on its own represent the speech rate sufficiently. In the literature [6] effects of F0 level and F0 movement on speech rate perception have been reported. Therefore we included these cues in our linear combination model. Our results show 1) that the duration of speech stimuli has a strong influence on the perception of speech rate, 2) that the linear combination of local syllable rate and phone rate is wellcorrelated with perceptual local...},
	author = {Pfitzinger, Hartmut},
	month = aug,
	year = {2000},
}

@article{aldholmi_perception_2016,
	title = {Perception of speech rate in speech rate perception},
	volume = {140},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4970623},
	doi = {10.1121/1.4970623},
	language = {en},
	number = {4},
	urldate = {2022-11-20},
	journal = {The Journal of the Acoustical Society of America},
	author = {Aldholmi, Yahya and Park, Hanyong},
	month = oct,
	year = {2016},
	pages = {3333--3333},
}

@book{matejka_phonotactic_2005,
	title = {Phonotactic language identification using high quality phoneme recognition},
	abstract = {Phoneme Recognizers followed by Language Modeling (PRLM) have consistently yielded top performance in language identification (LID) task. Parallel ordering of PRLMs (PPRLM) improves performance even more. Since tokenizer is the most important part of LID system the high quality phoneme rec-ognizer is employed. Two different multilingual databases for training phoneme recognizers are compared and the amount of sufficient training data is studied. Reported results are on data from NIST 2003 LID evaluation. Our four PRLM systems have Equal Error Rate (EER) of 2.4\% on 12 languages task. This re-sult compares favorably to the best known result from this task.},
	author = {Matejka, Pavel and Schwarz, Petr and Cernocký, Jan and Chytil, Pavel},
	month = jan,
	year = {2005},
	note = {Pages: 2240},
}

@inproceedings{liang_wang_multi-lingual_2006,
	address = {Hong Kong, China},
	title = {Multi-lingual {Phoneme} {Recognition} and {Language} {Identification} {Using} {Phonotactic} {Information}},
	isbn = {978-0-7695-2521-1},
	url = {http://ieeexplore.ieee.org/document/1699826/},
	doi = {10.1109/ICPR.2006.823},
	urldate = {2022-11-20},
	booktitle = {18th {International} {Conference} on {Pattern} {Recognition} ({ICPR}'06)},
	publisher = {IEEE},
	author = {{Liang Wang} and Ambikairajah, E. and Choi, E.H.C.},
	year = {2006},
	pages = {245--248},
}

@inproceedings{adell_database_2006,
	address = {Toulouse, France},
	title = {Database {Pruning} for {Unsupervised} {Building} of {Text}-{To}-{Speech} {Voices}},
	volume = {1},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1660164/},
	doi = {10.1109/ICASSP.2006.1660164},
	urldate = {2022-11-20},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Adell, J. and Aguero, P.D. and Bonafonte, A.},
	year = {2006},
	pages = {I--889--I--892},
}

@article{ramig_effects_1983,
	title = {Effects of {Physiological} {Aging} on {Selected} {Acoustic} {Characteristics} of {Voice}},
	volume = {26},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.2601.22},
	doi = {10.1044/jshr.2601.22},
	abstract = {The relationship between age-related changes in body physiology and certain acoustic characteristics of voice was studied in a sample of 48 men representing three chronological age grouping (25–35, 45–55 and 65–75) and two levels of physical condition (good and poor). A fundamental frequency analysis program (SEARP) was used to measure mean fundamental frequency, jitter, shimmer, and phonation range from samples of connected speech and sustained vowel production. Subjects in good physical condition produced maximum duration vowel phonation with significantly less jitter and shimmer and had larger phonation ranges than did subjects of similar chronological ages who were in poor physical condition. These differences were most apparent in the production of the elderly subjects. While chronological aging is undoubtedly a contributor to such changes in the acoustic characteristics of voice, these results suggest that age-related changes in body physiology, or physiological aging, also must be considered.},
	language = {en},
	number = {1},
	urldate = {2022-11-20},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Ramig, Lorraine A. and Ringel, Robert L.},
	month = mar,
	year = {1983},
	pages = {22--30},
}

@book{bruckl_aging_2003,
	title = {Aging female voices: {An} acoustic and perceptive analysis},
	shorttitle = {Aging female voices},
	abstract = {This study examines the changes in adult female voices due to the aging process. Acoustic cues in voices that enable listeners to recognize a speaker's vocal age are specified as well as acoustic cues that straightly indicate the speaker's chronological age. The analysed data are recordings of the voices of 56 female speakers differing in age. The recorded speech samples include sustained vowels, read speech and spontaneous speech. Our methods are acoustic analyses and perception tests. The perception tests are designed to analyse the influence of the vowel onset on the amount of information about aging transferred by sustained vowels. The acoustic evaluation comprises phonic parameters like measurements of stability of voice or of vocal tremor which are supposed to reflect voice qualities that are expected to vary with age. Changes in tempo of articulation are also investigated. We found that increasing amplitude perturbation is an indicator of increasing age even on the basis of spontaneous speech. Reading rate decreases with increasing age, whereas there is no significant change in articulation rate of spontaneous speech in women's voices. Based on sustained vowels of female voices, the frequency tremor intensity index indicates age more accurately than F0 and amplitude perturbations. We also found ev-idence for the relevance of the vowel onset to recognize age more accurately.},
	author = {Brückl, Markus and Sendlmeier, Walter},
	month = aug,
	year = {2003},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\USHRR5AA\\Brückl and Sendlmeier - 2003 - Aging female voices An acoustic and perceptive an.pdf:application/pdf},
}

@book{winkler_aging_2003,
	title = {The aging voice: an acoustic, electroglottographic and perceptive analysis of male and female voices},
	shorttitle = {The aging voice},
	abstract = {This study examines the differences between young and old adult voices. Acoustic cues in voices that enable listeners to recognize a speaker's vocal age are specified as well as acoustic cues that straightly indicate the speaker's chronological age. Electroglottographic data were used to directly examine glottal behaviour in aging voices. We found a strong spectral attenuation of high frequencies in aging male voices assumed to result from rather sinusoidal glottal excitation. Increasing amplitude perturbation is an indicator of increasing age even on the basis of spontaneous speech. Reading rate decreases with increasing age, whereas there is no significant change in articulation rate of spontaneous speech in women's voices. Based on sustained vowels of female voices, the frequency tremor intensity index indicates age more accurately than F0 and amplitude perturbations. We also found evidence for the relevance of the vowel onset to recognize age more accurately.},
	author = {Winkler, Ralf and Brückl, Markus and Sendlmeier, Walter},
	month = jan,
	year = {2003},
	note = {Journal Abbreviation: Proceedings of The IEEE - PIEEE
Publication Title: Proceedings of The IEEE - PIEEE},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\ZH8D4LWQ\\Winkler et al. - 2003 - The aging voice an acoustic, electroglottographic.pdf:application/pdf},
}

@inproceedings{vaseghi_speech_2006,
	address = {Toulouse, France},
	title = {Speech {Bandwidth} {Extension}: {Extrapolations} of {Spectral} {Envelop} and {Harmonicity} {Quality} of {Excitation}},
	volume = {3},
	isbn = {978-1-4244-0469-8},
	shorttitle = {Speech {Bandwidth} {Extension}},
	url = {http://ieeexplore.ieee.org/document/1660786/},
	doi = {10.1109/ICASSP.2006.1660786},
	urldate = {2022-11-20},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Vaseghi, S. and Zavarehei, E. and {Qin Yan}},
	year = {2006},
	pages = {III--844--III--847},
}

@inproceedings{srinivasan_harmonicity_2004,
	address = {Montreal, Que., Canada},
	title = {Harmonicity and dynamics-based features for audio},
	volume = {4},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1326828/},
	doi = {10.1109/ICASSP.2004.1326828},
	urldate = {2022-11-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Srinivasan, H. and Kankanhalli, M.},
	year = {2004},
	pages = {iv--321--iv--324},
}

@inproceedings{srinivasan_harmonicity_2003,
	address = {Hong Kong, China},
	title = {Harmonicity and dynamics based audio separation},
	volume = {5},
	isbn = {978-0-7803-7663-2},
	url = {http://ieeexplore.ieee.org/document/1200052/},
	doi = {10.1109/ICASSP.2003.1200052},
	urldate = {2022-11-20},
	booktitle = {2003 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}, 2003. {Proceedings}. ({ICASSP} '03).},
	publisher = {IEEE},
	author = {Srinivasan, S.H. and Kankanhalli, M.},
	year = {2003},
	pages = {V--640--3},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\AR44HZB7\\Srinivasan and Kankanhalli - 2003 - Harmonicity and dynamics based audio separation.pdf:application/pdf},
}

@misc{noauthor_unilever_nodate,
	title = {Unilever {Pakistan} {Homepage}},
	url = {https://www.unilever.pk/undefined},
	abstract = {At Unilever we meet everyday needs for nutrition, hygiene and personal care with brands that help people feel good, look good and get more out of life.},
	language = {en-GB},
	urldate = {2022-11-22},
	journal = {Unilever},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\3YUJBJNC\\www.unilever.pk.html:text/html},
}

@techreport{forrester_human_2017,
	title = {Human vs. {Machines}: {How} to stop your virtual agent from lagging behind},
	url = {https://telecoms.com/wp-content/blogs.dir/1/files/2017/10/Amdocs-TLP-Virtual-Agents.pdf},
	urldate = {2022-02-12},
	institution = {AMDOCS},
	author = {Forrester},
	year = {2017},
	note = {www.forrester.com},
}

@inproceedings{vasilescu_perceptual_2009,
	title = {A perceptual investigation of speech transcription errors involving frequent near-homophones in {French} and american {English}},
	url = {https://www.isca-speech.org/archive/interspeech_2009/vasilescu09_interspeech.html},
	doi = {10.21437/Interspeech.2009-53},
	language = {en},
	urldate = {2022-12-02},
	booktitle = {Interspeech 2009},
	publisher = {ISCA},
	author = {Vasilescu, Ioana and Adda-Decker, Martine and Lamel, Lori and Hallé, Pierre},
	month = sep,
	year = {2009},
	pages = {144--147},
}

@incollection{ide_corpus-based_2000,
	address = {Dordrecht},
	title = {A {Corpus}-{Based} {Approach} to the {Study} of {Speaking} {Style}},
	volume = {14},
	isbn = {978-90-481-5562-0 978-94-015-9413-4},
	url = {http://link.springer.com/10.1007/978-94-015-9413-4_12},
	urldate = {2022-12-02},
	booktitle = {Prosody: {Theory} and {Experiment}},
	publisher = {Springer Netherlands},
	author = {Hirschberg, Julia},
	editor = {Ide, Nancy and Véronis, Jean and Horne, Merle},
	year = {2000},
	doi = {10.1007/978-94-015-9413-4_12},
	note = {Series Title: Text, Speech and Language Technology},
	pages = {335--350},
}

@article{ermakova_corpus-based_2018,
	title = {Corpus-based research of spoken language: {The} state-of-the-art for {Czech} and {English}},
	volume = {79},
	shorttitle = {Corpus-based research of spoken language},
	abstract = {The article aims to review corpus-based research on spoken language, emphasizing issues in description and conceptualization of the grammar of spoken language in relation to the grammar of written language. The review first briefy looks at the development of spoken corpora, from simply transcribed corpora without sound alignment to today's sophisticated multi-modal corpora. The main part of the article deals with issues concerning the metalanguage for the description of spoken language, the choice of its basic descriptive unit, the status of basic linguistic categories such as part-of-speech, and typical lexical and grammatical devices. The existing extensive research on spoken English is reviewed and in line with it, illustrative examples based on Czech spoken corpora are provided. These are further contrasted with examples from written data to enhance the inherent differences between spoken and written language and the need to adjust the metalanguage of the description. © 2018 Czech Language Institute of the Academy of Sciences. All rights reserved.},
	journal = {Slovo a Slovesnost},
	author = {Ermáková, A. and Kopřivova, Marie},
	month = jan,
	year = {2018},
	pages = {217--240},
}

@article{servan_corpus-based_2008,
	title = {Corpus-based spoken language understanding for mixed initiative spoken dialog systems},
	abstract = {Spoken dialogues systems are interfaces between users and services. Simple examples of services for which theses dialogue systems can be used include : banking, booking (hotels, trains, flights), etc. Dialogue systems are composed of a number of modules. The main modules include Automatic Speech Recognition (ASR), Spoken Language Understanding (SLU), Dialogue Management and Speech Generation. In this thesis, we concentrate on the Spoken Language Understanding component of dialogue systems. In the past, it has usual to separate the Spoken Language Understanding process from that of Automatic Speech Recognition. First, the Automatic Speech Recognition process finds the best word hypothesis. Given this hypothesis, we then find the best semantic interpretation. This thesis presents a method for the robust extraction of basic conceptual constituents (or concepts) from an audio message. The conceptual decoding model proposed follows a stochastic paradigm and is directly integrated into the Automatic Speech Recognition process. This approach allows us to keep the probabilistic search space on sequences of words produced by the Automatic Speech Recognition module, and to project it to a probabilistic search space of sequences of concepts. The experiments carried out on the French spoken dialogue corpus MEDIA, available through ELDA, show that the performance reached by our new approach is better than the traditional sequential approach. As a starting point for evaluation, the effect that deterioration of word error rate (WER) has on SLU systems is examined though use of different ASR outputs. The SLU performance appears to decrease lineary as a function of ASR word error rate.We show, however, that the proposed integrated method of searching for both words and concets, gives better results to that of a traditionnanl sequential approach. In order to validate our approach, we conduct experiments on the MEDIA corpus in the same assessment conditions used during the MEDIA campaign. The goal is toproduce error-free semantic interpretations from transcripts. The results show that the performance achieved by our model is as good as the systems involved in the evaluation campaign. Studies made on the MEDIA corpus show the concept error rate is related to the word error rate, the size of the training corpus and a priori knwoledge added to conceptual model languages. Error analyses show the interest of modifying the probabilities of word lattice with triggers, a template cache or by using arbitrary rules requiring passage through a portion of the graph and applying the presence of triggers (words or concepts) based on history. Methods based on machine learning are generally quite demanding in terms of amount of training data required. By changing the size of the training corpus, the minimum and the optimal number of dialogues needed for training conceptual language models can be measured. Research conducted in this thesis aims to determine the size of corpus necessary for training conceptual language models from which the semantic evaluation scores stagnated. A correlation is established between the necessary corpus size for learning and the corpus size necessary to validate the manual annotations. In the case of the MEDIA evaluation campaign, it took roughly the same number of examples, first to validate the semantic annotations and, secondly, to obtain a "quality" corpus-trained stochastic model. The addition of a priori knowledge to our stochastic models reduce significantly the size of the training corpus needed to achieve the same scores as a fully stochastic system (nearly half the size for the same score). It allows us to confirm that the addition of basic intuitive rules (numbers, zip codes, dates) gives very encouraging results. It leeds us to create a hybrid system combining corpus-based and knowledge-based models. The second part of the thesis examines the application of the understanding module to another simple dialogue system task, a callrouting system. A problem with this specific task is a lack of data available for training the requiered language models. We attempt to resolve this issue by supplementing he in-domain data with various other generic corpora already available, and data from the MEDIA campaing. We show the benefits of integrating a call classification task in a SLU process. Unfortunately, we have very little training corpus in the field under consideration. By using our integrated approach to decode concepts, along with an integrated process, we propose a bag of words and concepts approach. This approach used by a classifier achieved encouraging call classification rates on the test corpus, while the WER was relativelyhigh. The methods developed are shown to improve the call routing system process robustness.},
	author = {Servan, Christophe},
	month = dec,
	year = {2008},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PQFJUAD3\\Servan - 2008 - Corpus-based spoken language understanding for mix.pdf:application/pdf},
}

@article{allwood_transliteration_2005,
	title = {Transliteration between spoken language corpora},
	volume = {28},
	issn = {0332-5865, 1502-4717},
	url = {https://www.cambridge.org/core/product/identifier/S0332586505001307/type/journal_article},
	doi = {10.1017/S0332586505001307},
	abstract = {Comparison of languages and linguistic data is essential if progress in our understanding of the nature of spoken languages is to be made. We understand phenomena better through comparison and contrast. This paper discusses problems that arise in trying to transfer a spoken language corpus transcribed and formatted according to one standard into the standard and format of another corpus. The problems that arise are related both to the differences that exist between the standards of the corpora and to human errors leading to lack of reliability in creating the transcriptions. Although the discussion is based on transfer and transliteration between two specific corpora (the Danish BySoc, BySociolingvistisk Korpus, and the Swedish GSLC, Göteborg Spoken Language Corpus), we believe that the discussion in the article documents and highlights problems of a general kind which have to be faced whenever spoken language corpora of different formats are to be compared.},
	language = {en},
	number = {1},
	urldate = {2022-12-02},
	journal = {Nordic Journal of Linguistics},
	author = {Allwood, Jens and Henrichsen, Peter Juel and Grönqvist, Leif and Ahlsén, Elisabeth and Gunnarsson, Magnus},
	month = jun,
	year = {2005},
	pages = {5--36},
}

@article{allwood_spoken_2000,
	title = {The {Spoken} {Language} {Corpus} at the {Department} of {Linguistics}, {Göteborg} {University}},
	volume = {1},
	abstract = {This paper summarizes work on spoken language at the Department of Linguistics Göteborg University. In addition to describing the recordings contained in the Spoken Language Corpus of Swedish at Göteborg University, we discuss the standard of transcription (MSO) which is used in creating the transcriptions, as well as some types of quantitative and qualitative analysis that have been done. Finally, we describe the computer tools that have been developed to support transcription, coding and analysis and briefly mention some of the results which have been obtained.
URN: urn:nbn:de:0114-fqs000391},
	journal = {Forum: Qualitative Social Research},
	author = {Allwood, Jens and Björnberg, Maria and Grönqvist, Leif and Ahlsen, Elisabeth and Ottesjö, Cajsa},
	month = dec,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5B67WBYS\\Allwood et al. - 2000 - The Spoken Language Corpus at the Department of Li.pdf:application/pdf},
}

@article{yanli_research_2021,
	title = {Research on {Spoken} {Language} {Understanding} {Based} on {Deep} {Learning}},
	volume = {2021},
	issn = {1875-919X, 1058-9244},
	url = {https://www.hindawi.com/journals/sp/2021/8900304/},
	doi = {10.1155/2021/8900304},
	abstract = {Aiming at solving the problem that the recognition effect of rare slot values in spoken language is poor, which affects the accuracy of oral understanding task, a spoken language understanding method is designed based on deep learning. The local features of semantic text are extracted and classified to make the classification results match the dialogue task. An intention recognition algorithm is designed for the classification results. Each datum has a corresponding intention label to complete the task of semantic slot filling. The attention mechanism is applied to the recognition of rare slot value information, the weight of hidden state and corresponding slot characteristics are obtained, and the updated slot value is used to represent the tracking state. An auxiliary gate unit is constructed between the upper and lower slots of historical dialogue, and the word vector is trained based on deep learning to complete the task of spoken language understanding. The simulation results show that the proposed method can realize multiple rounds of man-machine spoken language. Compared with the spoken language understanding methods based on cyclic network, context information, and label decomposition, it has higher accuracy and F1 value and has higher practical application value.},
	language = {en},
	urldate = {2022-12-02},
	journal = {Scientific Programming},
	author = {Yanli, Hui},
	editor = {Ding, Bai Yuan},
	month = oct,
	year = {2021},
	pages = {1--9},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\3RPQEHS4\\Yanli - 2021 - Research on Spoken Language Understanding Based on.pdf:application/pdf},
}

@incollection{hutchison_multilingual_2006,
	address = {Berlin, Heidelberg},
	title = {Multilingual {Spoken} {Language} {Corpus} {Development} for {Communication} {Research}},
	volume = {4274},
	isbn = {978-3-540-49665-6 978-3-540-49666-3},
	url = {http://link.springer.com/10.1007/11939993_78},
	urldate = {2022-12-02},
	booktitle = {Chinese {Spoken} {Language} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Takezawa, Toshiyuki},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Huo, Qiang and Ma, Bin and Chng, Eng-Siong and Li, Haizhou},
	year = {2006},
	doi = {10.1007/11939993_78},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {781--791},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\EE4F747D\\Takezawa - 2006 - Multilingual Spoken Language Corpus Development fo.pdf:application/pdf},
}

@book{izreel_search_2020,
	address = {Amsterdam},
	series = {Studies in {Corpus} {Linguistics}},
	title = {In {Search} of {Basic} {Units} of {Spoken} {Language}: {A} corpus-driven approach},
	volume = {94},
	isbn = {978-90-272-0497-4 978-90-272-6153-3},
	shorttitle = {In {Search} of {Basic} {Units} of {Spoken} {Language}},
	url = {http://www.jbe-platform.com/content/books/9789027261533},
	language = {en},
	urldate = {2022-12-02},
	publisher = {John Benjamins Publishing Company},
	editor = {Izre'el, Shlomo and Mello, Heliana and Panunzi, Alessandro and Raso, Tommaso},
	month = jun,
	year = {2020},
	doi = {10.1075/scl.94},
}

@article{allwood_spoken_2000-1,
	title = {The {Spoken} {Language} {Corpus} at the {Linguistics} {Department}, {Göteborg} {University}},
	volume = {1},
	copyright = {Copyright (c) 2000 Jens Allwood, Leif Grönqvist, Maria Björnberg, Elisabeth Ahlsen, Cajsa Ottesjö},
	issn = {1438-5627},
	url = {https://www.qualitative-research.net/index.php/fqs/article/view/1026},
	doi = {10.17169/fqs-1.3.1026},
	abstract = {This paper summarizes work on spoken language at the Department of Linguistics Göteborg University. In addition to describing the recordings contained in the Spoken Language Corpus of Swedish at Göteborg University, we discuss the standard of transcription (MSO) which is used in creating the transcriptions, as well as some types of quantitative and qualitative analysis that have been done. Finally, we describe the computer tools that have been developed to support transcription, coding and analysis and briefly mention some of the results which have been obtained.
URN: urn:nbn:de:0114-fqs000391},
	language = {en},
	number = {3},
	urldate = {2022-12-02},
	journal = {Forum Qualitative Sozialforschung / Forum: Qualitative Social Research},
	author = {Allwood, Jens and Grönqvist, Leif and Björnberg, Maria and Ahlsen, Elisabeth and Ottesjö, Cajsa},
	month = dec,
	year = {2000},
	note = {Number: 3},
	keywords = {coding, computer tool, corpus, Göteborg, MSO, qualitative analysis, quantitative analysis, spoken language, transcription standard},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\GTJ6LQZ3\\Allwood et al. - 2000 - The Spoken Language Corpus at the Linguistics Depa.pdf:application/pdf},
}

@book{strazny_encyclopedia_2005,
	address = {New York},
	title = {Encyclopedia of linguistics},
	isbn = {978-1-57958-391-0 978-1-57958-450-4 978-1-57958-451-1},
	publisher = {Fitzroy Dearborn},
	editor = {Strazny, Philipp},
	year = {2005},
	keywords = {Encyclopedias, Linguistics},
	annote = {v. 1. A-L-- v. 2 M-Z},
}

@inproceedings{k_kirchhoff_processing_2007,
	title = {Processing {Morphologically}-{Rich} {Languages}},
	author = {K. Kirchhoff and R. Sarikay},
	year = {2007},
	note = {https://www.researchgate.net/publication/224506277\_Introduction\_to\_the\_Special\_Issue\_on\_Processing\_Morphologically\_Rich\_Languages},
}

@article{sarikaya_introduction_2009,
	title = {Introduction to the {Special} {Issue} on {Processing} {Morphologically} {Rich} {Languages}},
	volume = {17},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5075779/},
	doi = {10.1109/TASL.2009.2023314},
	number = {5},
	urldate = {2022-12-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Sarikaya, Ruhi and Kirchhoff, Katrin and Schultz, Tanja and Hakkani-Tur, Dilek},
	month = jul,
	year = {2009},
	pages = {861--862},
}

@article{mihajlik_improved_2010,
	title = {Improved {Recognition} of {Spontaneous} {Hungarian} {Speech}—{Morphological} and {Acoustic} {Modeling} {Techniques} for a {Less} {Resourced} {Task}},
	volume = {18},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5356225/},
	doi = {10.1109/TASL.2009.2038807},
	number = {6},
	urldate = {2022-12-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Mihajlik, P and Tuske, Z and Tarján, B and Németh, B and Fegyó, T},
	month = aug,
	year = {2010},
	pages = {1588--1600},
}

@article{arisoy_turkish_2009,
	title = {Turkish {Broadcast} {News} {Transcription} and {Retrieval}},
	volume = {17},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5071138/},
	doi = {10.1109/TASL.2008.2012313},
	number = {5},
	urldate = {2022-12-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Arisoy, Ebru and Can, Dogan and Parlak, Siddika and Sak, Hasim and Saraclar, Murat},
	month = jul,
	year = {2009},
	pages = {874--883},
}

@inproceedings{kurimo_unlimited_2006,
	address = {New York, New York},
	title = {Unlimited vocabulary speech recognition for agglutinative languages},
	url = {http://portal.acm.org/citation.cfm?doid=1220835.1220897},
	doi = {10.3115/1220835.1220897},
	language = {en},
	urldate = {2022-12-02},
	booktitle = {Proceedings of the main conference on {Human} {Language} {Technology} {Conference} of the {North} {American} {Chapter} of the {Association} of {Computational} {Linguistics}  -},
	publisher = {Association for Computational Linguistics},
	author = {Kurimo, Mikko and Puurula, Antti and Arisoy, Ebru and Siivola, Vesa and Hirsimäki, Teemu and Pylkkönen, Janne and Alumäe, Tanel and Saraclar, Murat},
	year = {2006},
	pages = {487--494},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\Q3WL2Q6U\\Kurimo et al. - 2006 - Unlimited vocabulary speech recognition for agglut.pdf:application/pdf},
}

@book{tarjan_morph_2010,
	title = {On {Morph} {Based} {LVCSR} {Improvements}},
	abstract = {Efficient large vocabulary continuous speech recognition of morphologically rich languages is a big challenge due to the rapid vocabulary growth. To improve the results various subword units -called as morphs -are applied as basic language elements. The improvements over the word baseline, however, are changing from negative to error rate halving across languages and tasks. In this paper we make an attempt to explore the source of this variability. Different LVCSR tasks of an agglutinative language are investigated in numerous experiments using full vocabularies. The improvement results are compared to pre-existing other language results, as well. Important correlations are found between the morph-based improvements and between the vocabulary growths and the corpus sizes.},
	author = {Tarján, Balázs and Mihajlik, Péter},
	month = jan,
	year = {2010},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\87XBNSGN\\Tarján and Mihajlik - 2010 - On Morph Based LVCSR Improvements.pdf:application/pdf},
}

@inproceedings{gebreegziabher_sub-word_2020,
	address = {Toronto, ON, Canada},
	title = {Sub-word {Based} {End}-to-{End} {Speech} {Recognition} for an {Under}-{Resourced} {Language}: {Amharic}},
	isbn = {978-1-72818-526-2},
	shorttitle = {Sub-word {Based} {End}-to-{End} {Speech} {Recognition} for an {Under}-{Resourced} {Language}},
	url = {https://ieeexplore.ieee.org/document/9283401/},
	doi = {10.1109/SMC42975.2020.9283401},
	urldate = {2022-12-02},
	booktitle = {2020 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	publisher = {IEEE},
	author = {Gebreegziabher, Nirayo Hailu and Nurnberger, Andreas},
	month = oct,
	year = {2020},
	pages = {3466--3470},
}

@article{toth_speech_2010,
	title = {Speech {Recognition} {Experiments} with {Audiobooks}.},
	volume = {19},
	abstract = {Under real-life conditions several factors may be present that make the automatic recognition of speech difficult. The most obvious examples are background noise, peculiarities of the speaker's voice, sloppy articulation and strong emotional load. These all pose difficult problems for robust speech recognition, but it is not exactly clear how much each contributes to the difficulty of the task. In this paper we examine the abilities of our best recognition technologies under near-ideal conditions. The optimal conditions will be simulated by working with the sound material of an audiobook, in which most of the disturbing factors mentioned above are absent. Firstly pure phone recognition experiments will be performed, where neural net-based technologies will also be tried as well as the conventional Hidden Markov Models. Then we move on to large vocabulary recognition, where morphbased language models are applied to improve the performance of the standard word-based technology. The tests clearly justify our assertion that audiobooks pose a much easier recognition task than real-life databases. In both types of tasks we report the lowest error rates we have achieved so far in Hungarian continuous speech recognition.},
	journal = {Acta Cybern.},
	author = {Tóth, László and Tarján, Balázs and Sárosi, Gellért and Mihajlik, Péter},
	month = jan,
	year = {2010},
	pages = {695--713},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IMQP2BWU\\Tóth et al. - 2010 - Speech Recognition Experiments with Audiobooks..pdf:application/pdf},
}

@book{tarjan_evaluation_2011,
	title = {Evaluation of lexical models for {Hungarian} {Broadcast} speech transcription and spoken term detection},
	abstract = {In this paper, we re-evaluate morph (data-driven subword) and word lexical models used for large vocabulary continuous speech recognition of agglutinative languages. Since such speech recognition systems are applied mostly for information retrieval purposes we use evaluation metrics accordingly. Standard 3-gram language model with one million words vocabulary is used for words whereas statistical morph-based models are applied with smaller vocabularies and with higher order of n-gram models. Fostering real life applicability, the computational time and memory usage of the various approaches is kept below real-time and 1.5 GB, respectively. The lexical modeling approaches are tested on Hungarian Broadcast News and Broadcast Conversation speech. In our setup, although word-based models outperformed morph-based ones in terms of both word error rate and spoken term detection measures, a search-cascade of the word and morph approaches improved the latter results significantly.},
	author = {Tarján, Balázs and Mihajlik, Péter and Balog, András and Fegyó, Tibor},
	month = aug,
	year = {2011},
	note = {Pages: 5},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PF38AIDL\\Tarján et al. - 2011 - Evaluation of lexical models for Hungarian Broadca.pdf:application/pdf},
}

@article{guglani_automatic_2020,
	title = {Automatic speech recognition system with pitch dependent features for {Punjabi} language on {KALDI} toolkit},
	volume = {167},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X20304904},
	doi = {10.1016/j.apacoust.2020.107386},
	language = {en},
	urldate = {2022-12-02},
	journal = {Applied Acoustics},
	author = {Guglani, Jyoti and Mishra, A.N.},
	month = oct,
	year = {2020},
	pages = {107386},
}

@article{kaur_automatic_2021,
	title = {Automatic {Speech} {Recognition} {System} for {Tonal} {Languages}: {State}-of-the-{Art} {Survey}},
	volume = {28},
	issn = {1134-3060, 1886-1784},
	shorttitle = {Automatic {Speech} {Recognition} {System} for {Tonal} {Languages}},
	url = {https://link.springer.com/10.1007/s11831-020-09414-4},
	doi = {10.1007/s11831-020-09414-4},
	language = {en},
	number = {3},
	urldate = {2022-12-02},
	journal = {Archives of Computational Methods in Engineering},
	author = {Kaur, Jaspreet and Singh, Amitoj and Kadyan, Virender},
	month = may,
	year = {2021},
	pages = {1039--1068},
}

@book{winskel_south_2013,
	edition = {1},
	title = {South and {Southeast} {Asian} {Psycholinguistics}},
	isbn = {978-1-139-08464-2 978-1-107-01776-4},
	url = {https://www.cambridge.org/core/product/identifier/9781139084642/type/book},
	abstract = {A large body of knowledge has accumulated in recent years on the cognitive processes underlying language, much of which comes from studies of Indo-European languages, in particular English. This groundbreaking volume explores the languages of South and Southeast Asia, which differ significantly from Indo-European languages in their grammar, lexicon and spoken forms. This book raises new questions in psycholinguistics and enables readers to re-evaluate previous models in light of new research. With thirty-six chapters divided into three parts - Language Acquisition, Language Processing and Language and Brain - it examines contemporary topics alongside new findings in areas such as first and second language acquisition, the development of literacy, the diagnosis of language and reading disorders, and the relationship between language, brain, culture and cognition. It will be invaluable to all those interested in the languages of South and Southeast Asia, as well as psychologists, linguists, educationalists, speech therapists and neuroscientists.},
	urldate = {2022-12-02},
	publisher = {Cambridge University Press},
	editor = {Winskel, Heather and Padakannaya, Prakash},
	month = nov,
	year = {2013},
	doi = {10.1017/CBO9781139084642},
}

@inproceedings{benzeghiba_impact_2006,
	address = {Saint-Petersburg, Russia},
	title = {Impact of variabilities on speech recognition},
	abstract = {Major progress is being recorded regularly on both the technol-ogy and exploitation of Automatic Speech Recognition (ASR) and spoken language systems. However, there are still techno-logical barriers to flexible solutions and user satisfaction under some circumstances. This is related to several factors, such as the sensitivity to the environment (background noise or chan-nel variability), or the weak representation of grammatical and semantic knowledge. Current research is also emphasizing deficiencies in deal-ing with variation naturally present in speech. For instance, the lack of robustness to foreign accents precludes the use by spe-cific populations. There are actually many factors affecting the speech realization: regional, sociolinguistic, or related to the environment or the speaker itself. These create a wide range of variations that may not be modeled correctly (speaker, gen-der, speech rate, vocal effort, regional accents, speaking style, non stationarity...), especially when resources for system train-ing are scarce. This paper outlines some current advances related to vari-abilities in ASR.},
	publisher = {SPECOM'2006},
	author = {Benzeghiba, Mohamed and De Mori, Renato and Deroo, Olivier and Dupont, Stéphane and Jouvet, Denis and Fissore, Luciano and Laface, Pietro and Ris, Alfred and Rose, Richard and Tyagi, Vivek and Wellekens, Christian},
	month = jun,
	year = {2006},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\7NQI8C76\\Benzeghiba et al. - 2022 - Impact of variabilities on speech recognition.pdf:application/pdf},
}

@book{divenyi_speech_2005,
	address = {Boston, MA},
	title = {Speech {Separation} by {Humans} and {Machines}},
	isbn = {978-1-4020-8001-2 978-0-387-22794-8},
	url = {http://link.springer.com/10.1007/b99695},
	language = {en},
	urldate = {2022-12-02},
	publisher = {Springer US},
	editor = {Divenyi, Pierre},
	year = {2005},
	doi = {10.1007/b99695},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\STDG3E65\\Divenyi - 2005 - Speech Separation by Humans and Machines.pdf:application/pdf},
}

@article{noauthor_international_2011,
	title = {International {Journal} of {Computer} {Science} {Issues}},
	volume = {8},
	issn = {1694-0814},
	url = {www.IJCSI.org},
	number = {5 No. 3},
	month = sep,
	year = {2011},
}

@article{jabloun_large_1999,
	title = {Large {Vocabulary} {Speech} {Recognition} {In} {Noisy} {Environments}},
	abstract = {LARGE VOCABULARY SPEECH RECOGNITION IN NOISY ENVIRONMENTS Firas Jabloun M.S. in Electrical and Electronics Engineering Supervisor: A. Enis Cetin, Ph. D. July 1998 A new set of speech feature parameters based on multirate subband analysis and the Teager Energy Operator (TEO) is developed. The speech signal is first divided into nonuniform subbands in mel-scale using a multirate filter-bank, then the Teager energies of the subsignals are estimated. Finally, the feature vector is constructed by logcompression and inverse DCT computation. The new feature parameters (TEOCEP) have a robust speech recognition performance in car engine noise which has a low pass nature. In this thesis, we also present some solutions to the problem of large vocabulary speech recognition. Triphone-based Hidden Markov Models (HMM) are used to model the vocabulary words. Although the straight forward parallel search strategy gives good recognition performance, the processing time required is found to be long...},
	author = {Jabloun, Firas and Cetin, A. and (supervisor, Ph and Arikan, Orhan and Demirekler, Mubeccel},
	month = oct,
	year = {1999},
}

@article{zouhir_feature_2016,
	title = {Feature {Extraction} {Method} for {Improving} {Speech} {Recognition} in {Noisy} {Environments}},
	volume = {12},
	issn = {1549-3636},
	url = {http://thescipub.com/abstract/10.3844/jcssp.2016.56.61},
	doi = {10.3844/jcssp.2016.56.61},
	number = {2},
	urldate = {2022-12-02},
	journal = {Journal of Computer Science},
	author = {Zouhir, Youssef and Ouni, Kaïs},
	month = feb,
	year = {2016},
	pages = {56--61},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\BAPYGRW4\\Zouhir and Ouni - 2016 - Feature Extraction Method for Improving Speech Rec.pdf:application/pdf},
}

@article{kim_auditory_1996,
	title = {Auditory {Representations} for {Robust} {Speech} {Recognition} in {Noisy} {Environments}},
	volume = {15},
	abstract = {An auditory model is proposed for robust speech recognition in noisy environments. The model consists of cochlear bandpass filters and nonlinear stages, and represents frequency and intensity information efficiently even in noisy environments. Frequency information of the signal is obtained by zero-crossing intervals, and intensity information is also incorporated by peak detectors and saturating nonlinearities. Also, the robustness of the zero-crossings in estimating frequency is verified by the developed analytic relationship of the variance of the level-crossing interval perturbations as a function of the crossing level values. The proposed auditory model is computationally efficient and free from many unknown parameters compared with other auditory models. Speaker-independent speech recognition experiments demonstrate the robustness of the proposed method.},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Kim, Doh-Suk and Lee, Soo-Young and Kil, Rhee},
	month = jan,
	year = {1996},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IU5LJCZE\\Kim et al. - 1996 - Auditory Representations for Robust Speech Recogni.pdf:application/pdf},
}

@article{kumar_hindi_2021,
	title = {Hindi speech recognition in noisy environment using hybrid technique},
	volume = {13},
	issn = {2511-2104, 2511-2112},
	url = {https://link.springer.com/10.1007/s41870-020-00586-7},
	doi = {10.1007/s41870-020-00586-7},
	language = {en},
	number = {2},
	urldate = {2022-12-02},
	journal = {International Journal of Information Technology},
	author = {Kumar, Ashok and Mittal, Vikas},
	month = apr,
	year = {2021},
	pages = {483--492},
}

@article{singh_computational_2022,
	title = {Computational intelligence in processing of speech acoustics: a survey},
	volume = {8},
	issn = {2199-4536, 2198-6053},
	shorttitle = {Computational intelligence in processing of speech acoustics},
	url = {https://link.springer.com/10.1007/s40747-022-00665-1},
	doi = {10.1007/s40747-022-00665-1},
	abstract = {Abstract
            Speech recognition of a language is a key area in the field of pattern recognition. This paper presents a comprehensive survey on the speech recognition techniques for non-Indian and Indian languages, and compiled some of the computational models used for processing speech acoustics. An immense number of frameworks are available for speech processing and recognition for languages persisting around the globe. However, a limited number of automatic speech recognition systems are available for commercial use. The gap between the languages being spoken around the globe and the technical support available to these languages are very few. This paper examined major challenges for speech recognition for different languages. Analysis of the literature shows that lack of standard databases availability of minority languages hinder the research recognition research across the globe. When compared with non-Indian languages, the research on speech recognition of Indian languages (except Hindi) has not achieved the expected milestone yet. Combination of MFCC and DNN–HMM classifier is most commonly used system for developing ASR minority languages, whereas in some of the majority languages, researchers are using much advance algorithms of DNN. It has also been observed that the research in this field is quite thin and still more research needs to be carried out, particularly in the case of minority languages.},
	language = {en},
	number = {3},
	urldate = {2022-12-02},
	journal = {Complex \& Intelligent Systems},
	author = {Singh, Amitoj and Kaur, Navkiran and Kukreja, Vinay and Kadyan, Virender and Kumar, Munish},
	month = jun,
	year = {2022},
	pages = {2623--2661},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CKYWJ53Y\\Singh et al. - 2022 - Computational intelligence in processing of speech.pdf:application/pdf},
}

@phdthesis{pedro_j_moreno_speech_1996,
	address = {Pittsburgh, Pennsylvania, 15213},
	title = {Speech {Recognition} in {Noisy} {Environments}},
	url = {http://www.cs.cmu.edu/~robust/Thesis/pjm_thesis.pdf},
	school = {Carnegie Mellon University},
	author = {Pedro J Moreno},
	month = apr,
	year = {1996},
	note = {Departement of Electrical and Computer Engineering},
}

@article{loizou_reasons_2011,
	title = {Reasons why {Current} {Speech}-{Enhancement} {Algorithms} do not {Improve} {Speech} {Intelligibility} and {Suggested} {Solutions}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5428850/},
	doi = {10.1109/TASL.2010.2045180},
	number = {1},
	urldate = {2022-12-04},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Loizou, Philipos C. and Kim, Gibak},
	month = jan,
	year = {2011},
	pages = {47--56},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\HFPMJZDZ\\Loizou and Kim - 2011 - Reasons why Current Speech-Enhancement Algorithms .pdf:application/pdf},
}

@incollection{shi_noise_2010,
	address = {Berlin, Heidelberg},
	title = {Noise {Estimation} and {Noise} {Removal} {Techniques} for {Speech} {Recognition} in {Adverse} {Environment}},
	volume = {340},
	isbn = {978-3-642-16326-5 978-3-642-16327-2},
	url = {http://link.springer.com/10.1007/978-3-642-16327-2_40},
	urldate = {2022-12-04},
	booktitle = {Intelligent {Information} {Processing} {V}},
	publisher = {Springer Berlin Heidelberg},
	author = {Shrawankar, Urmila and Thakare, Vilas},
	editor = {Shi, Zhongzhi and Vadera, Sunil and Aamodt, Agnar and Leake, David},
	year = {2010},
	doi = {10.1007/978-3-642-16327-2_40},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {336--342},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ESHRZH64\\Shrawankar and Thakare - 2010 - Noise Estimation and Noise Removal Techniques for .pdf:application/pdf},
}

@article{schuller_recognition_2009-1,
	title = {Recognition of {Noisy} {Speech}: {A} {Comparative} {Survey} of {Robust} {Model} {Architecture} and {Feature} {Enhancement}},
	volume = {2009},
	issn = {1687-4722},
	shorttitle = {Recognition of {Noisy} {Speech}},
	url = {http://asmp.eurasipjournals.com/content/2009/1/942617},
	doi = {10.1155/2009/942617},
	language = {en},
	number = {1},
	urldate = {2022-12-04},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Schuller, Björn and Wöllmer, Martin and Moosmayr, Tobias and Rigoll, Gerhard},
	year = {2009},
	pages = {942617},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\VEKLJABS\\Schuller et al. - 2009 - Recognition of Noisy Speech A Comparative Survey .pdf:application/pdf},
}

@article{haton_problems_1994-1,
	title = {Problems and solutions for noisy speech recognition},
	volume = {04},
	issn = {1155-4339},
	url = {http://www.edpsciences.org/10.1051/jp4:1994592},
	doi = {10.1051/jp4:1994592},
	number = {C5},
	urldate = {2022-12-04},
	journal = {Le Journal de Physique IV},
	author = {Haton, J.-P.},
	month = may,
	year = {1994},
	pages = {C5--439--C5--448},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\8C39VFKG\\Haton - 1994 - Problems and solutions for noisy speech recognitio.pdf:application/pdf},
}

@article{stern_compensation_2022,
	title = {{COMPENSATION} {FOR} {ENVIRONMENTAL} {DEGRADATION} {IN} {AUTOMATIC} {SPEECH} {RECOGNITION}},
	abstract = {The accuracy of speech recognition systems degrades when operated in adverse acoustical environments. This paper reviews various methods by which more detailed mathematical descriptions of the effects of environmental degradation can improve speech recognition accuracy using both "data-driven" and "model-based" compensation strategies. Data-driven meth- ods learn environmental characteristics through direct compari- sons of speech recorded in the noisy environment with the same speech recorded under optimal conditions. Model-based methods use a mathematical model of the environment and attempt to use samples of the degraded speech to estimate model parameters. These general approaches to environmental compensation are discussed in terms of recent research in envi- ronmental robustness at CMU, and in terms of similar efforts at other sites. These compensation algorithms are evaluated in a series of experiments measuring recognition accuracy for speech from the ARPA Wall Street Journal database that is cor- rupted by artificially-added noise at various signal-to-noise ratios (SNRs), and in more natural speech recognition tasks.},
	author = {Stern, Richard and Raj, Bhiksha and Moreno, Pedro},
	month = dec,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\5S3VJVM3\\Stern et al. - 2022 - COMPENSATION FOR ENVIRONMENTAL DEGRADATION IN AUTO.pdf:application/pdf},
}

@article{bahari_speaker_2014,
	title = {Speaker age estimation using i-vectors},
	volume = {34},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197614001018},
	doi = {10.1016/j.engappai.2014.05.003},
	language = {en},
	urldate = {2022-12-04},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Bahari, Mohamad Hasan and McLaren, Mitchell and Van hamme, Hugo and van Leeuwen, David A.},
	month = sep,
	year = {2014},
	pages = {99--108},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\QMGUFD4Q\\Bahari et al. - 2014 - Speaker age estimation using i-vectors.pdf:application/pdf},
}

@article{urmila_shrawankar_voice_2010,
	title = {Voice {Activity} {Detector} and {Noise} {Trackers} for {Speech} {Recognition} {System} in {Noisy} {Environment}},
	volume = {2},
	issn = {2005-8039, 2233-9337},
	url = {http://www.aicit.org/ijact/paper_detail.html?q=92},
	doi = {10.4156/ijact.vol2.issue4.11},
	number = {4},
	urldate = {2022-12-04},
	journal = {International Journal of Advancements in Computing Technology},
	author = {Urmila Shrawankar, Vilas Thakare},
	month = oct,
	year = {2010},
	pages = {107--114},
}

@techreport{jasha_droppo_comparing_nodate,
	title = {Comparing {Human} and {Machine} {Errors} in {Conversational} {Speech} {Transcription}},
	url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-revised2.pdf},
	institution = {Microsoft AI and Research, Redmond, WA, USA},
	author = {Jasha Droppo and Andreas Stolcke},
}

@article{plaza_influence_2021,
	title = {Influence of the {Contact} {Center} {Systems} {Development} on {Key} {Performance} {Indicators}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9381200/},
	doi = {10.1109/ACCESS.2021.3066801},
	urldate = {2022-12-06},
	journal = {IEEE Access},
	author = {Plaza, Miroslaw and Pawlik, Lukasz},
	year = {2021},
	pages = {44580--44591},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\XV48KS4C\\Plaza and Pawlik - 2021 - Influence of the Contact Center Systems Developmen.pdf:application/pdf},
}

@article{plaza_call_2021-1,
	title = {Call {Transcription} {Methodology} for {Contact} {Center} {Systems}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9508438/},
	doi = {10.1109/ACCESS.2021.3102502},
	urldate = {2022-12-06},
	journal = {IEEE Access},
	author = {Plaza, Miroslaw and Pawlik, Lukasz and Deniziak, Stanislaw},
	year = {2021},
	pages = {110975--110988},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\T9DWNZTZ\\Plaza et al. - 2021 - Call Transcription Methodology for Contact Center .pdf:application/pdf},
}

@incollection{hu_analysis_2021,
	address = {Cham},
	title = {Analysis of {Key} {Elements} for {Modern} {Contact} {Center} {Systems} to {Improve} {Quality}},
	volume = {83},
	isbn = {978-3-030-80471-8 978-3-030-80472-5},
	url = {https://link.springer.com/10.1007/978-3-030-80472-5_14},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Advances in {Computer} {Science} for {Engineering} and {Education} {IV}},
	publisher = {Springer International Publishing},
	author = {Aitchanov, Bekmurza and Baimuratov, Olimzhon and Zhussupekov, Muratbek and Aitchanov, Tley},
	editor = {Hu, Zhengbing and Petoukhov, Sergey and Dychka, Ivan and He, Matthew},
	year = {2021},
	doi = {10.1007/978-3-030-80472-5_14},
	note = {Series Title: Lecture Notes on Data Engineering and Communications Technologies},
	pages = {162--174},
}

@inproceedings{mamou_spoken_2006,
	address = {Seattle, Washington, USA},
	title = {Spoken document retrieval from call-center conversations},
	isbn = {978-1-59593-369-0},
	url = {http://portal.acm.org/citation.cfm?doid=1148170.1148183},
	doi = {10.1145/1148170.1148183},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '06},
	publisher = {ACM Press},
	author = {Mamou, Jonathan and Carmel, David and Hoory, Ron},
	year = {2006},
	pages = {51},
}

@inproceedings{mishne_automatic_2005,
	address = {Bremen, Germany},
	title = {Automatic analysis of call-center conversations},
	isbn = {978-1-59593-140-5},
	url = {http://portal.acm.org/citation.cfm?doid=1099554.1099684},
	doi = {10.1145/1099554.1099684},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the 14th {ACM} international conference on {Information} and knowledge management  - {CIKM} '05},
	publisher = {ACM Press},
	author = {Mishne, Gilad and Carmel, David and Hoory, Ron and Roytman, Alexey and Soffer, Aya},
	year = {2005},
	pages = {453},
}

@inproceedings{busemann_message_2000,
	address = {Seattle, Washington},
	title = {Message classification in the call center},
	url = {http://portal.acm.org/citation.cfm?doid=974147.974169},
	doi = {10.3115/974147.974169},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the sixth conference on {Applied} natural language processing  -},
	publisher = {Association for Computational Linguistics},
	author = {Busemann, Stephan and Schmeier, Sven and Arens, Roman G.},
	year = {2000},
	pages = {158--165},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\7PBEW9N9\\Busemann et al. - 2000 - Message classification in the call center.pdf:application/pdf},
}

@inproceedings{park_automatic_2007,
	address = {Lisbon, Portugal},
	title = {Automatic call section segmentation for contact-center calls},
	isbn = {978-1-59593-803-9},
	url = {http://portal.acm.org/citation.cfm?doid=1321440.1321459},
	doi = {10.1145/1321440.1321459},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the sixteenth {ACM} conference on {Conference} on information and knowledge management  - {CIKM} '07},
	publisher = {ACM Press},
	author = {Park, Youngja},
	year = {2007},
	pages = {117},
}

@article{hu_neural_2022,
	title = {Neural {Architecture} {Search} for {LF}-{MMI} {Trained} {Time} {Delay} {Neural} {Networks}},
	volume = {30},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9721103/},
	doi = {10.1109/TASLP.2022.3153253},
	urldate = {2022-12-06},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hu, Shoukang and Xie, Xurong and Cui, Mingyu and Deng, Jiajun and Liu, Shansong and Yu, Jianwei and Geng, Mengzhe and Liu, Xunying and Meng, Helen},
	year = {2022},
	pages = {1093--1107},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\9AQIYJXB\\Hu et al. - 2022 - Neural Architecture Search for LF-MMI Trained Time.pdf:application/pdf},
}

@misc{linguistic_data_consortium_2000_2002,
	title = {2000 {HUB5} {English} {Evaluation} {Transcripts}},
	url = {https://catalog.ldc.upenn.edu/LDC2002T43},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}2000 HUB5 English Evaluation Transcripts was developed by the Linguistic Data Consortium (LDC)\&nbsp; and consists of transcripts of 40 English telephone conversations used in the 2000 HUB5 evaluation sponsored by\&nbsp;{\textless}a href="http://nist.gov/itl/iad/mig/"{\textgreater}NIST{\textless}/a{\textgreater} (National Institute of Standards and Technology).\&nbsp;{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The Hub5 evaluation series focused on conversational speech over the telephone with the particular task of transcribing conversational speech into text. Its goals were to explore promising new areas in the recognition of conversational speech, to develop advanced technology incorporating those ideas and to measure the performance of new technology.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Data{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}This release contains transcripts in .txt format for the 40 source speech data files used in the evaluation: (1) 20 unreleased telephone conversations from the Swtichboard studies in which recruited speakers were connected through a robot operator to carry on casual conversations about a daily topic announced by the robot operator at the start of the call; and (2) 20 telephone conversations from {\textless}a href="http://catalog.ldc.upenn.edu/LDC97S42" rel="nofollow"{\textgreater}CALLHOME American English Speech{\textless}/a{\textgreater} which consists of unscripted telephone conversations between native English speakers.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The corresponding speech files are availalbe in 2000 HUB5 English Evaluation Speech ({\textless}a href="../../../LDC2002S09"{\textgreater}LDC2002S09{\textless}/a{\textgreater}).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Please view this {\textless}a href="desc/addenda/LDC2002T43.txt"{\textgreater}sample{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Updates{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}There are no updates at this time.{\textless}/p{\textgreater}{\textless}/br{\textgreater} 
Portions © 1997, 2000, 2002 Trustees of the University of Pennsylvania},
	urldate = {2022-12-06},
	publisher = {Linguistic Data Consortium},
	author = {Linguistic Data Consortium},
	month = jan,
	year = {2002},
	doi = {10.35111/8C95-0C55},
	note = {Artwork Size: 4848 KB
Pages: 4848 KB
Type: dataset},
}

@misc{fiscus_jonathan_g_2003_2007,
	title = {2003 {NIST} {Rich} {Transcription} {Evaluation} {Data}},
	url = {https://catalog.ldc.upenn.edu/LDC2007S10},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}2003 NIST Rich Transcription Evaluation Data contains the test material used in the 2003 Rich Transcription Spring and Fall evaluations administered by the {\textless}a href="http://www.nist.gov/speech" rel="nofollow"{\textgreater}NIST (National Institute of Standards and Technology) Speech Group{\textless}/a{\textgreater}. The Spring evaluation (RT-03S), implemented in March-April 2003, focused on Speech-To-Text (STT) tasks for broadcast news speech and conversational telephone speech in three languages: English, Mandarin Chinese and Arabic. That evaluation also included one Metadata Extraction (MDE) task, speaker diarization for broadcast news speech and conversational telephone speech in English. The Fall evaluation (RT-03F), implemented in October 2003, focused on MDE tasks including speaker diarization, speaker-attributed STT, SU (sentence/semantic unit) detection and disfluency detection for broadcast news speech and conversational telephone speech in English. For complete information about the evaluations, see the {\textless}a href="https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation"{\textgreater}Rich Text Evaluation website{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Data{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The BN datasets were selected from {\textless}a href="http://projects.ldc.upenn.edu/TDT4/" rel="nofollow"{\textgreater}TDT-4{\textless}/a{\textgreater} sources collected in February 2001. The evaluation excerpts were transcribed to the nearest story boundary. The English BN dataset is approximately three hours long and is composed of 30-minute excerpts from six different broadcasts. The Mandarin Chinese BN dataset is approximately one hour long, consisting of 12-minute excerpts from five different broadcasts. The Arabic BN dataset is also approximately one hour long and contains 30-minute excerpts from two different broadcasts.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The CTS datasets consist of material from various LDC telephone speech data. All evaluation excerpts were transcribed to the nearest turn. The English CTS set is approximately 6 hours long and is composed of 5-minute excerpts from 72 different conversations: 36 from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC2001S13" rel="nofollow"{\textgreater}Switchboard Cellular{\textless}/a{\textgreater} collection and 36 from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC2004S13" rel="nofollow"{\textgreater}Fisher collection{\textless}/a{\textgreater}. The Mandarin Chinese CTS dataset is approximately one hour long and consists of 5-minute excerpts from 12 different conversations from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC96S55" rel="nofollow"{\textgreater}CallFriend Mandarin Chinese data{\textless}/a{\textgreater}. The Arabic CTS set is also approximately one hour long and contains 5-minute excerpts from 12 different conversations from the {\textless}a href="http://catalog.ldc.upenn.edu/LDC97S45" rel="nofollow"{\textgreater}CallHome Egyptian Arabic data{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}No manual (human-annotated) segmentations were provided. Sites were required to generate their own segmentations automatically.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Unlike the BN audio files where the full broadcasts were provided, the CTS audio files contain only the evaluation excerpts. Each audio excerpt is a SPHERE-headered, two channel interleaved 8-bit mulaw file.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}ul{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC2007S10.wav" rel="nofollow"{\textgreater}English Broacast News Audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC2007S10\_ind.txt" rel="nofollow"{\textgreater}Indices{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC2007S10.txt" rel="nofollow"{\textgreater}Transcriptions{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}/ul{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The World is a co-production of Public Radio International and the British Broadcasting Corporation and is produced at WGBH Boston.{\textless}/p{\textgreater}{\textless}/br{\textgreater} 
Portions © 2001 American Broadcasting Company, © 2001 Cable News Network, LP, LLLP, © 2001 China Broadcasting System (Taiwan), © 2001 China Central TV, © 2001 China National Radio, © 2001 China Television System (Taiwan), © 2001 National Broadcasting Company, © 2001 Nile TV, © 2001 Public Radio International, © 1996-2005, 2007 Trustees of the University of Pennsylvania{\textless}br{\textgreater}{\textless}br{\textgreater}The World is a co-production of Public Radio International and the British Broadcasting Corporation and is produced at WGBH Boston.},
	urldate = {2022-12-06},
	publisher = {Linguistic Data Consortium},
	author = {Fiscus, Jonathan G. and Doddington, George R. and Le, Audrey and Sanders, Greg and Przybocki, Mark and Pallett, David},
	month = aug,
	year = {2007},
	doi = {10.35111/V8J8-M006},
	note = {Artwork Size: 2097152 KB
Pages: 2097152 KB
Type: dataset},
}
