
@inproceedings{farooq_improving_2019,
	title = {Improving {Large} {Vocabulary} {Urdu} {Speech} {Recognition} {System} {Using} {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html},
	doi = {10.21437/Interspeech.2019-2629},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Rauf, Sahar and Hussain, Sarmad},
	month = sep,
	year = {2019},
	pages = {2978--2982},
}

@misc{nerd_of_the_rings_complete_2020,
	title = {The {Complete} {Travels} of {Galadriel} {\textbar} {Tolkien} {Explained}},
	url = {https://www.youtube.com/watch?v=yDX-2rTxE6Y},
	urldate = {2022-08-16},
	author = {{Nerd of the Rings}},
	month = nov,
	year = {2020},
}

@misc{alphacephei_alpha_nodate,
	title = {Alpha {Cephei}},
	url = {https://github.com/alphacep},
	abstract = {Alpha Cephei has 37 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2022-08-16},
	journal = {GitHub},
	author = {Alphacephei},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\C9UHNTWM\\alphacep.html:text/html},
}

@misc{alphacep_vosk_2022,
	title = {Vosk {Speech} {Recognition} {Toolkit}},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-api},
	abstract = {Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C\# and Node},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-09-03T17:48:42Z},
	keywords = {android, asr, deep-learning, deep-neural-networks, deepspeech, google-speech-to-text, ios, kaldi, offline, privacy, python, raspberry-pi, speaker-identification, speaker-verification, speech-recognition, speech-to-text, speech-to-text-android, stt, voice-recognition, vosk},
}

@misc{alphacep_alphacepvosk-server_2022,
	title = {alphacep/vosk-server},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-server},
	abstract = {WebSocket, gRPC and WebRTC speech recognition server based on Vosk and Kaldi libraries},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-05-07T17:24:55Z},
	keywords = {asr, kaldi, python, speech-recognition, vosk, grpc, saas, webrtc, websocket},
}

@misc{alphacep_vosk_nodate,
	title = {{VOSK} {Offline} {Speech} {Recognition} {API}},
	url = {https://alphacephei.com/vosk/},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\H4HQA2XC\\vosk.html:text/html},
}

@misc{alphacep_vosk_nodate-1,
	title = {{VOSK} {Models}},
	url = {https://alphacephei.com/vosk/models},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\J75LJ82E\\models.html:text/html},
}

@misc{daniel_povey_kaldi_nodate,
	title = {Kaldi: {Kaldi}},
	url = {https://kaldi-asr.org/doc/},
	urldate = {2022-08-16},
	author = {Daniel Povey},
	file = {Kaldi\: Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\QYDHVT6P\\doc.html:text/html},
}

@misc{noauthor_openslrorg_nodate,
	title = {openslr.org},
	url = {https://www.openslr.org/resources.php},
	urldate = {2022-08-16},
	file = {openslr.org:C\:\\Users\\DELL\\Zotero\\storage\\EJWY9PRN\\resources.html:text/html},
}

@misc{cmu_cmu_nodate,
	title = {{CMU} {Lexicon} {Tool}},
	url = {http://www.speech.cs.cmu.edu/tools/lextool.html},
	urldate = {2022-08-16},
	author = {CMU},
	file = {CMU Lexicon Tool:C\:\\Users\\DELL\\Zotero\\storage\\BTMXKZQL\\lextool.html:text/html},
}

@misc{coqui-ai_coqui-aistt_2022,
	title = {coqui-ai/{STT}},
	copyright = {MPL-2.0},
	url = {https://github.com/coqui-ai/STT},
	abstract = {��STT - The deep learning toolkit for Speech-to-Text. Training and deploying STT models has never been so easy.},
	urldate = {2022-08-16},
	publisher = {coqui},
	author = {coqui-ai},
	month = aug,
	year = {2022},
	note = {original-date: 2021-03-04T04:54:42Z},
	keywords = {asr, deep-learning, speech-recognition, speech-to-text, stt, voice-recognition, automatic-speech-recognition, speech-recognition-api, speech-recognizer, tensorflow},
}

@misc{noauthor_speechbrain_nodate,
	title = {{SpeechBrain}: {A} {PyTorch} {Speech} {Toolkit}},
	url = {https://speechbrain.github.io/},
	urldate = {2022-08-16},
}

@misc{piero_molino_ludwig_nodate,
	title = {Ludwig - code-free deep learning toolbox},
	url = {http://ludwig.ai},
	abstract = {Ludwig is a toolbox for training and testing deep learning models without writing code},
	language = {en},
	urldate = {2022-08-16},
	author = {Piero Molino},
}

@misc{noauthor_espnet_2022,
	title = {{ESPnet}: end-to-end speech processing toolkit},
	copyright = {Apache-2.0},
	shorttitle = {{ESPnet}},
	url = {https://github.com/espnet/espnet},
	abstract = {End-to-End Speech Processing Toolkit},
	urldate = {2022-08-16},
	publisher = {ESPnet},
	month = aug,
	year = {2022},
	note = {original-date: 2017-12-13T00:45:11Z},
	keywords = {deep-learning, kaldi, speech-recognition, chainer, end-to-end, machine-translation, pytorch, speech-enhancement, speech-separation, speech-synthesis, speech-translation, voice-conversion},
}

@misc{noauthor_espnet_nodate,
	title = {{ESPnet}: end-to-end speech processing toolkit — {ESPnet} 202207 documentation},
	url = {https://espnet.github.io/espnet/},
	urldate = {2022-08-16},
}

@article{georgescu_performance_2021,
	title = {Performance vs. hardware requirements in state-of-the-art automatic speech recognition},
	volume = {2021},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00217-4},
	doi = {10.1186/s13636-021-00217-4},
	abstract = {Abstract
            The last decade brought significant advances in automatic speech recognition (ASR) thanks to the evolution of deep learning methods. ASR systems evolved from pipeline-based systems, that modeled hand-crafted speech features with probabilistic frameworks and generated phone posteriors, to end-to-end (E2E) systems, that translate the raw waveform directly into words using one deep neural network (DNN). The transcription accuracy greatly increased, leading to ASR technology being integrated into many commercial applications. However, few of the existing ASR technologies are suitable for integration in embedded applications, due to their hard constrains related to computing power and memory usage. This overview paper serves as a guided tour through the recent literature on speech recognition and compares the most popular ASR implementations. The comparison emphasizes the trade-off between ASR performance and hardware requirements, to further serve decision makers in choosing the system which fits best their embedded application. To the best of our knowledge, this is the first study to provide this kind of trade-off analysis for state-of-the-art ASR systems.},
	language = {en},
	number = {1},
	urldate = {2022-08-16},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Georgescu, Alexandru-Lucian and Pappalardo, Alessandro and Cucu, Horia and Blott, Michaela},
	month = dec,
	year = {2021},
	pages = {28},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TRA6KAJY\\Georgescu et al. - 2021 - Performance vs. hardware requirements in state-of-.pdf:application/pdf},
}

@misc{cplc_cplc_nodate,
	title = {{CPLC} – {Citizens}-{Police} {Liaison} {Committee}},
	url = {http://www.cplc.org.pk/},
	urldate = {2022-08-16},
	author = {CPLC},
}

@misc{british_broadcast_bbc_nodate,
	title = {{BBC} - {Languages} - {Urdu} - {A} {Guide} to {Urdu} - 10 facts about the {Urdu} language},
	url = {https://www.bbc.co.uk/languages/other/urdu/guide/facts.shtml},
	abstract = {Discover surprising and revealing facts about Urdu, including Urdu words used in the English language and Urdu jokes and quotes.},
	language = {en},
	urldate = {2022-08-16},
	author = {British Broadcast},
	note = {Last Modified: 2008-01-30T12:35:00Z},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4CXS8LEU\\facts.html:text/html},
}

@misc{ethnologue_urdu_nodate,
	title = {Urdu {Language} - {Ethnologue} {Report}},
	url = {https://www.ethnologue.com/language/urdu},
	abstract = {A language profile for Urdu. Get a detailed look at the language, from population to dialects and usage.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Ethnologue},
	author = {Ethnologue},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\WGWY3RRQ\\urd.html:text/html},
}

@article{ali_automatic_2015,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-16},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@phdthesis{sehar_gul_detecting_2020,
	address = {Karachi},
	title = {{DETECTING} {MALICIOUS} {ACTIVITIES} {OVER} {TELEPHONE} {NETWORK} {FOR} {URDU} {SPEAKER}},
	abstract = {Telephone is one of most important invention in the fields of communication it is because on
this invention that we are able to connect with our friends and families without hassle of travelling
and going to their places,but some people are also using it for negative purpose therefore to secure
this medium of communication is one of the most important issue of today as many malicious
activities are taking place on this channel.Humanely it is not possible to tap each and every phone
call so that one could find malicious activities that are being done.In order to find such malicious
activities we need an automatic system that can automatically detect malicious voice activities,
for that we have decided to develop an automatic speech recognition system that will detect malicious
sentences in Urdu Language from the telephonic conversation which will then be processed
further.},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Sehar Gul},
	month = jun,
	year = {2020},
}

@incollection{hutchison_speaker_2010,
	address = {Berlin, Heidelberg},
	title = {Speaker {Independent} {Urdu} {Speech} {Recognition} {Using} {HMM}},
	volume = {6177},
	isbn = {978-3-642-13880-5},
	url = {http://link.springer.com/10.1007/978-3-642-13881-2_14},
	urldate = {2022-08-16},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ashraf, Javed and Iqbal, Naveed and Khattak, Naveed Sarfraz and Zaidi, Ather Mohsin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Hopfe, Christina J. and Rezgui, Yacine and Métais, Elisabeth and Preece, Alun and Li, Haijiang},
	year = {2010},
	doi = {10.1007/978-3-642-13881-2_14},
	note = {Series Title: Lecture Notes in Computer Science
ISBN2: 978-3-642-13881-2},
	pages = {140--148},
}

@inproceedings{sarfraz_large_2010,
	address = {Islamabad, Pakistan},
	title = {Large vocabulary continuous speech recognition for {Urdu}},
	isbn = {978-1-4503-0342-2},
	url = {http://portal.acm.org/citation.cfm?doid=1943628.1943629},
	doi = {10.1145/1943628.1943629},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Frontiers} of {Information} {Technology} - {FIT} '10},
	publisher = {ACM Press},
	author = {Sarfraz, Huda and Parveen, Rahila and Hussain, Sarmad and Bokhari, Riffat and Raza, Agha Ali and Ullah, Inam and Sarfraz, Zahid and Pervez, Sophia and Mustafa, Asad and Javed, Iqra},
	year = {2010},
	pages = {1--5},
}

@inproceedings{qasim_urdu_2016,
	address = {Bali, Indonesia},
	title = {Urdu speech recognition system for district names of {Pakistan}: {Development}, challenges and solutions},
	isbn = {978-1-5090-3516-8},
	shorttitle = {Urdu speech recognition system for district names of {Pakistan}},
	url = {http://ieeexplore.ieee.org/document/7918979/},
	doi = {10.1109/ICSDA.2016.7918979},
	urldate = {2022-08-16},
	booktitle = {2016 {Conference} of {The} {Oriental} {Chapter} of {International} {Committee} for {Coordination} and {Standardization} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Qasim, Muhammad and Nawaz, Sohaib and Hussain, Sarmad and Habib, Tania},
	month = oct,
	year = {2016},
	pages = {28--32},
}

@article{aguiar_de_lima_survey_2020,
	title = {A survey on automatic speech recognition systems for {Portuguese} language and its variations},
	volume = {62},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230819302992},
	doi = {10.1016/j.csl.2019.101055},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Aguiar de Lima, Thales and Da Costa-Abreu, Márjory},
	month = jul,
	year = {2020},
	pages = {101055},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\5YG7YNP8\\Aguiar de Lima and Da Costa-Abreu - 2020 - A survey on automatic speech recognition systems f.pdf:application/pdf},
}

@inproceedings{dash_automatic_2018,
	title = {Automatic {Speech} {Recognition} with {Articulatory} {Information} and a {Unified} {Dictionary} for {Hindi}, {Marathi}, {Bengali} and {Oriya}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/dash18_interspeech.html},
	doi = {10.21437/Interspeech.2018-2122},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Dash, Debadatta and Kim, Myungjong and Teplansky, Kristin and Wang, Jun},
	month = sep,
	year = {2018},
	pages = {1046--1050},
}

@inproceedings{patil_automatic_2016,
	address = {Pune, India},
	title = {Automatic {Speech} {Recognition} of isolated words in {Hindi} language using {MFCC}},
	isbn = {978-1-5090-1338-8},
	url = {http://ieeexplore.ieee.org/document/7915008/},
	doi = {10.1109/CAST.2016.7915008},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Computing}, {Analytics} and {Security} {Trends} ({CAST})},
	publisher = {IEEE},
	author = {Patil, U. G. and Shirbahadurkar, S. D. and Paithane, A. N.},
	month = dec,
	year = {2016},
	pages = {433--438},
}

@article{a_n_mishra_robust_2011,
	title = {Robust {Features} for {Connected} {Hindi} {Digits} {Recognition}},
	volume = {4},
	number = {2},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {A. N. Mishra and Mahesh Chandra and Astik Biswas and S. N. Sharan},
	month = jun,
	year = {2011},
}

@inproceedings{aggarwal_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	abstract = {The goal of automatic speech recognition (ASR) system is to accurately and efficiently convert a speech signal into a text message independent of the device, speaker or the environment. In general the speech signal is captured and pre-processed at front-end for feature extraction and evaluated at back-end using the Gaussian mixture hidden Markov model. In this statistical approach since the evaluation of Gaussian likelihoods dominate the total computational load, the appropriate selection of Gaussian mixtures is very important depending upon the amount of training data. As the small databases are available to train the Indian languages ASR system, the higher range of Gaussian mixtures (i.e. 64 and above), normally used for European languages, cannot be applied for them. This paper reviews the statistical framework and presents an iterative procedure to select an optimum number of Gaussian mixtures that exhibits maximum accuracy in the context of Hindi speech recognition system.},
	booktitle = {International {Journal} of {Signal} {Processing}, {Image} {Processing} and {Pattern} {Recognition}},
	author = {Aggarwal, R. K. and Dave, M.},
	year = {2011},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\FVSZRDG9\\Aggarwal and Dave - 2011 - Using Gaussian Mixtures for Hindi Speech Recogniti.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\NYC9U7U3\\summary.html:text/html},
}

@inproceedings{r_k_aggarwal_and_m_dave_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	author = {R. K. Aggarwal {and} M. Dave},
	year = {2011},
}

@inproceedings{b_venkataraman_sopc-based_2006,
	title = {{SOPC}-based speech-to-text conversion},
	booktitle = {Embedded {Processor} {Design} {Contest} {Outstanding} {Designs}},
	author = {B. Venkataraman},
	year = {2006},
}

@incollection{tanveer_continuous_2019,
	address = {Singapore},
	title = {Continuous {Hindi} {Speech} {Recognition} {Using} {Kaldi} {ASR} {Based} on {Deep} {Neural} {Network}},
	volume = {748},
	isbn = {9789811309229},
	url = {http://link.springer.com/10.1007/978-981-13-0923-6_26},
	urldate = {2022-08-16},
	booktitle = {Machine {Intelligence} and {Signal} {Analysis}},
	publisher = {Springer Singapore},
	author = {Upadhyaya, Prashant and Mittal, Sanjeev Kumar and Farooq, Omar and Varshney, Yash Vardhan and Abidi, Musiur Raza},
	editor = {Tanveer, M. and Pachori, Ram Bilas},
	year = {2019},
	doi = {10.1007/978-981-13-0923-6_26},
	note = {Series Title: Advances in Intelligent Systems and Computing
ISBN2: 9789811309236},
	pages = {303--311},
}

@inproceedings{karel_vesel_sequence-discriminative_2013,
	title = {Sequence-discriminative training of deep neural networks},
	abstract = {Luk Burget},
	author = {Karel Vesel and Arnab Ghoshal and Daniel Povey},
	year = {2013},
}

@inproceedings{k_v_s_parsad_and_s_m_virk_computational_2012,
	title = {Computational evidence that {Hindi} and {Urdu} share a grammar but not the lexicon},
	booktitle = {3rd {Workshop} on {South} and {Southeast} {Asian} {NLP}},
	author = {K. V. S. Parsad {and} S. M. Virk},
	year = {2012},
}

@article{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1402.1128},
	doi = {10.48550/ARXIV.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2022-08-16},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{abdel-hamid_exploring_2013,
	title = {Exploring {Convolutional} {Neural} {Network} {Structures} and {Optimization} {Techniques} for {Speech} {Recognition}},
	url = {https://www.microsoft.com/en-us/research/publication/exploring-convolutional-neural-network-structures-and-optimization-techniques-for-speech-recognition/},
	abstract = {Recently, convolutional neural networks (CNNs) have been shown to outperform the standard fully connected deep neural networks within the hybrid deep neural network / hidden Markov model (DNN/HMM) framework on the phone recognition task. In this paper, we extend the earlier basic form of the CNN and explore it in multiple ways. We first investigate several CNN architectures, including full and limited weight sharing, convolution along frequency and time axes, and stacking of several convolution layers. We then develop a novel weighted softmax pooling layer so that the size in the pooling layer can be automatically learned. Further, we evaluate the effect of CNN pretraining, which is achieved by using a convolutional version of the RBM. We show that all CNN architectures we have investigated outperform the earlier basic form of the DNN on both the phone recognition and large vocabulary speech recognition tasks. The architecture with limited weight sharing provides additional gains over the full weight sharing architecture. The softmax pooling layer performs as well as the best CNN with the manually tuned fixed-pooling size, and has a potential for further improvement. Finally, we show that CNN pretraining produces significantly better results on a large vocabulary speech recognition task.},
	booktitle = {Interspeech 2013},
	publisher = {ISCA},
	author = {Abdel-Hamid, Ossama and Deng, Li and Yu, Dong},
	month = aug,
	year = {2013},
	note = {Edition: Interspeech 2013},
}

@misc{noauthor_gram_nodate,
	title = {{GRAM} {VAANI} {ASR} {Challenge} 2022},
	url = {https://sites.google.com/view/gramvaaniasrchallenge/home},
	abstract = {A challenge on Automatic Speech Recognition for Hindi is being organized as part of INTERSPEECH 2022 by sharing the spontaneous telephone speech recordings collected by a social technology enterprise Gram Vaani. The regional variations of Hindi together with spontaneity of speech, natural},
	language = {en},
	urldate = {2022-08-16},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P4IHK3L5\\home.html:text/html},
}

@misc{noauthor_gramvaaniorg_nodate,
	title = {gramvaani.org},
	url = {https://gramvaani.org/},
	urldate = {2022-08-16},
}

@inproceedings{latif_cross_2018,
	address = {Islamabad, Pakistan},
	title = {Cross {Lingual} {Speech} {Emotion} {Recognition}: {Urdu} vs. {Western} {Languages}},
	isbn = {978-1-5386-9355-1},
	shorttitle = {Cross {Lingual} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/8616972/},
	doi = {10.1109/FIT.2018.00023},
	urldate = {2022-08-16},
	booktitle = {2018 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	publisher = {IEEE},
	author = {Latif, Siddique and Qayyum, Adnan and Usman, Muhammad and Qadir, Junaid},
	month = dec,
	year = {2018},
	pages = {88--93},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\NLRUGBJI\\Latif et al. - 2018 - Cross Lingual Speech Emotion Recognition Urdu vs..pdf:application/pdf},
}

@article{besacier_automatic_2014,
	title = {Automatic speech recognition for under-resourced languages: {A} survey},
	volume = {56},
	issn = {01676393},
	shorttitle = {Automatic speech recognition for under-resourced languages},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988},
	doi = {10.1016/j.specom.2013.07.008},
	language = {en},
	urldate = {2022-08-16},
	journal = {Speech Communication},
	author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
	month = jan,
	year = {2014},
	pages = {85--100},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YRF6UNN8\\Besacier et al. - 2014 - Automatic speech recognition for under-resourced l.pdf:application/pdf},
}

@article{lakshmi_sri_kaldi_2020,
	title = {Kaldi recipe in {Hindi} for word level recognition and phoneme level transcription},
	volume = {171},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050920312606},
	doi = {10.1016/j.procs.2020.04.268},
	language = {en},
	urldate = {2022-08-16},
	journal = {Procedia Computer Science},
	author = {Lakshmi Sri, Karra Venkata and Srinivasan, Mayuka and Nair, Radhika Rajeev and Priya, K. Jeeva and Gupta, Deepa},
	year = {2020},
	pages = {2476--2485},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5J9VQKIM\\Lakshmi Sri et al. - 2020 - Kaldi recipe in Hindi for word level recognition a.pdf:application/pdf},
}

@misc{qureshi_urdu_2021,
	title = {Urdu {Speech} {Recognition}},
	copyright = {MIT},
	url = {https://github.com/ZoraizQ/urdu-speech-recognition},
	abstract = {Urdu Speech Recognition using Kaldi ASR, by training Triphone Acoustic GMMs using the PRUS dataset.},
	urldate = {2022-08-16},
	author = {Qureshi, Zoraiz},
	month = oct,
	year = {2021},
	note = {original-date: 2021-04-21T10:11:17Z},
	keywords = {speech-recognition, kaldi-asr, multi-speaker, prus, urdu},
}

@inproceedings{asadullah_automatic_2016,
	address = {Portsmouth},
	title = {Automatic {Urdu} {Speech} {Recognition} using {Hidden} {Markov} {Model}},
	isbn = {978-1-5090-3755-1},
	url = {https://ieeexplore.ieee.org/document/7571287/},
	doi = {10.1109/ICIVC.2016.7571287},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Image}, {Vision} and {Computing} ({ICIVC})},
	publisher = {IEEE},
	author = {{Asadullah} and Shaukat, Arslan and Ali, Hazrat and Akram, Usman},
	month = aug,
	year = {2016},
	pages = {135--139},
}

@misc{chodroff_corpus_2018,
	title = {Corpus {Phonetics} {Tutorial}},
	url = {http://arxiv.org/abs/1811.05553},
	abstract = {Corpus phonetics has become an increasingly popular method of research in linguistic analysis. With advances in speech technology and computational power, large scale processing of speech data has become a viable technique. This tutorial introduces the speech scientist and engineer to various automatic speech processing tools. These include acoustic model creation and forced alignment using the Kaldi Automatic Speech Recognition Toolkit (Povey et al., 2011), forced alignment using FAVE-align (Rosenfelder et al., 2014), the Montreal Forced Aligner (McAuliffe et al., 2017), and the Penn Phonetics Lab Forced Aligner (Yuan \& Liberman, 2008), as well as stop consonant burst alignment using AutoVOT (Keshet et al., 2014). The tutorial provides a general overview of each program, step-by-step instructions for running the program, as well as several tips and tricks.},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Chodroff, Eleanor},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05553 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3GAPGJTY\\Chodroff - 2018 - Corpus Phonetics Tutorial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5EDZSUNT\\1811.html:text/html},
}

@article{alharbi_automatic_2021,
	title = {Automatic {Speech} {Recognition}: {Systematic} {Literature} {Review}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Automatic {Speech} {Recognition}},
	doi = {10.1109/ACCESS.2021.3112535},
	abstract = {A huge amount of research has been done in the field of speech signal processing in recent years. In particular, there has been increasing interest in the automatic speech recognition (ASR) technology field. ASR began with simple systems that responded to a limited number of sounds and has evolved into sophisticated systems that respond fluently to natural language. This systematic review of automatic speech recognition is provided to help other researchers with the most significant topics published in the last six years. This research will also help in identifying recent major ASR challenges in real-world environments. In addition, it discusses current research gaps in ASR. This review covers articles available in five research databases that were completed according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) protocol. The search strategy yielded 82 conferences and articles related to the study’s scope for the period 2015–2020. The results presented in this review shed light on research trends in the area of ASR and also suggest new research directions.},
	journal = {IEEE Access},
	author = {Alharbi, Sadeen and Alrazgan, Muna and Alrashed, Alanoud and Alnomasi, Turkiayh and Almojel, Raghad and Alharbi, Rimah and Alharbi, Saja and Alturki, Sahar and Alshehri, Fatimah and Almojil, Maha},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {ASR challenges, ASR systematic review, automatic speech recognition, Automatic speech recognition, Databases, Licenses, Nails, Quality assessment, Software, Speech recognition, Systematics},
	pages = {131858--131876},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\TLVJTS37\\9536732.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\7LCUHWDI\\Alharbi et al. - 2021 - Automatic Speech Recognition Systematic Literatur.pdf:application/pdf},
}

@article{alsayadi_arabic_2021,
	title = {Arabic speech recognition using end-to-end deep learning},
	volume = {15},
	issn = {1751-9683},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12057},
	doi = {10.1049/sil2.12057},
	abstract = {Arabic automatic speech recognition (ASR) methods with diacritics have the ability to be integrated with other systems better than Arabic ASR methods without diacritics. In this work, the application of state-of-the-art end-to-end deep learning approaches is investigated to build a robust diacritised Arabic ASR. These approaches are based on the Mel-Frequency Cepstral Coefficients and the log Mel-Scale Filter Bank energies as acoustic features. To the best of our knowledge, end-to-end deep learning approach has not been used in the task of diacritised Arabic automatic speech recognition. To fill this gap, this work presents a new CTC-based ASR, CNN-LSTM, and an attention-based end-to-end approach for improving diacritisedArabic ASR. In addition, a word-based language model is employed to achieve better results. The end-to-end approaches applied in this work are based on state-of-the-art frameworks, namely ESPnet and Espresso. Training and testing of these frameworks are performed based on the Standard Arabic Single Speaker Corpus (SASSC), which contains 7 h of modern standard Arabic speech. Experimental results show that the CNN-LSTM with an attention framework outperforms conventional ASR and the Joint CTC-attention ASR framework in the task of Arabic speech recognition. The CNN-LSTM with an attention framework could achieve a word error rate better than conventional ASR and the Joint CTC-attention ASR by 5.24\% and 2.62\%, respectively.},
	language = {en},
	number = {8},
	urldate = {2022-08-16},
	journal = {IET Signal Processing},
	author = {Alsayadi, Hamzah A. and Abdelhamid, Abdelaziz A. and Hegazy, Islam and Fayed, Zaki T.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/sil2.12057},
	pages = {521--534},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8MD9EIN6\\Alsayadi et al. - 2021 - Arabic speech recognition using end-to-end deep le.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8NYWWX8A\\sil2.html:text/html},
}

@inproceedings{raza_rapid_2018,
	title = {Rapid {Collection} of {Spontaneous} {Speech} {Corpora} {Using} {Telephonic} {Community} {Forums}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/raza18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1139},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Raza, Agha Ali and Athar, Awais and Randhawa, Shan and Tariq, Zain and Saleem, Muhammad Bilal and Bin Zia, Haris and Saif, Umar and Rosenfeld, Roni},
	month = sep,
	year = {2018},
	pages = {1021--1025},
}

@inproceedings{naeem_subspace_2020,
	title = {Subspace {Gaussian} {Mixture} {Model} for {Continuous} {Urdu} {Speech} {Recognition} using {Kaldi}},
	doi = {10.1109/ICOSST51357.2020.9333026},
	abstract = {Automatic Speech Recognition Systems (ASR) have significantly improved in recent years, where deep learning is playing an important role in the development of end to end ASR's. ASR is the task of converting spoken language into computer readable text. ASRs are becoming ever more prevalent way to interact with technology, thereby significantly closing the gap in terms of how humans interact with computers, making it more natural. Urdu is an under resourced language, for which training such a system requires a huge amount of data that is not readily available. In this paper we present improvements to the architecture of a statistical automatic speech recognition system for which the components involved in a statistical ASR have been explored in great detail. We also present the results on various statistical models that are trained for Urdu language. We choose the Kaldi toolkit for training the Urdu ASR using approximately 100 hours of transcribed data. The refined Subspace Gaussian Model gives a word error rate of 9\% on the test set.},
	booktitle = {2020 14th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Naeem, Saad and Iqbal, Majid and Saqib, Muhammad and Saad, Muhammad and Raza, Muhammad Soban and Ali, Zaid and Akhtar, Naveed and Beg, Mirza Omer and Shahzad, Waseem and Arshad, Muhhamad Umair},
	month = dec,
	year = {2020},
	keywords = {Adaptation models, Analytical models, Automatic Speech Recognition, Computational modeling, Context modeling, Hidden Markov models, Hidden Markov Models, Mel-frequency Cepstrum, Probabilistic logic, Subspace Gaussian Mixture Models, Training},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\3KACECHI\\9333026.html:text/html},
}

@inproceedings{khan_multi-genre_2021,
	address = {Singapore, Singapore},
	title = {A {Multi}-{Genre} {Urdu} {Broadcast} {Speech} {Recognition} {System}},
	isbn = {978-1-66540-870-7},
	url = {https://ieeexplore.ieee.org/document/9660552/},
	doi = {10.1109/O-COCOSDA202152914.2021.9660552},
	urldate = {2022-08-16},
	booktitle = {2021 24th {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Khan, Erbaz and Rauf, Sahar and Adeeba, Farah and Hussain, Sarmad},
	month = nov,
	year = {2021},
	pages = {25--30},
}

@incollection{somogyi_automatic_2021,
	address = {Cham},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-3-030-60031-0 978-3-030-60032-7},
	url = {http://link.springer.com/10.1007/978-3-030-60032-7_5},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Application} of {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Somogyi, Zoltán},
	collaborator = {Somogyi, Zoltán},
	year = {2021},
	doi = {10.1007/978-3-030-60032-7_5},
	pages = {145--171},
}

@incollection{chapelle_automatic_2020,
	edition = {1},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4051-9473-0 978-1-4051-9843-1},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781405198431.wbeal0066.pub2},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Encyclopedia} of {Applied} {Linguistics}},
	publisher = {Wiley},
	author = {Levis, John and Suvorov, Ruslan},
	editor = {Chapelle, Carol A.},
	month = dec,
	year = {2020},
	doi = {10.1002/9781405198431.wbeal0066.pub2},
	pages = {1--8},
}

@incollection{gold_brief_2011,
	address = {Hoboken, NJ, USA},
	title = {Brief {History} of {Automatic} {Speech} {Recognition}},
	isbn = {978-1-118-14288-2 978-0-470-19536-9},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118142882.ch4},
	urldate = {2022-08-16},
	booktitle = {Speech and {Audio} {Signal} {Processing}},
	publisher = {John Wiley \& Sons, Inc.},
	collaborator = {Gold, Ben and Morgan, Nelson and Ellis, Dan},
	month = oct,
	year = {2011},
	doi = {10.1002/9781118142882.ch4},
	pages = {40--58},
}

@misc{kincaid_brief_2018,
	title = {A {Brief} {History} of {ASR}: {Automatic} {Speech} {Recognition}},
	shorttitle = {A {Brief} {History} of {ASR}},
	url = {https://medium.com/descript/a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5},
	abstract = {This is the first post in a series on Automatic Speech Recognition, the foundational technology that makes Descript possible. We’ll be…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FHD2LASC\\a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5.html:text/html},
}

@article{suma_swamy_evolution_2013,
	title = {Evolution of {Speech} {Recognition} – {A} {Brief} {History} of {Technology} {Development}},
	volume = {60},
	journal = {Elixir Adv. Engg. Info.},
	author = {Suma Swamy and Ramakrishnan, Kollengode},
	month = jul,
	year = {2013},
}

@article{smit_advances_2021,
	title = {Advances in subword-based {HMM}-{DNN} speech recognition across languages},
	volume = {66},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300917},
	doi = {10.1016/j.csl.2020.101158},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Smit, Peter and Virpioja, Sami and Kurimo, Mikko},
	month = mar,
	year = {2021},
	pages = {101158},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC6HIS7B\\Smit et al. - 2021 - Advances in subword-based HMM-DNN speech recogniti.pdf:application/pdf},
}

@misc{kincaid_state_2018,
	title = {The {State} of {Automatic} {Speech} {Recognition}: {Q}\&{A} with {Kaldi}’s {Dan} {Povey}},
	shorttitle = {The {State} of {Automatic} {Speech} {Recognition}},
	url = {https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85},
	abstract = {This article continues our series on Automatic Speech Recognition, including our recent piece on the History of ASR.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AI8J5D9U\\the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85.html:text/html},
}

@misc{blog_machine_2022,
	title = {Machine {Learning} {Models} {Are} {Only} as {Good} as the {Data} {They} {Are} {Trained} {On}},
	url = {https://deepchecks.com/machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on/},
	abstract = {Learn about the importance of data validation in machine learning, look into the various tools for different sets of data validation techniques \& procedures.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Deepchecks},
	author = {Blog, Deepchecks Community},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5T5L9UMP\\machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on.html:text/html},
}

@misc{noauthor_machine_2018,
	title = {“{A} machine learning model is only as good as the data it is fed”},
	url = {https://devm.io/machine-learning/apache-spark-machine-learning-interview-143122},
	abstract = {Apache Spark 2.3 was released earlier this year; it marked a major milestone for Structured Streaming but there are a lot of other interesting features that deserve your attention. We talked with Reynold Xin, co-founder and Chief Architect at Databricks about the Databricks Runtime and other enhancements introduced in Apache Spark 2.3.},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {devmio - expand your knowledge},
	month = jun,
	year = {2018},
	note = {Section: Artikel},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HS46J6C4\\apache-spark-machine-learning-interview-143122.html:text/html},
}

@misc{brownlee_why_2020,
	title = {Why {Data} {Preparation} {Is} {So} {Important} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/data-preparation-is-important/},
	abstract = {On a predictive modeling project, machine learning algorithms learn a mapping from input variables to a target variable. The most common form of predictive modeling project involves so-called structured data or tabular data. This is data as it looks in a spreadsheet or a matrix, with rows of examples and columns of features for each […]},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HGMCR7CK\\data-preparation-is-important.html:text/html},
}

@book{ilyas_data_2019,
	address = {New York, NY},
	title = {Data {Cleaning}},
	isbn = {978-1-4503-7153-7},
	language = {English},
	publisher = {ACM Books},
	author = {Ilyas, Ihab F. and Chu, Xu},
	month = jun,
	year = {2019},
}

@misc{khan_introduction_2021,
	title = {An {Introduction} to {Classification} {Using} {Mislabeled} {Data}},
	url = {https://towardsdatascience.com/an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5},
	abstract = {The performance of any classifier, or for that matter any machine learning task, depends crucially on the quality of the available data…},
	language = {en},
	urldate = {2022-08-16},
	journal = {Medium},
	author = {Khan, Shihab Shahriar},
	month = mar,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SY93L99I\\an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5.html:text/html},
}

@article{cao_joint_2019,
	title = {Joint {Prostate} {Cancer} {Detection} and {Gleason} {Score} {Prediction} in mp-{MRI} via {FocalNet}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8653866/},
	doi = {10.1109/TMI.2019.2901928},
	number = {11},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Cao, Ruiming and Mohammadian Bajgiran, Amirhossein and Afshari Mirak, Sohrab and Shakeri, Sepideh and Zhong, Xinran and Enzmann, Dieter and Raman, Steven and Sung, Kyunghyun},
	month = nov,
	year = {2019},
	pages = {2496--2506},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WK7STA45\\Cao et al. - 2019 - Joint Prostate Cancer Detection and Gleason Score .pdf:application/pdf},
}

@article{fan_impact_2021,
	title = {The {Impact} of {Mislabeled} {Changes} by {SZZ} on {Just}-in-{Time} {Defect} {Prediction}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2019.2929761},
	abstract = {Just-in-Time (JIT) defect prediction-a technique which aims to predict bugs at change level-has been paid more attention. JIT defect prediction leverages the SZZ approach to identify bug-introducing changes. Recently, researchers found that the performance of SZZ (including its variants) is impacted by a large amount of noise. SZZ may considerably mislabel changes that are used to train a JIT defect prediction model, and thus impact the prediction accuracy. In this paper, we investigate the impact of the mislabeled changes by different SZZ variants on the performance and interpretation of JIT defect prediction models. We analyze four SZZ variants (i.e., B-SZZ, AG-SZZ, MA-SZZ, and RA-SZZ) that are proposed by prior studies. We build the prediction models using the labeled data by these four SZZ variants. Among the four SZZ variants, RA-SZZ is least likely to generate mislabeled changes, and we construct the testing set by using RA-SZZ. All of the four prediction models are then evaluated on the same testing set. We choose the prediction model built on the labeled data by RA-SZZ as the baseline model, and we compare the performance and metric importance of the models trained using the labeled data by the other three SZZ variants with the baseline model. Through a large-scale empirical study on a total of 126,526 changes from ten Apache open source projects, we find that in terms of various performance measures (AUC, F1-score, G-mean and Recall@20\%), the mislabeled changes by B-SZZ and MA-SZZ are not likely to cause a considerable performance reduction, while the mislabeled changes by AG-SZZ cause a statistically significant performance reduction with an average difference of 1-5 percent. When considering developers' inspection effort (measured by LOC) in practice, the changes mislabeled B-SZZ and AG-SZZ lead to 9-10 and 1-15 percent more wasted inspection effort, respectively. And the mislabeled changes by B-SZZ lead to significantly more wasted effort. The mislabeled changes by MA-SZZ do not cause considerably more wasted effort. We also find that the top-most important metric for identifying bug-introducing changes (i.e., number of files modified in a change) is robust to the mislabeling noise generated by SZZ. But the second- and third-most important metrics are more likely to be impacted by the mislabeling noise, unless random forest is used as the underlying classifier.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fan, Yuanrui and Xia, Xin and da Costa, Daniel Alencar and Lo, David and Hassan, Ahmed E. and Li, Shanping},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Analytical models, Computer bugs, Data models, Inspection, Just-in-time defect prediction, Measurement, mining software repositories, noisy data, Predictive models, SZZ, Testing},
	pages = {1559--1586},
}

@inproceedings{van_rooyen_learning_2015,
	title = {Learning with {Symmetric} {Label} {Noise}: {The} {Importance} of {Being} {Unhinged}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {van Rooyen, Brendan and Menon, Aditya and Williamson, Robert C},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{natarajan_learning_2013,
	title = {Learning with {Noisy} {Labels}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@article{frenay_classification_2014,
	title = {Classification in the {Presence} of {Label} {Noise}: {A} {Survey}},
	volume = {25},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Classification in the {Presence} of {Label} {Noise}},
	url = {http://ieeexplore.ieee.org/document/6685834/},
	doi = {10.1109/TNNLS.2013.2292894},
	number = {5},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Frenay, Benoit and Verleysen, Michel},
	month = may,
	year = {2014},
	pages = {845--869},
}

@misc{zhang_uberi_speechrecognition_nodate,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-08-16},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\9RSTEWBT\\SpeechRecognition.html:text/html},
}

@inproceedings{panayotov_librispeech_2015,
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	shorttitle = {Librispeech},
	doi = {10.1109/ICASSP.2015.7178964},
	abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {Bioinformatics, Blogs, Corpus, Electronic publishing, Genomics, Information services, LibriVox, Resource description framework, Speech Recognition},
	pages = {5206--5210},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\2AK7VZ3R\\7178964.html:text/html},
}

@article{backstrom_introduction_2022,
	title = {Introduction to {Speech} {Processing}: 2nd {Edition}},
	copyright = {Open Access},
	shorttitle = {Introduction to {Speech} {Processing}},
	url = {https://zenodo.org/record/6821775},
	doi = {10.5281/ZENODO.6821775},
	abstract = {This release is primarily about migrating all content to jupyter-books and git. The published version is now hosted at https://speechprocessingbook.aalto.fi. In addition to github, the release has long-term storage location at Zenodo, which also assigns a DOI to the release. We have some entirely new sections, such as Forensic speaker recognition. There are also plenty of small improvements everywhere.},
	language = {en},
	urldate = {2022-08-16},
	author = {Bäckström, Tom and Räsänen, Okko and Zewoudie, Abraham and Zarazaga, Pablo Pérez and Koivusalo, Liisa and Das, Sneha and Mellado, Esteban Gómez and Mariem Bouafif Mansali and Ramos, Daniel},
	month = jul,
	year = {2022},
	note = {Publisher: Zenodo
Version Number: v2},
	keywords = {speech processing},
}

@book{yu_automatic_2015,
	address = {London},
	series = {Signals and {Communication} {Technology}},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4471-5778-6},
	url = {http://link.springer.com/10.1007/978-1-4471-5779-3},
	urldate = {2022-08-16},
	publisher = {Springer London},
	author = {Yu, Dong and Deng, Li},
	year = {2015},
	note = {ISBN2: 978-1-4471-5779-3},
}

@article{park_review_2021,
	title = {A {Review} of {Speaker} {Diarization}: {Recent} {Advances} with {Deep} {Learning}},
	volume = {72},
	copyright = {arXiv.org perpetual, non-exclusive license},
	issn = {101317},
	shorttitle = {A {Review} of {Speaker} {Diarization}},
	url = {https://arxiv.org/abs/2101.09624},
	doi = {10.48550/ARXIV.2101.09624},
	abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
	urldate = {2022-08-17},
	journal = {Computer Speech \& Language},
	author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
This article is a preprint version of the article published in Computer Speech \& Language, Volume 72, March 2022, 101317},
}

@misc{juang_automatic_nodate,
	title = {Automatic {Speech} {Recognition} – {A} {Brief} {History} of the {Technology} {Development} {Abstract}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis [1, 2], the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. H. and Rabiner, Lawrence R.},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\3YU9F3ID\\Juang and Rabiner - Automatic Speech Recognition – A Brief History of .pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\I34GUPHY\\summary.html:text/html},
}

@incollection{maglogiannis__2020,
	address = {Cham},
	title = {Α {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4 978-3-030-49161-1},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\K3RKXDZZ\\Filippidou and Moussiades - 2020 - Α Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@inproceedings{morris_wer_2004,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{anusuya_speech_2010,
	title = {Speech {Recognition} by {Machine}, {A} {Review}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1001.2267},
	doi = {10.48550/ARXIV.1001.2267},
	abstract = {This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.},
	urldate = {2022-08-17},
	author = {Anusuya, M. A. and Katti, S. K.},
	year = {2010},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
25 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS December 2009, ISSN 1947 5500, http://sites.google.com/site/ijcsis/},
}

@article{upton_speech_1984,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages — this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-17},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@article{davis_automatic_1952,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@inproceedings{maja_popovic_hjerson_2011,
	title = {Hjerson: {An} {Open} {Source} {Tool} for {Automatic} {Error} {Classification} of {Machine} {Translation} {Output}},
	author = {Maja Popovic},
	year = {2011},
}

@article{errattahi_automatic_2018,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-17},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YWBD74CJ\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@inproceedings{povey_daniel_and_ghoshal_arnab_and_boulianne_gilles_and_burget_lukas_and_glembek_ondrej_and_goel_nagendra_and_hannemann_mirko_and_motlicek_petr_and_qian_yanmin_and_schwarz_petr__and_others_kaldi_2011,
	title = {The {Kaldi} speech recognition toolkit},
	booktitle = {Workshop on automatic speech recognition and understanding},
	author = {Povey, Daniel {and} Ghoshal, Arnab {and} Boulianne, Gilles {and} Burget, Lukas {and} Glembek, Ondrej {and} Goel, Nagendra {and} Hannemann, Mirko {and} Motlicek, Petr {and} Qian, Yanmin {and} Schwarz, Petr  {and} others},
	year = {2011},
}

@misc{sutherland_short_2017,
	title = {A short history of speech recognition},
	url = {https://medium.com/@sutherlandjamie/a-short-history-of-speech-recognition-9b8e78ad086a},
	abstract = {There has been more progress in speech recognition technology in the last 3 years than in the first 30 years. Computing power and…},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Sutherland, Jamie},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\LAELJJKS\\a-short-history-of-speech-recognition-9b8e78ad086a.html:text/html},
}

@inproceedings{xiong_microsoft_2017,
	address = {New Orleans, LA},
	title = {The microsoft 2016 conversational speech recognition system},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953159/},
	doi = {10.1109/ICASSP.2017.7953159},
	urldate = {2022-08-17},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Xiong, W. and Droppo, J. and Huang, X. and Seide, F. and Seltzer, M. and Stolcke, A. and Yu, D. and Zweig, G.},
	month = mar,
	year = {2017},
	pages = {5255--5259},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\ZVTCMYVF\\Xiong et al. - 2017 - The microsoft 2016 conversational speech recogniti.pdf:application/pdf},
}

@misc{qi_benchmarking_2021,
	title = {Benchmarking {Commercial} {Intent} {Detection} {Services} with {Practice}-{Driven} {Evaluations}},
	url = {http://arxiv.org/abs/2012.03929},
	abstract = {Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users' text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the intent detection models can see a different distribution of test data when being deployed in the real world, leading to poor accuracy. Finally, a practical intent detection model must be computationally efficient in both training and single query inference so that it can be used continuously and re-trained frequently. We benchmark intent detection methods on a variety of datasets. Our results show that Watson Assistant's intent detection model outperforms other commercial solutions and is comparable to large pretrained language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ.},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Qi, Haode and Pan, Lin and Sood, Atin and Shah, Abhishek and Kunc, Ladislav and Yu, Mo and Potdar, Saloni},
	month = jun,
	year = {2021},
	note = {arXiv:2012.03929 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at NAACL2021 Industry Track},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\XMUN9XIG\\Qi et al. - 2021 - Benchmarking Commercial Intent Detection Services .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5JX39X9L\\2012.html:text/html},
}

@inproceedings{arora_hint3_2020,
	address = {Online},
	title = {{HINT3}: {Raising} the bar for {Intent} {Detection} in the {Wild}},
	shorttitle = {{HINT3}},
	url = {https://www.aclweb.org/anthology/2020.insights-1.16},
	doi = {10.18653/v1/2020.insights-1.16},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Proceedings of the {First} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Arora, Gaurav and Jain, Chirag and Chaturvedi, Manas and Modi, Krupal},
	year = {2020},
	pages = {100--105},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\SYABT9YP\\Arora et al. - 2020 - HINT3 Raising the bar for Intent Detection in the.pdf:application/pdf},
}

@misc{patel_data-centric_2021,
	title = {Data-{Centric} {Approach} vs {Model}-{Centric} {Approach} in {Machine} {Learning}},
	url = {https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning},
	abstract = {Code and data are the foundations of the AI system. Both of these components play an important role in the development of a robust model but which one should you focus on more? In this article, we’ll go through the data-centric vs model-centric approaches, and see which one is better, we would also talk about […]},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {neptune.ai},
	author = {Patel, Harshil},
	month = dec,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ZT7NNEUA\\data-centric-vs-model-centric-machine-learning.html:text/html},
}

@misc{radecic_data-centric_2022,
	title = {Data-centric vs. {Model}-centric {AI}? {The} {Answer} is {Clear}},
	shorttitle = {Data-centric vs. {Model}-centric {AI}?},
	url = {https://towardsdatascience.com/data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67},
	abstract = {There’s something wrong with the current approach to AI. But there’s a solution.},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Radečić, Dario},
	month = mar,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\X4DU4GPV\\data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67.html:text/html},
}

@misc{noauthor_chat_nodate,
	title = {A {Chat} with {Andrew} on {MLOps}: {From} {Model}-centric to {Data}-centric {AI} - {YouTube}},
	url = {https://www.youtube.com/watch?v=06-AZXmwHjo&t=69s},
	urldate = {2022-08-17},
}

@misc{noauthor_data-centric_nodate,
	title = {Data-centric {Machine} {Learning}: {Making} customized {ML} solutions production-ready},
	shorttitle = {Data-centric {Machine} {Learning}},
	url = {https://dida.do/blog/data-centric-machine-learning},
	abstract = {In this article, we will see why many ML Projects do not make it into production, introduce the concepts of model- and data-centric ML, and give examples how we at dida improve projects by applying data-centric techniques.},
	language = {en},
	urldate = {2022-08-17},
	journal = {dida Machine Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\F2569RZH\\data-centric-machine-learning.html:text/html},
}

@misc{noauthor_significance_nodate,
	title = {The {Significance} of {Data}-centric {AI}},
	url = {https://www.kdnuggets.com/the-significance-of-data-centric-ai.html/},
	abstract = {How a systematic way of maintaining data quality can do wonders to your model performance.},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {KDnuggets},
	note = {Section: KDnuggets Originals},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SGUDCHX6\\significance-data-centric-ai.html:text/html},
}

@misc{deeplearningai_data-centric_2021,
	title = {Data-centric {AI}: {Real} {World} {Approaches}},
	shorttitle = {Data-centric {AI}},
	url = {https://www.youtube.com/watch?v=Yqj7Kyjznh4},
	urldate = {2022-08-17},
	author = {{DeepLearningAI}},
	month = aug,
	year = {2021},
}

@article{creutz_morph-based_2007,
	title = {Morph-based speech recognition and modeling of out-of-vocabulary words across languages},
	volume = {5},
	issn = {1550-4875, 1550-4883},
	url = {https://dl.acm.org/doi/10.1145/1322391.1322394},
	doi = {10.1145/1322391.1322394},
	abstract = {We explore the use of morph-based language models in large-vocabulary continuous-speech recognition systems across four so-called morphologically rich languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. The morphs are subword units discovered in an unsupervised, data-driven way using the
              Morfessor
              algorithm. By estimating
              n
              -gram language models over sequences of morphs instead of words, the quality of the language model is improved through better vocabulary coverage and reduced data sparsity. Standard word models suffer from high out-of-vocabulary (OOV) rates, whereas the morph models can recognize previously unseen word forms by concatenating morphs. It is shown that the morph models do perform fairly well on OOVs without compromising the recognition accuracy on in-vocabulary words. The Arabic experiment constitutes the only exception since here the standard word model outperforms the morph model. Differences in the datasets and the amount of data are discussed as a plausible explanation.},
	language = {en},
	number = {1},
	urldate = {2022-08-18},
	journal = {ACM Transactions on Speech and Language Processing},
	author = {Creutz, Mathias and Hirsimäki, Teemu and Kurimo, Mikko and Puurula, Antti and Pylkkönen, Janne and Siivola, Vesa and Varjokallio, Matti and Arisoy, Ebru and Saraçlar, Murat and Stolcke, Andreas},
	month = dec,
	year = {2007},
	pages = {1--29},
}

@misc{noauthor_mfcc_nodate,
	title = {{MFCC} vs {FBANK} for chain models ?},
	url = {https://groups.google.com/g/kaldi-help/c/_7hB74HKhC4},
	urldate = {2022-08-18},
	file = {MFCC vs FBANK for chain models ?:C\:\\Users\\DELL\\Zotero\\storage\\5FKKZNEQ\\_7hB74HKhC4.html:text/html},
}

@book{reithaug_orchestrating_2002,
	title = {Orchestrating {Success} in {Reading}},
	isbn = {978-0-9694974-4-8},
	url = {https://books.google.com.pk/books?id=\_YGZMQAACAAJ},
	publisher = {Stirling Head Enterprises},
	author = {Reithaug, D.},
	year = {2002},
}

@article{zia_pronouncur_2018,
	title = {{PronouncUR}: {An} {Urdu} {Pronunciation} {Lexicon} {Generator}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{PronouncUR}},
	url = {https://arxiv.org/abs/1801.00409},
	doi = {10.48550/ARXIV.1801.00409},
	abstract = {State-of-the-art speech recognition systems rely heavily on three basic components: an acoustic model, a pronunciation lexicon and a language model. To build these components, a researcher needs linguistic as well as technical expertise, which is a barrier in low-resource domains. Techniques to construct these three components without having expert domain knowledge are in great demand. Urdu, despite having millions of speakers all over the world, is a low-resource language in terms of standard publically available linguistic resources. In this paper, we present a grapheme-to-phoneme conversion tool for Urdu that generates a pronunciation lexicon in a form suitable for use with speech recognition systems from a list of Urdu words. The tool predicts the pronunciation of words using a LSTM-based model trained on a handcrafted expert lexicon of around 39,000 words and shows an accuracy of 64\% upon internal evaluation. For external evaluation on a speech recognition task, we obtain a word error rate comparable to one achieved using a fully handcrafted expert lexicon.},
	urldate = {2022-08-18},
	author = {Zia, Haris Bin and Raza, Agha Ali and Athar, Awais},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
5 pages, LREC 2018},
}

@book{jurafsky_speech_2009,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-187321-6},
	shorttitle = {Speech and language processing},
	publisher = {Pearson Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
	note = {OCLC: 213375806},
	keywords = {Automatic speech recognition, Computational linguistics},
}

@article{likhomanenko_rethinking_2020,
	title = {Rethinking {Evaluation} in {ASR}: {Are} {Our} {Models} {Robust} {Enough}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Rethinking {Evaluation} in {ASR}},
	url = {https://arxiv.org/abs/2010.11745},
	doi = {10.48550/ARXIV.2010.11745},
	abstract = {Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets - in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. We show that, in general, reverberative and additive noise augmentation improves generalization performance across domains. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world noisy data. Finally, we show that training a single acoustic model on the most widely-used datasets - combined - reaches competitive performance on both research and real-world benchmarks.},
	urldate = {2022-08-19},
	author = {Likhomanenko, Tatiana and Xu, Qiantong and Pratap, Vineel and Tomasello, Paden and Kahn, Jacob and Avidov, Gilad and Collobert, Ronan and Synnaeve, Gabriel},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD), 68T07, 68T10, I.2.6; I.5.4},
}

@incollection{hutchison_illustrated_2013,
	address = {Berlin, Heidelberg},
	title = {An {Illustrated} {Methodology} for {Evaluating} {ASR} {Systems}},
	volume = {7836},
	isbn = {978-3-642-37424-1},
	url = {http://link.springer.com/10.1007/978-3-642-37425-8_3},
	urldate = {2022-08-19},
	booktitle = {Adaptive {Multimedia} {Retrieval}. {Large}-{Scale} {Multimedia} {Retrieval} and {Evaluation}},
	publisher = {Springer Berlin Heidelberg},
	author = {González, María and Moreno, Julián and Martínez, José Luis and Martínez, Paloma},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Detyniecki, Marcin and García-Serrano, Ana and Nürnberger, Andreas and Stober, Sebastian},
	year = {2013},
	doi = {10.1007/978-3-642-37425-8_3},
	note = {Series Title: Lecture Notes in Computer Science
ISBN2: 978-3-642-37425-8},
	pages = {33--42},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\XVRNLJ7F\\González et al. - 2013 - An Illustrated Methodology for Evaluating ASR Syst.pdf:application/pdf},
}

@phdthesis{zhang_strategies_2019,
	type = {Thesis},
	title = {Strategies for {Handling} {Out}-of-{Vocabulary} {Words} in {Automatic} {Speech} {Recognition}},
	url = {https://jscholarship.library.jhu.edu/handle/1774.2/62275},
	abstract = {Nowadays, most ASR (automatic speech recognition) systems deployed in  industry are closed-vocabulary systems, meaning we have a limited vocabulary of words the system can recognize, and where pronunciations are provided to the system. Words out of this vocabulary are called out-of-vocabulary (OOV) words, for which either pronunciations or both spellings and pronunciations are not known to the system. The basic motivations of developing strategies to handle OOV words are: First, in the training phase, missing or wrong pronunciations of words in training data results in poor acoustic models. Second, in the test phase, words out of the vocabulary cannot be recognized at all, and mis-recognition of OOV words may affect recognition performance of its in-vocabulary neighbors as well. Therefore, this dissertation is dedicated to exploring strategies of handling OOV words in closed-vocabulary ASR. 

First, we investigate dealing with OOV words in ASR training data, by introducing an acoustic-data driven pronunciation learning framework using a likelihood-reduction based criterion for selecting pronunciation candidates from multiple sources, i.e. standard grapheme-to-phoneme algorithms (G2P) and phonetic decoding, in a greedy fashion. This framework effectively expands a small hand-crafted pronunciation lexicon to cover OOV words, for which the learned pronunciations have higher quality than approaches using G2P alone or using other baseline pruning criteria. Furthermore, applying the proposed framework to generate alternative pronunciations for in-vocabulary (IV) words improves both recognition performance on relevant words and overall acoustic model performance.

Second, we investigate dealing with OOV words in ASR test data, i.e. OOV detection and recovery. We first conduct a comparative study of a hybrid lexical model (HLM) approach for OOV detection, and several baseline approaches, with the conclusion that the HLM approach outperforms others in both OOV detection and first pass OOV recovery performance. Next, we introduce a grammar-decoding framework for efficient second pass OOV recovery, showing that with properly designed schemes of estimating OOV unigram probabilities, the framework significantly improves OOV recovery and overall decoding performance compared to first pass decoding.

Finally we propose an open-vocabulary word-level recurrent neural network language model (RNNLM) re-scoring framework, making it possible to re-score lattices containing recovered OOVs using a single word-level RNNLM, that was ignorant of OOVs when it was trained. Above all, the whole OOV recovery pipeline shows the potential of a highly efficient open-vocabulary word-level ASR decoding framework, tightly integrated into a standard WFST decoding pipeline.},
	language = {en\_US},
	urldate = {2022-08-19},
	school = {Johns Hopkins University},
	author = {Zhang, Xiaohui},
	month = oct,
	year = {2019},
	note = {Accepted: 2020-02-06T04:08:11Z},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\6E6BV6R6\\Zhang - 2019 - Strategies for Handling Out-of-Vocabulary Words in.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\QLFGJMPH\\62275.html:text/html},
}

@article{zhang_hello_2017,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@article{morgan_continuous_1995,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {1558-0792},
	doi = {10.1109/79.382443},
	abstract = {The authors focus on a tutorial description of the hybrid HMM/ANN method. The approach has been applied to large vocabulary continuous speech recognition, and variants are in use by many researchers, The method provides a mechanism for incorporating a range of sources of evidence without strong assumptions about their joint statistics, and may have applicability to much more complex systems that can incorporate deep acoustic and linguistic context. The method is inherently discriminant and conservative of parameters. Despite these potential advantages, the hybrid method has focused on implementing fairly simple systems, which do surprisingly well on large continuous speech recognition tasks, Researchers are only beginning to explore the use of more complex structures with this paradigm. In particular, they are just beginning to look at the connectionist inference of language models (including phonology) from data, which may be required in order to take advantage of locally discriminant probabilities rather than simply translating to likelihoods. Finally, the authors' current intuition is that more advanced versions of the hybrid method can greatly benefit from a perceptual perspective.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Speech recognition, Hidden Markov models, Statistics, Vocabulary},
	pages = {24--42},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\SIWUUI5M\\382443.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\99KFFZGH\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{nautsch_gdpr_2019,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\V8ZYWF7E\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@misc{noauthor_spsc_nodate,
	title = {{SPSC} {\textbar} {ISCA} {SIG}-{SPSC}},
	url = {https://www.spsc-sig.org/},
	urldate = {2022-08-19},
}

@misc{noauthor_mozilla_nodate,
	title = {Mozilla {Common} {Voice}},
	url = {https://commonvoice.mozilla.org/},
	language = {en},
	urldate = {2022-08-19},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2ZVA6IM8\\datasets.html:text/html},
}

@article{schertz_acoustic_2020,
	title = {Acoustic cues in production and perception of the four-way stop laryngeal contrast in {Hindi} and {Urdu}},
	volume = {81},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009544702030070X},
	doi = {10.1016/j.wocn.2020.100979},
	language = {en},
	urldate = {2022-08-19},
	journal = {Journal of Phonetics},
	author = {Schertz, Jessamyn and Khan, Sarah},
	month = jul,
	year = {2020},
	pages = {100979},
}

@incollection{dua_urdu_2006,
	title = {Urdu},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022446},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02244-6},
	pages = {269--275},
}

@incollection{dua_hindustani_2006,
	title = {Hindustani},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022203},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02220-3},
	pages = {309--312},
}

@incollection{annamalai_india_2006,
	title = {India: {Language} {Situation}},
	isbn = {978-0-08-044854-1},
	shorttitle = {India},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542046113},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Annamalai, E.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/04611-3},
	pages = {610--613},
}

@misc{noauthor_morfessor_nodate,
	title = {Morfessor 2.0 documentation — {Morfessor} 2.0.4 documentation},
	url = {https://morfessor.readthedocs.io/en/latest/},
	urldate = {2022-08-19},
}

@misc{noauthor_morpho_nodate,
	title = {Morpho project},
	url = {http://morpho.aalto.fi/projects/morpho/},
	urldate = {2022-08-19},
	file = {Morpho project:C\:\\Users\\DELL\\Zotero\\storage\\435PUDQA\\morpho.html:text/html},
}

@inproceedings{farooq_enhancing_2020,
	title = {Enhancing {Large} {Vocabulary} {Continuous} {Speech} {Recognition} {System} for {Urdu}-{English} {Conversational} {Code}-{Switched} {Speech}},
	doi = {10.1109/O-COCOSDA50338.2020.9295036},
	abstract = {This paper presents first step towards Large Vocabulary Continuous Speech Recognition (LVCSR) system for Urdu-English code-switched conversational speech. Urdu is the national language and lingua franca of Pakistan, with 100 million speakers worldwide. English, on the other hand, is official language of Pakistan and commonly mixed with Urdu in daily communication. Urdu, being under-resourced language, have no substantial Urdu-English code-switched corpus in hand to develop speech recognition system. In this research, readily available spontaneous Urdu speech corpus (25 hours) is revised to use it for enhancement of read speech Urdu LVCSR to recognize code-switched speech. This data set is split into 20 hours of train and 5 hours of test set. 10 hours of Urdu BroadCast (BC) data are collected and annotated in a semi-supervised way to enhance the system further. For acoustic modeling, state-of-the-art DNN-HMM modeling technique is used without any prior GMM-HMM training and alignments. Various techniques to improve language model using monolingual data are investigated. The overall percent Word Error Rate (WER) is reduced from 40.71\% to 26.95\% on test set.},
	booktitle = {2020 23rd {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Hussain, Sarmad and Rauf, Sahar and Khalid, Maryam},
	month = nov,
	year = {2020},
	note = {ISSN: 2472-7695},
	keywords = {Speech recognition, Data models, Vocabulary, Acoustics, Speech coding, Speech enhancement, Switches, under-resourced language, Urdu speech recognition, Urdu-English code-switching},
	pages = {155--159},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\88SVPLUM\\9295036.html:text/html},
}

@inproceedings{watanabe_-line_2009,
	address = {Taipei, Taiwan},
	title = {On-line adaptation and {Bayesian} detection of environmental changes based on a macroscopic time evolution system},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4960598/},
	doi = {10.1109/ICASSP.2009.4960598},
	urldate = {2022-08-19},
	booktitle = {2009 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Watanabe, Shinji and Nakamura, Atsushi},
	month = apr,
	year = {2009},
	pages = {4373--4376},
}

@article{davis_automatic_1952-1,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@article{s_review_2016,
	title = {A {Review} on {Automatic} {Speech} {Recognition} {Architecture} and {Approaches}},
	volume = {9},
	issn = {20054254, 20054254},
	url = {http://article.nadiapub.com/IJSIP/vol9_no4/34.pdf},
	doi = {10.14257/ijsip.2016.9.4.34},
	number = {4},
	urldate = {2022-08-19},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {S, Karpagavalli and E, Chandra},
	month = apr,
	year = {2016},
	pages = {393--404},
}

@incollection{maglogiannis__2020-1,
	address = {Cham},
	title = {Α {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology
ISBN2: 978-3-030-49161-1},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\79KCL3R7\\Filippidou and Moussiades - 2020 - Α Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@misc{amodei_deep_2015,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	shorttitle = {Deep {Speech} 2},
	url = {http://arxiv.org/abs/1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	month = dec,
	year = {2015},
	note = {arXiv:1512.02595 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\T5Y77XAY\\Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5HBNU4V7\\1512.html:text/html},
}

@article{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech}},
	url = {https://arxiv.org/abs/1412.5567},
	doi = {10.48550/ARXIV.1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	urldate = {2022-08-19},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{xiong_microsoft_2018,
	title = {The {Microsoft} 2017 {Conversational} {Speech} {Recognition} {System}},
	url = {https://www.microsoft.com/en-us/research/publication/conference-paper-microsoft-2017-conversational-speech-recognition-system/},
	abstract = {We describe the latest version of Microsoft's conversational speech recognition system for the Switchboard and CallHome domains. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby acoustic model posteriors are first combined at the senone/frame level,followed by a word-level voting via confusion networks. We also added another language model rescoring step following the confusion network combination. The resulting system yields a 5.1\% word error rate on the NIST 2000 Switchboard test set, and 9.8\% on the CallHome subset.},
	booktitle = {Proc. {IEEE} {ICASSP}},
	publisher = {IEEE},
	author = {Xiong, Wayne and Wu, Lingfeng and Droppo, Jasha and Huang, Xuedong and Stolcke, Andreas},
	month = apr,
	year = {2018},
	pages = {5934--5938},
}

@article{juang_automatic_2005,
	title = {Automatic {Speech} {Recognition} - {A} {Brief} {History} of the {Technology} {Development}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (1, 2), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. and Rabiner, Lawrence},
	month = jan,
	year = {2005},
}

@incollection{juang_speech_2006,
	address = {Oxford},
	title = {Speech {Recognition}, {Automatic}: {History}},
	isbn = {978-0-08-044854-1},
	shorttitle = {Speech {Recognition}, {Automatic}},
	url = {https://www.sciencedirect.com/science/article/pii/B0080448542009068},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (Dudley, 1939; Dudley et al., 1939), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface. Examples are automatic call processing in the telephone network and query-based information systems that provide updated travel information, stock price quotations, weather reports, etc. In this article we review some major highlights in the research and development of automatic speech recognition during the last few decades to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Juang, B. -H. and Rabiner, L. R.},
	editor = {Brown, Keith},
	month = jan,
	year = {2006},
	doi = {10.1016/B0-08-044854-2/00906-8},
	keywords = {Speech recognition, acoustic modeling, automatic transcription, dialog systems, finite state network, hidden Markov models, keyword spotting, language modeling, neural networks, office automation, pattern recognition, spectral analysis, speech understanding, statistical modeling, time normalization},
	pages = {806--819},
	file = {ScienceDirect Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VWPXBBVS\\B0080448542009068.html:text/html},
}

@book{brown_encyclopedia_2006,
	address = {Amsterdam},
	edition = {2nd ed},
	title = {Encyclopedia of language \& linguistics},
	isbn = {978-0-08-044854-1},
	language = {eng},
	publisher = {Elsevier},
	author = {Brown, E. K. and Anderson, Anne},
	year = {2006},
}

@article{upton_speech_1984-1,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages — this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-19},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@inproceedings{morris_wer_2004-1,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{zhang_hello_2017-1,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@inproceedings{wu_deep_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Deep neural networks employing {Multi}-{Task} {Learning} and stacked bottleneck features for speech synthesis},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178814/},
	doi = {10.1109/ICASSP.2015.7178814},
	urldate = {2022-08-19},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Zhizheng and Valentini-Botinhao, Cassia and Watts, Oliver and King, Simon},
	month = apr,
	year = {2015},
	pages = {4460--4464},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\GFDVBE7Z\\Wu et al. - 2015 - Deep neural networks employing Multi-Task Learning.pdf:application/pdf},
}

@article{pohjalainen_feature_2015,
	title = {Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits},
	volume = {29},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230813001113},
	doi = {10.1016/j.csl.2013.11.004},
	language = {en},
	number = {1},
	urldate = {2022-08-19},
	journal = {Computer Speech \& Language},
	author = {Pohjalainen, Jouni and Räsänen, Okko and Kadioglu, Serdar},
	month = jan,
	year = {2015},
	pages = {145--171},
}

@inproceedings{eyben_opensmile_2010,
	address = {Firenze, Italy},
	title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
	isbn = {978-1-60558-933-6},
	shorttitle = {Opensmile},
	url = {http://dl.acm.org/citation.cfm?doid=1873951.1874246},
	doi = {10.1145/1873951.1874246},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the international conference on {Multimedia} - {MM} '10},
	publisher = {ACM Press},
	author = {Eyben, Florian and Wöllmer, Martin and Schuller, Björn},
	year = {2010},
	pages = {1459},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4F8ACZKL\\Eyben et al. - 2010 - Opensmile the munich versatile and fast open-sour.pdf:application/pdf},
}

@article{chung_unsupervised_2019,
	title = {An {Unsupervised} {Autoregressive} {Model} for {Speech} {Representation} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1904.03240},
	doi = {10.48550/ARXIV.1904.03240},
	abstract = {This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.},
	urldate = {2022-08-19},
	author = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Accepted to Interspeech 2019. Code available at: https://github.com/iamyuanchung/Autoregressive-Predictive-Coding},
}

@inproceedings{boser_training_1992,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {A training algorithm for optimal margin classifiers},
	isbn = {978-0-89791-497-0},
	url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
	doi = {10.1145/130385.130401},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the fifth annual workshop on {Computational} learning theory  - {COLT} '92},
	publisher = {ACM Press},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	year = {1992},
	pages = {144--152},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\29LCGTE4\\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf:application/pdf},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {wav2vec 2.0},
	url = {https://arxiv.org/abs/2006.11477},
	doi = {10.48550/ARXIV.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2022-08-19},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@article{soldz_big_1999,
	title = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}: {A} 45-{Year} {Longitudinal} {Study}},
	volume = {33},
	issn = {00926566},
	shorttitle = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656699922432},
	doi = {10.1006/jrpe.1999.2243},
	language = {en},
	number = {2},
	urldate = {2022-08-19},
	journal = {Journal of Research in Personality},
	author = {Soldz, Stephen and Vaillant, George E.},
	month = jun,
	year = {1999},
	pages = {208--232},
}

@inproceedings{zhou_security_2010,
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	abstract = {Cloud Computing is becoming a well-known buzzword nowadays. Many companies, such as Amazon, Google, Microsoft and so on, accelerate their paces in developing Cloud Computing systems and enhancing their services to provide for a larger amount of users. However, security and privacy issues present a strong barrier for users to adapt into Cloud Computing systems. In this paper, we investigate several Cloud Computing system providers about their concerns on security and privacy issues. We find those concerns are not adequate and more should be added in terms of five aspects (i.e., availability, confidentiality, data integrity, control, audit) for security. Moreover, released acts on privacy are out of date to protect users' private information in the new environment (i.e., Cloud Computing system environment) since they are no longer applicable to the new relationship between users and providers, which contains three parties (i.e., Cloud service user, Cloud service provider/Cloud user, Cloud provider). Multi located data storage and services (i.e., applications) in the Cloud make privacy issues even worse. Hence, adapting released acts for new scenarios in the Cloud, it will result in more users to step into Cloud. We claim that the prosperity in Cloud Computing literature is to be coming after those security and privacy issues having be resolved.},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = dec,
	year = {2010},
	note = {Journal Abbreviation: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
Publication Title: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
DOI:},
	pages = {112},
}

@inproceedings{zhou_security_2010-1,
	address = {Beijing, China},
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	isbn = {978-1-4244-8125-5},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	url = {http://ieeexplore.ieee.org/document/5663489/},
	doi = {10.1109/SKG.2010.19},
	urldate = {2022-08-19},
	booktitle = {2010 {Sixth} {International} {Conference} on {Semantics}, {Knowledge} and {Grids}},
	publisher = {IEEE},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = nov,
	year = {2010},
	pages = {105--112},
}

@inproceedings{karimov_cloud_2021,
	title = {Cloud {Computing} {Security} {Challenges} and {Solutions}},
	doi = {10.1109/ICISCT52966.2021.9670220},
	abstract = {in this paper focuses on development of cloud computing, privacy security issues have become increasingly prominent, which is of concern to industry and academia. We review the research progress on privacy security issues from the perspective of several privacy security protection technologies in cloud computing. First, we introduce some privacy security risks of cloud computing and propose a comprehensive privacy security protection framework.},
	booktitle = {2021 {International} {Conference} on {Information} {Science} and {Communications} {Technologies} ({ICISCT})},
	author = {Karimov, Abdukodir and Olimov, Iskandar and Berdiyev, Khusniddin and Tojiakbarova, Umida and Tursunov, Otabek},
	month = nov,
	year = {2021},
	keywords = {Computational modeling, Access control, Attribute-based encryption, Cloud computing, Communications technology, Industries, Information science, Privacy, Privacy security},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\6B73HGPD\\9670220.html:text/html},
}

@incollection{takabi_introduction_2019,
	title = {Introduction to the {Cloud} and {Fundamental} {Security} and {Privacy} {Issues} of the {Cloud}},
	isbn = {978-1-119-05340-8},
	url = {https://ieeexplore.ieee.org/document/9821151},
	abstract = {Cloud Computing is the most important solution to extend Information Technology's (IT) capabilities. However, Cloud is still vulnerable to a variety of threats and attacks that affects the growth of cloud computing in recent years. Therefore, the security concerns should be considered to improve the assurance of required security for the cloud customers. The cloud security consists of different aspects such as: infrastructure, information, and identity. In this chapter, we provide an introduction to the Cloud and its fundamental security and privacy issues, investigate security issues in different cloud services delivery models and introduce Cloud security standards.},
	urldate = {2022-08-19},
	booktitle = {Security, {Privacy}, and {Digital} {Forensics} in the {Cloud}},
	publisher = {Wiley},
	author = {Takabi, Hassan and GhasemiGol, Mohammad},
	year = {2019},
	doi = {10.1002/9781119053385.ch1},
	note = {Conference Name: Security, Privacy, and Digital Forensics in the Cloud},
	keywords = {Computational modeling, Cloud computing, Privacy, Operating systems, Organizations, Security, Software as a service},
	pages = {1--22},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\NNG85SBV\\9821151.html:text/html},
}

@inproceedings{kumar_systematic_2020,
	title = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}: {Data} {Integrity}, {Confidentiality} and {Availability}},
	shorttitle = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}},
	doi = {10.1109/GUCON48875.2020.9231255},
	abstract = {The cloud computing plays the prominent role in many organizations and researchers were focus on securing the cloud computing. The privacy preserving is the major challenge that grows exponentially with increases in user. In this paper, the depth survey is conducted on the recent methodologies of the cloud storage security related with the cloud computing. The overview of the cloud computing and security issues is analyzed in this paper. The key security requirements such as data integrity, availability and confidentiality. Security issues in the recent methodologies of cloud security is analyzed. The challenges in the cloud security is analyzed and possible future scope of the method is discussed. The paper involves in analyzing the state-of-art method to investigate the advantages and limitations.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Computing}, {Power} and {Communication} {Technologies} ({GUCON})},
	author = {Kumar, Rajeev and Bhatia, M P S},
	month = oct,
	year = {2020},
	keywords = {Systematics, Privacy, Organizations, Cloud Computing, Cloud computing security, Cloud Storage Security, Conferences, Confidentiality, Data integrity, Data Integrity, Data transfer, Privacy Preserving},
	pages = {334--337},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\MT8C247U\\9231255.html:text/html},
}

@inproceedings{godfrey_switchboard_1992,
	address = {San Francisco, CA, USA},
	title = {{SWITCHBOARD}: telephone speech corpus for research and development},
	isbn = {978-0-7803-0532-8},
	shorttitle = {{SWITCHBOARD}},
	url = {http://ieeexplore.ieee.org/document/225858/},
	doi = {10.1109/ICASSP.1992.225858},
	urldate = {2022-08-20},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
	year = {1992},
	pages = {517--520 vol.1},
}

@article{zhang_hello_2017-2,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-20},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Code available in github at https://github.com/ARM-software/ML-KWS-for-MCU},
}

@inproceedings{lin_self-attentive_2020,
	title = {Self-{Attentive} {Similarity} {Measurement} {Strategies} in {Speaker} {Diarization}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1908},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Lin, Qingjian and Hou, Yu and Li, Ming},
	month = oct,
	year = {2020},
	pages = {284--288},
}

@article{schuller_interspeech_2013,
	title = {The interspeech 2013 computational paralinguistics challenge: {Social} signals, conflict, emotion, autism},
	shorttitle = {The interspeech 2013 computational paralinguistics challenge},
	journal = {Proceedings of Interspeech},
	author = {Schuller, Björn and Steidl, S. and Batliner, Anton and Vinciarelli, Alessandro and Scherer, K. and Ringeval, Fabien and Chetouani, Mohamed and Weninger, F. and Eyben, Florian and Marchi, Erik and Mortillaro, Marcello and Salamin, H. and Polychroniou, Anna and Valente, F. and Kim, S.},
	month = jan,
	year = {2013},
	pages = {148--152},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IJ9QFIG3\\Schuller et al. - 2013 - The interspeech 2013 computational paralinguistics.pdf:application/pdf},
}

@inproceedings{misra_spectral_2004,
	address = {Montreal, Que., Canada},
	title = {Spectral entropy based feature for robust {ASR}},
	volume = {1},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1325955/},
	doi = {10.1109/ICASSP.2004.1325955},
	urldate = {2022-08-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Misra, H. and Ikbal, S. and Bourlard, H. and Hermansky, H.},
	year = {2004},
	pages = {I--193--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\YLKP828K\\Misra et al. - 2004 - Spectral entropy based feature for robust ASR.pdf:application/pdf},
}

@article{hunt_spectral_2000,
	title = {Spectral {Signal} {Processing} for {ASR}},
	abstract = {The paper begins by discussing the difficulties in obtaining repeatable results in speech recognition. Theoretical arguments are presented for and against copying human auditory properties in automatic speech recognition. The "standard" acoustic analysis for automatic speech recognition, consisting of melscale cepstrum coefficients and their temporal derivatives, is described. Some variations and extensions of the standard analysis --- PLP, cepstrum correlation methods, LDA, and variants on log power --- are then discussed. These techniques pass the test of having been found useful at multiple sites, especially with noisy speech. The extent to which auditory properties can account for the advantage found for particular techniques is considered. It is concluded that the advantages do not in fact stem from auditory properties, and that there is so far little or no evidence that the study of the human auditory system has contributed to advances in automatic speech recognition. Contributio...},
	author = {Hunt, Melvyn},
	month = aug,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\Z9XRFNJG\\Hunt - 2000 - Spectral Signal Processing for ASR.pdf:application/pdf},
}

@article{biswas_speaker_2021,
	title = {Speaker recognition: an enhanced approach to identify singer voice using neural network},
	volume = {24},
	issn = {1381-2416, 1572-8110},
	shorttitle = {Speaker recognition},
	url = {http://link.springer.com/10.1007/s10772-020-09698-8},
	doi = {10.1007/s10772-020-09698-8},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {International Journal of Speech Technology},
	author = {Biswas, Sharmila and Solanki, Sandeep Singh},
	month = mar,
	year = {2021},
	pages = {9--21},
}

@incollection{penn_computational_2012,
	title = {Computational {Linguistics}},
	isbn = {978-0-444-51747-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444517470500056},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Philosophy of {Linguistics}},
	publisher = {Elsevier},
	author = {Penn, Gerald},
	year = {2012},
	doi = {10.1016/B978-0-444-51747-0.50005-6},
	pages = {143--173},
}

@inproceedings{brill_improved_2000,
	address = {Hong Kong},
	title = {An improved error model for noisy channel spelling correction},
	url = {http://portal.acm.org/citation.cfm?doid=1075218.1075255},
	doi = {10.3115/1075218.1075255},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Proceedings of the 38th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '00},
	publisher = {Association for Computational Linguistics},
	author = {Brill, Eric and Moore, Robert C.},
	year = {2000},
	pages = {286--293},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YP8NG3XW\\Brill and Moore - 2000 - An improved error model for noisy channel spelling.pdf:application/pdf},
}

@misc{noauthor_type_2017,
	title = {Type less, talk more},
	url = {https://blog.google/products/search/type-less-talk-more/},
	abstract = {We’re bringing voice typing (aka talking to your phone instead of typing) to 30 new languages and locales around the world, covering more than a billion people.},
	language = {en-us},
	urldate = {2022-08-20},
	journal = {Google},
	month = aug,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L9T2I37L\\type-less-talk-more.html:text/html},
}

@article{noauthor_amazon_2019,
	title = {Amazon {Workers} {Are} {Listening} to {What} {You} {Tell} {Alexa}},
	url = {https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio},
	abstract = {A global team reviews audio clips in an effort to help the voice-activated assistant respond to commands.},
	language = {en},
	urldate = {2022-08-20},
	journal = {Bloomberg.com},
	month = apr,
	year = {2019},
	keywords = {Software, Privacy, ALPHABET INC-CL A, AMAZON.COM INC, APPLE INC, Boston, business, Costa Rica, India, markets, Policy, Romania, technology},
}

@misc{noauthor_amazon_nodate,
	title = {Amazon {Sends} 1,700 {Alexa} {Voice} {Recordings} to a {Random} {Person}},
	url = {https://threatpost.com/amazon-1700-alexa-voice-recordings/140201/},
	abstract = {The intimate recordings paint a detailed picture of a man's life.},
	language = {en},
	urldate = {2022-08-20},
}

@article{wolfson_amazons_2018,
	chapter = {Technology},
	title = {Amazon's {Alexa} recorded private conversation and sent it to random contact},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2018/may/24/amazon-alexa-recorded-conversation},
	abstract = {The company, which has insisted its Echo devices aren’t always recording, has confirmed the audio was sent},
	language = {en-GB},
	urldate = {2022-08-20},
	journal = {The Guardian},
	author = {Wolfson, Sam},
	month = may,
	year = {2018},
	keywords = {Privacy, Amazon, Amazon Alexa, Internet, Surveillance, Technology, US news},
}

@article{stupp_fraudsters_2019,
	chapter = {WSJ Pro},
	title = {Fraudsters {Used} {AI} to {Mimic} {CEO}’s {Voice} in {Unusual} {Cybercrime} {Case}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402},
	abstract = {Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of funds in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.},
	language = {en-US},
	urldate = {2022-08-20},
	journal = {Wall Street Journal},
	author = {Stupp, Catherine},
	month = aug,
	year = {2019},
	keywords = {artificial intelligence, Artificial Intelligence/Machine Learning, Bobby Filar, business in europe, Business in Europe, business in the u.k., Business in the U.K., c\&e executive news filter, C\&E Executive News Filter, c\&e industry news filter, C\&E Industry News Filter, computer science, Computer Science, content types, Content Types, corporate, corporate crime, Corporate Crime/Legal Action, Corporate/Industrial News, crime, Crime/Legal Action, cybercrime, Cybercrime/Hacking, Euler Hermes Group, factiva filters, Factiva Filters, financial services, Financial Services, fraud, Fraud, general news, hacking, humanities, industrial news, insurance, Insurance, Irakli Beridze, legal action, machine learning, management, Management, non-life insurance, Non-life Insurance, Philipp Amann, political, Political/General News, PRO, Rüdiger Kirsch, sciences, Sciences/Humanities, senior level management, Senior Level Management, trade credit insurance, Trade Credit Insurance, WSJ-PRO-CYBER, WSJ-PRO-WSJ.com},
}

@book{petronio_boundaries_2002,
	address = {Albany},
	series = {{SUNY} series in communication studies},
	title = {Boundaries of privacy: dialectics of disclosure},
	isbn = {978-0-7914-5515-9},
	shorttitle = {Boundaries of privacy},
	publisher = {State University of New York Press},
	author = {Petronio, Sandra Sporbert},
	year = {2002},
	note = {ISBN2: 978-0-7914-5516-6},
	keywords = {Privacy, Interpersonal communication, Secrecy, Self-disclosure},
}

@article{xie_how_2009,
	title = {How to repair customer trust after negative publicity: {The} roles of competence, integrity, benevolence, and forgiveness},
	volume = {26},
	issn = {07426046, 15206793},
	shorttitle = {How to repair customer trust after negative publicity},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mar.20289},
	doi = {10.1002/mar.20289},
	language = {en},
	number = {7},
	urldate = {2022-08-20},
	journal = {Psychology and Marketing},
	author = {Xie, Yi and Peng, Siqing},
	month = jul,
	year = {2009},
	pages = {572--589},
}

@article{shi_edge_2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	url = {http://ieeexplore.ieee.org/document/7488250/},
	doi = {10.1109/JIOT.2016.2579198},
	number = {5},
	urldate = {2022-08-20},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	month = oct,
	year = {2016},
	pages = {637--646},
}

@misc{noauthor_mydata_2015,
	type = {Muut julkaisut},
	title = {{MyData} – {A} {Nordic} {Model} for human-centered personal data management and processing},
	copyright = {This publication is copyrighted. You may download, display and print it for Your own personal use. Commercial use is prohibited.},
	url = {https://julkaisut.valtioneuvosto.fi/handle/10024/78439},
	abstract = {This white paper presents a framework, principles, and a model for a human-centric approach to the managing and processing of personal information. The approach – defined as MyData – is based on the right of individuals to access the data collected about them. The core idea is that individuals should be in control of their own data. The MyData approach aims at strengthening digital human rights while opening new opportunities for businesses to develop innovative personal data based services built on mutual trust.},
	language = {en},
	urldate = {2022-08-20},
	year = {2015},
	note = {Accepted: 2016-11-11T10:03:41Z
ISBN: 9789522434555
Publisher: liikenne- ja viestintäministeriö},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\UBFJDIUP\\2015 - MyData – A Nordic Model for human-centered persona.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\KEV9QSG2\\78439.html:text/html},
}

@inproceedings{nautsch_gdpr_2019-1,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} towards a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {http://arxiv.org/abs/1907.03458},
	doi = {10.21437/Interspeech.2019-2647},
	abstract = {Privacy preservation and the protection of speech data is in high demand, not least as a result of recent regulation, e.g. the General Data Protection Regulation (GDPR) in the EU. While there has been a period with which to prepare for its implementation, its implications for speech data is poorly understood. This assertion applies to both the legal and technology communities, and is hardly surprising since there is no universal definition of 'privacy', let alone a clear understanding of when or how the GDPR applies to the capture, storage and processing of speech data. In aiming to initiate the discussion that is needed to establish a level of harmonisation that is thus far lacking, this contribution presents some reflections of both legal and technology communities on the implications of the GDPR as regards speech data. The article outlines the need for taxonomies at the intersection of speech technology and data privacy - a discussion that is still very much in its infancy - and describes the ways to safeguards and priorities for future research. In being agnostic to any specific application, the treatment should be of interest to the speech communication community at large.},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2019},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	note = {arXiv:1907.03458 [cs, eess]},
	keywords = {Computer Science - Computers and Society, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {3695--3699},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\W7WB9QLH\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\3UUJ3JJX\\1907.html:text/html},
}

@book{european_data_protection_supervisor_edps_2019,
	address = {LU},
	series = {{EDPS} {TechDispatch}},
	title = {{EDPS} {TechDispatch}},
	url = {https://data.europa.eu/doi/10.2804/004275},
	language = {eng},
	urldate = {2022-08-20},
	publisher = {Publications Office},
	author = {{European Data Protection Supervisor}},
	year = {2019},
}

@article{konig_automatic_2015,
	title = {Automatic speech analysis for the assessment of patients with predementia and {Alzheimer}'s disease},
	volume = {1},
	issn = {2352-8729, 2352-8729},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/j.dadm.2014.11.012},
	doi = {10.1016/j.dadm.2014.11.012},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	author = {König, Alexandra and Satt, Aharon and Sorin, Alexander and Hoory, Ron and Toledo‐Ronen, Orith and Derreumaux, Alexandre and Manera, Valeria and Verhey, Frans and Aalten, Pauline and Robert, Phillipe H. and David, Renaud},
	month = mar,
	year = {2015},
	pages = {112--124},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WZVVDNX2\\König et al. - 2015 - Automatic speech analysis for the assessment of pa.pdf:application/pdf},
}

@article{konig_automatic_2015-1,
	title = {Automatic speech analysis for the assessment of patients with predementia and {Alzheimer}'s disease},
	volume = {1},
	issn = {2352-8729, 2352-8729},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/j.dadm.2014.11.012},
	doi = {10.1016/j.dadm.2014.11.012},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	author = {König, Alexandra and Satt, Aharon and Sorin, Alexander and Hoory, Ron and Toledo‐Ronen, Orith and Derreumaux, Alexandre and Manera, Valeria and Verhey, Frans and Aalten, Pauline and Robert, Phillipe H. and David, Renaud},
	month = mar,
	year = {2015},
	pages = {112--124},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\PGX9R2SR\\König et al. - 2015 - Automatic speech analysis for the assessment of pa.pdf:application/pdf},
}

@incollection{gutwirth_seven_2013,
	address = {Dordrecht},
	title = {Seven {Types} of {Privacy}},
	isbn = {978-94-007-5184-2 978-94-007-5170-5},
	url = {http://link.springer.com/10.1007/978-94-007-5170-5_1},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {European {Data} {Protection}: {Coming} of {Age}},
	publisher = {Springer Netherlands},
	author = {Finn, Rachel L. and Wright, David and Friedewald, Michael},
	editor = {Gutwirth, Serge and Leenes, Ronald and de Hert, Paul and Poullet, Yves},
	year = {2013},
	doi = {10.1007/978-94-007-5170-5_1},
	pages = {3--32},
}

@article{chen_no_2003,
	title = {[{No} title found]},
	volume = {4},
	issn = {1385951X},
	url = {http://link.springer.com/10.1023/A:1022962631249},
	doi = {10.1023/A:1022962631249},
	number = {2/3},
	urldate = {2022-08-20},
	journal = {Information Technology and Management},
	author = {Chen, Sandy C. and Dhillon, Gurpreet S.},
	year = {2003},
	pages = {303--318},
}

@misc{reuter_guide_2015,
	title = {A {Guide} to {Fully} {Homomorphic} {Encryption}},
	url = {https://eprint.iacr.org/2015/1192},
	abstract = {Fully homomorphic encryption (FHE) has been dubbed the holy grail of cryptography, an elusive goal which could solve the IT world's problems of security and trust. Research in the area exploded after 2009 when Craig Gentry showed that FHE can be realised in principle. Since that time considerable progress has been made in finding more practical and more efficient solutions. Whilst research quickly developed, terminology and concepts became diverse and confusing so that today it can be difficult to understand what the achievements of different works actually are. The purpose of this paper is to address three fundamental questions: What is FHE? What can FHE be used for? What is the state of FHE today? As well as surveying the field, we clarify different terminology in use and prove connections between different FHE notions.Updated the acknowledgements.},
	urldate = {2022-08-20},
	author = {Reuter, Colin Boyd, Christopher Carr, Kristian Gjøsteen, Angela Jäschke, Christian A., Frederik Armknecht and Strand, Martin},
	year = {2015},
	note = {Report Number: 1192},
	keywords = {Fully Homomorphic Encryption},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PE6U2WJV\\Reuter and Strand - 2015 - A Guide to Fully Homomorphic Encryption.pdf:application/pdf},
}

@book{jessica_gasiorek_message_2018,
	title = {Message {Processing}: {The} {Science} of {Creating} {Understanding}},
	copyright = {Creative Commons Attribution 4.0 International License},
	url = {http://pressbooks-dev.oer.hawaii.edu/messageprocessing/},
	publisher = {UH Mānoa Outreach College},
	author = {Jessica Gasiorek and R. Kelly Aune},
	year = {2018},
	note = {https://pressbooks.oer.hawaii.edu/messageprocessing/\#:{\textasciitilde}:text=Book\%20Title\%3A\%20Message\%20Processing\%3A\%20The\%20Science\%20of\%20Creating\%20Understanding\&text=Book\%20Description\%3A\%20The\%20text\%20provides,on\%20how\%20people\%20create\%20understanding.},
}

@misc{noauthor_kunstliche_nodate,
	title = {Künstliche {Intelligenz} kommt in {Unternehmen} allmählich voran {\textbar} {Bitkom} e.{V}.},
	url = {https://www.bitkom.org/Presse/Presseinformation/Kuenstliche-Intelligenz-kommt-in-Unternehmen-allmaehlich-voran},
	abstract = {Zwei Drittel halten KI für die wichtigste Zukunftstechnologie Bislang nutzen 8 Prozent KI-Anwendungen, jedes vierte Unternehmen will investieren Bitkom-Präsident Berg: „KI braucht noch mehr Schwung“},
	language = {de},
	urldate = {2022-08-21},
}

@misc{noauthor_scaling_nodate,
	title = {Scaling {AI}: {From} {Experimental} to {Exponential}},
	shorttitle = {Scaling {AI}},
	url = {https://www.accenture.com/us-en/insights/artificial-intelligence/ai-investments},
	abstract = {Most businesses deploy pilot \#AI programs, but they struggle when it comes to scaling it. A new Accenture report explains the 3 critical factors for scaling AI.},
	language = {en},
	urldate = {2022-08-21},
}

@article{ardila_end--end_2019,
	title = {End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-019-0447-x},
	doi = {10.1038/s41591-019-0447-x},
	language = {en},
	number = {6},
	urldate = {2022-08-21},
	journal = {Nature Medicine},
	author = {Ardila, Diego and Kiraly, Atilla P. and Bharadwaj, Sujeeth and Choi, Bokyung and Reicher, Joshua J. and Peng, Lily and Tse, Daniel and Etemadi, Mozziyar and Ye, Wenxing and Corrado, Greg and Naidich, David P. and Shetty, Shravya},
	month = jun,
	year = {2019},
	pages = {954--961},
}

@article{fukushima_neocognitron_1988,
	title = {Neocognitron: {A} hierarchical neural network capable of visual pattern recognition},
	volume = {1},
	issn = {08936080},
	shorttitle = {Neocognitron},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
	doi = {10.1016/0893-6080(88)90014-7},
	language = {en},
	number = {2},
	urldate = {2022-08-21},
	journal = {Neural Networks},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {1988},
	pages = {119--130},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2022-08-21},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@inproceedings{ghahremani_acoustic_2016,
	title = {Acoustic {Modelling} from the {Signal} {Domain} {Using} {CNNs}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/ghahremani16_interspeech.html},
	doi = {10.21437/Interspeech.2016-1495},
	language = {en},
	urldate = {2022-08-21},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Ghahremani, Pegah and Manohar, Vimal and Povey, Daniel and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {3434--3438},
}

@misc{hou_audio-visual_2018,
	title = {Audio-{Visual} {Speech} {Enhancement} {Using} {Multimodal} {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.10893},
	abstract = {Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus only on addressing audio information. In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. We also propose a multi-task learning framework for reconstructing audio and visual signals at the output layer. Precisely speaking, the proposed AVDCNN model is structured as an audio-visual encoder-decoder network, in which audio and visual data are first processed using individual CNNs, and then fused into a joint network to generate enhanced speech (the primary task) and reconstructed images (the secondary task) at the output layer. The model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five instrumental criteria. Results show that the AVDCNN model yields a notably superior performance compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process. In addition, the AVDCNN model also outperforms an existing audio-visual SE model, confirming its capability of effectively combining audio and visual information in SE.},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Hou, Jen-Cheng and Wang, Syu-Siang and Lai, Ying-Hui and Tsao, Yu and Chang, Hsiu-Wen and Wang, Hsin-Min},
	month = jan,
	year = {2018},
	note = {arXiv:1703.10893 [cs, stat]},
	keywords = {Computer Science - Multimedia, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: To appear in IEEE Transactions on Emerging Topics in Computational Intelligence. Some audio samples can be reached in this link: https://sites.google.com/view/avse2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\RZW9TSJU\\Hou et al. - 2018 - Audio-Visual Speech Enhancement Using Multimodal D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\W8FSCKT6\\1703.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2022-08-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{errattahi_automatic_2018-1,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-21},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\HJ993EDS\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@article{morgan_continuous_1995-1,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {10535888},
	url = {http://ieeexplore.ieee.org/document/382443/},
	doi = {10.1109/79.382443},
	number = {3},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	pages = {24--42},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\8AZVI7RK\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{li_hybrid_2013,
	address = {Geneva, Switzerland},
	title = {Hybrid {Deep} {Neural} {Network}--{Hidden} {Markov} {Model} ({DNN}-{HMM}) {Based} {Speech} {Emotion} {Recognition}},
	isbn = {978-0-7695-5048-0},
	url = {http://ieeexplore.ieee.org/document/6681449/},
	doi = {10.1109/ACII.2013.58},
	urldate = {2022-08-21},
	booktitle = {2013 {Humaine} {Association} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction}},
	publisher = {IEEE},
	author = {Li, Longfei and Zhao, Yong and Jiang, Dongmei and Zhang, Yanning and Wang, Fengna and Gonzalez, Isabel and Valentin, Enescu and Sahli, Hichem},
	month = sep,
	year = {2013},
	pages = {312--317},
}

@article{ochiai_speaker_2016,
	title = {Speaker {Adaptive} {Training} {Localizing} {Speaker} {Modules} in {DNN} for {Hybrid} {DNN}-{HMM} {Speech} {Recognizers}},
	volume = {E99.D},
	issn = {0916-8532, 1745-1361},
	url = {https://www.jstage.jst.go.jp/article/transinf/E99.D/10/E99.D_2016SLP0010/_article},
	doi = {10.1587/transinf.2016SLP0010},
	language = {en},
	number = {10},
	urldate = {2022-08-21},
	journal = {IEICE Transactions on Information and Systems},
	author = {Ochiai, Tsubasa and Matsuda, Shigeki and Watanabe, Hideyuki and Lu, Xugang and Hori, Chiori and Kawai, Hisashi and Katagiri, Shigeru},
	year = {2016},
	pages = {2431--2443},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\8EY8WZF7\\Ochiai et al. - 2016 - Speaker Adaptive Training Localizing Speaker Modul.pdf:application/pdf},
}

@book{muller_fundamentals_2021,
	address = {Cham, Switzerland},
	edition = {Second edition},
	title = {Fundamentals of music processing: using {Python} and {Jupyter} notebooks},
	isbn = {978-3-030-69807-2},
	shorttitle = {Fundamentals of music processing},
	language = {eng},
	publisher = {Springer},
	author = {Müller, Meinard},
	year = {2021},
}

@inproceedings{seide_feature_2011,
	address = {Waikoloa, HI, USA},
	title = {Feature engineering in {Context}-{Dependent} {Deep} {Neural} {Networks} for conversational speech transcription},
	isbn = {978-1-4673-0367-5},
	url = {http://ieeexplore.ieee.org/document/6163899/},
	doi = {10.1109/ASRU.2011.6163899},
	urldate = {2022-08-21},
	booktitle = {2011 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} \& {Understanding}},
	publisher = {IEEE},
	author = {Seide, Frank and Li, Gang and Chen, Xie and Yu, Dong},
	month = dec,
	year = {2011},
	note = {ISBN2: 978-1-4673-0365-1 
ISBN3: 978-1-4673-0366-8},
	pages = {24--29},
}

@article{dua_developing_2022,
	title = {Developing a {Speech} {Recognition} {System} for {Recognizing} {Tonal} {Speech} {Signals} {Using} a {Convolutional} {Neural} {Network}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/12/6223},
	doi = {10.3390/app12126223},
	abstract = {Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15\% accuracy rate and a 10.56\% WER for continuous and extensive vocabulary sentences of speech signals with different tones.},
	language = {en},
	number = {12},
	urldate = {2022-08-21},
	journal = {Applied Sciences},
	author = {Dua, Sakshi and Kumar, Sethuraman Sambath and Albagory, Yasser and Ramalingam, Rajakumar and Dumka, Ankur and Singh, Rajesh and Rashid, Mamoon and Gehlot, Anita and Alshamrani, Sultan S. and AlGhamdi, Ahmed Saeed},
	month = jun,
	year = {2022},
	pages = {6223},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\97ILRS3G\\Dua et al. - 2022 - Developing a Speech Recognition System for Recogni.pdf:application/pdf},
}

@misc{garofolo_john_s_timit_1993,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}ul{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.phn" rel="nofollow"{\textgreater}phonemes{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.txt" rel="nofollow"{\textgreater}transcripts{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wav" rel="nofollow"{\textgreater}audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wrd" rel="nofollow"{\textgreater}word list{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}/ul{\textgreater}{\textless}/br{\textgreater} 
Portions © 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	year = {1993},
	doi = {10.35111/17GK-BN40},
	note = {Artwork Size: 715776 KB
Pages: 715776 KB
Type: dataset},
}

@article{bell_adaptation_2020,
	title = {Adaptation algorithms for neural network-based speech recognition: {An} overview},
	volume = {2},
	shorttitle = {Adaptation algorithms for neural network-based speech recognition},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {33--66},
}

@article{bell_adaptation_2021,
	title = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}: {An} {Overview}},
	volume = {2},
	issn = {2644-1322},
	shorttitle = {Adaptation {Algorithms} for {Neural} {Network}-{Based} {Speech} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9296327/},
	doi = {10.1109/OJSP.2020.3045349},
	urldate = {2022-08-21},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
	year = {2021},
	pages = {33--66},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\X557RF2Q\\Bell et al. - 2021 - Adaptation Algorithms for Neural Network-Based Spe.pdf:application/pdf},
}

@misc{garofolo_john_s_csr-i_2007,
	title = {{CSR}-{I} ({WSJ0}) {Complete}},
	url = {https://catalog.ldc.upenn.edu/LDC93S6A},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}LDC93S6A - Complete CSR-I corpus {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6B" rel="nofollow"{\textgreater}LDC93S6B{\textless}/a{\textgreater} - CSR-I Sennheiser speech {\textless}a href="http://catalog.ldc.upenn.edu/LDC93S6C" rel="nofollow"{\textgreater}LDC93S6C{\textless}/a{\textgreater} - CSR-I other speech{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}During 1991, the DARPA Spoken Language Program initiated efforts to build a new corpus to support research on large-vocabulary Continuous Speech Recognition (CSR) systems.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The first two CSR Corpora consist primarily of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news text and are thus often known as WSJ0 and WSJ1. (Later sections of the CSR set of corpora, however, will consist of read texts from other sources of North American business news and eventually from other news domains).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The texts to be read were selected to fall within either a 5,000-word or a 20,000-word subset of the WSJ text corpus. (See the documentation for details). Some spontaneous dictation is included in addition to the read speech. The dictation portion was collected using journalists who dictated hypothetical news articles.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Two microphones are used throughout: a close-talking Sennheiser HMD414 and a secondary microphone, which may vary. The corpora are thus offered in three configurations: the speech from the Sennheiser, the speech from the other microphone and the speech from both; all three sets include all transcriptions, tests, documentation, etc.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}In general, transcriptions of the speech, test data from ARPA evaluations, scores achieved by various speech recognition systems and software used in scoring are included on separate discs from the waveform data.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}Please listen to this {\textless}a href="desc/addenda/LDC93S6A.wav"{\textgreater}audio sample{\textless}/a{\textgreater}.{\textless}/p{\textgreater}{\textless}/br{\textgreater} 
Portions © 1987-1989 Dow Jones \& Company, Inc., © 1992, 1993 Trustees of the University of Pennsylvania},
	urldate = {2022-08-21},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Graff, David and Paul, Doug and Pallett, David},
	month = may,
	year = {2007},
	doi = {10.35111/EWKM-CG47},
	note = {Artwork Size: 9542041 KB
Pages: 9542041 KB
Type: dataset},
}

@article{li_deng_mnist_2012,
	title = {The {MNIST} {Database} of {Handwritten} {Digit} {Images} for {Machine} {Learning} {Research} [{Best} of the {Web}]},
	volume = {29},
	issn = {1053-5888},
	url = {http://ieeexplore.ieee.org/document/6296535/},
	doi = {10.1109/MSP.2012.2211477},
	number = {6},
	urldate = {2022-08-21},
	journal = {IEEE Signal Processing Magazine},
	author = {{Li Deng}},
	month = nov,
	year = {2012},
	pages = {141--142},
}

@inproceedings{deng_imagenet_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	urldate = {2022-08-21},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
}

@misc{noauthor_center_nodate,
	title = {Center for {Language} {Engineering}},
	url = {https://cle.org.pk/},
	urldate = {2022-08-22},
	file = {Center for Language Engineering:C\:\\Users\\DELL\\Zotero\\storage\\LV3V3DMG\\cle.org.pk.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub}: {Where} the world builds software},
	shorttitle = {{GitHub}},
	url = {https://github.com/},
	abstract = {GitHub is where over 83 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositories, review code like a pro, track bugs and feat...},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {citation-265913669.bib:C\:\\Users\\DELL\\Zotero\\storage\\YSPPY2BA\\citation-265913669.bib:application/x-bibtex;citation.bib:C\:\\Users\\DELL\\Zotero\\storage\\6H4SBI5D\\citation.bib:application/x-bibtex},
}

@misc{noauthor_kaggle_nodate,
	title = {Kaggle: {Your} {Machine} {Learning} and {Data} {Science} {Community}},
	shorttitle = {Kaggle},
	url = {https://www.kaggle.com/},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	language = {en},
	urldate = {2022-08-22},
}

@article{verma_i-vectors_2015,
	title = {i-{Vectors} in speech processing applications: a survey},
	volume = {18},
	issn = {1381-2416, 1572-8110},
	shorttitle = {i-{Vectors} in speech processing applications},
	url = {http://link.springer.com/10.1007/s10772-015-9295-3},
	doi = {10.1007/s10772-015-9295-3},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {International Journal of Speech Technology},
	author = {Verma, Pulkit and Das, Pradip K.},
	month = dec,
	year = {2015},
	pages = {529--546},
}

@article{verma_i-vectors_2015-1,
	title = {i-{Vectors} in speech processing applications: a survey},
	volume = {18},
	issn = {1381-2416, 1572-8110},
	shorttitle = {i-{Vectors} in speech processing applications},
	url = {http://link.springer.com/10.1007/s10772-015-9295-3},
	doi = {10.1007/s10772-015-9295-3},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {International Journal of Speech Technology},
	author = {Verma, Pulkit and Das, Pradip K.},
	month = dec,
	year = {2015},
	pages = {529--546},
}

@article{joshi_modified_2016,
	title = {Modified {Mean} and {Variance} {Normalization}: {Transforming} to {Utterance}-{Specific} {Estimates}},
	volume = {35},
	issn = {0278-081X, 1531-5878},
	shorttitle = {Modified {Mean} and {Variance} {Normalization}},
	url = {http://link.springer.com/10.1007/s00034-015-0129-y},
	doi = {10.1007/s00034-015-0129-y},
	language = {en},
	number = {5},
	urldate = {2022-08-22},
	journal = {Circuits, Systems, and Signal Processing},
	author = {Joshi, Vikas and Prasad, N. Vishnu and Umesh, S.},
	month = may,
	year = {2016},
	pages = {1593--1609},
}

@book{institute_of_electrical_and_electronics_engineers_2013_2013,
	address = {Piscataway, NJ},
	title = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013): {Olomouc}, {Czech} {Republic}, 8 - 12 {December} 2013},
	isbn = {978-1-4799-2756-2 978-1-4799-2757-9},
	shorttitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({ASRU} 2013)},
	language = {eng},
	publisher = {IEEE},
	editor = {Institute of Electrical {and} Electronics Engineers},
	year = {2013},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\K3IX7I3E\\Institute of Electrical and Electronics Engineers - 2013 - 2013 IEEE Workshop on Automatic Speech Recognition.pdf:application/pdf},
}

@misc{noauthor_urdu_nodate,
	title = {Urdu {Speech} {Dataset}},
	url = {https://www.kaggle.com/datasets/hazrat/urdu-speech-dataset},
	abstract = {2,500 Urdu audio samples},
	language = {en},
	urldate = {2022-08-22},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\G6KKGYW6\\urdu-speech-dataset.html:text/html},
}

@misc{noauthor_gramvaani_hindi_asrkaldiasr_nodate,
	title = {gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	url = {https://github.com/anish9208/gramvaani_hindi_asr},
	abstract = {This repo contains the baseline model recipes and pre-trained model for GramVanni hindi ASR challenge  - gramvaani\_hindi\_asr/kaldi/asr at main · anish9208/gramvaani\_hindi\_asr},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\MXCR2LTK\\asr.html:text/html},
}

@inproceedings{peinl_open_2020,
	address = {Deggendorf, Germany},
	title = {Open {Source} {Speech} {Recognition} on {Edge} {Devices}},
	isbn = {978-1-72816-759-6 978-1-72816-760-2},
	url = {https://ieeexplore.ieee.org/document/9208978/},
	doi = {10.1109/ACIT49673.2020.9208978},
	urldate = {2022-08-22},
	booktitle = {2020 10th {International} {Conference} on {Advanced} {Computer} {Information} {Technologies} ({ACIT})},
	publisher = {IEEE},
	author = {Peinl, Rene and Rizk, Basem and Szabad, Robert},
	month = sep,
	year = {2020},
	pages = {441--445},
}

@inproceedings{christian_gaida_comparing_2014,
	title = {Comparing {Open}-{Source} {Speech} {Recognition} {Toolkits}},
	author = {Christian Gaida and P. Lange and Rico Petrick and Patrick Proba and Ahmed Malatawy and David Suendermann-Oeft},
	year = {2014},
}

@misc{noauthor_tdnn_nodate,
	title = {{TDNN} --{\textgreater} {CNN}},
	url = {https://groups.google.com/g/kaldi-help/c/jsg1Oo4bNGQ/m/uwvFw5PtBwAJ},
	urldate = {2022-08-22},
	file = {TDNN --> CNN:C\:\\Users\\DELL\\Zotero\\storage\\X6FFGDN4\\uwvFw5PtBwAJ.html:text/html},
}

@inproceedings{kreyssig_improved_2018,
	address = {Calgary, AB},
	title = {Improved {Tdnns} {Using} {Deep} {Kernels} and {Frequency} {Dependent} {Grid}-{RNNS}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462523/},
	doi = {10.1109/ICASSP.2018.8462523},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kreyssig, F. L. and Zhang, C. and Woodland, P. C.},
	month = apr,
	year = {2018},
	pages = {4864--4868},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\MDGVZMYB\\Kreyssig et al. - 2018 - Improved Tdnns Using Deep Kernels and Frequency De.pdf:application/pdf},
}

@inproceedings{biswas_semi-supervised_2019,
	title = {Semi-{Supervised} {Acoustic} {Model} {Training} for {Five}-{Lingual} {Code}-{Switched} {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html},
	doi = {10.21437/Interspeech.2019-1325},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Biswas, Astik and Yılmaz, Emre and Wet, Febe de and Westhuizen, Ewald van der and Niesler, Thomas},
	month = sep,
	year = {2019},
	pages = {3745--3749},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5DDQYLKT\\Biswas et al. - 2019 - Semi-Supervised Acoustic Model Training for Five-L.pdf:application/pdf},
}

@inproceedings{zorila_investigation_2019,
	address = {SG, Singapore},
	title = {An {Investigation} into the {Effectiveness} of {Enhancement} in {ASR} {Training} and {Test} for {Chime}-5 {Dinner} {Party} {Transcription}},
	isbn = {978-1-72810-306-8},
	url = {https://ieeexplore.ieee.org/document/9003785/},
	doi = {10.1109/ASRU46091.2019.9003785},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Zorila, Catalin and Boeddeker, Christoph and Doddipatla, Rama and Haeb-Umbach, Reinhold},
	month = dec,
	year = {2019},
	pages = {47--53},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\IE9HPYPR\\Zorila et al. - 2019 - An Investigation into the Effectiveness of Enhance.pdf:application/pdf},
}

@article{abdel-hamid_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Speech} {Recognition}},
	volume = {22},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/6857341/},
	doi = {10.1109/TASLP.2014.2339736},
	number = {10},
	urldate = {2022-08-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
	month = oct,
	year = {2014},
	pages = {1533--1545},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\MQX94C3D\\Abdel-Hamid et al. - 2014 - Convolutional Neural Networks for Speech Recogniti.pdf:application/pdf},
}

@article{eeckt_continual_2021,
	title = {Continual {Learning} for {Monolingual} {End}-to-{End} {Automatic} {Speech} {Recognition}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2112.09427},
	doi = {10.48550/ARXIV.2112.09427},
	abstract = {Adapting Automatic Speech Recognition (ASR) models to new domains results in a deterioration of performance on the original domain(s), a phenomenon called Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to new accents, dialects, topics, etc. without suffering from CF, making them unable to be continually enhanced without storing all past data. Fortunately, Continual Learning (CL) methods, which aim to enable continual adaptation while overcoming CF, can be used. In this paper, we implement an extensive number of CL methods for End-to-End ASR and test and compare their ability to extend a monolingual Hybrid CTC-Transformer model across four new tasks. We find that the best performing CL method closes the gap between the fine-tuned model (lower bound) and the model trained jointly on all tasks (upper bound) by more than 40\%, while requiring access to only 0.6\% of the original data.},
	urldate = {2022-08-22},
	author = {Eeckt, Steven Vander and Van hamme, Hugo},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering},
	annote = {Other
Accepted at EUSIPCO 2022. 5 pages, 1 figure},
}

@article{ali_automatic_2015-1,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-22},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@inproceedings{shaik_riyaz_automatic_2019,
	title = {Automatic {Speaker} {Recognition} {System} in {Urdu} using {MFCC} {\textbackslash}\& {HMM}},
	author = {Shaik Riyaz and Bathula Lakshmi Bhavani and S. Venkatrama Phani Kumar},
	year = {2019},
}

@misc{noauthor_federated_nodate,
	title = {Federated {Learning}: {Collaborative} {Machine} {Learning} without {Centralized} {Training} {Data}},
	shorttitle = {Federated {Learning}},
	url = {http://ai.googleblog.com/2017/04/federated-learning-collaborative.html},
	abstract = {Posted by Brendan McMahan and Daniel Ramage, Research Scientists Standard machine learning approaches require centralizing the training data...},
	language = {en},
	urldate = {2022-08-22},
	journal = {Google AI Blog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\CPB7S2RU\\federated-learning-collaborative.html:text/html},
}

@inproceedings{andreas_stolcke_srilm_2002,
	title = {{SRILM} -- {An} xtensible language modeling toolkit},
	url = {https://www.sri.com/platform/srilm/},
	author = {Andreas Stolcke},
	year = {2002},
}

@inproceedings{ali_complete_2014,
	address = {South Lake Tahoe, NV, USA},
	title = {A complete {KALDI} recipe for building {Arabic} speech recognition systems},
	isbn = {978-1-4799-7129-9},
	url = {http://ieeexplore.ieee.org/document/7078629/},
	doi = {10.1109/SLT.2014.7078629},
	urldate = {2022-08-23},
	booktitle = {2014 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Ali, Ahmed and Zhang, Yifan and Cardinal, Patrick and Dahak, Najim and Vogel, Stephan and Glass, James},
	month = dec,
	year = {2014},
	pages = {525--529},
}

@article{amodei_deep_2015-1,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech} 2},
	url = {https://arxiv.org/abs/1512.02595},
	doi = {10.48550/ARXIV.1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-23},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{alhanai_development_2016,
	address = {San Diego, CA},
	title = {Development of the {MIT} {ASR} system for the 2016 {Arabic} {Multi}-genre {Broadcast} {Challenge}},
	isbn = {978-1-5090-4903-5},
	url = {http://ieeexplore.ieee.org/document/7846280/},
	doi = {10.1109/SLT.2016.7846280},
	urldate = {2022-08-23},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {AlHanai, Tuka and Hsu, Wei-Ning and Glass, James},
	month = dec,
	year = {2016},
	pages = {299--304},
}

@phdthesis{meyerjosh_multi-task_2019,
	title = {Multi-{Task} and {Transfer} {Learning} in {Low}-{Resource} {Speech} {Recognition}},
	copyright = {Pro Quest LLC},
	url = {https://www.proquest.com/openview/0d416c7a0cb00a3f6069c467ad545db5/1?pq-origsite=gscholar&cbl=18750&diss=y},
	abstract = {This thesis investigates methods for Acoustic Modeling in Automatic Speech Recognition, assuming limited access to training data in the target domain. The Acoustic
Models of interest are Deep Neural Network Acoustic Models (in both the Hybrid
and End-to-End approaches), and the target domains in question are either different
languages or different speakers. Inductive bias is transfered from a source domain
during training, via Multi-Task Learning or Transfer Learning.
With regards to Multi-Task Learning, Chapter (5) presents experiments which
explicitly incorporate linguistic knowledge (i.e. phonetics and phonology) into an
auxiliary task during neural Acoustic Model training. In Chapter (6), I investigate
Multi-Task methods which do not rely on expert knowledge (linguistic or otherwise),
by re-using existing parts of the Hybrid training pipeline. In Chapter (7), new tasks
are discovered using unsupervised learning. In Chapter (8), using the “copy-paste”
Transfer Learning approach, I demonstrate that with an appropriate early-stopping
criteria, cross-lingual transfer is possible to both large and small target datasets.
The methods and intuitions which rely on linguistic knowledge are of interest to
the Speech Recognition practitioner working in low-resource domains. These same
sections may be of interest to the theoretical linguist, as a study of the relative import
of phonetic categories in classification. To the Machine Learning practitioner, I hope
to offer approaches which can be easily ported over to other classification tasks. To
the Machine Learning researcher, I hope to inspire new ideas on addressing the small
data problem.},
	language = {en},
	urldate = {2022-08-23},
	school = {University of Arizona},
	author = {Meyer,Josh},
	year = {2019},
	note = {http://jrmeyer.github.io/misc/MEYER\_dissertation\_2019.pdf
Published by Pro Quest LLC},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\IWJEWKS8\\1.html:text/html},
}

@book{international_speech_communication_association_speech_2016,
	address = {Red Hook, NY},
	title = {Speech beyond speech towards a better understanding of the most important biosignal: 16th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({INTERSPEECH} 2015): {Dresden}, {Germany}, 6-10 {September} 2015. {Volume} 5},
	volume = {5},
	isbn = {978-1-5108-1790-6},
	shorttitle = {Speech beyond speech towards a better understanding of the most important biosignal},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {International Speech Communication Association},
	year = {2016},
	note = {Meeting Name: International Speech Communication Association},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\QHEKNZCX\\International Speech Communication Association - 2016 - Speech beyond speech towards a better understandin.pdf:application/pdf},
}

@inproceedings{shaik_improvements_2015,
	title = {Improvements in {RWTH} {LVCSR} evaluation systems for {Polish}, {Portuguese}, {English}, urdu, and {Arabic}},
	url = {https://www.isca-speech.org/archive/interspeech_2015/shaik15_interspeech.html},
	doi = {10.21437/Interspeech.2015-635},
	language = {en},
	urldate = {2022-08-23},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Shaik, M. Ali Basha and Tüske, Zoltán and Tahir, M. Ali and Nußbaum-Thom, Markus and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2015},
	pages = {3154--3158},
}

@article{kumar_large-vocabulary_2004,
	title = {A large-vocabulary continuous speech recognition system for {Hindi}},
	volume = {48},
	issn = {0018-8646, 0018-8646},
	url = {http://ieeexplore.ieee.org/document/5388836/},
	doi = {10.1147/rd.485.0703},
	number = {5.6},
	urldate = {2022-08-23},
	journal = {IBM Journal of Research and Development},
	author = {Kumar, M. and Rajput, N. and Verma, A.},
	month = sep,
	year = {2004},
	pages = {703--715},
}

@article{ming_speech_2017,
	title = {Speech {Enhancement} {Based} on {Full}-{Sentence} {Correlation} and {Clean} {Speech} {Recognition}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7814226/},
	doi = {10.1109/TASLP.2017.2651406},
	number = {3},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Ming, Ji and Crookes, Danny},
	month = mar,
	year = {2017},
	pages = {531--543},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\FKM8TYUH\\Ming and Crookes - 2017 - Speech Enhancement Based on Full-Sentence Correlat.pdf:application/pdf},
}

@article{ganapathy_multivariate_2017,
	title = {Multivariate {Autoregressive} {Spectrogram} {Modeling} for {Noisy} {Speech} {Recognition}},
	volume = {24},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7973047/},
	doi = {10.1109/LSP.2017.2724561},
	number = {9},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Ganapathy, Sriram},
	month = sep,
	year = {2017},
	pages = {1373--1377},
}

@article{lee_dnn-based_2016,
	title = {{DNN}-{Based} {Feature} {Enhancement} {Using} {DOA}-{Constrained} {ICA} for {Robust} {Speech} {Recognition}},
	volume = {23},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/7497454/},
	doi = {10.1109/LSP.2016.2583658},
	number = {8},
	urldate = {2022-08-23},
	journal = {IEEE Signal Processing Letters},
	author = {Lee, Ho-Yong and Cho, Ji-Won and Kim, Minook and Park, Hyung-Min},
	month = aug,
	year = {2016},
	pages = {1091--1095},
}

@article{lee_threshold-based_2018,
	title = {Threshold-{Based} {Noise} {Detection} and {Reduction} for {Automatic} {Speech} {Recognition} {System} in {Human}-{Robot} {Interactions}},
	volume = {18},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/18/7/2068},
	doi = {10.3390/s18072068},
	language = {en},
	number = {7},
	urldate = {2022-08-23},
	journal = {Sensors},
	author = {Lee, Sheng-Chieh and Wang, Jhing-Fa and Chen, Miao-Hia},
	month = jun,
	year = {2018},
	pages = {2068},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TB36Q4AT\\Lee et al. - 2018 - Threshold-Based Noise Detection and Reduction for .pdf:application/pdf},
}

@article{gosztolya_domain_2016,
	title = {Domain {Adaptation} of {Deep} {Neural} {Networks} for {Automatic} {Speech} {Recognition} via {Wireless} {Sensors}},
	volume = {67},
	issn = {1339-309X},
	url = {https://www.sciendo.com/article/10.1515/jee-2016-0017},
	doi = {10.1515/jee-2016-0017},
	abstract = {Abstract
            Wireless sensors are recent, portable, low-powered devices, designed to record and transmit observations of their environment such as speech. To allow portability they are designed to have a small size and weight; this, however, along with their low power consumption, usually means that they have only quite basic recording equipment (e.g. microphone) installed. Recent speech technology applications typically require several dozen hours of audio recordings (nowadays even hundreds of hours is common), which is usually not available as recorded material by such sensors. Since systems trained with studio-level utterances tend to perform suboptimally for such recordings, a sensible idea is to adapt models which were trained on existing, larger, noise-free corpora. In this study, we experimented with adapting Deep Neural Network-based acoustic models trained on noise-free speech data to perform speech recognition on utterances recorded by wireless sensors. In the end, we were able to achieve a 5\% gain in terms of relative error reduction compared to training only on the sensor-recorded, restricted utterance subset.},
	language = {en},
	number = {2},
	urldate = {2022-08-23},
	journal = {Journal of Electrical Engineering},
	author = {Gosztolya, Gábor and Grósz, Tamás},
	month = apr,
	year = {2016},
	pages = {124--130},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CSYDKLJL\\Gosztolya and Grósz - 2016 - Domain Adaptation of Deep Neural Networks for Auto.pdf:application/pdf},
}

@article{chen_progressive_2018,
	title = {Progressive {Joint} {Modeling} in {Unsupervised} {Single}-{Channel} {Overlapped} {Speech} {Recognition}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/8080252/},
	doi = {10.1109/TASLP.2017.2765834},
	number = {1},
	urldate = {2022-08-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Chen, Zhehuai and Droppo, Jasha and Li, Jinyu and Xiong, Wayne},
	month = jan,
	year = {2018},
	pages = {184--196},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\U75U9AZK\\Chen et al. - 2018 - Progressive Joint Modeling in Unsupervised Single-.pdf:application/pdf},
}

@misc{dautume_episodic_2019,
	title = {Episodic {Memory} in {Lifelong} {Language} {Learning}},
	url = {http://arxiv.org/abs/1906.01076},
	abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\textasciitilde}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {d'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
	month = nov,
	year = {2019},
	note = {arXiv:1906.01076 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Proceedings of NeurIPS 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\F68HIZ99\\d'Autume et al. - 2019 - Episodic Memory in Lifelong Language Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\U9A7WITC\\1906.html:text/html},
}

@misc{chang_towards_2021,
	title = {Towards {Lifelong} {Learning} of {End}-to-end {ASR}},
	url = {http://arxiv.org/abs/2104.01616},
	abstract = {Automatic speech recognition (ASR) technologies today are primarily optimized for given datasets; thus, any changes in the application environment (e.g., acoustic conditions or topic domains) may inevitably degrade the performance. We can collect new data describing the new environment and fine-tune the system, but this naturally leads to higher error rates for the earlier datasets, referred to as catastrophic forgetting. The concept of lifelong learning (LLL) aiming to enable a machine to sequentially learn new tasks from new datasets describing the changing real world without forgetting the previously learned knowledge is thus brought to attention. This paper reports, to our knowledge, the first effort to extensively consider and analyze the use of various approaches of LLL in end-to-end (E2E) ASR, including proposing novel methods in saving data for past domains to mitigate the catastrophic forgetting problem. An overall relative reduction of 28.7\% in WER was achieved compared to the fine-tuning baseline when sequentially learning on three very different benchmark corpora. This can be the first step toward the highly desired ASR technologies capable of synchronizing with the continuously changing real world.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Chang, Heng-Jui and Lee, Hung-yi and Lee, Lin-shan},
	month = jul,
	year = {2021},
	note = {arXiv:2104.01616 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Interspeech 2021. We acknowledge the support of Salesforce Research Deep Learning Grant},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\NZP72WVC\\Chang et al. - 2021 - Towards Lifelong Learning of End-to-end ASR.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\PTMWQGE5\\2104.html:text/html},
}

@misc{yang_online_2022,
	title = {Online {Continual} {Learning} of {End}-to-{End} {Speech} {Recognition} {Models}},
	url = {http://arxiv.org/abs/2207.05071},
	abstract = {Continual Learning, also known as Lifelong Learning, aims to continually learn from new data as it becomes available. While prior research on continual learning in automatic speech recognition has focused on the adaptation of models across multiple different speech recognition tasks, in this paper we propose an experimental setting for {\textbackslash}textit\{online continual learning\} for automatic speech recognition of a single task. Specifically focusing on the case where additional training data for the same task becomes available incrementally over time, we demonstrate the effectiveness of performing incremental model updates to end-to-end speech recognition models with an online Gradient Episodic Memory (GEM) method. Moreover, we show that with online continual learning and a selective sampling strategy, we can maintain an accuracy that is similar to retraining a model from scratch while requiring significantly lower computation costs. We have also verified our method with self-supervised learning (SSL) features.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Yang, Muqiao and Lane, Ian and Watanabe, Shinji},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05071 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at InterSpeech 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D5PLRPCB\\Yang et al. - 2022 - Online Continual Learning of End-to-End Speech Rec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\Q2KHNKDV\\2207.html:text/html},
}

@article{kumar_leveraging_2020,
	title = {Leveraging {Linguistic} {Context} in {Dyadic} {Interactions} to {Improve} {Automatic} {Speech} {Recognition} for {Children}},
	volume = {63},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300346},
	doi = {10.1016/j.csl.2020.101101},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Lyon, Thomas D. and Narayanan, Shrikanth},
	month = sep,
	year = {2020},
	pages = {101101},
}

@article{pironkov_hybrid-task_2020,
	title = {Hybrid-task learning for robust automatic speech recognition},
	volume = {64},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S088523082030036X},
	doi = {10.1016/j.csl.2020.101103},
	language = {en},
	urldate = {2022-08-23},
	journal = {Computer Speech \& Language},
	author = {Pironkov, Gueorgui and Wood, Sean UN and Dupont, Stéphane},
	month = nov,
	year = {2020},
	pages = {101103},
}

@article{wang_wavenet_2020,
	title = {{WaveNet} {With} {Cross}-{Attention} for {Audiovisual} {Speech} {Recognition}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9197622/},
	doi = {10.1109/ACCESS.2020.3024218},
	urldate = {2022-08-23},
	journal = {IEEE Access},
	author = {Wang, Hui and Gao, Fei and Zhao, Yue and Wu, Licheng},
	year = {2020},
	pages = {169160--169168},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\FRECEY55\\Wang et al. - 2020 - WaveNet With Cross-Attention for Audiovisual Speec.pdf:application/pdf},
}

@inproceedings{sarfraz_huda_speech_2016,
	address = {Khatmandu, Nepal},
	title = {Speech {Corpus} {Development} for a {Speaker} {Independent} {Spontaneous} {Urdu} {Speech} {Recognition} {System}},
	booktitle = {Proceedings of the {O}-{COCOSDA}},
	author = {Sarfraz, Huda and Hussain, Sarmad and Bokhari, Riffat and Agha, Ali and Agha Ali Raza and Inamullah and Sarfaraz, Zahid and Parvez, Sophia and Mustafa, Asad and Javed, Iqra and Parveen, Raheela},
	month = mar,
	year = {2016},
}

@misc{mozilla_deep_nodate,
	title = {Deep {Speech} {Documentation}},
	url = {https://deepspeech.readthedocs.io/en/r0.9/?badge=latest},
	urldate = {2022-08-23},
	author = {Mozilla},
}

@inproceedings{wang_application_2015,
	address = {Beijing},
	title = {The {Application} of {Data} {Mining} {Technology} for the {Judgment} of {Poisoning} {Cases}},
	isbn = {978-1-4673-7211-4},
	url = {https://ieeexplore.ieee.org/document/7518465/},
	doi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.284},
	urldate = {2022-08-24},
	booktitle = {2015 {IEEE} 12th {Intl} {Conf} on {Ubiquitous} {Intelligence} and {Computing} and 2015 {IEEE} 12th {Intl} {Conf} on {Autonomic} and {Trusted} {Computing} and 2015 {IEEE} 15th {Intl} {Conf} on {Scalable} {Computing} and {Communications} and {Its} {Associated} {Workshops} ({UIC}-{ATC}-{ScalCom})},
	publisher = {IEEE},
	author = {Wang, Jiong and Zhang, Yunfeng and Wang, Fanglin and Gao, Bin},
	month = aug,
	year = {2015},
	pages = {1567--1571},
}

@article{zhao_data_2022,
	title = {Data {Poisoning} {Attacks} and {Defenses} in {Dynamic} {Crowdsourcing} with {Online} {Data} {Quality} {Learning}},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/9640529/},
	doi = {10.1109/TMC.2021.3133365},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Zhao, Yuxi and Gong, Xiaowen and Lin, Fuhong and Chen, Xu},
	year = {2022},
	pages = {1--1},
}

@inproceedings{hu_data_2020,
	title = {Data {Poisoning} on {Deep} {Learning} {Models}},
	doi = {10.1109/CSCI51800.2020.00111},
	abstract = {Deep learning is a form of artificial intelligence (AI) that has seen rapid development and deployment in computer software as a means to implementing AI functionality with greater efficiency and ease as compared to other alternative AI solutions, with usage seen in systems varying from search and recommendation engines to autonomous vehicles. With the demand for deep learning algorithms that can perform increasingly complex tasks in a shorter time frame growing at an exponential pace, the developments in the efficiency and productivity of algorithms has far outpaced that of the security of such algorithms, drawing concerns over the many unaddressed vulnerabilities that may be exploited to compromise the integrity of these software. This study investigated the ability of poisoning attacks, a form of attack targeting the vulnerability of deep learning training data, to compromise the integrity of a deep learning model's classificational functionality. Experimentation involved the processing of training data sets with varying deep learning models and the incremental introduction of poisoned data sets to view the efficacy of a poisoning attack under multiple circumstances and correlate such with aspects of the model's design conditions. Analysis of results showed evidence of a decrease of classificational ability correlating with an increase of poison percentage in the training data sets, but the scale of which the decrease occurred varied with the specified parameters in the model design. Based on this, it was concluded that poisoning can provide varying levels of damage to deep learning classificational ability depending on the parameters utilized in the model design, and methods to countermeasure such were proposed, such as increasing epoch count, implementing mechanisms bolstering model fit, and integrating input level filtration systems.},
	booktitle = {2020 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Hu, Charles and Hu, Yen-Hung Frank},
	month = dec,
	year = {2020},
	keywords = {Software, Computational modeling, Data models, artificial intelligence, machine learning, data poisoning, deep learning, Deep learning, Software algorithms, Toxicology, Training data},
	pages = {628--632},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\EU5RHS82\\9457922.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\2MNURGVH\\Hu and Hu - 2020 - Data Poisoning on Deep Learning Models.pdf:application/pdf},
}

@inproceedings{uprety_mitigating_2021,
	title = {Mitigating {Poisoning} {Attack} in {Federated} {Learning}},
	doi = {10.1109/SSCI50451.2021.9659839},
	abstract = {Adversarial machine learning (AML) has emerged as one of the significant research areas in machine learning (ML) because models we train lack robustness and trustworthiness. Federated learning (FL) trains models over distributed devices and model parameters are shared instead of actual data in a privacy-preserving manner. Unfortunately, FL is also vulnerable to attacks including parameter/data poisoning attacks. In this paper, we first analyze the impact of the data poisoning attack on this training method with a label-flipping attack. We propose a poisoning attack mitigation technique based on the reputation of nodes' involved in the training process. The reputation score for each client is calculated using the beta probability distribution method. This is the first work to show the removal of malicious nodes with the poisoned dataset from the training environment based on the calculated reputation score. The improvement in model performance after filtering malicious nodes is validated using the benchmark MNIST dataset. At the same time, our work contributes to preventing denial of service attacks by considering a blockchain-based server network. Our results hold for two different attack settings with different proportions of poisoned data samples.},
	booktitle = {2021 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Uprety, Aashma and Rawat, Danda B.},
	month = dec,
	year = {2021},
	keywords = {Computational modeling, Training, Data models, Collaborative work, Data poisoning attack, Data privacy, Distance learning, Filtering, reputation model, secure federated learning},
	pages = {01--07},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\PM3PPZV8\\Uprety and Rawat - 2021 - Mitigating Poisoning Attack in Federated Learning.pdf:application/pdf},
}

@inproceedings{seetharaman_influence_2022,
	title = {Influence {Based} {Defense} {Against} {Data} {Poisoning} {Attacks} in {Online} {Learning}},
	doi = {10.1109/COMSNETS53615.2022.9668557},
	abstract = {Data poisoning is a type of adversarial attack on training data where an attacker manipulates a fraction of data to degrade the performance of machine learning model. There are several known defensive mechanisms for handling offline attacks, however defensive measures for online learning, where data points arrive sequentially, have not garnered similar interest. In this work, we propose a defense mechanism to minimize the degradation caused by the poisoned training data on a learner's model in an online setup. Our proposed method utilizes an influence function which is a classic technique in robust statistics. Further, we supplement it with the existing data sanitization methods for filtering out some of the poisoned data points. We study the effectiveness of our defense mechanism on multiple datasets and across multiple attack strategies against an online learner.},
	booktitle = {2022 14th {International} {Conference} on {COMmunication} {Systems} \& {NETworkS} ({COMSNETS})},
	author = {Seetharaman, Sanjay and Malaviya, Shubham and Vasu, Rosni and Shukla, Manish and Lodha, Sachin},
	month = jan,
	year = {2022},
	note = {ISSN: 2155-2509},
	keywords = {Data models, Data integrity, Training data, Filtering, Adversarial Machine Learning, Data Poisoning, Degradation, Influence Function, Linear programming, Machine learning, Online Learning},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\QI6QPWYV\\Seetharaman et al. - 2022 - Influence Based Defense Against Data Poisoning Att.pdf:application/pdf},
}

@article{zhao_garbage_2021,
	title = {Garbage {In}, {Garbage} {Out}: {Poisoning} {Attacks} {Disguised} {With} {Plausible} {Mobility} in {Data} {Aggregation}},
	volume = {8},
	issn = {2327-4697, 2334-329X},
	shorttitle = {Garbage {In}, {Garbage} {Out}},
	url = {https://ieeexplore.ieee.org/document/9511094/},
	doi = {10.1109/TNSE.2021.3103919},
	number = {3},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Zhao, Ping and Jiang, Hongbo and Li, Jie and Xiao, Zhu and Liu, Daibo and Ren, Ju and Guo, Deke},
	month = jul,
	year = {2021},
	pages = {2679--2693},
}

@inproceedings{franci_influence-driven_2022,
	title = {Influence-{Driven} {Data} {Poisoning} in {Graph}-{Based} {Semi}-{Supervised} {Classifiers}},
	abstract = {Graph-based Semi-Supervised Learning (GSSL) is a practical solution to learn from a limited amount of labelled data together with a vast amount of unlabelled data. However, due to their reliance on the known labels to infer the unknown labels, these algorithms are sensitive to data quality. It is therefore essential to study the potential threats related to the labelled data, more specifically, label poisoning. In this paper, we propose a novel data poisoning method which efficiently approximates the result of label inference to identify the inputs which, if poisoned, would produce the highest number of incorrectly inferred labels. We extensively evaluate our approach on three classification problems under 24 different experimental settings each. Compared to the state of the art, our influence-driven attack produces an average increase of error rate 50\% higher, while being faster by multiple orders of magnitude. Moreover, our method can inform engineers of inputs that deserve investigation (relabelling them) before training the learning model. We show that relabelling one-third of the poisoned inputs (selected based on their influence) reduces the poisoning effect by 50\%. ACM Reference Format: Adriano Franci, Maxime Cordy, Martin Gubri, Mike Papadakis, and Yves Le Traon. 2022. Influence-Driven Data Poisoning in Graph-Based Semi-Supervised Classifiers. In 1st Conference on AI Engineering - Software Engineering for AI (CAIN’22), May 16–24, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3522664.3528606},
	booktitle = {2022 {IEEE}/{ACM} 1st {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Franci, Adriano and Cordy, Maxime and Gubri, Martin and Papadakis, Mike and Traon, Yves Le},
	month = may,
	year = {2022},
	keywords = {Training, Measurement, Data integrity, data poisoning, Machine learning, Approximation algorithms, Error analysis, Inference algorithms, semi-supervised learning, Semisupervised learning},
	pages = {77--87},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\J9JGMGKF\\Franci et al. - 2022 - Influence-Driven Data Poisoning in Graph-Based Sem.pdf:application/pdf},
}

@article{zhang_poisongan_2021,
	title = {{PoisonGAN}: {Generative} {Poisoning} {Attacks} {Against} {Federated} {Learning} in {Edge} {Computing} {Systems}},
	volume = {8},
	issn = {2327-4662, 2372-2541},
	shorttitle = {{PoisonGAN}},
	url = {https://ieeexplore.ieee.org/document/9194010/},
	doi = {10.1109/JIOT.2020.3023126},
	number = {5},
	urldate = {2022-08-24},
	journal = {IEEE Internet of Things Journal},
	author = {Zhang, Jiale and Chen, Bing and Cheng, Xiang and Binh, Huynh Thi Thanh and Yu, Shui},
	month = mar,
	year = {2021},
	pages = {3310--3322},
}

@inproceedings{doku_mitigating_2021,
	address = {Las Vegas, NV, USA},
	title = {Mitigating {Data} {Poisoning} {Attacks} {On} a {Federated} {Learning}-{Edge} {Computing} {Network}},
	isbn = {978-1-72819-794-4},
	url = {https://ieeexplore.ieee.org/document/9369581/},
	doi = {10.1109/CCNC49032.2021.9369581},
	urldate = {2022-08-24},
	booktitle = {2021 {IEEE} 18th {Annual} {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	publisher = {IEEE},
	author = {Doku, Ronald and Rawat, Danda B.},
	month = jan,
	year = {2021},
	pages = {1--6},
}

@article{wen_great_2021,
	title = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}: {Efficient} {Poisoning} {Attacks} and {Defenses} for {Linear} {Regression} {Models}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {With {Great} {Dispersion} {Comes} {Greater} {Resilience}},
	url = {https://ieeexplore.ieee.org/document/9448089/},
	doi = {10.1109/TIFS.2021.3087332},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Wen, Jialin and Zhao, Benjamin Zi Hao and Xue, Minhui and Oprea, Alina and Qian, Haifeng},
	year = {2021},
	pages = {3709--3723},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\PWEN89AG\\Wen et al. - 2021 - With Great Dispersion Comes Greater Resilience Ef.pdf:application/pdf},
}

@article{chen_-pois_2021,
	title = {De-{Pois}: {An} {Attack}-{Agnostic} {Defense} against {Data} {Poisoning} {Attacks}},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	shorttitle = {De-{Pois}},
	url = {https://ieeexplore.ieee.org/document/9431105/},
	doi = {10.1109/TIFS.2021.3080522},
	urldate = {2022-08-24},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Chen, Jian and Zhang, Xuxin and Zhang, Rui and Wang, Chen and Liu, Ling},
	year = {2021},
	pages = {3412--3425},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\AK6SW628\\Chen et al. - 2021 - De-Pois An Attack-Agnostic Defense against Data P.pdf:application/pdf},
}

@inproceedings{kwon_selective_2019,
	address = {Sardinia, Italy},
	title = {Selective {Poisoning} {Attack} on {Deep} {Neural} {Network} to {Induce} {Fine}-{Grained} {Recognition} {Error}},
	isbn = {978-1-72811-488-0},
	url = {https://ieeexplore.ieee.org/document/8791700/},
	doi = {10.1109/AIKE.2019.00033},
	urldate = {2022-08-24},
	booktitle = {2019 {IEEE} {Second} {International} {Conference} on {Artificial} {Intelligence} and {Knowledge} {Engineering} ({AIKE})},
	publisher = {IEEE},
	author = {Kwon, Hyun and Yoon, Hyunsoo and Park, Ki-Woong},
	month = jun,
	year = {2019},
	pages = {136--139},
}

@inproceedings{kontopoulos_countering_2018,
	address = {Athens},
	title = {Countering {Real}-{Time} {Stream} {Poisoning}: {An} {Architecture} for {Detecting} {Vessel} {Spoofing} in {Streams} of {AIS} {Data}},
	isbn = {978-1-5386-7518-2},
	shorttitle = {Countering {Real}-{Time} {Stream} {Poisoning}},
	url = {https://ieeexplore.ieee.org/document/8512006/},
	doi = {10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00139},
	urldate = {2022-08-24},
	booktitle = {2018 {IEEE} 16th {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, 16th {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, 4th {Intl} {Conf} on {Big} {Data} {Intelligence} and {Computing} and {Cyber} {Science} and {Technology} {Congress}({DASC}/{PiCom}/{DataCom}/{CyberSciTech})},
	publisher = {IEEE},
	author = {Kontopoulos, Ioannis and Spiliopoulos, Giannis and Zissis, Dimitrios and Chatzikokolakis, Konstantinos and Artikis, Alexander},
	month = aug,
	year = {2018},
	pages = {981--986},
}

@misc{noauthor_adoption_nodate,
	title = {Adoption of {AI} advances, but foundational barriers remain {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/featured-insights/artificial-intelligence/ai-adoption-advances-but-foundational-barriers-remain},
	urldate = {2022-08-24},
	file = {Adoption of AI advances, but foundational barriers remain | McKinsey:C\:\\Users\\DELL\\Zotero\\storage\\7BXXNTS6\\ai-adoption-advances-but-foundational-barriers-remain.html:text/html},
}

@misc{noauthor_exclusive_nodate,
	title = {Exclusive: {What} is data poisoning and why should we be concerned? - {International} {Security} {Journal} ({ISJ})},
	shorttitle = {Exclusive},
	url = {https://internationalsecurityjournal.com/what-is-data-poisoning/},
	abstract = {Machine learning could be one of the most disruptive technologies the world has seen in decades. Virtually every industry can benefit from these artificial},
	language = {en-GB},
	urldate = {2022-08-24},
	note = {Section: AI \& Deep Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VYNPR2KK\\what-is-data-poisoning.html:text/html},
}

@misc{vincent_twitter_2016,
	title = {Twitter taught {Microsoft}’s friendly {AI} chatbot to be a racist asshole in less than a day},
	url = {https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist},
	abstract = {It took less than 24 hours for Twitter to corrupt an innocent AI chatbot. Yesterday, Microsoft unveiled Tay — a Twitter bot that the company described as an experiment in "conversational...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Vincent, James},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5ZCPBDM9\\tay-microsoft-chatbot-racist.html:text/html},
}

@misc{kastrenakes_microsoft_2016,
	title = {Microsoft made a chatbot that tweets like a teen},
	url = {https://www.theverge.com/2016/3/23/11290200/tay-ai-chatbot-released-microsoft},
	abstract = {Microsoft is trying to create AI that can pass for a teen. Its research team launched a chatbot this morning called Tay, which is meant to test and improve Microsoft's understanding of...},
	language = {en},
	urldate = {2022-08-24},
	journal = {The Verge},
	author = {Kastrenakes, Jacob},
	month = mar,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SJ6QZ3R3\\tay-ai-chatbot-released-microsoft.html:text/html},
}

@article{steinhardt_certified_2017,
	title = {Certified {Defenses} for {Data} {Poisoning} {Attacks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03691},
	doi = {10.48550/ARXIV.1706.03691},
	abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.},
	urldate = {2022-08-24},
	author = {Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
	annote = {Other
Appeared at NIPS 2017},
}

@article{newman_ai_nodate,
	title = {{AI} {Can} {Help} {Cybersecurity}—{If} {It} {Can} {Fight} {Through} the {Hype}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/ai-machine-learning-cybersecurity/},
	abstract = {There are a ton of claims around AI and cybersecurity that don't quite add up. Here's what's really going on.},
	language = {en-US},
	urldate = {2022-08-24},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {artificial intelligence, machine learning, ai, cybersecurity},
}

@misc{ilmoi_evasion_2019,
	title = {Evasion attacks on {Machine} {Learning} (or “{Adversarial} {Examples}”)},
	url = {https://towardsdatascience.com/evasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1},
	abstract = {Your ML model is easier to fool than you think},
	language = {en},
	urldate = {2022-08-24},
	journal = {Medium},
	author = {ilmoi},
	month = jul,
	year = {2019},
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	copyright = {Creative Commons Attribution 3.0 Unported},
	url = {https://arxiv.org/abs/1312.6199},
	doi = {10.48550/ARXIV.1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2022-08-24},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV)},
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1412.6572},
	doi = {10.48550/ARXIV.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2022-08-24},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{bursztein_attacks_nodate,
	title = {Attacks against machine learning — an overview},
	url = {https://elie.net/blog/ai/attacks-against-machine-learning-an-overview/},
	abstract = {This blog post surveys the attacks techniques that target AI (Artificial Intelligence) systems and how to protect against them.},
	language = {en},
	urldate = {2022-08-24},
	journal = {Elie Bursztein's site},
	author = {Bursztein, Elie},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P5FGBPU5\\attacks-against-machine-learning-an-overview.html:text/html},
}

@article{goh_comprehensive_2015,
	title = {Comprehensive {Literature} {Review} on {Machine} {Learning} {Structures} for {Web} {Spam} {Classification}},
	volume = {70},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915032330},
	doi = {10.1016/j.procs.2015.10.069},
	language = {en},
	urldate = {2022-08-24},
	journal = {Procedia Computer Science},
	author = {Goh, Kwang Leng and Singh, Ashutosh Kumar},
	year = {2015},
	pages = {434--441},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\ZZY6ZDLK\\Goh and Singh - 2015 - Comprehensive Literature Review on Machine Learnin.pdf:application/pdf},
}

@article{jo_measuring_2017,
	title = {Measuring the tendency of {CNNs} to {Learn} {Surface} {Statistical} {Regularities}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.11561},
	doi = {10.48550/ARXIV.1711.11561},
	abstract = {Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset. We are concerned with the following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities. For the SVHN and CIFAR-10 datasets, we present two Fourier filtered variants: a low frequency variant and a randomly filtered variant. Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects. Our main finding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap across the various test sets. Moreover, we observe that significantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap. Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts.},
	urldate = {2022-08-24},
	author = {Jo, Jason and Bengio, Yoshua},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	annote = {Other
Submitted},
}

@inproceedings{florian_tramer_stealing_2016,
	address = {Austin, Texas, USA},
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	isbn = {978-1-931971-32-4},
	url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer},
	urldate = {2022-08-24},
	publisher = {USENIX Association},
	author = {Florian Tramer and Fan Zhang and Ari Juels and Michael K. Reiter and Thomas Ristenpart},
	month = aug,
	year = {2016},
	pages = {601--618},
	file = {Stealing Machine Learning Models via Prediction APIs | USENIX:C\:\\Users\\DELL\\Zotero\\storage\\KYVFLJL7\\tramer.html:text/html},
}

@misc{papernot_semi-supervised_2017,
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	url = {http://arxiv.org/abs/1610.05755},
	abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
	month = mar,
	year = {2017},
	note = {arXiv:1610.05755 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted to ICLR 17 as an oral},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\B2VJHRWF\\Papernot et al. - 2017 - Semi-supervised Knowledge Transfer for Deep Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\JK48PTLF\\1610.html:text/html},
}

@misc{papernot_scalable_2018,
	title = {Scalable {Private} {Learning} with {PATE}},
	url = {http://arxiv.org/abs/1802.08908},
	abstract = {The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a "student" model the knowledge of an ensemble of "teacher" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy (\${\textbackslash}varepsilon\$ {\textless} 1.0).},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Úlfar},
	month = feb,
	year = {2018},
	note = {arXiv:1802.08908 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Published as a conference paper at ICLR 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Y9TDCBWP\\Papernot et al. - 2018 - Scalable Private Learning with PATE.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\L8FAK7HV\\1802.html:text/html},
}

@article{wang_poisoning_2022,
	title = {Poisoning attacks and countermeasures in intelligent networks: {Status} quo and prospects},
	volume = {8},
	issn = {23528648},
	shorttitle = {Poisoning attacks and countermeasures in intelligent networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S235286482100050X},
	doi = {10.1016/j.dcan.2021.07.009},
	language = {en},
	number = {2},
	urldate = {2022-08-24},
	journal = {Digital Communications and Networks},
	author = {Wang, Chen and Chen, Jian and Yang, Yang and Ma, Xiaoqiang and Liu, Jiangchuan},
	month = apr,
	year = {2022},
	pages = {225--234},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5924FJWC\\Wang et al. - 2022 - Poisoning attacks and countermeasures in intellige.pdf:application/pdf},
}

@misc{noauthor_google_nodate,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/emergency-response/},
	urldate = {2022-08-24},
	file = {Google - Site Reliability Engineering:C\:\\Users\\DELL\\Zotero\\storage\\27RWN7KZ\\emergency-response.html:text/html},
}

@misc{noauthor_google_nodate-1,
	title = {Google - {Site} {Reliability} {Engineering}},
	url = {https://sre.google/sre-book/managing-incidents/},
	urldate = {2022-08-24},
}

@misc{gaudesi_channelaugment_2021,
	title = {{ChannelAugment}: {Improving} generalization of multi-channel {ASR} by training with input channel randomization},
	shorttitle = {{ChannelAugment}},
	url = {http://arxiv.org/abs/2109.11225},
	abstract = {End-to-end (E2E) multi-channel ASR systems show state-of-the-art performance in far-field ASR tasks by joint training of a multi-channel front-end along with the ASR model. The main limitation of such systems is that they are usually trained with data from a fixed array geometry, which can lead to degradation in accuracy when a different array is used in testing. This makes it challenging to deploy these systems in practice, as it is costly to retrain and deploy different models for various array configurations. To address this, we present a simple and effective data augmentation technique, which is based on randomly dropping channels in the multi-channel audio input during training, in order to improve the robustness to various array configurations at test time. We call this technique ChannelAugment, in contrast to SpecAugment (SA) which drops time and/or frequency components of a single channel input audio. We apply ChannelAugment to the Spatial Filtering (SF) and Minimum Variance Distortionless Response (MVDR) neural beamforming approaches. For SF, we observe 10.6\% WER improvement across various array configurations employing different numbers of microphones. For MVDR, we achieve a 74\% reduction in training time without causing degradation of recognition accuracy.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Gaudesi, Marco and Weninger, Felix and Sharma, Dushyant and Zhan, Puming},
	month = sep,
	year = {2021},
	note = {arXiv:2109.11225 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	annote = {Comment: To appear in ASRU 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\D2RMIMH8\\Gaudesi et al. - 2021 - ChannelAugment Improving generalization of multi-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4F2SMWT4\\2109.html:text/html},
}

@article{lin_ml_2021,
	title = {{ML} {Attack} {Models}: {Adversarial} {Attacks} and {Data} {Poisoning} {Attacks}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{ML} {Attack} {Models}},
	url = {https://arxiv.org/abs/2112.02797},
	doi = {10.48550/ARXIV.2112.02797},
	abstract = {Many state-of-the-art ML models have outperformed humans in various tasks such as image classification. With such outstanding performance, ML models are widely used today. However, the existence of adversarial attacks and data poisoning attacks really questions the robustness of ML models. For instance, Engstrom et al. demonstrated that state-of-the-art image classifiers could be easily fooled by a small rotation on an arbitrary image. As ML systems are being increasingly integrated into safety and security-sensitive applications, adversarial attacks and data poisoning attacks pose a considerable threat. This chapter focuses on the two broad and important areas of ML security: adversarial attacks and data poisoning attacks.},
	urldate = {2022-08-24},
	author = {Lin, Jing and Dang, Long and Rahouti, Mohamed and Xiong, Kaiqi},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Cryptography and Security (cs.CR)},
}

@misc{noauthor_why_nodate,
	title = {Why does kaldi require mono channel audio for training instead of stereo or surround?},
	url = {https://groups.google.com/g/kaldi-help/c/92-jEzqyNb4},
	urldate = {2022-08-24},
	file = {Why does kaldi require mono channel audio for training instead of stereo or surround?:C\:\\Users\\DELL\\Zotero\\storage\\8P3Y6TTD\\92-jEzqyNb4.html:text/html},
}

@misc{noauthor_kaldi_nodate,
	title = {Kaldi: {Data} preparation},
	url = {https://kaldi-asr.org/doc/data_prep.html},
	urldate = {2022-08-24},
	file = {Kaldi\: Data preparation:C\:\\Users\\DELL\\Zotero\\storage\\QLKDHK6G\\data_prep.html:text/html},
}

@book{international_speech_communication_association_speech_2016-1,
	address = {Red Hook, NY},
	title = {Speech beyond speech towards a better understanding of the most important biosignal: 16th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({INTERSPEECH} 2015): {Dresden}, {Germany}, 6-10 {September} 2015. {Volume} 5},
	isbn = {978-1-5108-1790-6},
	shorttitle = {Speech beyond speech towards a better understanding of the most important biosignal},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {International Speech Communication Association},
	year = {2016},
	note = {Meeting Name: International Speech Communication Association},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:C\:\\Users\\DELL\\Zotero\\storage\\22CXWDL9\\International Speech Communication Association - 2016 - Speech beyond speech towards a better understandin.pdf:application/pdf},
}

@article{kocon_offensive_2021,
	title = {Offensive, aggressive, and hate speech analysis: {From} data-centric to human-centered approach},
	volume = {58},
	issn = {03064573},
	shorttitle = {Offensive, aggressive, and hate speech analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457321001333},
	doi = {10.1016/j.ipm.2021.102643},
	language = {en},
	number = {5},
	urldate = {2022-08-26},
	journal = {Information Processing \& Management},
	author = {Kocoń, Jan and Figas, Alicja and Gruza, Marcin and Puchalska, Daria and Kajdanowicz, Tomasz and Kazienko, Przemysław},
	month = sep,
	year = {2021},
	pages = {102643},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\NSSERT5Y\\Kocoń et al. - 2021 - Offensive, aggressive, and hate speech analysis F.pdf:application/pdf},
}

@inproceedings{ghahremani_investigation_2017,
	address = {Okinawa},
	title = {Investigation of transfer learning for {ASR} using {LF}-{MMI} trained neural networks},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268947/},
	doi = {10.1109/ASRU.2017.8268947},
	urldate = {2022-09-04},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Ghahremani, Pegah and Manohar, Vimal and Hadian, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = dec,
	year = {2017},
	pages = {279--286},
}

@inproceedings{wallington_learning_2021,
	title = {On the {Learning} {Dynamics} of {Semi}-{Supervised} {Training} for {ASR}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/wallington21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1777},
	language = {en},
	urldate = {2022-09-04},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Wallington, Electra and Kershenbaum, Benji and Klejch, Ondřej and Bell, Peter},
	month = aug,
	year = {2021},
	pages = {716--720},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\F2TLM7QF\\Wallington et al. - 2021 - On the Learning Dynamics of Semi-Supervised Traini.pdf:application/pdf},
}

@inproceedings{sarkar_novel_2014,
	title = {A novel boosting algorithm for improved i-vector based speaker verification in noisy environments},
	booktitle = {Proceedings of the {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {INTERSPEECH}},
	author = {Sarkar, Sourjya and Rao, K.},
	month = sep,
	year = {2014},
}

@inproceedings{sarkar_study_2012,
	title = {Study of the {Effect} of {I}-vector {Modeling} on {Short} and {Mismatch} {Utterance} {Duration} for {Speaker} {Verification}},
	booktitle = {{INTERSPEECH}},
	author = {Sarkar, Achintya Kumar and Matrouf, Driss and Bousquet, Pierre-Michel and Bonastre, Jean-François},
	year = {2012},
}

@misc{zhang_uberi_speechrecognition_nodate-1,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-09-18},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4MV996BJ\\SpeechRecognition.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What {Is} {Automatic} {Speech} {Recognition}? - {Alexa} {Skills} {Kit} {Official} {Site}},
	shorttitle = {What {Is} {Automatic} {Speech} {Recognition}?},
	url = {https://developer.amazon.com/en-US/alexa/alexa-skills-kit/asr.html},
	abstract = {Automatic speech recognition (ASR) is technology that converts spoken words into text. Explore the topic of ASR and learn about building for voice.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Amazon (Alexa)},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\EA9BYMAB\\asr.html:text/html},
}

@misc{noauthor_speech--text_nodate,
	title = {Speech-to-{Text}: {Automatic} {Speech} {Recognition}},
	shorttitle = {Speech-to-{Text}},
	url = {https://cloud.google.com/speech-to-text},
	abstract = {Accurately convert voice to text in over 125 languages and variants by applying Google’s powerful machine learning models with an easy-to-use API.},
	language = {en},
	urldate = {2022-09-19},
	journal = {Google Cloud},
}

@misc{noauthor_siri_nodate,
	title = {Siri},
	url = {https://www.apple.com/siri/},
	abstract = {Siri is an easy way to make calls, send texts, use apps, and get things done with just your voice. And Siri is the most private intelligent assistant.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {Apple},
}

@misc{noauthor_cortana_nodate,
	title = {Cortana - {Your} personal productivity assistant},
	url = {https://www.microsoft.com/en-us/cortana},
	abstract = {Cortana helps you achieve more with less effort. Your personal productivity assistant helps you stay on top of what matters, follow through, and do your best work.},
	language = {en-us},
	urldate = {2022-09-19},
	journal = {Cortana - Your personal productivity assistant},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\APLZ98WM\\cortana.html:text/html},
}

@misc{noauthor_jarvis_nodate,
	title = {Jarvis {\textbar} {NVIDIA} {NGC}},
	url = {https://catalog.ngc.nvidia.com/orgs/nvidia/collections/jarvis},
	abstract = {NVIDIA Jarvis is a framework for production-grade conversational AI inference. The Jarvis Collection on NGC includes all the resources required for getting started with Jarvis.},
	language = {en},
	urldate = {2022-09-19},
	journal = {NVIDIA NGC Catalog},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8U4FAMTW\\jarvis.html:text/html},
}

@misc{noauthor_sox_nodate,
	title = {{SoX} - {Sound} {eXchange} {\textbar} {HomePage}},
	url = {http://sox.sourceforge.net/},
	urldate = {2022-09-19},
	file = {SoX - Sound eXchange | HomePage:C\:\\Users\\DELL\\Zotero\\storage\\Y5XJDGHX\\sox.sourceforge.net.html:text/html},
}

@misc{raj_note_nodate,
	title = {A note on {MFCCs} and delta features},
	url = {https://desh2608.github.io/2019-07-26-delta-feats/},
	abstract = {What are MFCCs and how are they computed? Feature extraction is the first step in any automatic speech recognition (ASR) pipeline. The objective is to compute features from speech waveforms which contain relevant information about the linguistic content of the speech, and ignore information about the background noise, emotions, etc....},
	language = {en},
	urldate = {2022-09-19},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\R4WGBL8Q\\2019-07-26-delta-feats.html:text/html},
}

@inproceedings{menne_analysis_2019,
	title = {Analysis of {Deep} {Clustering} as {Preprocessing} for {Automatic} {Speech} {Recognition} of {Sparsely} {Overlapping} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html},
	doi = {10.21437/Interspeech.2019-1728},
	language = {en},
	urldate = {2022-09-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Menne, Tobias and Sklyar, Ilya and Schlüter, Ralf and Ney, Hermann},
	month = sep,
	year = {2019},
	pages = {2638--2642},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\5X2I92JR\\Menne et al. - 2019 - Analysis of Deep Clustering as Preprocessing for A.pdf:application/pdf},
}

@article{li_tenet_2019,
	title = {{TEnet}: target speaker extraction network with accumulated speaker embedding for automatic speech recognition},
	volume = {55},
	issn = {0013-5194, 1350-911X},
	shorttitle = {{TEnet}},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/el.2019.1228},
	doi = {10.1049/el.2019.1228},
	language = {en},
	number = {14},
	urldate = {2022-09-19},
	journal = {Electronics Letters},
	author = {Li, Wenjie and Zhang, Pengyuan and Yan, Yonghong},
	month = jul,
	year = {2019},
	pages = {816--819},
}

@article{van_wyk_multivaluedness_2021,
	title = {Multivaluedness in {Networks}: {Shannon}’s {Noisy}-{Channel} {Coding} {Theorem}},
	volume = {68},
	issn = {1549-7747, 1558-3791},
	shorttitle = {Multivaluedness in {Networks}},
	url = {https://ieeexplore.ieee.org/document/9410598/},
	doi = {10.1109/TCSII.2021.3074925},
	number = {10},
	urldate = {2022-09-19},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {van Wyk, Michael Antonie and Ping, Li and Chen, Guanrong},
	month = oct,
	year = {2021},
	pages = {3234--3235},
}

@inproceedings{nautsch_gdpr_2019-2,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-09-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\GPBKHSGG\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@inproceedings{mohamed_understanding_2012,
	address = {Kyoto, Japan},
	title = {Understanding how {Deep} {Belief} {Networks} perform acoustic modelling},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	url = {http://ieeexplore.ieee.org/document/6288863/},
	doi = {10.1109/ICASSP.2012.6288863},
	urldate = {2022-09-19},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Mohamed, Abdel-rahman and Hinton, Geoffrey and Penn, Gerald},
	month = mar,
	year = {2012},
	pages = {4273--4276},
}

@misc{shrawankar_adverse_2013,
	title = {Adverse {Conditions} and {ASR} {Techniques} for {Robust} {Speech} {User} {Interface}},
	url = {http://arxiv.org/abs/1303.5515},
	abstract = {The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.},
	urldate = {2022-09-19},
	publisher = {arXiv},
	author = {Shrawankar, Urmila and Thakare, V. M.},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	annote = {Comment: 10 pages 2 Tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Y9HZ2V8C\\Shrawankar and Thakare - 2013 - Adverse Conditions and ASR Techniques for Robust S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AWG8ADWP\\1303.html:text/html},
}

@article{benesty_springer_2009,
	title = {Springer {Handbook} of {Speech} {Processing}},
	volume = {126},
	issn = {00014966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/126/4/10.1121/1.3203918},
	doi = {10.1121/1.3203918},
	language = {en},
	number = {4},
	urldate = {2022-09-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Benesty, Jacob and Sondhi, Mohan M. and Huang, Yiteng and Greenberg, Steven},
	year = {2009},
	pages = {2130},
}

@misc{marvel_jrvis_nodate,
	title = {J.{A}.{R}.{V}.{I}.{S}.},
	url = {https://ironman.fandom.com/wiki/J.A.R.V.I.S.},
	abstract = {Just A Rather Very Intelligent System (J.A.R.V.I.S.) was originally Tony Stark's natural-language user interface computer system, named after Edwin Jarvis, the butler who worked for Howard Stark. Over time, he was upgraded into an artificially intelligent system, tasked with running business for Stark Industries as well as security for Tony Stark's Mansion and Stark Tower. After creating the Mark II armor, Stark uploaded J.A.R.V.I.S. into all of the Iron Man Armors, as well as allowing him to in},
	language = {en},
	urldate = {2022-09-25},
	journal = {Iron Man Wiki},
	author = {Marvel, Fandom},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2AT6NTE6\\J.A.R.V.I.S..html:text/html},
}

@misc{noauthor_linux_nodate,
	title = {Linux},
	url = {https://www.audacityteam.org/download/linux/},
	abstract = {Thank you for downloading Audacity

Your download will start in 5 seconds. 
  Problems with the download? Please use this direct link


AppImage
Audacity 3.2.0 is available as an AppImage. The AppImage should run on most modern Linux distributions. To run AppImage:

Left click the link below.
Make t},
	language = {en-US},
	urldate = {2022-09-27},
	journal = {Audacity ®},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\TZSQDL3M\\linux.html:text/html},
}

@article{hussein_arabic_2022,
	title = {Arabic speech recognition by end-to-end, modular systems and human},
	volume = {71},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230821000760},
	doi = {10.1016/j.csl.2021.101272},
	language = {en},
	urldate = {2022-10-02},
	journal = {Computer Speech \& Language},
	author = {Hussein, Amir and Watanabe, Shinji and Ali, Ahmed},
	month = jan,
	year = {2022},
	pages = {101272},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\WJXMYF47\\Hussein et al. - 2022 - Arabic speech recognition by end-to-end, modular s.pdf:application/pdf},
}

@article{dahl_context-dependent_2012,
	title = {Context-{Dependent} {Pre}-{Trained} {Deep} {Neural} {Networks} for {Large}-{Vocabulary} {Speech} {Recognition}},
	volume = {20},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5740583/},
	doi = {10.1109/TASL.2011.2134090},
	number = {1},
	urldate = {2022-10-02},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Dahl, G. E. and {Dong Yu} and {Li Deng} and Acero, A.},
	month = jan,
	year = {2012},
	pages = {30--42},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\CT4BGLDU\\Dahl et al. - 2012 - Context-Dependent Pre-Trained Deep Neural Networks.pdf:application/pdf},
}

@article{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1303.5778},
	doi = {10.48550/ARXIV.1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates {\textbackslash}emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	urldate = {2022-10-02},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	year = {2013},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},
	annote = {Other
To appear in ICASSP 2013},
}

@article{hifny_unified_2015,
	title = {Unified {Acoustic} {Modeling} using {Deep} {Conditional} {Random} {Fields}},
	issn = {20547390},
	url = {http://scholarpublishing.org/index.php/TMLAI/article/view/1124},
	doi = {10.14738/tmlai.32.1124},
	urldate = {2022-10-02},
	journal = {Transactions on Machine Learning and Artificial Intelligence},
	author = {Hifny, Yasser},
	month = apr,
	year = {2015},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\V8U7GBTW\\Hifny - 2015 - Unified Acoustic Modeling using Deep Conditional R.pdf:application/pdf},
}

@inproceedings{povey_purely_2016,
	title = {Purely {Sequence}-{Trained} {Neural} {Networks} for {ASR} {Based} on {Lattice}-{Free} {MMI}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/povey16_interspeech.html},
	doi = {10.21437/Interspeech.2016-595},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Povey, Daniel and Peddinti, Vijayaditya and Galvez, Daniel and Ghahremani, Pegah and Manohar, Vimal and Na, Xingyu and Wang, Yiming and Khudanpur, Sanjeev},
	month = sep,
	year = {2016},
	pages = {2751--2755},
}

@inproceedings{vijayaditya_time_2015,
	title = {A time delay neural network architecture for efficient modeling of long temporal contexts},
	url = {https://www.semanticscholar.org/paper/A-time-delay-neural-network-architecture-for-of-Peddinti-Povey/3a79ac688f2558b2d9693e434f010e041eba0fae},
	author = {Vijayaditya and Peddinti and Sanjeev Khudanpur and Daniel Povey},
	year = {2015},
}

@article{ali_speech_2017,
	title = {Speech {Recognition} {Challenge} in the {Wild}: {Arabic} {MGB}-3},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Speech {Recognition} {Challenge} in the {Wild}},
	url = {https://arxiv.org/abs/1709.07276},
	doi = {10.48550/ARXIV.1709.07276},
	abstract = {This paper describes the Arabic MGB-3 Challenge - Arabic Speech Recognition in the Wild. Unlike last year's Arabic MGB-2 Challenge, for which the recognition task was based on more than 1,200 hours broadcast TV news recordings from Aljazeera Arabic TV programs, MGB-3 emphasises dialectal Arabic using a multi-genre collection of Egyptian YouTube videos. Seven genres were used for the data collection: comedy, cooking, family/kids, fashion, drama, sports, and science (TEDx). A total of 16 hours of videos, split evenly across the different genres, were divided into adaptation, development and evaluation data sets. The Arabic MGB-Challenge comprised two tasks: A) Speech transcription, evaluated on the MGB-3 test set, along with the 10 hour MGB-2 test set to report progress on the MGB-2 evaluation; B) Arabic dialect identification, introduced this year in order to distinguish between four major Arabic dialects - Egyptian, Levantine, North African, Gulf, as well as Modern Standard Arabic. Two hours of audio per dialect were released for development and a further two hours were used for evaluation. For dialect identification, both lexical features and i-vector bottleneck features were shared with participants in addition to the raw audio recordings. Overall, thirteen teams submitted ten systems to the challenge. We outline the approaches adopted in each system, and summarise the evaluation results.},
	urldate = {2022-10-02},
	author = {Ali, Ahmed and Vogel, Stephan and Renals, Steve},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{khurana_qcri_2016,
	address = {San Diego, CA},
	title = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition: {MGB}-2 challenge},
	isbn = {978-1-5090-4903-5},
	shorttitle = {{QCRI} advanced transcription system ({QATS}) for the {Arabic} {Multi}-{Dialect} {Broadcast} media recognition},
	url = {http://ieeexplore.ieee.org/document/7846279/},
	doi = {10.1109/SLT.2016.7846279},
	urldate = {2022-10-02},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Khurana, Sameer and Ali, Ahmed},
	month = dec,
	year = {2016},
	pages = {292--298},
}

@inproceedings{smit_aalto_2017,
	address = {Okinawa},
	title = {Aalto system for the 2017 {Arabic} multi-genre broadcast challenge},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268955/},
	doi = {10.1109/ASRU.2017.8268955},
	urldate = {2022-10-02},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Smit, Peter and Gangireddy, Siva Reddy and Enarvi, Seppo and Virpioja, Sami and Kurimo, Mikko},
	month = dec,
	year = {2017},
	pages = {338--345},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\YUJC9CV7\\Smit et al. - 2017 - Aalto system for the 2017 Arabic multi-genre broad.pdf:application/pdf},
}

@inproceedings{snyder_speaker_2019,
	address = {Brighton, United Kingdom},
	title = {Speaker {Recognition} for {Multi}-speaker {Conversations} {Using} {X}-vectors},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683760/},
	doi = {10.1109/ICASSP.2019.8683760},
	urldate = {2022-10-02},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and McCree, Alan and Povey, Daniel and Khudanpur, Sanjeev},
	month = may,
	year = {2019},
	pages = {5796--5800},
}

@article{dutta_performance_2021,
	title = {Performance analysis of {ASR} system in hybrid {DNN}-{HMM} framework using a {PWL} euclidean activation function},
	volume = {15},
	issn = {2095-2228, 2095-2236},
	url = {https://link.springer.com/10.1007/s11704-020-9419-z},
	doi = {10.1007/s11704-020-9419-z},
	language = {en},
	number = {4},
	urldate = {2022-10-02},
	journal = {Frontiers of Computer Science},
	author = {Dutta, Anirban and Ashishkumar, Gudmalwar and Rao, Ch V. Rama},
	month = aug,
	year = {2021},
	pages = {154705},
}

@inproceedings{georgescu_kaldi-based_2019,
	address = {Timisoara, Romania},
	title = {Kaldi-based {DNN} {Architectures} for {Speech} {Recognition} in {Romanian}},
	isbn = {978-1-72810-984-8},
	url = {https://ieeexplore.ieee.org/document/8906555/},
	doi = {10.1109/SPED.2019.8906555},
	urldate = {2022-10-04},
	booktitle = {2019 {International} {Conference} on {Speech} {Technology} and {Human}-{Computer} {Dialogue} ({SpeD})},
	publisher = {IEEE},
	author = {Georgescu, Alexandru-Lucian and Cucu, Horia and Burileanu, Corneliu},
	month = oct,
	year = {2019},
	pages = {1--6},
}

@incollection{ekstein_cnn-tdnn-based_2021,
	address = {Cham},
	title = {{CNN}-{TDNN}-{Based} {Architecture} for {Speech} {Recognition} {Using} {Grapheme} {Models} in {Bilingual} {Czech}-{Slovak} {Task}},
	volume = {12848},
	isbn = {978-3-030-83526-2 978-3-030-83527-9},
	url = {https://link.springer.com/10.1007/978-3-030-83527-9_45},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {Text, {Speech}, and {Dialogue}},
	publisher = {Springer International Publishing},
	author = {Psutka, Josef V. and Švec, Jan and Pražák, Aleš},
	editor = {Ekštein, Kamil and Pártl, František and Konopík, Miloslav},
	year = {2021},
	doi = {10.1007/978-3-030-83527-9_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {523--533},
}

@misc{noauthor_lattice_nodate,
	title = {On lattice free {MMI} and {Chain} models in {Kaldi}},
	url = {https://desh2608.github.io/2019-05-21-chain/},
	urldate = {2022-10-04},
	file = {On lattice free MMI and Chain models in Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\WVI67A8B\\2019-05-21-chain.html:text/html},
}

@misc{raj_experiments_nodate,
	title = {Experiments with {Subword} {Modeling}},
	url = {https://desh2608.github.io/2018-11-22-subword-segmentation/},
	abstract = {Think about tasks such as machine translation (MT), automatic speech recognition (ASR), or handwriting recognition (HWR). While these appear very distinct, on abstraction they share the same pipeline wherein given an input signal, we are required to predict some text. The difference only lies in the form of the input...},
	language = {en},
	urldate = {2022-10-04},
	author = {Raj, Desh},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ID8BRFHZ\\2018-11-22-subword-segmentation.html:text/html},
}

@inproceedings{tian_consistent_2022,
	address = {Singapore, Singapore},
	title = {Consistent {Training} and {Decoding} for {End}-to-{End} {Speech} {Recognition} {Using} {Lattice}-{Free} {MMI}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9746579/},
	doi = {10.1109/ICASSP43922.2022.9746579},
	urldate = {2022-10-05},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Tian, Jinchuan and Yu, Jianwei and Weng, Chao and Zhang, Shi-Xiong and Su, Dan and Yu, Dong and Zou, Yuexian},
	month = may,
	year = {2022},
	pages = {7782--7786},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\UFAHYT8M\\Tian et al. - 2022 - Consistent Training and Decoding for End-to-End Sp.pdf:application/pdf},
}

@misc{fayek_speech_2016,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}’s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2022-10-06},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
}

@misc{wiesner_lattice_2020,
	title = {Lattice {Free} {Maximum} {Mutual} {Information} ({LF}-{MMI})},
	url = {https://m-wiesner.github.io/LF-MMI/},
	abstract = {Everything about LF-MMI},
	language = {en},
	urldate = {2022-10-08},
	journal = {Matthew Wiesner},
	author = {Wiesner, Matthew and MatthewWiesner},
	month = jan,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ELAZNE4P\\LF-MMI.html:text/html},
}

@article{liu_time_2019,
	title = {Time {Delay} {Recurrent} {Neural} {Network} for {Speech} {Recognition}},
	volume = {1229},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1229/1/012078},
	doi = {10.1088/1742-6596/1229/1/012078},
	abstract = {Abstract
            In Automatic Speech Recognition(ASR), Time Delay Neural Network (TDNN) has been proven to be an efficient network structure for its strong ability in context modeling. In addition, as a feed-forward neural architecture, it is faster to train TDNN, compared with recurrent neural networks such as Long Short-Term Memory (LSTM). However, different from recurrent neural networks, the context in TDNN is carefully designed and is limited. Although stacking Long Short-Term Memory (LSTM) together with TDNN in order to extend the context information have been proven to be useful, it is too complex and is hard to train. In this paper, we focus on directly extending the context modeling capability of TDNNs by adding recurrent connections. Several new network architectures were investigated. The results on the Switchboard show that the best model significantly outperforms the base line TDNN system and is comparable with TDNN-LSTM architecture. In addition, the training process is much simpler than that of TDNN-LSTM.},
	number = {1},
	urldate = {2022-10-12},
	journal = {Journal of Physics: Conference Series},
	author = {Liu, Boji and Zhang, Weibin and Xu, Xiangming and Chen, Dongpeng},
	month = may,
	year = {2019},
	pages = {012078},
}

@phdthesis{ritter_neural_2019,
	title = {Neural {Architecture} {Search} for {Finding} the {Best} {Time} {Delay} {Neural} {Network} {Acoustic} {Model} for {Speech} {Recognition}},
	abstract = {Time Delay Neural Network (TDNN) is a popular type of acoustic model used for speech recognition applications. Their popularity is mainly due to their faster training and decoding times, with word error rates (WER) comparable to a long-short term memory (LSTM) based acoustic model. The popularity of TDNNs picked up in 2015 when a more efficient setup was proposed, which is known as a TDNN with sub-sampling scheme. However, the 2015 proposal with sub-sampling scheme does not offer any details on the sub-sampling used. Neural Architecture Search (NAS) is a new research field that has garnered a lot of attention with successful results in computer vision research. Despite this, NAS has received little attention in speech recognition, where design architectures for acoustic models is crucial. TDNNs are provided in the Kaldi speech recognition toolkit to be used for research or deployment purposes. From the literature, we have observed that it is common for a TDNN to be used in Kaldi as is, with no additional tuning of its hyperparameters. For this reason, this project aims to investigate if the Kaldi baseline TDNN is actually the best configuration to be used for a speech recognition application. To do so, we have made use of a recently proposed algorithm which integrates reinforcement learning in its training process. Specifically, we have used Neural Architecture Search (NAS) to target and improve automatically the sub-sampling scheme of the Kaldi TDNN. For reproducibility we have based all our results on the standard Wall Street Journal database. We performed experiments by setting a RNN based TDNN architecture generator, with the added constraint that the TDNNs evaluated have the same number of frames as the one provided with Kaldi. Our results show that NAS is able to sample a better TDNN architecture than the one provided by Kaldi in less than 40,000 iterations, achieving a 0.19 WER reduction. i},
	author = {Ritter, Fabian},
	month = aug,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8Y4PMA9Y\\Ritter - 2019 - Neural Architecture Search for Finding the Best Ti.pdf:application/pdf},
}

@misc{fayek_speech_2016-1,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}’s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2022-10-13},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\GNNWTTYV\\speech-processing-for-machine-learning.html:text/html},
}

@misc{shrawankar_adverse_2013-1,
	title = {Adverse {Conditions} and {ASR} {Techniques} for {Robust} {Speech} {User} {Interface}},
	url = {http://arxiv.org/abs/1303.5515},
	abstract = {The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Shrawankar, Urmila and Thakare, V. M.},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	annote = {Comment: 10 pages 2 Tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\Q3SPV23K\\Shrawankar and Thakare - 2013 - Adverse Conditions and ASR Techniques for Robust S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\82368FRG\\1303.html:text/html},
}

@article{vipperla_ageing_2010,
	title = {Ageing {Voices}: {The} {Effect} of {Changes} in {Voice} {Parameters} on {ASR} {Performance}},
	volume = {2010},
	issn = {1687-4714, 1687-4722},
	shorttitle = {Ageing {Voices}},
	url = {http://asmp.eurasipjournals.com/content/2010/1/525783},
	doi = {10.1155/2010/525783},
	language = {en},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Vipperla, Ravichander and Renals, Steve and Frankel, Joe},
	year = {2010},
	pages = {1--10},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\G6TNUYYM\\Vipperla et al. - 2010 - Ageing Voices The Effect of Changes in Voice Para.pdf:application/pdf},
}

@techreport{markus_forsberg_why_2003,
	title = {Why is {Speech} {Recognition} {Difficult}?},
	url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.3677},
	institution = {Chalmers University of Technology},
	author = {Markus Forsberg},
	month = feb,
	year = {2003},
}

@article{schuller_recognition_2009,
	title = {Recognition of {Noisy} {Speech}: {A} {Comparative} {Survey} of {Robust} {Model} {Architecture} and {Feature} {Enhancement}},
	volume = {2009},
	issn = {1687-4722},
	shorttitle = {Recognition of {Noisy} {Speech}},
	url = {http://asmp.eurasipjournals.com/content/2009/1/942617},
	doi = {10.1155/2009/942617},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Schuller, Björn and Wöllmer, Martin and Moosmayr, Tobias and Rigoll, Gerhard},
	year = {2009},
	pages = {942617},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\9UQKPJZ2\\Schuller et al. - 2009 - Recognition of Noisy Speech A Comparative Survey .pdf:application/pdf},
}

@article{hoge_basic_2007,
	title = {Basic parameters in speech processing. {The} need for evaluation},
	volume = {32},
	copyright = {Copyright on any open access article in the Archives of Acoustics published by Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society is retained by the author(s).   Authors grant Polish Academy of Sciences and Institute of Fundamental Technological Research Polish Acoustical Society a license to publish the article and identify itself as the original publisher.   Authors also grant any user the right to use the article freely as long as its integrity is maintained and its original authors, citation details and publisher are identified.           The Creative Commons Attribution License-ShareAlike 4.0 formalizes these and other terms and conditions of publishing articles.    Exceptions to copyright policy    For the articles which were previously published, before year 2019, policies that are different from the above. In all such, access to these articles is free from fees or any other access restrictions.   Permissions for the use of the texts published in that journal may be sought directly from the Editorial Office of Archives of Acoustics},
	issn = {2300-262X},
	url = {https://acoustics.ippt.pan.pl/index.php/aa/article/view/766},
	abstract = {As basic parameters in speech processing we regard pitch, duration, intensity, voice quality, signal to noise ratio, voice activity detection and strength of Lombard effect. Taking in account also adverse conditions the performance of many published algorithms to extract those parameters from the speech signal automatically is not known. A framework based on competitive evaluation is proposed to push algorithmic research and to make progress comparable.},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {Archives of Acoustics},
	author = {Höge, Harald},
	year = {2007},
	note = {Number: 1},
	pages = {67},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\9Z4BXXSK\\Höge - 2007 - Basic parameters in speech processing. The need fo.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FSTAYK66\\766.html:text/html},
}

@article{venkatagiri_speech_2002,
	title = {Speech {Recognition} {Technology} {Applications} in {Communication} {Disorders}},
	volume = {11},
	issn = {1058-0360, 1558-9110},
	url = {http://pubs.asha.org/doi/10.1044/1058-0360%282002/037%29},
	doi = {10.1044/1058-0360(2002/037)},
	abstract = {Speech recognition (SR) is the process whereby a microprocessor-based system, typically a computer with sound processing hardware and speech recognition software, responds in predictable ways to spoken commands and/or converts speech into text. This tutorial describes the types and the general uses of SR and provides an explanation of the technology behind it. The emerging applications of SR technology for dictation, articulation training, language and literacy development, environmental control, and communication augmentation are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-10-13},
	journal = {American Journal of Speech-Language Pathology},
	author = {Venkatagiri, H. S.},
	month = nov,
	year = {2002},
	pages = {323--332},
}

@techreport{noauthor_alternative_nodate,
	title = {Alternative {Control} {Technologies}: {Human} {Factors} {Issues}},
	shorttitle = {Alternative {Control} {Technologies}},
	url = {https://apps.dtic.mil/sti/citations/ADA355911},
	abstract = {With the increasing intelligence of computer systems, it is becoming more desirable to have an operator communicate with machines rather than simply operate them. In combat aircraft, this need to communicate is made quite crucial due to high temporal pressure and workload during critical phases of the flight ingress, engagement, deployment of self-defense. The HOTAS concept, with manual controls fitted on the stick and throttle, has been widely used in modern fighters such as F16, F18, EFA and Rafale. This concept allows pilots to input real time commands to the aircraft system. However, it increases the complexity of the pilot task due to inflation of real time controls, with some controls being multifunction. It is therefore desirable, in the framework of ecological interfaces, to introduce alternative input channels in order to reduce the complexity of manual control in the HOTAS concept and allow more direct and natural access to the aircraft systems. Control and display technologies are the critical enablers for these advanced interfaces. There are a variety of novel alternative control technologies that when integrated usefully with critical mission tasks can make natural use of the innate potential of human sensory and motor systems. Careful design and integration of candidate control technologies will result in human-machine interfaces which are natural, easier to learn, easier to use, and less prone to error. Significant progress is being made on using signals from the brain, muscles, voice, lip, head position, eye position and gestures for the control of computers and other devices. Judicious application of alternative control technologies has the potential to increase the bandwidth of operator-system interaction, improve the effectiveness of military systems, and realize cost savings. Alternative controls can reduce workload and improve efficiency within the cockpit, directly supporting the warfighter.},
	language = {en},
	urldate = {2022-10-13},
	note = {Section: Technical Reports},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\96Z76MGM\\ADA355911.html:text/html},
}

@book{vasilescu_cross-lingual_2011,
	title = {Cross-{Lingual} {Study} of {ASR} {Errors}: {On} the {Role} of the {Context} in {Human} {Perception} of {Near}-{Homophones}.},
	shorttitle = {Cross-{Lingual} {Study} of {ASR} {Errors}},
	author = {Vasilescu, Ioana and Yahia, Dahbia and Snoeren, Natalie and Adda-Decker, Martine and Lamel, Lori},
	month = aug,
	year = {2011},
	note = {Pages: 1952},
}

@inproceedings{povey_semi-orthogonal_2018,
	title = {Semi-{Orthogonal} {Low}-{Rank} {Matrix} {Factorization} for {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/povey18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1417},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Povey, Daniel and Cheng, Gaofeng and Wang, Yiming and Li, Ke and Xu, Hainan and Yarmohammadi, Mahsa and Khudanpur, Sanjeev},
	month = sep,
	year = {2018},
	pages = {3743--3747},
}

@inproceedings{yeh_taiwanese_2020,
	address = {Taipei, Taiwan},
	title = {Taiwanese {Speech} {Recognition} {Based} on {Hybrid} {Deep} {Neural} {Network} {Architecture}},
	url = {https://aclanthology.org/2020.rocling-1.11},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 32nd {Conference} on {Computational} {Linguistics} and {Speech} {Processing} ({ROCLING} 2020)},
	publisher = {The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)},
	author = {Yeh, Yu-Fu and Su, Bo-Hao and Ou, Yang-Yen and Wang, Jhing-Fa and Tsai, An-Chao},
	month = sep,
	year = {2020},
	pages = {102--113},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\MTSZB5PS\\Yeh et al. - 2020 - Taiwanese Speech Recognition Based on Hybrid Deep .pdf:application/pdf},
}
