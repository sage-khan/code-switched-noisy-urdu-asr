<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>Bibliography</title>
</head>
<body>
<div class="csl-bib-body" style="line-height: 1.35; ">
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">J. Kincaid, “A Brief History of ASR: Automatic Speech Recognition,” <i>Descript</i>, Jul. 12, 2018. <a href="https://medium.com/descript/a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5">https://medium.com/descript/a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=A%20Brief%20History%20of%20ASR%3A%20Automatic%20Speech%20Recognition&amp;rft.description=This%20is%20the%20first%20post%20in%20a%20series%20on%20Automatic%20Speech%20Recognition%2C%20the%20foundational%20technology%20that%20makes%20Descript%20possible.%20We%E2%80%99ll%20be%E2%80%A6&amp;rft.identifier=https%3A%2F%2Fmedium.com%2Fdescript%2Fa-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5&amp;rft.aufirst=Jason&amp;rft.aulast=Kincaid&amp;rft.au=Jason%20Kincaid&amp;rft.date=2018-07-12&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[2]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“A Chat with Andrew on MLOps: From Model-centric to Data-centric AI - YouTube.” <a href="https://www.youtube.com/watch?v=06-AZXmwHjo&amp;t=69s">https://www.youtube.com/watch?v=06-AZXmwHjo&amp;t=69s</a> (accessed Aug. 17, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=A%20Chat%20with%20Andrew%20on%20MLOps%3A%20From%20Model-centric%20to%20Data-centric%20AI%20-%20YouTube&amp;rft.identifier=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D06-AZXmwHjo%26t%3D69s"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[3]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“‘A machine learning model is only as good as the data it is fed,’” <i>devmio - expand your knowledge</i>, Jun. 04, 2018. <a href="https://devm.io/machine-learning/apache-spark-machine-learning-interview-143122">https://devm.io/machine-learning/apache-spark-machine-learning-interview-143122</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=%E2%80%9CA%20machine%20learning%20model%20is%20only%20as%20good%20as%20the%20data%20it%20is%20fed%E2%80%9D&amp;rft.description=Apache%20Spark%202.3%20was%20released%20earlier%20this%20year%3B%20it%20marked%20a%20major%20milestone%20for%20Structured%20Streaming%20but%20there%20are%20a%20lot%20of%20other%20interesting%20features%20that%20deserve%20your%20attention.%20We%20talked%20with%20Reynold%20Xin%2C%20co-founder%20and%20Chief%20Architect%20at%20Databricks%20about%20the%20Databricks%20Runtime%20and%20other%20enhancements%20introduced%20in%20Apache%20Spark%202.3.&amp;rft.identifier=https%3A%2F%2Fdevm.io%2Fmachine-learning%2Fapache-spark-machine-learning-interview-143122&amp;rft.date=2018-06-04&amp;rft.language=en-US"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[4]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">E. Khan, S. Rauf, F. Adeeba, and S. Hussain, “A Multi-Genre Urdu Broadcast Speech Recognition System,” in <i>2021 24th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)</i>, Singapore, Singapore, Nov. 2021, pp. 25–30. doi: <a href="https://doi.org/10.1109/O-COCOSDA202152914.2021.9660552">10.1109/O-COCOSDA202152914.2021.9660552</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FO-COCOSDA202152914.2021.9660552&amp;rft_id=urn%3Aisbn%3A978-1-66540-870-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=A%20Multi-Genre%20Urdu%20Broadcast%20Speech%20Recognition%20System&amp;rft.btitle=2021%2024th%20Conference%20of%20the%20Oriental%20COCOSDA%20International%20Committee%20for%20the%20Co-ordination%20and%20Standardisation%20of%20Speech%20Databases%20and%20Assessment%20Techniques%20(O-COCOSDA)&amp;rft.place=Singapore%2C%20Singapore&amp;rft.publisher=IEEE&amp;rft.aufirst=Erbaz&amp;rft.aulast=Khan&amp;rft.au=Erbaz%20Khan&amp;rft.au=Sahar%20Rauf&amp;rft.au=Farah%20Adeeba&amp;rft.au=Sarmad%20Hussain&amp;rft.date=2021-11-18&amp;rft.pages=25-30&amp;rft.spage=25&amp;rft.epage=30&amp;rft.isbn=978-1-66540-870-7"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[5]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, “A Review of Speaker Diarization: Recent Advances with Deep Learning,” 2021, doi: <a href="https://doi.org/10.48550/ARXIV.2101.09624">10.48550/ARXIV.2101.09624</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FARXIV.2101.09624&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A%20Review%20of%20Speaker%20Diarization%3A%20Recent%20Advances%20with%20Deep%20Learning&amp;rft.aufirst=Tae%20Jin&amp;rft.aulast=Park&amp;rft.au=Tae%20Jin%20Park&amp;rft.au=Naoyuki%20Kanda&amp;rft.au=Dimitrios%20Dimitriadis&amp;rft.au=Kyu%20J.%20Han&amp;rft.au=Shinji%20Watanabe&amp;rft.au=Shrikanth%20Narayanan&amp;rft.date=2021"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[6]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">T. Aguiar de Lima and M. Da Costa-Abreu, “A survey on automatic speech recognition systems for Portuguese language and its variations,” <i>Computer Speech &amp; Language</i>, vol. 62, p. 101055, Jul. 2020, doi: <a href="https://doi.org/10.1016/j.csl.2019.101055">10.1016/j.csl.2019.101055</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.csl.2019.101055&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A%20survey%20on%20automatic%20speech%20recognition%20systems%20for%20Portuguese%20language%20and%20its%20variations&amp;rft.jtitle=Computer%20Speech%20%26%20Language&amp;rft.stitle=Computer%20Speech%20%26%20Language&amp;rft.volume=62&amp;rft.aufirst=Thales&amp;rft.aulast=Aguiar%20de%20Lima&amp;rft.au=Thales%20Aguiar%20de%20Lima&amp;rft.au=M%C3%A1rjory%20Da%20Costa-Abreu&amp;rft.date=2020-07&amp;rft.pages=101055&amp;rft.issn=08852308&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[7]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">P. Smit, S. Virpioja, and M. Kurimo, “Advances in subword-based HMM-DNN speech recognition across languages,” <i>Computer Speech &amp; Language</i>, vol. 66, p. 101158, Mar. 2021, doi: <a href="https://doi.org/10.1016/j.csl.2020.101158">10.1016/j.csl.2020.101158</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.csl.2020.101158&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Advances%20in%20subword-based%20HMM-DNN%20speech%20recognition%20across%20languages&amp;rft.jtitle=Computer%20Speech%20%26%20Language&amp;rft.stitle=Computer%20Speech%20%26%20Language&amp;rft.volume=66&amp;rft.aufirst=Peter&amp;rft.aulast=Smit&amp;rft.au=Peter%20Smit&amp;rft.au=Sami%20Virpioja&amp;rft.au=Mikko%20Kurimo&amp;rft.date=2021-03&amp;rft.pages=101158&amp;rft.issn=08852308&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[8]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Alphacephei, “Alpha Cephei,” <i>GitHub</i>. <a href="https://github.com/alphacep">https://github.com/alphacep</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Alpha%20Cephei&amp;rft.description=Alpha%20Cephei%20has%2037%20repositories%20available.%20Follow%20their%20code%20on%20GitHub.&amp;rft.identifier=https%3A%2F%2Fgithub.com%2Falphacep&amp;rft.aulast=Alphacephei&amp;rft.au=Alphacephei&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[9]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">alphacep, “alphacep/vosk-server.” Alpha Cephei, Aug. 14, 2022. Accessed: Aug. 16, 2022. [Online]. Available: <a href="https://github.com/alphacep/vosk-server">https://github.com/alphacep/vosk-server</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=computerProgram&amp;rft.title=alphacep%2Fvosk-server&amp;rft.rights=Apache-2.0&amp;rft.description=WebSocket%2C%20gRPC%20and%20WebRTC%20speech%20recognition%20server%20based%20on%20Vosk%20and%20Kaldi%20libraries&amp;rft.identifier=https%3A%2F%2Fgithub.com%2Falphacep%2Fvosk-server&amp;rft.aulast=alphacep&amp;rft.au=alphacep&amp;rft.date=2022-08-14"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[10]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">S. S. Khan, “An Introduction to Classification Using Mislabeled Data,” <i>Medium</i>, Mar. 18, 2021. <a href="https://towardsdatascience.com/an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5">https://towardsdatascience.com/an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=An%20Introduction%20to%20Classification%20Using%20Mislabeled%20Data&amp;rft.description=The%20performance%20of%20any%20classifier%2C%20or%20for%20that%20matter%20any%20machine%20learning%20task%2C%20depends%20crucially%20on%20the%20quality%20of%20the%20available%20data%E2%80%A6&amp;rft.identifier=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-classification-using-mislabeled-data-581a6c09f9f5&amp;rft.aufirst=Shihab%20Shahriar&amp;rft.aulast=Khan&amp;rft.au=Shihab%20Shahriar%20Khan&amp;rft.date=2021-03-18&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[11]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">H. A. Alsayadi, A. A. Abdelhamid, I. Hegazy, and Z. T. Fayed, “Arabic speech recognition using end-to-end deep learning,” <i>IET Signal Processing</i>, vol. 15, no. 8, pp. 521–534, 2021, doi: <a href="https://doi.org/10.1049/sil2.12057">10.1049/sil2.12057</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1049%2Fsil2.12057&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Arabic%20speech%20recognition%20using%20end-to-end%20deep%20learning&amp;rft.jtitle=IET%20Signal%20Processing&amp;rft.volume=15&amp;rft.issue=8&amp;rft.aufirst=Hamzah%20A.&amp;rft.aulast=Alsayadi&amp;rft.au=Hamzah%20A.%20Alsayadi&amp;rft.au=Abdelaziz%20A.%20Abdelhamid&amp;rft.au=Islam%20Hegazy&amp;rft.au=Zaki%20T.%20Fayed&amp;rft.date=2021&amp;rft.pages=521-534&amp;rft.spage=521&amp;rft.epage=534&amp;rft.issn=1751-9683&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[12]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">J. Levis and R. Suvorov, “Automatic Speech Recognition,” in <i>The Encyclopedia of Applied Linguistics</i>, 1st ed., C. A. Chapelle, Ed. Wiley, 2020, pp. 1–8. doi: <a href="https://doi.org/10.1002/9781405198431.wbeal0066.pub2">10.1002/9781405198431.wbeal0066.pub2</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-4051-9473-0%20978-1-4051-9843-1&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Automatic%20Speech%20Recognition&amp;rft.publisher=Wiley&amp;rft.edition=1&amp;rft.aufirst=John&amp;rft.aulast=Levis&amp;rft.au=Carol%20A.%20Chapelle&amp;rft.au=John%20Levis&amp;rft.au=Ruslan%20Suvorov&amp;rft.date=2020-12-22&amp;rft.pages=1-8&amp;rft.spage=1&amp;rft.epage=8&amp;rft.isbn=978-1-4051-9473-0%20978-1-4051-9843-1&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[13]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Z. Somogyi, “Automatic Speech Recognition,” in <i>The Application of Artificial Intelligence</i>, Cham: Springer International Publishing, 2021, pp. 145–171. doi: <a href="https://doi.org/10.1007/978-3-030-60032-7_5">10.1007/978-3-030-60032-7_5</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-3-030-60031-0%20978-3-030-60032-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Automatic%20Speech%20Recognition&amp;rft.place=Cham&amp;rft.publisher=Springer%20International%20Publishing&amp;rft.aufirst=Zolt%C3%A1n&amp;rft.aulast=Somogyi&amp;rft.au=Zolt%C3%A1n%20Somogyi&amp;rft.au=Zolt%C3%A1n%20Somogyi&amp;rft.date=2021&amp;rft.pages=145-171&amp;rft.spage=145&amp;rft.epage=171&amp;rft.isbn=978-3-030-60031-0%20978-3-030-60032-7&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[14]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">D. Yu and L. Deng, <i>Automatic Speech Recognition</i>. London: Springer London, 2015. doi: <a href="https://doi.org/10.1007/978-1-4471-5779-3">10.1007/978-1-4471-5779-3</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-4471-5778-6%20978-1-4471-5779-3&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Automatic%20Speech%20Recognition&amp;rft.place=London&amp;rft.publisher=Springer%20London&amp;rft.series=Signals%20and%20Communication%20Technology&amp;rft.aufirst=Dong&amp;rft.aulast=Yu&amp;rft.au=Dong%20Yu&amp;rft.au=Li%20Deng&amp;rft.date=2015&amp;rft.isbn=978-1-4471-5778-6%20978-1-4471-5779-3"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[15]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">U. G. Patil, S. D. Shirbahadurkar, and A. N. Paithane, “Automatic Speech Recognition of isolated words in Hindi language using MFCC,” in <i>2016 International Conference on Computing, Analytics and Security Trends (CAST)</i>, Pune, India, Dec. 2016, pp. 433–438. doi: <a href="https://doi.org/10.1109/CAST.2016.7915008">10.1109/CAST.2016.7915008</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FCAST.2016.7915008&amp;rft_id=urn%3Aisbn%3A978-1-5090-1338-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Automatic%20Speech%20Recognition%20of%20isolated%20words%20in%20Hindi%20language%20using%20MFCC&amp;rft.btitle=2016%20International%20Conference%20on%20Computing%2C%20Analytics%20and%20Security%20Trends%20(CAST)&amp;rft.place=Pune%2C%20India&amp;rft.publisher=IEEE&amp;rft.aufirst=U.%20G.&amp;rft.aulast=Patil&amp;rft.au=U.%20G.%20Patil&amp;rft.au=S.%20D.%20Shirbahadurkar&amp;rft.au=A.%20N.%20Paithane&amp;rft.date=2016-12&amp;rft.pages=433-438&amp;rft.spage=433&amp;rft.epage=438&amp;rft.isbn=978-1-5090-1338-8"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[16]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">H. Ali, N. Ahmad, and X. Zhou, “Automatic speech recognition of Urdu words using linear discriminant analysis,” <i>IFS</i>, vol. 28, no. 5, pp. 2369–2375, Jun. 2015, doi: <a href="https://doi.org/10.3233/IFS-151554">10.3233/IFS-151554</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.3233%2FIFS-151554&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Automatic%20speech%20recognition%20of%20Urdu%20words%20using%20linear%20discriminant%20analysis&amp;rft.jtitle=Journal%20of%20Intelligent%20%26%20Fuzzy%20Systems&amp;rft.stitle=IFS&amp;rft.volume=28&amp;rft.issue=5&amp;rft.aufirst=Hazrat&amp;rft.aulast=Ali&amp;rft.au=Hazrat%20Ali&amp;rft.au=Nasir%20Ahmad&amp;rft.au=Xianwei%20Zhou&amp;rft.date=2015-06-23&amp;rft.pages=2369-2375&amp;rft.spage=2369&amp;rft.epage=2375&amp;rft.issn=10641246%2C%2018758967"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[17]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">D. Dash, M. Kim, K. Teplansky, and J. Wang, “Automatic Speech Recognition with Articulatory Information and a Unified Dictionary for Hindi, Marathi, Bengali and Oriya,” in <i>Interspeech 2018</i>, Sep. 2018, pp. 1046–1050. doi: <a href="https://doi.org/10.21437/Interspeech.2018-2122">10.21437/Interspeech.2018-2122</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.21437%2FInterspeech.2018-2122&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Automatic%20Speech%20Recognition%20with%20Articulatory%20Information%20and%20a%20Unified%20Dictionary%20for%20Hindi%2C%20Marathi%2C%20Bengali%20and%20Oriya&amp;rft.btitle=Interspeech%202018&amp;rft.publisher=ISCA&amp;rft.aufirst=Debadatta&amp;rft.aulast=Dash&amp;rft.au=Debadatta%20Dash&amp;rft.au=Myungjong%20Kim&amp;rft.au=Kristin%20Teplansky&amp;rft.au=Jun%20Wang&amp;rft.date=2018-09-02&amp;rft.pages=1046-1050&amp;rft.spage=1046&amp;rft.epage=1050&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[18]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">S. Alharbi <i>et al.</i>, “Automatic Speech Recognition: Systematic Literature Review,” <i>IEEE Access</i>, vol. 9, pp. 131858–131876, 2021, doi: <a href="https://doi.org/10.1109/ACCESS.2021.3112535">10.1109/ACCESS.2021.3112535</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FACCESS.2021.3112535&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Automatic%20Speech%20Recognition%3A%20Systematic%20Literature%20Review&amp;rft.jtitle=IEEE%20Access&amp;rft.volume=9&amp;rft.aufirst=Sadeen&amp;rft.aulast=Alharbi&amp;rft.au=Sadeen%20Alharbi&amp;rft.au=Muna%20Alrazgan&amp;rft.au=Alanoud%20Alrashed&amp;rft.au=Turkiayh%20Alnomasi&amp;rft.au=Raghad%20Almojel&amp;rft.au=Rimah%20Alharbi&amp;rft.au=Saja%20Alharbi&amp;rft.au=Sahar%20Alturki&amp;rft.au=Fatimah%20Alshehri&amp;rft.au=Maha%20Almojil&amp;rft.date=2021&amp;rft.pages=131858-131876&amp;rft.spage=131858&amp;rft.epage=131876&amp;rft.issn=2169-3536"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[19]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Asadullah, A. Shaukat, H. Ali, and U. Akram, “Automatic Urdu Speech Recognition using Hidden Markov Model,” in <i>2016 International Conference on Image, Vision and Computing (ICIVC)</i>, Portsmouth, Aug. 2016, pp. 135–139. doi: <a href="https://doi.org/10.1109/ICIVC.2016.7571287">10.1109/ICIVC.2016.7571287</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FICIVC.2016.7571287&amp;rft_id=urn%3Aisbn%3A978-1-5090-3755-1&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Automatic%20Urdu%20Speech%20Recognition%20using%20Hidden%20Markov%20Model&amp;rft.btitle=2016%20International%20Conference%20on%20Image%2C%20Vision%20and%20Computing%20(ICIVC)&amp;rft.place=Portsmouth&amp;rft.publisher=IEEE&amp;rft.au=undefined&amp;rft.au=Arslan%20Shaukat&amp;rft.au=Hazrat%20Ali&amp;rft.au=Usman%20Akram&amp;rft.date=2016-08&amp;rft.pages=135-139&amp;rft.spage=135&amp;rft.epage=139&amp;rft.isbn=978-1-5090-3755-1"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[20]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">British Broadcast, “BBC - Languages - Urdu - A Guide to Urdu - 10 facts about the Urdu language.” <a href="https://www.bbc.co.uk/languages/other/urdu/guide/facts.shtml">https://www.bbc.co.uk/languages/other/urdu/guide/facts.shtml</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=BBC%20-%20Languages%20-%20Urdu%20-%20A%20Guide%20to%20Urdu%20-%2010%20facts%20about%20the%20Urdu%20language&amp;rft.description=Discover%20surprising%20and%20revealing%20facts%20about%20Urdu%2C%20including%20Urdu%20words%20used%20in%20the%20English%20language%20and%20Urdu%20jokes%20and%20quotes.&amp;rft.identifier=https%3A%2F%2Fwww.bbc.co.uk%2Flanguages%2Fother%2Furdu%2Fguide%2Ffacts.shtml&amp;rft.aulast=British%20Broadcast&amp;rft.au=British%20Broadcast&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[21]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“Brief History of Automatic Speech Recognition,” in <i>Speech and Audio Signal Processing</i>, Hoboken, NJ, USA: John Wiley &amp; Sons, Inc., 2011, pp. 40–58. doi: <a href="https://doi.org/10.1002/9781118142882.ch4">10.1002/9781118142882.ch4</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-118-14288-2%20978-0-470-19536-9&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Brief%20History%20of%20Automatic%20Speech%20Recognition&amp;rft.place=Hoboken%2C%20NJ%2C%20USA&amp;rft.publisher=John%20Wiley%20%26%20Sons%2C%20Inc.&amp;rft.au=Ben%20Gold&amp;rft.au=Nelson%20Morgan&amp;rft.au=Dan%20Ellis&amp;rft.date=2011-10-14&amp;rft.pages=40-58&amp;rft.spage=40&amp;rft.epage=58&amp;rft.isbn=978-1-118-14288-2%20978-0-470-19536-9"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[22]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">B. Frenay and M. Verleysen, “Classification in the Presence of Label Noise: A Survey,” <i>IEEE Trans. Neural Netw. Learning Syst.</i>, vol. 25, no. 5, pp. 845–869, May 2014, doi: <a href="https://doi.org/10.1109/TNNLS.2013.2292894">10.1109/TNNLS.2013.2292894</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FTNNLS.2013.2292894&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Classification%20in%20the%20Presence%20of%20Label%20Noise%3A%20A%20Survey&amp;rft.jtitle=IEEE%20Transactions%20on%20Neural%20Networks%20and%20Learning%20Systems&amp;rft.stitle=IEEE%20Trans.%20Neural%20Netw.%20Learning%20Syst.&amp;rft.volume=25&amp;rft.issue=5&amp;rft.aufirst=Benoit&amp;rft.aulast=Frenay&amp;rft.au=Benoit%20Frenay&amp;rft.au=Michel%20Verleysen&amp;rft.date=2014-05&amp;rft.pages=845-869&amp;rft.spage=845&amp;rft.epage=869&amp;rft.issn=2162-237X%2C%202162-2388"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[23]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">CMU, “CMU Lexicon Tool.” <a href="http://www.speech.cs.cmu.edu/tools/lextool.html">http://www.speech.cs.cmu.edu/tools/lextool.html</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=CMU%20Lexicon%20Tool&amp;rft.identifier=http%3A%2F%2Fwww.speech.cs.cmu.edu%2Ftools%2Flextool.html&amp;rft.aulast=CMU&amp;rft.au=CMU"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[24]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">K. V. S. Parsad and S. M. Virk, “Computational evidence that Hindi and Urdu share a grammar but not the lexicon,” presented at the COLING, 2012.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Computational%20evidence%20that%20Hindi%20and%20Urdu%20share%20a%20grammar%20but%20not%20the%20lexicon&amp;rft.btitle=3rd%20Workshop%20on%20South%20and%20Southeast%20Asian%20NLP&amp;rft.aulast=K.%20V.%20S.%20Parsad%20and%20S.%20M.%20Virk&amp;rft.au=K.%20V.%20S.%20Parsad%20and%20S.%20M.%20Virk&amp;rft.date=2012"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[25]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">P. Upadhyaya, S. K. Mittal, O. Farooq, Y. V. Varshney, and M. R. Abidi, “Continuous Hindi Speech Recognition Using Kaldi ASR Based on Deep Neural Network,” in <i>Machine Intelligence and Signal Analysis</i>, vol. 748, M. Tanveer and R. B. Pachori, Eds. Singapore: Springer Singapore, 2019, pp. 303–311. doi: <a href="https://doi.org/10.1007/978-981-13-0923-6_26">10.1007/978-981-13-0923-6_26</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A9789811309229%209789811309236&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Continuous%20Hindi%20Speech%20Recognition%20Using%20Kaldi%20ASR%20Based%20on%20Deep%20Neural%20Network&amp;rft.place=Singapore&amp;rft.publisher=Springer%20Singapore&amp;rft.aufirst=Prashant&amp;rft.aulast=Upadhyaya&amp;rft.au=M.%20Tanveer&amp;rft.au=Ram%20Bilas%20Pachori&amp;rft.au=Prashant%20Upadhyaya&amp;rft.au=Sanjeev%20Kumar%20Mittal&amp;rft.au=Omar%20Farooq&amp;rft.au=Yash%20Vardhan%20Varshney&amp;rft.au=Musiur%20Raza%20Abidi&amp;rft.date=2019&amp;rft.pages=303-311&amp;rft.spage=303&amp;rft.epage=311&amp;rft.isbn=9789811309229%209789811309236"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[26]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">coqui-ai, “coqui-ai/STT.” coqui, Aug. 16, 2022. Accessed: Aug. 16, 2022. [Online]. Available: <a href="https://github.com/coqui-ai/STT">https://github.com/coqui-ai/STT</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=computerProgram&amp;rft.title=coqui-ai%2FSTT&amp;rft.rights=MPL-2.0&amp;rft.description=%F0%9F%90%B8STT%20-%20The%20deep%20learning%20toolkit%20for%20Speech-to-Text.%20Training%20and%20deploying%20STT%20models%20has%20never%20been%20so%20easy.&amp;rft.identifier=https%3A%2F%2Fgithub.com%2Fcoqui-ai%2FSTT&amp;rft.aulast=coqui-ai&amp;rft.au=coqui-ai&amp;rft.date=2022-08-16"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[27]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">E. Chodroff, “Corpus Phonetics Tutorial.” arXiv, Nov. 13, 2018. Accessed: Aug. 16, 2022. [Online]. Available: <a href="http://arxiv.org/abs/1811.05553">http://arxiv.org/abs/1811.05553</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Corpus%20Phonetics%20Tutorial&amp;rft.description=Corpus%20phonetics%20has%20become%20an%20increasingly%20popular%20method%20of%20research%20in%20linguistic%20analysis.%20With%20advances%20in%20speech%20technology%20and%20computational%20power%2C%20large%20scale%20processing%20of%20speech%20data%20has%20become%20a%20viable%20technique.%20This%20tutorial%20introduces%20the%20speech%20scientist%20and%20engineer%20to%20various%20automatic%20speech%20processing%20tools.%20These%20include%20acoustic%20model%20creation%20and%20forced%20alignment%20using%20the%20Kaldi%20Automatic%20Speech%20Recognition%20Toolkit%20(Povey%20et%20al.%2C%202011)%2C%20forced%20alignment%20using%20FAVE-align%20(Rosenfelder%20et%20al.%2C%202014)%2C%20the%20Montreal%20Forced%20Aligner%20(McAuliffe%20et%20al.%2C%202017)%2C%20and%20the%20Penn%20Phonetics%20Lab%20Forced%20Aligner%20(Yuan%20%26%20Liberman%2C%202008)%2C%20as%20well%20as%20stop%20consonant%20burst%20alignment%20using%20AutoVOT%20(Keshet%20et%20al.%2C%202014).%20The%20tutorial%20provides%20a%20general%20overview%20of%20each%20program%2C%20step-by-step%20instructions%20for%20running%20the%20program%2C%20as%20well%20as%20several%20tips%20and%20tricks.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F1811.05553&amp;rft.aufirst=Eleanor&amp;rft.aulast=Chodroff&amp;rft.au=Eleanor%20Chodroff&amp;rft.date=2018-11-13"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[28]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">CPLC, “CPLC – Citizens-Police Liaison Committee.” <a href="http://www.cplc.org.pk/">http://www.cplc.org.pk/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=CPLC%20%E2%80%93%20Citizens-Police%20Liaison%20Committee&amp;rft.identifier=http%3A%2F%2Fwww.cplc.org.pk%2F&amp;rft.aulast=CPLC&amp;rft.au=CPLC"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[29]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">S. Latif, A. Qayyum, M. Usman, and J. Qadir, “Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages,” in <i>2018 International Conference on Frontiers of Information Technology (FIT)</i>, Islamabad, Pakistan, Dec. 2018, pp. 88–93. doi: <a href="https://doi.org/10.1109/FIT.2018.00023">10.1109/FIT.2018.00023</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FFIT.2018.00023&amp;rft_id=urn%3Aisbn%3A978-1-5386-9355-1&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Cross%20Lingual%20Speech%20Emotion%20Recognition%3A%20Urdu%20vs.%20Western%20Languages&amp;rft.btitle=2018%20International%20Conference%20on%20Frontiers%20of%20Information%20Technology%20(FIT)&amp;rft.place=Islamabad%2C%20Pakistan&amp;rft.publisher=IEEE&amp;rft.aufirst=Siddique&amp;rft.aulast=Latif&amp;rft.au=Siddique%20Latif&amp;rft.au=Adnan%20Qayyum&amp;rft.au=Muhammad%20Usman&amp;rft.au=Junaid%20Qadir&amp;rft.date=2018-12&amp;rft.pages=88-93&amp;rft.spage=88&amp;rft.epage=93&amp;rft.isbn=978-1-5386-9355-1"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[30]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">I. F. Ilyas and X. Chu, <i>Data Cleaning</i>. New York, NY: ACM Books, 2019.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-4503-7153-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Data%20Cleaning&amp;rft.place=New%20York%2C%20NY&amp;rft.publisher=ACM%20Books&amp;rft.aufirst=Ihab%20F.&amp;rft.aulast=Ilyas&amp;rft.au=Ihab%20F.%20Ilyas&amp;rft.au=Xu%20Chu&amp;rft.date=2019-06-18&amp;rft.tpages=282&amp;rft.isbn=978-1-4503-7153-7&amp;rft.language=English"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[31]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;"><i>Data-centric AI: Real World Approaches</i>, (Aug. 11, 2021). Accessed: Aug. 17, 2022. [Online Video]. Available: <a href="https://www.youtube.com/watch?v=Yqj7Kyjznh4">https://www.youtube.com/watch?v=Yqj7Kyjznh4</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=videoRecording&amp;rft.title=Data-centric%20AI%3A%20Real%20World%20Approaches&amp;rft.identifier=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYqj7Kyjznh4&amp;rft.au=undefined&amp;rft.date=2021-08-11"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[32]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">H. Patel, “Data-Centric Approach vs Model-Centric Approach in Machine Learning,” <i>neptune.ai</i>, Dec. 30, 2021. <a href="https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning">https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning</a> (accessed Aug. 17, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Data-Centric%20Approach%20vs%20Model-Centric%20Approach%20in%20Machine%20Learning&amp;rft.description=Code%20and%20data%20are%20the%20foundations%20of%20the%20AI%20system.%20Both%20of%20these%20components%20play%20an%20important%20role%20in%20the%20development%20of%20a%20robust%20model%20but%20which%20one%20should%20you%20focus%20on%20more%3F%20In%20this%20article%2C%20we%E2%80%99ll%20go%20through%20the%20data-centric%20vs%20model-centric%20approaches%2C%20and%20see%20which%20one%20is%20better%2C%20we%20would%20also%20talk%20about%20%5B%E2%80%A6%5D&amp;rft.identifier=https%3A%2F%2Fneptune.ai%2Fblog%2Fdata-centric-vs-model-centric-machine-learning&amp;rft.aufirst=Harshil&amp;rft.aulast=Patel&amp;rft.au=Harshil%20Patel&amp;rft.date=2021-12-30&amp;rft.language=en-US"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[33]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“Data-centric Machine Learning: Making customized ML solutions production-ready,” <i>dida Machine Learning</i>. <a href="https://dida.do/blog/data-centric-machine-learning">https://dida.do/blog/data-centric-machine-learning</a> (accessed Aug. 17, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Data-centric%20Machine%20Learning%3A%20Making%20customized%20ML%20solutions%20production-ready&amp;rft.description=In%20this%20article%2C%20we%20will%20see%20why%20many%20ML%20Projects%20do%20not%20make%20it%20into%20production%2C%20introduce%20the%20concepts%20of%20model-%20and%20data-centric%20ML%2C%20and%20give%20examples%20how%20we%20at%20dida%20improve%20projects%20by%20applying%20data-centric%20techniques.&amp;rft.identifier=https%3A%2F%2Fdida.do%2Fblog%2Fdata-centric-machine-learning&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[34]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">D. Radečić, “Data-centric vs. Model-centric AI? The Answer is Clear,” <i>Medium</i>, Mar. 23, 2022. <a href="https://towardsdatascience.com/data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67">https://towardsdatascience.com/data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67</a> (accessed Aug. 17, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Data-centric%20vs.%20Model-centric%20AI%3F%20The%20Answer%20is%20Clear&amp;rft.description=There%E2%80%99s%20something%20wrong%20with%20the%20current%20approach%20to%20AI.%20But%20there%E2%80%99s%20a%20solution.&amp;rft.identifier=https%3A%2F%2Ftowardsdatascience.com%2Fdata-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67&amp;rft.aufirst=Dario&amp;rft.aulast=Rade%C4%8Di%C4%87&amp;rft.au=Dario%20Rade%C4%8Di%C4%87&amp;rft.date=2022-03-23&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[35]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Sehar Gul, “DETECTING MALICIOUS ACTIVITIES OVER TELEPHONE NETWORK FOR URDU SPEAKER,” National University of Science and Technology - Pakistan Navy Engineering College, Karachi, 2020.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=DETECTING%20MALICIOUS%20ACTIVITIES%20OVER%20TELEPHONE%20NETWORK%20FOR%20URDU%20SPEAKER&amp;rft.aulast=Sehar%20Gul&amp;rft.au=Sehar%20Gul&amp;rft.date=2020-06"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[36]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“ESPnet: end-to-end speech processing toolkit.” ESPnet, Aug. 16, 2022. Accessed: Aug. 16, 2022. [Online]. Available: <a href="https://github.com/espnet/espnet">https://github.com/espnet/espnet</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=computerProgram&amp;rft.title=ESPnet%3A%20end-to-end%20speech%20processing%20toolkit&amp;rft.rights=Apache-2.0&amp;rft.description=End-to-End%20Speech%20Processing%20Toolkit&amp;rft.identifier=https%3A%2F%2Fgithub.com%2Fespnet%2Fespnet&amp;rft.date=2022-08-16"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[37]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“ESPnet: end-to-end speech processing toolkit — ESPnet 202207 documentation.” <a href="https://espnet.github.io/espnet/">https://espnet.github.io/espnet/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=ESPnet%3A%20end-to-end%20speech%20processing%20toolkit%20%E2%80%94%20ESPnet%20202207%20documentation&amp;rft.identifier=https%3A%2F%2Fespnet.github.io%2Fespnet%2F"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[38]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Suma Swamy and Ramakrishnan, Kollengode, “Evolution of Speech Recognition – A Brief History of Technology Development,” <i>Elixir Adv. Engg. Info.</i>, vol. 60, Jul. 2013.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Evolution%20of%20Speech%20Recognition%20%E2%80%93%20A%20Brief%20History%20of%20Technology%20Development&amp;rft.jtitle=Elixir%20Adv.%20Engg.%20Info.&amp;rft.volume=60&amp;rft.aulast=Suma%20Swamy&amp;rft.au=Suma%20Swamy&amp;rft.au=Ramakrishnan%2C%20Kollengode&amp;rft.date=2013-07"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[39]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">O. Abdel-Hamid, L. Deng, and D. Yu, “Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition,” Aug. 2013, Interspeech 2013. [Online]. Available: <a href="https://www.microsoft.com/en-us/research/publication/exploring-convolutional-neural-network-structures-and-optimization-techniques-for-speech-recognition/">https://www.microsoft.com/en-us/research/publication/exploring-convolutional-neural-network-structures-and-optimization-techniques-for-speech-recognition/</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Exploring%20Convolutional%20Neural%20Network%20Structures%20and%20Optimization%20Techniques%20for%20Speech%20Recognition&amp;rft.btitle=Interspeech%202013&amp;rft.publisher=ISCA&amp;rft.aufirst=Ossama&amp;rft.aulast=Abdel-Hamid&amp;rft.au=Ossama%20Abdel-Hamid&amp;rft.au=Li%20Deng&amp;rft.au=Dong%20Yu&amp;rft.date=2013-08"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[40]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“GRAM VAANI ASR Challenge 2022.” <a href="https://sites.google.com/view/gramvaaniasrchallenge/home">https://sites.google.com/view/gramvaaniasrchallenge/home</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=GRAM%20VAANI%20ASR%20Challenge%202022&amp;rft.description=A%20challenge%20on%20Automatic%20Speech%20Recognition%20for%20Hindi%20is%20being%20organized%20as%20part%20of%20INTERSPEECH%202022%20by%20sharing%20the%20spontaneous%20telephone%20speech%20recordings%20collected%20by%20a%20social%20technology%20enterprise%20Gram%20Vaani.%20The%20regional%20variations%20of%20Hindi%20together%20with%20spontaneity%20of%20speech%2C%20natural&amp;rft.identifier=https%3A%2F%2Fsites.google.com%2Fview%2Fgramvaaniasrchallenge%2Fhome&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[41]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“gramvaani.org.” <a href="https://gramvaani.org/">https://gramvaani.org/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=gramvaani.org&amp;rft.identifier=https%3A%2F%2Fgramvaani.org%2F"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[42]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">M. U. Farooq, F. Adeeba, S. Rauf, and S. Hussain, “Improving Large Vocabulary Urdu Speech Recognition System Using Deep Neural Networks,” in <i>Interspeech 2019</i>, Sep. 2019, pp. 2978–2982. doi: <a href="https://doi.org/10.21437/Interspeech.2019-2629">10.21437/Interspeech.2019-2629</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.21437%2FInterspeech.2019-2629&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Improving%20Large%20Vocabulary%20Urdu%20Speech%20Recognition%20System%20Using%20Deep%20Neural%20Networks&amp;rft.btitle=Interspeech%202019&amp;rft.publisher=ISCA&amp;rft.aufirst=Muhammad%20Umar&amp;rft.aulast=Farooq&amp;rft.au=Muhammad%20Umar%20Farooq&amp;rft.au=Farah%20Adeeba&amp;rft.au=Sahar%20Rauf&amp;rft.au=Sarmad%20Hussain&amp;rft.date=2019-09-15&amp;rft.pages=2978-2982&amp;rft.spage=2978&amp;rft.epage=2982&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[43]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">T. Bäckström <i>et al.</i>, “Introduction to Speech Processing: 2nd Edition,” Jul. 2022, doi: <a href="https://doi.org/10.5281/ZENODO.6821775">10.5281/ZENODO.6821775</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.5281%2FZENODO.6821775&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Introduction%20to%20Speech%20Processing%3A%202nd%20Edition&amp;rft.aufirst=Tom&amp;rft.aulast=B%C3%A4ckstr%C3%B6m&amp;rft.au=Tom%20B%C3%A4ckstr%C3%B6m&amp;rft.au=Okko%20R%C3%A4s%C3%A4nen&amp;rft.au=Abraham%20Zewoudie&amp;rft.au=Pablo%20P%C3%A9rez%20Zarazaga&amp;rft.au=Liisa%20Koivusalo&amp;rft.au=Sneha%20Das&amp;rft.au=Esteban%20G%C3%B3mez%20Mellado&amp;rft.au=Mariem%20Bouafif%20Mansali&amp;rft.au=Daniel%20Ramos&amp;rft.date=2022-07-12&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[44]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">R. Cao <i>et al.</i>, “Joint Prostate Cancer Detection and Gleason Score Prediction in mp-MRI via FocalNet,” <i>IEEE Trans. Med. Imaging</i>, vol. 38, no. 11, pp. 2496–2506, Nov. 2019, doi: <a href="https://doi.org/10.1109/TMI.2019.2901928">10.1109/TMI.2019.2901928</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FTMI.2019.2901928&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Joint%20Prostate%20Cancer%20Detection%20and%20Gleason%20Score%20Prediction%20in%20mp-MRI%20via%20FocalNet&amp;rft.jtitle=IEEE%20Transactions%20on%20Medical%20Imaging&amp;rft.stitle=IEEE%20Trans.%20Med.%20Imaging&amp;rft.volume=38&amp;rft.issue=11&amp;rft.aufirst=Ruiming&amp;rft.aulast=Cao&amp;rft.au=Ruiming%20Cao&amp;rft.au=Amirhossein%20Mohammadian%20Bajgiran&amp;rft.au=Sohrab%20Afshari%20Mirak&amp;rft.au=Sepideh%20Shakeri&amp;rft.au=Xinran%20Zhong&amp;rft.au=Dieter%20Enzmann&amp;rft.au=Steven%20Raman&amp;rft.au=Kyunghyun%20Sung&amp;rft.date=2019-11&amp;rft.pages=2496-2506&amp;rft.spage=2496&amp;rft.epage=2506&amp;rft.issn=0278-0062%2C%201558-254X"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[45]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">K. V. Lakshmi Sri, M. Srinivasan, R. R. Nair, K. J. Priya, and D. Gupta, “Kaldi recipe in Hindi for word level recognition and phoneme level transcription,” <i>Procedia Computer Science</i>, vol. 171, pp. 2476–2485, 2020, doi: <a href="https://doi.org/10.1016/j.procs.2020.04.268">10.1016/j.procs.2020.04.268</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.procs.2020.04.268&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Kaldi%20recipe%20in%20Hindi%20for%20word%20level%20recognition%20and%20phoneme%20level%20transcription&amp;rft.jtitle=Procedia%20Computer%20Science&amp;rft.stitle=Procedia%20Computer%20Science&amp;rft.volume=171&amp;rft.aufirst=Karra%20Venkata&amp;rft.aulast=Lakshmi%20Sri&amp;rft.au=Karra%20Venkata%20Lakshmi%20Sri&amp;rft.au=Mayuka%20Srinivasan&amp;rft.au=Radhika%20Rajeev%20Nair&amp;rft.au=K.%20Jeeva%20Priya&amp;rft.au=Deepa%20Gupta&amp;rft.date=2020&amp;rft.pages=2476-2485&amp;rft.spage=2476&amp;rft.epage=2485&amp;rft.issn=18770509&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[46]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Daniel Povey, “Kaldi: Kaldi.” <a href="https://kaldi-asr.org/doc/">https://kaldi-asr.org/doc/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Kaldi%3A%20Kaldi&amp;rft.identifier=https%3A%2F%2Fkaldi-asr.org%2Fdoc%2F&amp;rft.aulast=Daniel%20Povey&amp;rft.au=Daniel%20Povey"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[47]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">H. Sarfraz <i>et al.</i>, “Large vocabulary continuous speech recognition for Urdu,” in <i>Proceedings of the 8th International Conference on Frontiers of Information Technology - FIT ’10</i>, Islamabad, Pakistan, 2010, pp. 1–5. doi: <a href="https://doi.org/10.1145/1943628.1943629">10.1145/1943628.1943629</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F1943628.1943629&amp;rft_id=urn%3Aisbn%3A978-1-4503-0342-2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Large%20vocabulary%20continuous%20speech%20recognition%20for%20Urdu&amp;rft.btitle=Proceedings%20of%20the%208th%20International%20Conference%20on%20Frontiers%20of%20Information%20Technology%20-%20FIT%20'10&amp;rft.place=Islamabad%2C%20Pakistan&amp;rft.publisher=ACM%20Press&amp;rft.aufirst=Huda&amp;rft.aulast=Sarfraz&amp;rft.au=Huda%20Sarfraz&amp;rft.au=Rahila%20Parveen&amp;rft.au=Sarmad%20Hussain&amp;rft.au=Riffat%20Bokhari&amp;rft.au=Agha%20Ali%20Raza&amp;rft.au=Inam%20Ullah&amp;rft.au=Zahid%20Sarfraz&amp;rft.au=Sophia%20Pervez&amp;rft.au=Asad%20Mustafa&amp;rft.au=Iqra%20Javed&amp;rft.date=2010&amp;rft.pages=1-5&amp;rft.spage=1&amp;rft.epage=5&amp;rft.isbn=978-1-4503-0342-2&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[48]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learning with Noisy Labels,” in <i>Advances in Neural Information Processing Systems</i>, 2013, vol. 26. [Online]. Available: <a href="https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf">https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Learning%20with%20Noisy%20Labels&amp;rft.btitle=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;rft.publisher=Curran%20Associates%2C%20Inc.&amp;rft.aufirst=Nagarajan&amp;rft.aulast=Natarajan&amp;rft.au=Nagarajan%20Natarajan&amp;rft.au=Inderjit%20S%20Dhillon&amp;rft.au=Pradeep%20K%20Ravikumar&amp;rft.au=Ambuj%20Tewari&amp;rft.au=C.%20J.%20Burges&amp;rft.au=L.%20Bottou&amp;rft.au=M.%20Welling&amp;rft.au=Z.%20Ghahramani&amp;rft.au=K.%20Q.%20Weinberger&amp;rft.date=2013"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[49]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">B. van Rooyen, A. Menon, and R. C. Williamson, “Learning with Symmetric Label Noise: The Importance of Being Unhinged,” in <i>Advances in Neural Information Processing Systems</i>, 2015, vol. 28. [Online]. Available: <a href="https://proceedings.neurips.cc/paper/2015/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf">https://proceedings.neurips.cc/paper/2015/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Learning%20with%20Symmetric%20Label%20Noise%3A%20The%20Importance%20of%20Being%20Unhinged&amp;rft.btitle=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;rft.publisher=Curran%20Associates%2C%20Inc.&amp;rft.aufirst=Brendan&amp;rft.aulast=van%20Rooyen&amp;rft.au=Brendan%20van%20Rooyen&amp;rft.au=Aditya%20Menon&amp;rft.au=Robert%20C%20Williamson&amp;rft.au=C.%20Cortes&amp;rft.au=N.%20Lawrence&amp;rft.au=D.%20Lee&amp;rft.au=M.%20Sugiyama&amp;rft.au=R.%20Garnett&amp;rft.date=2015"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[50]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An ASR corpus based on public domain audio books,” in <i>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, Apr. 2015, pp. 5206–5210. doi: <a href="https://doi.org/10.1109/ICASSP.2015.7178964">10.1109/ICASSP.2015.7178964</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FICASSP.2015.7178964&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Librispeech%3A%20An%20ASR%20corpus%20based%20on%20public%20domain%20audio%20books&amp;rft.btitle=2015%20IEEE%20International%20Conference%20on%20Acoustics%2C%20Speech%20and%20Signal%20Processing%20(ICASSP)&amp;rft.aufirst=Vassil&amp;rft.aulast=Panayotov&amp;rft.au=Vassil%20Panayotov&amp;rft.au=Guoguo%20Chen&amp;rft.au=Daniel%20Povey&amp;rft.au=Sanjeev%20Khudanpur&amp;rft.date=2015-04&amp;rft.pages=5206-5210&amp;rft.spage=5206&amp;rft.epage=5210"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[51]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">H. Sak, A. Senior, and F. Beaufays, “Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition,” 2014, doi: <a href="https://doi.org/10.48550/ARXIV.1402.1128">10.48550/ARXIV.1402.1128</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FARXIV.1402.1128&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Long%20Short-Term%20Memory%20Based%20Recurrent%20Neural%20Network%20Architectures%20for%20Large%20Vocabulary%20Speech%20Recognition&amp;rft.aufirst=Ha%C5%9Fim&amp;rft.aulast=Sak&amp;rft.au=Ha%C5%9Fim%20Sak&amp;rft.au=Andrew%20Senior&amp;rft.au=Fran%C3%A7oise%20Beaufays&amp;rft.date=2014"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[52]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Piero Molino, “Ludwig - code-free deep learning toolbox.” <a href="http://ludwig.ai">http://ludwig.ai</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Ludwig%20-%20code-free%20deep%20learning%20toolbox&amp;rft.description=Ludwig%20is%20a%20toolbox%20for%20training%20and%20testing%20deep%20learning%20models%20without%20writing%20code&amp;rft.identifier=http%3A%2F%2Fludwig.ai&amp;rft.aulast=Piero%20Molino&amp;rft.au=Piero%20Molino&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[53]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">D. C. Blog, “Machine Learning Models Are Only as Good as the Data They Are Trained On,” <i>Deepchecks</i>, Apr. 19, 2022. <a href="https://deepchecks.com/machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on/">https://deepchecks.com/machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Machine%20Learning%20Models%20Are%20Only%20as%20Good%20as%20the%20Data%20They%20Are%20Trained%20On&amp;rft.description=Learn%20about%20the%20importance%20of%20data%20validation%20in%20machine%20learning%2C%20look%20into%20the%20various%20tools%20for%20different%20sets%20of%20data%20validation%20techniques%20%26%20procedures.&amp;rft.identifier=https%3A%2F%2Fdeepchecks.com%2Fmachine-learning-models-are-only-as-good-as-the-data-they-are-trained-on%2F&amp;rft.aufirst=Deepchecks%20Community&amp;rft.aulast=Blog&amp;rft.au=Deepchecks%20Community%20Blog&amp;rft.date=2022-04-19&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[54]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“openslr.org.” <a href="https://www.openslr.org/resources.php">https://www.openslr.org/resources.php</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=openslr.org&amp;rft.identifier=https%3A%2F%2Fwww.openslr.org%2Fresources.php"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[55]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">A.-L. Georgescu, A. Pappalardo, H. Cucu, and M. Blott, “Performance vs. hardware requirements in state-of-the-art automatic speech recognition,” <i>J AUDIO SPEECH MUSIC PROC.</i>, vol. 2021, no. 1, p. 28, Dec. 2021, doi: <a href="https://doi.org/10.1186/s13636-021-00217-4">10.1186/s13636-021-00217-4</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1186%2Fs13636-021-00217-4&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Performance%20vs.%20hardware%20requirements%20in%20state-of-the-art%20automatic%20speech%20recognition&amp;rft.jtitle=EURASIP%20Journal%20on%20Audio%2C%20Speech%2C%20and%20Music%20Processing&amp;rft.stitle=J%20AUDIO%20SPEECH%20MUSIC%20PROC.&amp;rft.volume=2021&amp;rft.issue=1&amp;rft.aufirst=Alexandru-Lucian&amp;rft.aulast=Georgescu&amp;rft.au=Alexandru-Lucian%20Georgescu&amp;rft.au=Alessandro%20Pappalardo&amp;rft.au=Horia%20Cucu&amp;rft.au=Michaela%20Blott&amp;rft.date=2021-12&amp;rft.pages=28&amp;rft.issn=1687-4722&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[56]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">A. A. Raza <i>et al.</i>, “Rapid Collection of Spontaneous Speech Corpora Using Telephonic Community Forums,” in <i>Interspeech 2018</i>, Sep. 2018, pp. 1021–1025. doi: <a href="https://doi.org/10.21437/Interspeech.2018-1139">10.21437/Interspeech.2018-1139</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.21437%2FInterspeech.2018-1139&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Rapid%20Collection%20of%20Spontaneous%20Speech%20Corpora%20Using%20Telephonic%20Community%20Forums&amp;rft.btitle=Interspeech%202018&amp;rft.publisher=ISCA&amp;rft.aufirst=Agha%20Ali&amp;rft.aulast=Raza&amp;rft.au=Agha%20Ali%20Raza&amp;rft.au=Awais%20Athar&amp;rft.au=Shan%20Randhawa&amp;rft.au=Zain%20Tariq&amp;rft.au=Muhammad%20Bilal%20Saleem&amp;rft.au=Haris%20Bin%20Zia&amp;rft.au=Umar%20Saif&amp;rft.au=Roni%20Rosenfeld&amp;rft.date=2018-09-02&amp;rft.pages=1021-1025&amp;rft.spage=1021&amp;rft.epage=1025&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[57]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Mishra, Achyuta and Chandra, Mahesh and Biswas, Astik and Sharan, “Robust Features for Connected Hindi Digits Recognition,” <i>International Journal of Signal Processing, Image Processing and Pattern Recognition</i>, vol. 4, no. 2, Jun. 2011.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Robust%20Features%20for%20Connected%20Hindi%20Digits%20Recognition&amp;rft.jtitle=International%20Journal%20of%20Signal%20Processing%2C%20Image%20Processing%20and%20Pattern%20Recognition&amp;rft.volume=4&amp;rft.issue=2&amp;rft.aulast=Mishra%2C%20Achyuta%20and%20Chandra%2C%20Mahesh%20and%20Biswas%2C%20Astik%20and%20Sharan&amp;rft.au=Mishra%2C%20Achyuta%20and%20Chandra%2C%20Mahesh%20and%20Biswas%2C%20Astik%20and%20Sharan&amp;rft.date=2011-06"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[58]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Karel Vesel and Arnab Ghoshal and Luk Burget and Daniel Povey, “Sequence-discriminative training of deep neural networks,” presented at the Interspeech, 2013.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Sequence-discriminative%20training%20of%20deep%20neural%20networks&amp;rft.aulast=Karel%20Vesel%20and%20Arnab%20Ghoshal%20and%20Luk%20Burget%20and%20Daniel%20Povey&amp;rft.au=Karel%20Vesel%20and%20Arnab%20Ghoshal%20and%20Luk%20Burget%20and%20Daniel%20Povey&amp;rft.date=2013"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[59]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">B. Venkataraman, “SOPC-based speech-to-text conversion,” presented at the Nios II, 2006.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=SOPC-based%20speech-to-text%20conversion&amp;rft.btitle=Embedded%20Processor%20Design%20Contest%20Outstanding%20Designs&amp;rft.aulast=B.%20Venkataraman&amp;rft.au=B.%20Venkataraman&amp;rft.date=2006"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[60]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">J. Ashraf, N. Iqbal, N. S. Khattak, and A. M. Zaidi, “Speaker Independent Urdu Speech Recognition Using HMM,” in <i>Natural Language Processing and Information Systems</i>, vol. 6177, C. J. Hopfe, Y. Rezgui, E. Métais, A. Preece, and H. Li, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010, pp. 140–148. doi: <a href="https://doi.org/10.1007/978-3-642-13881-2_14">10.1007/978-3-642-13881-2_14</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-3-642-13880-5%20978-3-642-13881-2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Speaker%20Independent%20Urdu%20Speech%20Recognition%20Using%20HMM&amp;rft.place=Berlin%2C%20Heidelberg&amp;rft.publisher=Springer%20Berlin%20Heidelberg&amp;rft.aufirst=Javed&amp;rft.aulast=Ashraf&amp;rft.au=David%20Hutchison&amp;rft.au=Takeo%20Kanade&amp;rft.au=Josef%20Kittler&amp;rft.au=Jon%20M.%20Kleinberg&amp;rft.au=Friedemann%20Mattern&amp;rft.au=John%20C.%20Mitchell&amp;rft.au=Moni%20Naor&amp;rft.au=Oscar%20Nierstrasz&amp;rft.au=C.%20Pandu%20Rangan&amp;rft.au=Bernhard%20Steffen&amp;rft.au=Madhu%20Sudan&amp;rft.au=Demetri%20Terzopoulos&amp;rft.au=Doug%20Tygar&amp;rft.au=Moshe%20Y.%20Vardi&amp;rft.au=Gerhard%20Weikum&amp;rft.au=Christina%20J.%20Hopfe&amp;rft.au=Yacine%20Rezgui&amp;rft.au=Elisabeth%20M%C3%A9tais&amp;rft.au=Alun%20Preece&amp;rft.au=Haijiang%20Li&amp;rft.au=Javed%20Ashraf&amp;rft.au=Naveed%20Iqbal&amp;rft.au=Naveed%20Sarfraz%20Khattak&amp;rft.au=Ather%20Mohsin%20Zaidi&amp;rft.date=2010&amp;rft.pages=140-148&amp;rft.spage=140&amp;rft.epage=148&amp;rft.isbn=978-3-642-13880-5%20978-3-642-13881-2"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[61]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“SpeechBrain: A PyTorch Speech Toolkit.” <a href="https://speechbrain.github.io/">https://speechbrain.github.io/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=SpeechBrain%3A%20A%20PyTorch%20Speech%20Toolkit&amp;rft.identifier=https%3A%2F%2Fspeechbrain.github.io%2F"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[62]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">A. Zhang (Uberi), “SpeechRecognition: Library for performing speech recognition, with support for several engines and APIs, online and offline.” Accessed: Aug. 16, 2022. [MacOS :: MacOS X, Microsoft :: Windows, Other OS, POSIX :: Linux]. Available: <a href="https://github.com/Uberi/speech_recognition#readme">https://github.com/Uberi/speech_recognition#readme</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=computerProgram&amp;rft.title=SpeechRecognition%3A%20Library%20for%20performing%20speech%20recognition%2C%20with%20support%20for%20several%20engines%20and%20APIs%2C%20online%20and%20offline.&amp;rft.rights=BSD%20License&amp;rft.identifier=https%3A%2F%2Fgithub.com%2FUberi%2Fspeech_recognition%23readme&amp;rft.aufirst=Anthony&amp;rft.aulast=Zhang%20(Uberi)&amp;rft.au=Anthony%20Zhang%20(Uberi)"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[63]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">S. Naeem <i>et al.</i>, “Subspace Gaussian Mixture Model for Continuous Urdu Speech Recognition using Kaldi,” in <i>2020 14th International Conference on Open Source Systems and Technologies (ICOSST)</i>, Dec. 2020, pp. 1–7. doi: <a href="https://doi.org/10.1109/ICOSST51357.2020.9333026">10.1109/ICOSST51357.2020.9333026</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FICOSST51357.2020.9333026&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Subspace%20Gaussian%20Mixture%20Model%20for%20Continuous%20Urdu%20Speech%20Recognition%20using%20Kaldi&amp;rft.btitle=2020%2014th%20International%20Conference%20on%20Open%20Source%20Systems%20and%20Technologies%20(ICOSST)&amp;rft.aufirst=Saad&amp;rft.aulast=Naeem&amp;rft.au=Saad%20Naeem&amp;rft.au=Majid%20Iqbal&amp;rft.au=Muhammad%20Saqib&amp;rft.au=Muhammad%20Saad&amp;rft.au=Muhammad%20Soban%20Raza&amp;rft.au=Zaid%20Ali&amp;rft.au=Naveed%20Akhtar&amp;rft.au=Mirza%20Omer%20Beg&amp;rft.au=Waseem%20Shahzad&amp;rft.au=Muhhamad%20Umair%20Arshad&amp;rft.date=2020-12&amp;rft.pages=1-7&amp;rft.spage=1&amp;rft.epage=7"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[64]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Y. Fan, X. Xia, D. A. da Costa, D. Lo, A. E. Hassan, and S. Li, “The Impact of Mislabeled Changes by SZZ on Just-in-Time Defect Prediction,” <i>IEEE Transactions on Software Engineering</i>, vol. 47, no. 8, pp. 1559–1586, Aug. 2021, doi: <a href="https://doi.org/10.1109/TSE.2019.2929761">10.1109/TSE.2019.2929761</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FTSE.2019.2929761&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The%20Impact%20of%20Mislabeled%20Changes%20by%20SZZ%20on%20Just-in-Time%20Defect%20Prediction&amp;rft.jtitle=IEEE%20Transactions%20on%20Software%20Engineering&amp;rft.volume=47&amp;rft.issue=8&amp;rft.aufirst=Yuanrui&amp;rft.aulast=Fan&amp;rft.au=Yuanrui%20Fan&amp;rft.au=Xin%20Xia&amp;rft.au=Daniel%20Alencar%20da%20Costa&amp;rft.au=David%20Lo&amp;rft.au=Ahmed%20E.%20Hassan&amp;rft.au=Shanping%20Li&amp;rft.date=2021-08&amp;rft.pages=1559-1586&amp;rft.spage=1559&amp;rft.epage=1586&amp;rft.issn=1939-3520"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[65]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">“The Significance of Data-centric AI,” <i>KDnuggets</i>. <a href="https://www.kdnuggets.com/the-significance-of-data-centric-ai.html/">https://www.kdnuggets.com/the-significance-of-data-centric-ai.html/</a> (accessed Aug. 17, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=The%20Significance%20of%20Data-centric%20AI&amp;rft.description=How%20a%20systematic%20way%20of%20maintaining%20data%20quality%20can%20do%20wonders%20to%20your%20model%20performance.&amp;rft.identifier=https%3A%2F%2Fwww.kdnuggets.com%2Fthe-significance-of-data-centric-ai.html%2F&amp;rft.language=en-US"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[66]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">J. Kincaid, “The State of Automatic Speech Recognition: Q&amp;A with Kaldi’s Dan Povey,” <i>Descript</i>, Jul. 17, 2018. <a href="https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85">https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=The%20State%20of%20Automatic%20Speech%20Recognition%3A%20Q%26A%20with%20Kaldi%E2%80%99s%20Dan%20Povey&amp;rft.description=This%20article%20continues%20our%20series%20on%20Automatic%20Speech%20Recognition%2C%20including%20our%20recent%20piece%20on%20the%20History%20of%20ASR.&amp;rft.identifier=https%3A%2F%2Fmedium.com%2Fdescript%2Fthe-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85&amp;rft.aufirst=Jason&amp;rft.aulast=Kincaid&amp;rft.au=Jason%20Kincaid&amp;rft.date=2018-07-17&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[67]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Ethnologue, “Urdu Language - Ethnologue Report,” <i>Ethnologue</i>. <a href="https://www.ethnologue.com/language/urdu">https://www.ethnologue.com/language/urdu</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Urdu%20Language%20-%20Ethnologue%20Report&amp;rft.description=A%20language%20profile%20for%20Urdu.%20Get%20a%20detailed%20look%20at%20the%20language%2C%20from%20population%20to%20dialects%20and%20usage.&amp;rft.identifier=https%3A%2F%2Fwww.ethnologue.com%2Flanguage%2Furdu&amp;rft.aulast=Ethnologue&amp;rft.au=Ethnologue&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[68]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">Z. Qureshi, “Urdu Speech Recognition.” Oct. 13, 2021. Accessed: Aug. 16, 2022. [Online]. Available: <a href="https://github.com/ZoraizQ/urdu-speech-recognition">https://github.com/ZoraizQ/urdu-speech-recognition</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=computerProgram&amp;rft.title=Urdu%20Speech%20Recognition&amp;rft.rights=MIT&amp;rft.description=Urdu%20Speech%20Recognition%20using%20Kaldi%20ASR%2C%20by%20training%20Triphone%20Acoustic%20GMMs%20using%20the%20PRUS%20dataset.&amp;rft.identifier=https%3A%2F%2Fgithub.com%2FZoraizQ%2Furdu-speech-recognition&amp;rft.aufirst=Zoraiz&amp;rft.aulast=Qureshi&amp;rft.au=Zoraiz%20Qureshi&amp;rft.date=2021-10-13"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[69]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">M. Qasim, S. Nawaz, S. Hussain, and T. Habib, “Urdu speech recognition system for district names of Pakistan: Development, challenges and solutions,” in <i>2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA)</i>, Bali, Indonesia, Oct. 2016, pp. 28–32. doi: <a href="https://doi.org/10.1109/ICSDA.2016.7918979">10.1109/ICSDA.2016.7918979</a>.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FICSDA.2016.7918979&amp;rft_id=urn%3Aisbn%3A978-1-5090-3516-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Urdu%20speech%20recognition%20system%20for%20district%20names%20of%20Pakistan%3A%20Development%2C%20challenges%20and%20solutions&amp;rft.btitle=2016%20Conference%20of%20The%20Oriental%20Chapter%20of%20International%20Committee%20for%20Coordination%20and%20Standardization%20of%20Speech%20Databases%20and%20Assessment%20Techniques%20(O-COCOSDA)&amp;rft.place=Bali%2C%20Indonesia&amp;rft.publisher=IEEE&amp;rft.aufirst=Muhammad&amp;rft.aulast=Qasim&amp;rft.au=Muhammad%20Qasim&amp;rft.au=Sohaib%20Nawaz&amp;rft.au=Sarmad%20Hussain&amp;rft.au=Tania%20Habib&amp;rft.date=2016-10&amp;rft.pages=28-32&amp;rft.spage=28&amp;rft.epage=32&amp;rft.isbn=978-1-5090-3516-8"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[70]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">R. K. Aggarwal and M. Dave, “Using Gaussian Mixtures for Hindi Speech Recognition System,” 2011.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Using%20Gaussian%20Mixtures%20for%20Hindi%20Speech%20Recognition%20System&amp;rft.btitle=International%20Journal%20of%20Signal%20Processing%2C%20Image%20Processing%20and%20Pattern%20Recognition&amp;rft.aufirst=R.%20K.&amp;rft.aulast=Aggarwal&amp;rft.au=R.%20K.%20Aggarwal&amp;rft.au=M.%20Dave&amp;rft.date=2011"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[71]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">R. K. Aggarwal and M. Dave, “Using Gaussian Mixtures for Hindi Speech Recognition System,” presented at the International Journal of Signal Processing, Image Processing and Pattern Recognition, 2011.</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Using%20Gaussian%20Mixtures%20for%20Hindi%20Speech%20Recognition%20System&amp;rft.aulast=R.%20K.%20Aggarwal%20and%20M.%20Dave&amp;rft.au=R.%20K.%20Aggarwal%20and%20M.%20Dave&amp;rft.date=2011"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[72]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">alphacep, “VOSK Models,” <i>VOSK Offline Speech Recognition API</i>. <a href="https://alphacephei.com/vosk/models">https://alphacephei.com/vosk/models</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=VOSK%20Models&amp;rft.description=Accurate%20speech%20recognition%20for%20Android%2C%20iOS%2C%20Raspberry%20Pi%20and%20servers%20with%20Python%2C%20Java%2C%20C%23%2C%20Swift%20and%20Node.&amp;rft.identifier=https%3A%2F%2Falphacephei.com%2Fvosk%2Fmodels&amp;rft.aulast=alphacep&amp;rft.au=alphacep&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[73]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">alphacep, “VOSK Offline Speech Recognition API,” <i>VOSK Offline Speech Recognition API</i>. <a href="https://alphacephei.com/vosk/">https://alphacephei.com/vosk/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=VOSK%20Offline%20Speech%20Recognition%20API&amp;rft.description=Accurate%20speech%20recognition%20for%20Android%2C%20iOS%2C%20Raspberry%20Pi%20and%20servers%20with%20Python%2C%20Java%2C%20C%23%2C%20Swift%20and%20Node.&amp;rft.identifier=https%3A%2F%2Falphacephei.com%2Fvosk%2F&amp;rft.aulast=alphacep&amp;rft.au=alphacep&amp;rft.language=en"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[74]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">alphacep, “Vosk Speech Recognition Toolkit.” Alpha Cephei, Aug. 16, 2022. Accessed: Aug. 16, 2022. [Online]. Available: <a href="https://github.com/alphacep/vosk-api">https://github.com/alphacep/vosk-api</a></div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=computerProgram&amp;rft.title=Vosk%20Speech%20Recognition%20Toolkit&amp;rft.rights=Apache-2.0&amp;rft.description=Offline%20speech%20recognition%20API%20for%20Android%2C%20iOS%2C%20Raspberry%20Pi%20and%20servers%20with%20Python%2C%20Java%2C%20C%23%20and%20Node&amp;rft.identifier=https%3A%2F%2Fgithub.com%2Falphacep%2Fvosk-api&amp;rft.aulast=alphacep&amp;rft.au=alphacep&amp;rft.date=2022-08-16"></span>
  <div class="csl-entry" style="clear: left; ">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em;text-align: right; width: 2em;">[75]</div><div class="csl-right-inline" style="margin: 0 .4em 0 2.5em;">J. Brownlee, “Why Data Preparation Is So Important in Machine Learning,” <i>Machine Learning Mastery</i>, Jun. 14, 2020. <a href="https://machinelearningmastery.com/data-preparation-is-important/">https://machinelearningmastery.com/data-preparation-is-important/</a> (accessed Aug. 16, 2022).</div>
  </div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Why%20Data%20Preparation%20Is%20So%20Important%20in%20Machine%20Learning&amp;rft.description=On%20a%20predictive%20modeling%20project%2C%20machine%20learning%20algorithms%20learn%20a%20mapping%20from%20input%20variables%20to%20a%20target%20variable.%20The%20most%20common%20form%20of%20predictive%20modeling%20project%20involves%20so-called%20structured%20data%20or%20tabular%20data.%20This%20is%20data%20as%20it%20looks%20in%20a%20spreadsheet%20or%20a%20matrix%2C%20with%20rows%20of%20examples%20and%20columns%20of%20features%20for%20each%20%5B%E2%80%A6%5D&amp;rft.identifier=https%3A%2F%2Fmachinelearningmastery.com%2Fdata-preparation-is-important%2F&amp;rft.aufirst=Jason&amp;rft.aulast=Brownlee&amp;rft.au=Jason%20Brownlee&amp;rft.date=2020-06-14&amp;rft.language=en-US"></span>
</div></body>
</html>
