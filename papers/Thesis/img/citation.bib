
@inproceedings{farooq_improving_2019,
	title = {Improving {Large} {Vocabulary} {Urdu} {Speech} {Recognition} {System} {Using} {Deep} {Neural} {Networks}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html},
	doi = {10.21437/Interspeech.2019-2629},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Rauf, Sahar and Hussain, Sarmad},
	month = sep,
	year = {2019},
	pages = {2978--2982},
}

@misc{nerd_of_the_rings_complete_2020,
	title = {The {Complete} {Travels} of {Galadriel} {\textbar} {Tolkien} {Explained}},
	url = {https://www.youtube.com/watch?v=yDX-2rTxE6Y},
	urldate = {2022-08-16},
	author = {{Nerd of the Rings}},
	month = nov,
	year = {2020},
}

@misc{alphacephei_alpha_nodate,
	title = {Alpha {Cephei}},
	url = {https://github.com/alphacep},
	abstract = {Alpha Cephei has 37 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2022-08-16},
	journal = {GitHub},
	author = {Alphacephei},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\C9UHNTWM\\alphacep.html:text/html},
}

@misc{alphacep_vosk_2022,
	title = {Vosk {Speech} {Recognition} {Toolkit}},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-api},
	abstract = {Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C\# and Node},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-09-03T17:48:42Z},
	keywords = {android, asr, deep-learning, deep-neural-networks, deepspeech, google-speech-to-text, ios, kaldi, offline, privacy, python, raspberry-pi, speaker-identification, speaker-verification, speech-recognition, speech-to-text, speech-to-text-android, stt, voice-recognition, vosk},
}

@misc{alphacep_alphacepvosk-server_2022,
	title = {alphacep/vosk-server},
	copyright = {Apache-2.0},
	url = {https://github.com/alphacep/vosk-server},
	abstract = {WebSocket, gRPC and WebRTC speech recognition server based on Vosk and Kaldi libraries},
	urldate = {2022-08-16},
	publisher = {Alpha Cephei},
	author = {alphacep},
	month = aug,
	year = {2022},
	note = {original-date: 2019-05-07T17:24:55Z},
	keywords = {asr, kaldi, python, speech-recognition, vosk, grpc, saas, webrtc, websocket},
}

@misc{alphacep_vosk_nodate,
	title = {{VOSK} {Offline} {Speech} {Recognition} {API}},
	url = {https://alphacephei.com/vosk/},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\H4HQA2XC\\vosk.html:text/html},
}

@misc{alphacep_vosk_nodate-1,
	title = {{VOSK} {Models}},
	url = {https://alphacephei.com/vosk/models},
	abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
	language = {en},
	urldate = {2022-08-16},
	journal = {VOSK Offline Speech Recognition API},
	author = {alphacep},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\J75LJ82E\\models.html:text/html},
}

@misc{daniel_povey_kaldi_nodate,
	title = {Kaldi: {Kaldi}},
	url = {https://kaldi-asr.org/doc/},
	urldate = {2022-08-16},
	author = {Daniel Povey},
	file = {Kaldi\: Kaldi:C\:\\Users\\DELL\\Zotero\\storage\\QYDHVT6P\\doc.html:text/html},
}

@misc{noauthor_openslrorg_nodate,
	title = {openslr.org},
	url = {https://www.openslr.org/resources.php},
	urldate = {2022-08-16},
	file = {openslr.org:C\:\\Users\\DELL\\Zotero\\storage\\EJWY9PRN\\resources.html:text/html},
}

@misc{cmu_cmu_nodate,
	title = {{CMU} {Lexicon} {Tool}},
	url = {http://www.speech.cs.cmu.edu/tools/lextool.html},
	urldate = {2022-08-16},
	author = {CMU},
	file = {CMU Lexicon Tool:C\:\\Users\\DELL\\Zotero\\storage\\BTMXKZQL\\lextool.html:text/html},
}

@misc{coqui-ai_coqui-aistt_2022,
	title = {coqui-ai/{STT}},
	copyright = {MPL-2.0},
	url = {https://github.com/coqui-ai/STT},
	abstract = {üê∏STT - The deep learning toolkit for Speech-to-Text. Training and deploying STT models has never been so easy.},
	urldate = {2022-08-16},
	publisher = {coqui},
	author = {coqui-ai},
	month = aug,
	year = {2022},
	note = {original-date: 2021-03-04T04:54:42Z},
	keywords = {asr, deep-learning, speech-recognition, speech-to-text, stt, voice-recognition, automatic-speech-recognition, speech-recognition-api, speech-recognizer, tensorflow},
}

@misc{noauthor_speechbrain_nodate,
	title = {{SpeechBrain}: {A} {PyTorch} {Speech} {Toolkit}},
	url = {https://speechbrain.github.io/},
	urldate = {2022-08-16},
}

@misc{piero_molino_ludwig_nodate,
	title = {Ludwig - code-free deep learning toolbox},
	url = {http://ludwig.ai},
	abstract = {Ludwig is a toolbox for training and testing deep learning models without writing code},
	language = {en},
	urldate = {2022-08-16},
	author = {Piero Molino},
}

@misc{noauthor_espnet_2022,
	title = {{ESPnet}: end-to-end speech processing toolkit},
	copyright = {Apache-2.0},
	shorttitle = {{ESPnet}},
	url = {https://github.com/espnet/espnet},
	abstract = {End-to-End Speech Processing Toolkit},
	urldate = {2022-08-16},
	publisher = {ESPnet},
	month = aug,
	year = {2022},
	note = {original-date: 2017-12-13T00:45:11Z},
	keywords = {deep-learning, kaldi, speech-recognition, chainer, end-to-end, machine-translation, pytorch, speech-enhancement, speech-separation, speech-synthesis, speech-translation, voice-conversion},
}

@misc{noauthor_espnet_nodate,
	title = {{ESPnet}: end-to-end speech processing toolkit ‚Äî {ESPnet} 202207 documentation},
	url = {https://espnet.github.io/espnet/},
	urldate = {2022-08-16},
}

@article{georgescu_performance_2021,
	title = {Performance vs. hardware requirements in state-of-the-art automatic speech recognition},
	volume = {2021},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00217-4},
	doi = {10.1186/s13636-021-00217-4},
	abstract = {Abstract
            The last decade brought significant advances in automatic speech recognition (ASR) thanks to the evolution of deep learning methods. ASR systems evolved from pipeline-based systems, that modeled hand-crafted speech features with probabilistic frameworks and generated phone posteriors, to end-to-end (E2E) systems, that translate the raw waveform directly into words using one deep neural network (DNN). The transcription accuracy greatly increased, leading to ASR technology being integrated into many commercial applications. However, few of the existing ASR technologies are suitable for integration in embedded applications, due to their hard constrains related to computing power and memory usage. This overview paper serves as a guided tour through the recent literature on speech recognition and compares the most popular ASR implementations. The comparison emphasizes the trade-off between ASR performance and hardware requirements, to further serve decision makers in choosing the system which fits best their embedded application. To the best of our knowledge, this is the first study to provide this kind of trade-off analysis for state-of-the-art ASR systems.},
	language = {en},
	number = {1},
	urldate = {2022-08-16},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Georgescu, Alexandru-Lucian and Pappalardo, Alessandro and Cucu, Horia and Blott, Michaela},
	month = dec,
	year = {2021},
	pages = {28},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TRA6KAJY\\Georgescu et al. - 2021 - Performance vs. hardware requirements in state-of-.pdf:application/pdf},
}

@misc{cplc_cplc_nodate,
	title = {{CPLC} ‚Äì {Citizens}-{Police} {Liaison} {Committee}},
	url = {http://www.cplc.org.pk/},
	urldate = {2022-08-16},
	author = {CPLC},
}

@misc{british_broadcast_bbc_nodate,
	title = {{BBC} - {Languages} - {Urdu} - {A} {Guide} to {Urdu} - 10 facts about the {Urdu} language},
	url = {https://www.bbc.co.uk/languages/other/urdu/guide/facts.shtml},
	abstract = {Discover surprising and revealing facts about Urdu, including Urdu words used in the English language and Urdu jokes and quotes.},
	language = {en},
	urldate = {2022-08-16},
	author = {British Broadcast},
	note = {Last Modified: 2008-01-30T12:35:00Z},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\4CXS8LEU\\facts.html:text/html},
}

@misc{ethnologue_urdu_nodate,
	title = {Urdu {Language} - {Ethnologue} {Report}},
	url = {https://www.ethnologue.com/language/urdu},
	abstract = {A language profile for Urdu. Get a detailed look at the language, from population to dialects and usage.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Ethnologue},
	author = {Ethnologue},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\WGWY3RRQ\\urd.html:text/html},
}

@article{ali_automatic_2015,
	title = {Automatic speech recognition of {Urdu} words using linear discriminant analysis},
	volume = {28},
	issn = {10641246, 18758967},
	url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IFS-151554},
	doi = {10.3233/IFS-151554},
	number = {5},
	urldate = {2022-08-16},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ali, Hazrat and Ahmad, Nasir and Zhou, Xianwei},
	month = jun,
	year = {2015},
	pages = {2369--2375},
}

@phdthesis{sehar_gul_detecting_2020,
	address = {Karachi},
	title = {{DETECTING} {MALICIOUS} {ACTIVITIES} {OVER} {TELEPHONE} {NETWORK} {FOR} {URDU} {SPEAKER}},
	abstract = {Telephone is one of most important invention in the fields of communication it is because on
this invention that we are able to connect with our friends and families without hassle of travelling
and going to their places,but some people are also using it for negative purpose therefore to secure
this medium of communication is one of the most important issue of today as many malicious
activities are taking place on this channel.Humanely it is not possible to tap each and every phone
call so that one could find malicious activities that are being done.In order to find such malicious
activities we need an automatic system that can automatically detect malicious voice activities,
for that we have decided to develop an automatic speech recognition system that will detect malicious
sentences in Urdu Language from the telephonic conversation which will then be processed
further.},
	school = {National University of Science and Technology - Pakistan Navy Engineering College},
	author = {Sehar Gul},
	month = jun,
	year = {2020},
}

@incollection{hutchison_speaker_2010,
	address = {Berlin, Heidelberg},
	title = {Speaker {Independent} {Urdu} {Speech} {Recognition} {Using} {HMM}},
	volume = {6177},
	isbn = {978-3-642-13880-5 978-3-642-13881-2},
	url = {http://link.springer.com/10.1007/978-3-642-13881-2_14},
	urldate = {2022-08-16},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ashraf, Javed and Iqbal, Naveed and Khattak, Naveed Sarfraz and Zaidi, Ather Mohsin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Hopfe, Christina J. and Rezgui, Yacine and M√©tais, Elisabeth and Preece, Alun and Li, Haijiang},
	year = {2010},
	doi = {10.1007/978-3-642-13881-2_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {140--148},
}

@inproceedings{sarfraz_large_2010,
	address = {Islamabad, Pakistan},
	title = {Large vocabulary continuous speech recognition for {Urdu}},
	isbn = {978-1-4503-0342-2},
	url = {http://portal.acm.org/citation.cfm?doid=1943628.1943629},
	doi = {10.1145/1943628.1943629},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Frontiers} of {Information} {Technology} - {FIT} '10},
	publisher = {ACM Press},
	author = {Sarfraz, Huda and Parveen, Rahila and Hussain, Sarmad and Bokhari, Riffat and Raza, Agha Ali and Ullah, Inam and Sarfraz, Zahid and Pervez, Sophia and Mustafa, Asad and Javed, Iqra},
	year = {2010},
	pages = {1--5},
}

@inproceedings{qasim_urdu_2016,
	address = {Bali, Indonesia},
	title = {Urdu speech recognition system for district names of {Pakistan}: {Development}, challenges and solutions},
	isbn = {978-1-5090-3516-8},
	shorttitle = {Urdu speech recognition system for district names of {Pakistan}},
	url = {http://ieeexplore.ieee.org/document/7918979/},
	doi = {10.1109/ICSDA.2016.7918979},
	urldate = {2022-08-16},
	booktitle = {2016 {Conference} of {The} {Oriental} {Chapter} of {International} {Committee} for {Coordination} and {Standardization} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Qasim, Muhammad and Nawaz, Sohaib and Hussain, Sarmad and Habib, Tania},
	month = oct,
	year = {2016},
	pages = {28--32},
}

@article{aguiar_de_lima_survey_2020,
	title = {A survey on automatic speech recognition systems for {Portuguese} language and its variations},
	volume = {62},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230819302992},
	doi = {10.1016/j.csl.2019.101055},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Aguiar de Lima, Thales and Da Costa-Abreu, M√°rjory},
	month = jul,
	year = {2020},
	pages = {101055},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\5YG7YNP8\\Aguiar de Lima and Da Costa-Abreu - 2020 - A survey on automatic speech recognition systems f.pdf:application/pdf},
}

@inproceedings{dash_automatic_2018,
	title = {Automatic {Speech} {Recognition} with {Articulatory} {Information} and a {Unified} {Dictionary} for {Hindi}, {Marathi}, {Bengali} and {Oriya}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/dash18_interspeech.html},
	doi = {10.21437/Interspeech.2018-2122},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Dash, Debadatta and Kim, Myungjong and Teplansky, Kristin and Wang, Jun},
	month = sep,
	year = {2018},
	pages = {1046--1050},
}

@inproceedings{patil_automatic_2016,
	address = {Pune, India},
	title = {Automatic {Speech} {Recognition} of isolated words in {Hindi} language using {MFCC}},
	isbn = {978-1-5090-1338-8},
	url = {http://ieeexplore.ieee.org/document/7915008/},
	doi = {10.1109/CAST.2016.7915008},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Computing}, {Analytics} and {Security} {Trends} ({CAST})},
	publisher = {IEEE},
	author = {Patil, U. G. and Shirbahadurkar, S. D. and Paithane, A. N.},
	month = dec,
	year = {2016},
	pages = {433--438},
}

@article{mishra_achyuta_and_chandra_mahesh_and_biswas_astik_and_sharan_robust_2011,
	title = {Robust {Features} for {Connected} {Hindi} {Digits} {Recognition}},
	volume = {4},
	number = {2},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {Mishra, Achyuta {and} Chandra, Mahesh {and} Biswas, Astik {and} Sharan},
	month = jun,
	year = {2011},
}

@inproceedings{aggarwal_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	abstract = {The goal of automatic speech recognition (ASR) system is to accurately and efficiently convert a speech signal into a text message independent of the device, speaker or the environment. In general the speech signal is captured and pre-processed at front-end for feature extraction and evaluated at back-end using the Gaussian mixture hidden Markov model. In this statistical approach since the evaluation of Gaussian likelihoods dominate the total computational load, the appropriate selection of Gaussian mixtures is very important depending upon the amount of training data. As the small databases are available to train the Indian languages ASR system, the higher range of Gaussian mixtures (i.e. 64 and above), normally used for European languages, cannot be applied for them. This paper reviews the statistical framework and presents an iterative procedure to select an optimum number of Gaussian mixtures that exhibits maximum accuracy in the context of Hindi speech recognition system.},
	booktitle = {International {Journal} of {Signal} {Processing}, {Image} {Processing} and {Pattern} {Recognition}},
	author = {Aggarwal, R. K. and Dave, M.},
	year = {2011},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\FVSZRDG9\\Aggarwal and Dave - 2011 - Using Gaussian Mixtures for Hindi Speech Recogniti.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\NYC9U7U3\\summary.html:text/html},
}

@inproceedings{r_k_aggarwal_and_m_dave_using_2011,
	title = {Using {Gaussian} {Mixtures} for {Hindi} {Speech} {Recognition} {System}},
	author = {R. K. Aggarwal {and} M. Dave},
	year = {2011},
}

@inproceedings{b_venkataraman_sopc-based_2006,
	title = {{SOPC}-based speech-to-text conversion},
	booktitle = {Embedded {Processor} {Design} {Contest} {Outstanding} {Designs}},
	author = {B. Venkataraman},
	year = {2006},
}

@incollection{tanveer_continuous_2019,
	address = {Singapore},
	title = {Continuous {Hindi} {Speech} {Recognition} {Using} {Kaldi} {ASR} {Based} on {Deep} {Neural} {Network}},
	volume = {748},
	isbn = {9789811309229 9789811309236},
	url = {http://link.springer.com/10.1007/978-981-13-0923-6_26},
	urldate = {2022-08-16},
	booktitle = {Machine {Intelligence} and {Signal} {Analysis}},
	publisher = {Springer Singapore},
	author = {Upadhyaya, Prashant and Mittal, Sanjeev Kumar and Farooq, Omar and Varshney, Yash Vardhan and Abidi, Musiur Raza},
	editor = {Tanveer, M. and Pachori, Ram Bilas},
	year = {2019},
	doi = {10.1007/978-981-13-0923-6_26},
	note = {Series Title: Advances in Intelligent Systems and Computing},
	pages = {303--311},
}

@inproceedings{karel_vesel_and_arnab_ghoshal_and_luk_burget_and_daniel_povey_sequence-discriminative_2013,
	title = {Sequence-discriminative training of deep neural networks},
	author = {Karel Vesel {and} Arnab Ghoshal {and} Luk Burget {and} Daniel Povey},
	year = {2013},
}

@inproceedings{k_v_s_parsad_and_s_m_virk_computational_2012,
	title = {Computational evidence that {Hindi} and {Urdu} share a grammar but not the lexicon},
	booktitle = {3rd {Workshop} on {South} and {Southeast} {Asian} {NLP}},
	author = {K. V. S. Parsad {and} S. M. Virk},
	year = {2012},
}

@article{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1402.1128},
	doi = {10.48550/ARXIV.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2022-08-16},
	author = {Sak, Ha≈üim and Senior, Andrew and Beaufays, Fran√ßoise},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{abdel-hamid_exploring_2013,
	title = {Exploring {Convolutional} {Neural} {Network} {Structures} and {Optimization} {Techniques} for {Speech} {Recognition}},
	url = {https://www.microsoft.com/en-us/research/publication/exploring-convolutional-neural-network-structures-and-optimization-techniques-for-speech-recognition/},
	abstract = {Recently, convolutional neural networks (CNNs) have been shown to outperform the standard fully connected deep neural networks within the hybrid deep neural network / hidden Markov model (DNN/HMM) framework on the phone recognition task. In this paper, we extend the earlier basic form of the CNN and explore it in multiple ways. We first investigate several CNN architectures, including full and limited weight sharing, convolution along frequency and time axes, and stacking of several convolution layers. We then develop a novel weighted softmax pooling layer so that the size in the pooling layer can be automatically learned. Further, we evaluate the effect of CNN pretraining, which is achieved by using a convolutional version of the RBM. We show that all CNN architectures we have investigated outperform the earlier basic form of the DNN on both the phone recognition and large vocabulary speech recognition tasks. The architecture with limited weight sharing provides additional gains over the full weight sharing architecture. The softmax pooling layer performs as well as the best CNN with the manually tuned fixed-pooling size, and has a potential for further improvement. Finally, we show that CNN pretraining produces significantly better results on a large vocabulary speech recognition task.},
	booktitle = {Interspeech 2013},
	publisher = {ISCA},
	author = {Abdel-Hamid, Ossama and Deng, Li and Yu, Dong},
	month = aug,
	year = {2013},
	note = {Edition: Interspeech 2013},
}

@misc{noauthor_gram_nodate,
	title = {{GRAM} {VAANI} {ASR} {Challenge} 2022},
	url = {https://sites.google.com/view/gramvaaniasrchallenge/home},
	abstract = {A challenge on Automatic Speech Recognition for Hindi is being organized as part of INTERSPEECH 2022 by sharing the spontaneous telephone speech recordings collected by a social technology enterprise Gram Vaani. The regional variations of Hindi together with spontaneity of speech, natural},
	language = {en},
	urldate = {2022-08-16},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\P4IHK3L5\\home.html:text/html},
}

@misc{noauthor_gramvaaniorg_nodate,
	title = {gramvaani.org},
	url = {https://gramvaani.org/},
	urldate = {2022-08-16},
}

@inproceedings{latif_cross_2018,
	address = {Islamabad, Pakistan},
	title = {Cross {Lingual} {Speech} {Emotion} {Recognition}: {Urdu} vs. {Western} {Languages}},
	isbn = {978-1-5386-9355-1},
	shorttitle = {Cross {Lingual} {Speech} {Emotion} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/8616972/},
	doi = {10.1109/FIT.2018.00023},
	urldate = {2022-08-16},
	booktitle = {2018 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	publisher = {IEEE},
	author = {Latif, Siddique and Qayyum, Adnan and Usman, Muhammad and Qadir, Junaid},
	month = dec,
	year = {2018},
	pages = {88--93},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\NLRUGBJI\\Latif et al. - 2018 - Cross Lingual Speech Emotion Recognition Urdu vs..pdf:application/pdf},
}

@article{besacier_automatic_2014,
	title = {Automatic speech recognition for under-resourced languages: {A} survey},
	volume = {56},
	issn = {01676393},
	shorttitle = {Automatic speech recognition for under-resourced languages},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988},
	doi = {10.1016/j.specom.2013.07.008},
	language = {en},
	urldate = {2022-08-16},
	journal = {Speech Communication},
	author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
	month = jan,
	year = {2014},
	pages = {85--100},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YRF6UNN8\\Besacier et al. - 2014 - Automatic speech recognition for under-resourced l.pdf:application/pdf},
}

@article{lakshmi_sri_kaldi_2020,
	title = {Kaldi recipe in {Hindi} for word level recognition and phoneme level transcription},
	volume = {171},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050920312606},
	doi = {10.1016/j.procs.2020.04.268},
	language = {en},
	urldate = {2022-08-16},
	journal = {Procedia Computer Science},
	author = {Lakshmi Sri, Karra Venkata and Srinivasan, Mayuka and Nair, Radhika Rajeev and Priya, K. Jeeva and Gupta, Deepa},
	year = {2020},
	pages = {2476--2485},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\5J9VQKIM\\Lakshmi Sri et al. - 2020 - Kaldi recipe in Hindi for word level recognition a.pdf:application/pdf},
}

@misc{qureshi_urdu_2021,
	title = {Urdu {Speech} {Recognition}},
	copyright = {MIT},
	url = {https://github.com/ZoraizQ/urdu-speech-recognition},
	abstract = {Urdu Speech Recognition using Kaldi ASR, by training Triphone Acoustic GMMs using the PRUS dataset.},
	urldate = {2022-08-16},
	author = {Qureshi, Zoraiz},
	month = oct,
	year = {2021},
	note = {original-date: 2021-04-21T10:11:17Z},
	keywords = {speech-recognition, kaldi-asr, multi-speaker, prus, urdu},
}

@inproceedings{asadullah_automatic_2016,
	address = {Portsmouth},
	title = {Automatic {Urdu} {Speech} {Recognition} using {Hidden} {Markov} {Model}},
	isbn = {978-1-5090-3755-1},
	url = {https://ieeexplore.ieee.org/document/7571287/},
	doi = {10.1109/ICIVC.2016.7571287},
	urldate = {2022-08-16},
	booktitle = {2016 {International} {Conference} on {Image}, {Vision} and {Computing} ({ICIVC})},
	publisher = {IEEE},
	author = {{Asadullah} and Shaukat, Arslan and Ali, Hazrat and Akram, Usman},
	month = aug,
	year = {2016},
	pages = {135--139},
}

@misc{chodroff_corpus_2018,
	title = {Corpus {Phonetics} {Tutorial}},
	url = {http://arxiv.org/abs/1811.05553},
	abstract = {Corpus phonetics has become an increasingly popular method of research in linguistic analysis. With advances in speech technology and computational power, large scale processing of speech data has become a viable technique. This tutorial introduces the speech scientist and engineer to various automatic speech processing tools. These include acoustic model creation and forced alignment using the Kaldi Automatic Speech Recognition Toolkit (Povey et al., 2011), forced alignment using FAVE-align (Rosenfelder et al., 2014), the Montreal Forced Aligner (McAuliffe et al., 2017), and the Penn Phonetics Lab Forced Aligner (Yuan \& Liberman, 2008), as well as stop consonant burst alignment using AutoVOT (Keshet et al., 2014). The tutorial provides a general overview of each program, step-by-step instructions for running the program, as well as several tips and tricks.},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Chodroff, Eleanor},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05553 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\3GAPGJTY\\Chodroff - 2018 - Corpus Phonetics Tutorial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5EDZSUNT\\1811.html:text/html},
}

@article{alharbi_automatic_2021,
	title = {Automatic {Speech} {Recognition}: {Systematic} {Literature} {Review}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Automatic {Speech} {Recognition}},
	doi = {10.1109/ACCESS.2021.3112535},
	abstract = {A huge amount of research has been done in the field of speech signal processing in recent years. In particular, there has been increasing interest in the automatic speech recognition (ASR) technology field. ASR began with simple systems that responded to a limited number of sounds and has evolved into sophisticated systems that respond fluently to natural language. This systematic review of automatic speech recognition is provided to help other researchers with the most significant topics published in the last six years. This research will also help in identifying recent major ASR challenges in real-world environments. In addition, it discusses current research gaps in ASR. This review covers articles available in five research databases that were completed according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) protocol. The search strategy yielded 82 conferences and articles related to the study‚Äôs scope for the period 2015‚Äì2020. The results presented in this review shed light on research trends in the area of ASR and also suggest new research directions.},
	journal = {IEEE Access},
	author = {Alharbi, Sadeen and Alrazgan, Muna and Alrashed, Alanoud and Alnomasi, Turkiayh and Almojel, Raghad and Alharbi, Rimah and Alharbi, Saja and Alturki, Sahar and Alshehri, Fatimah and Almojil, Maha},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {ASR challenges, ASR systematic review, automatic speech recognition, Automatic speech recognition, Databases, Licenses, Nails, Quality assessment, Software, Speech recognition, Systematics},
	pages = {131858--131876},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\TLVJTS37\\9536732.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\7LCUHWDI\\Alharbi et al. - 2021 - Automatic Speech Recognition Systematic Literatur.pdf:application/pdf},
}

@article{alsayadi_arabic_2021,
	title = {Arabic speech recognition using end-to-end deep learning},
	volume = {15},
	issn = {1751-9683},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12057},
	doi = {10.1049/sil2.12057},
	abstract = {Arabic automatic speech recognition (ASR) methods with diacritics have the ability to be integrated with other systems better than Arabic ASR methods without diacritics. In this work, the application of state-of-the-art end-to-end deep learning approaches is investigated to build a robust diacritised Arabic ASR. These approaches are based on the Mel-Frequency Cepstral Coefficients and the log Mel-Scale Filter Bank energies as acoustic features. To the best of our knowledge, end-to-end deep learning approach has not been used in the task of diacritised Arabic automatic speech recognition. To fill this gap, this work presents a new CTC-based ASR, CNN-LSTM, and an attention-based end-to-end approach for improving diacritisedArabic ASR. In addition, a word-based language model is employed to achieve better results. The end-to-end approaches applied in this work are based on state-of-the-art frameworks, namely ESPnet and Espresso. Training and testing of these frameworks are performed based on the Standard Arabic Single Speaker Corpus (SASSC), which contains 7 h of modern standard Arabic speech. Experimental results show that the CNN-LSTM with an attention framework outperforms conventional ASR and the Joint CTC-attention ASR framework in the task of Arabic speech recognition. The CNN-LSTM with an attention framework could achieve a word error rate better than conventional ASR and the Joint CTC-attention ASR by 5.24\% and 2.62\%, respectively.},
	language = {en},
	number = {8},
	urldate = {2022-08-16},
	journal = {IET Signal Processing},
	author = {Alsayadi, Hamzah A. and Abdelhamid, Abdelaziz A. and Hegazy, Islam and Fayed, Zaki T.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/sil2.12057},
	pages = {521--534},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\8MD9EIN6\\Alsayadi et al. - 2021 - Arabic speech recognition using end-to-end deep le.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\8NYWWX8A\\sil2.html:text/html},
}

@inproceedings{raza_rapid_2018,
	title = {Rapid {Collection} of {Spontaneous} {Speech} {Corpora} {Using} {Telephonic} {Community} {Forums}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/raza18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1139},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Raza, Agha Ali and Athar, Awais and Randhawa, Shan and Tariq, Zain and Saleem, Muhammad Bilal and Bin Zia, Haris and Saif, Umar and Rosenfeld, Roni},
	month = sep,
	year = {2018},
	pages = {1021--1025},
}

@inproceedings{naeem_subspace_2020,
	title = {Subspace {Gaussian} {Mixture} {Model} for {Continuous} {Urdu} {Speech} {Recognition} using {Kaldi}},
	doi = {10.1109/ICOSST51357.2020.9333026},
	abstract = {Automatic Speech Recognition Systems (ASR) have significantly improved in recent years, where deep learning is playing an important role in the development of end to end ASR's. ASR is the task of converting spoken language into computer readable text. ASRs are becoming ever more prevalent way to interact with technology, thereby significantly closing the gap in terms of how humans interact with computers, making it more natural. Urdu is an under resourced language, for which training such a system requires a huge amount of data that is not readily available. In this paper we present improvements to the architecture of a statistical automatic speech recognition system for which the components involved in a statistical ASR have been explored in great detail. We also present the results on various statistical models that are trained for Urdu language. We choose the Kaldi toolkit for training the Urdu ASR using approximately 100 hours of transcribed data. The refined Subspace Gaussian Model gives a word error rate of 9\% on the test set.},
	booktitle = {2020 14th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Naeem, Saad and Iqbal, Majid and Saqib, Muhammad and Saad, Muhammad and Raza, Muhammad Soban and Ali, Zaid and Akhtar, Naveed and Beg, Mirza Omer and Shahzad, Waseem and Arshad, Muhhamad Umair},
	month = dec,
	year = {2020},
	keywords = {Adaptation models, Analytical models, Automatic Speech Recognition, Computational modeling, Context modeling, Hidden Markov models, Hidden Markov Models, Mel-frequency Cepstrum, Probabilistic logic, Subspace Gaussian Mixture Models, Training},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\3KACECHI\\9333026.html:text/html},
}

@inproceedings{khan_multi-genre_2021,
	address = {Singapore, Singapore},
	title = {A {Multi}-{Genre} {Urdu} {Broadcast} {Speech} {Recognition} {System}},
	isbn = {978-1-66540-870-7},
	url = {https://ieeexplore.ieee.org/document/9660552/},
	doi = {10.1109/O-COCOSDA202152914.2021.9660552},
	urldate = {2022-08-16},
	booktitle = {2021 24th {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	publisher = {IEEE},
	author = {Khan, Erbaz and Rauf, Sahar and Adeeba, Farah and Hussain, Sarmad},
	month = nov,
	year = {2021},
	pages = {25--30},
}

@incollection{somogyi_automatic_2021,
	address = {Cham},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-3-030-60031-0 978-3-030-60032-7},
	url = {http://link.springer.com/10.1007/978-3-030-60032-7_5},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Application} of {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Somogyi, Zolt√°n},
	collaborator = {Somogyi, Zolt√°n},
	year = {2021},
	doi = {10.1007/978-3-030-60032-7_5},
	pages = {145--171},
}

@incollection{chapelle_automatic_2020,
	edition = {1},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4051-9473-0 978-1-4051-9843-1},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781405198431.wbeal0066.pub2},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {The {Encyclopedia} of {Applied} {Linguistics}},
	publisher = {Wiley},
	author = {Levis, John and Suvorov, Ruslan},
	editor = {Chapelle, Carol A.},
	month = dec,
	year = {2020},
	doi = {10.1002/9781405198431.wbeal0066.pub2},
	pages = {1--8},
}

@incollection{gold_brief_2011,
	address = {Hoboken, NJ, USA},
	title = {Brief {History} of {Automatic} {Speech} {Recognition}},
	isbn = {978-1-118-14288-2 978-0-470-19536-9},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118142882.ch4},
	urldate = {2022-08-16},
	booktitle = {Speech and {Audio} {Signal} {Processing}},
	publisher = {John Wiley \& Sons, Inc.},
	collaborator = {Gold, Ben and Morgan, Nelson and Ellis, Dan},
	month = oct,
	year = {2011},
	doi = {10.1002/9781118142882.ch4},
	pages = {40--58},
}

@misc{kincaid_brief_2018,
	title = {A {Brief} {History} of {ASR}: {Automatic} {Speech} {Recognition}},
	shorttitle = {A {Brief} {History} of {ASR}},
	url = {https://medium.com/descript/a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5},
	abstract = {This is the first post in a series on Automatic Speech Recognition, the foundational technology that makes Descript possible. We‚Äôll be‚Ä¶},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\FHD2LASC\\a-brief-history-of-asr-automatic-speech-recognition-b8f338d4c0e5.html:text/html},
}

@article{suma_swamy_evolution_2013,
	title = {Evolution of {Speech} {Recognition} ‚Äì {A} {Brief} {History} of {Technology} {Development}},
	volume = {60},
	journal = {Elixir Adv. Engg. Info.},
	author = {Suma Swamy and Ramakrishnan, Kollengode},
	month = jul,
	year = {2013},
}

@article{smit_advances_2021,
	title = {Advances in subword-based {HMM}-{DNN} speech recognition across languages},
	volume = {66},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300917},
	doi = {10.1016/j.csl.2020.101158},
	language = {en},
	urldate = {2022-08-16},
	journal = {Computer Speech \& Language},
	author = {Smit, Peter and Virpioja, Sami and Kurimo, Mikko},
	month = mar,
	year = {2021},
	pages = {101158},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\TC6HIS7B\\Smit et al. - 2021 - Advances in subword-based HMM-DNN speech recogniti.pdf:application/pdf},
}

@misc{kincaid_state_2018,
	title = {The {State} of {Automatic} {Speech} {Recognition}: {Q}\&{A} with {Kaldi}‚Äôs {Dan} {Povey}},
	shorttitle = {The {State} of {Automatic} {Speech} {Recognition}},
	url = {https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85},
	abstract = {This article continues our series on Automatic Speech Recognition, including our recent piece on the History of ASR.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Descript},
	author = {Kincaid, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\AI8J5D9U\\the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85.html:text/html},
}

@misc{blog_machine_2022,
	title = {Machine {Learning} {Models} {Are} {Only} as {Good} as the {Data} {They} {Are} {Trained} {On}},
	url = {https://deepchecks.com/machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on/},
	abstract = {Learn about the importance of data validation in machine learning, look into the various tools for different sets of data validation techniques \& procedures.},
	language = {en},
	urldate = {2022-08-16},
	journal = {Deepchecks},
	author = {Blog, Deepchecks Community},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5T5L9UMP\\machine-learning-models-are-only-as-good-as-the-data-they-are-trained-on.html:text/html},
}

@misc{noauthor_machine_2018,
	title = {‚Äú{A} machine learning model is only as good as the data it is fed‚Äù},
	url = {https://devm.io/machine-learning/apache-spark-machine-learning-interview-143122},
	abstract = {Apache Spark 2.3 was released earlier this year; it marked a major milestone for Structured Streaming but there are a lot of other interesting features that deserve your attention. We talked with Reynold Xin, co-founder and Chief Architect at Databricks about the Databricks Runtime and other enhancements introduced in Apache Spark 2.3.},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {devmio - expand your knowledge},
	month = jun,
	year = {2018},
	note = {Section: Artikel},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HS46J6C4\\apache-spark-machine-learning-interview-143122.html:text/html},
}

@misc{brownlee_why_2020,
	title = {Why {Data} {Preparation} {Is} {So} {Important} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/data-preparation-is-important/},
	abstract = {On a predictive modeling project, machine learning algorithms learn a mapping from input variables to a target variable. The most common form of predictive modeling project involves so-called structured data or tabular data. This is data as it looks in a spreadsheet or a matrix, with rows of examples and columns of features for each [‚Ä¶]},
	language = {en-US},
	urldate = {2022-08-16},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\HGMCR7CK\\data-preparation-is-important.html:text/html},
}

@book{ilyas_data_2019,
	address = {New York, NY},
	title = {Data {Cleaning}},
	isbn = {978-1-4503-7153-7},
	language = {English},
	publisher = {ACM Books},
	author = {Ilyas, Ihab F. and Chu, Xu},
	month = jun,
	year = {2019},
}

@misc{khan_introduction_2021,
	title = {An {Introduction} to {Classification} {Using} {Mislabeled} {Data}},
	url = {https://towardsdatascience.com/an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5},
	abstract = {The performance of any classifier, or for that matter any machine learning task, depends crucially on the quality of the available data‚Ä¶},
	language = {en},
	urldate = {2022-08-16},
	journal = {Medium},
	author = {Khan, Shihab Shahriar},
	month = mar,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SY93L99I\\an-introduction-to-classification-using-mislabeled-data-581a6c09f9f5.html:text/html},
}

@article{cao_joint_2019,
	title = {Joint {Prostate} {Cancer} {Detection} and {Gleason} {Score} {Prediction} in mp-{MRI} via {FocalNet}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8653866/},
	doi = {10.1109/TMI.2019.2901928},
	number = {11},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Cao, Ruiming and Mohammadian Bajgiran, Amirhossein and Afshari Mirak, Sohrab and Shakeri, Sepideh and Zhong, Xinran and Enzmann, Dieter and Raman, Steven and Sung, Kyunghyun},
	month = nov,
	year = {2019},
	pages = {2496--2506},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\WK7STA45\\Cao et al. - 2019 - Joint Prostate Cancer Detection and Gleason Score .pdf:application/pdf},
}

@article{fan_impact_2021,
	title = {The {Impact} of {Mislabeled} {Changes} by {SZZ} on {Just}-in-{Time} {Defect} {Prediction}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2019.2929761},
	abstract = {Just-in-Time (JIT) defect prediction-a technique which aims to predict bugs at change level-has been paid more attention. JIT defect prediction leverages the SZZ approach to identify bug-introducing changes. Recently, researchers found that the performance of SZZ (including its variants) is impacted by a large amount of noise. SZZ may considerably mislabel changes that are used to train a JIT defect prediction model, and thus impact the prediction accuracy. In this paper, we investigate the impact of the mislabeled changes by different SZZ variants on the performance and interpretation of JIT defect prediction models. We analyze four SZZ variants (i.e., B-SZZ, AG-SZZ, MA-SZZ, and RA-SZZ) that are proposed by prior studies. We build the prediction models using the labeled data by these four SZZ variants. Among the four SZZ variants, RA-SZZ is least likely to generate mislabeled changes, and we construct the testing set by using RA-SZZ. All of the four prediction models are then evaluated on the same testing set. We choose the prediction model built on the labeled data by RA-SZZ as the baseline model, and we compare the performance and metric importance of the models trained using the labeled data by the other three SZZ variants with the baseline model. Through a large-scale empirical study on a total of 126,526 changes from ten Apache open source projects, we find that in terms of various performance measures (AUC, F1-score, G-mean and Recall@20\%), the mislabeled changes by B-SZZ and MA-SZZ are not likely to cause a considerable performance reduction, while the mislabeled changes by AG-SZZ cause a statistically significant performance reduction with an average difference of 1-5 percent. When considering developers' inspection effort (measured by LOC) in practice, the changes mislabeled B-SZZ and AG-SZZ lead to 9-10 and 1-15 percent more wasted inspection effort, respectively. And the mislabeled changes by B-SZZ lead to significantly more wasted effort. The mislabeled changes by MA-SZZ do not cause considerably more wasted effort. We also find that the top-most important metric for identifying bug-introducing changes (i.e., number of files modified in a change) is robust to the mislabeling noise generated by SZZ. But the second- and third-most important metrics are more likely to be impacted by the mislabeling noise, unless random forest is used as the underlying classifier.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fan, Yuanrui and Xia, Xin and da Costa, Daniel Alencar and Lo, David and Hassan, Ahmed E. and Li, Shanping},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Analytical models, Computer bugs, Data models, Inspection, Just-in-time defect prediction, Measurement, mining software repositories, noisy data, Predictive models, SZZ, Testing},
	pages = {1559--1586},
}

@inproceedings{van_rooyen_learning_2015,
	title = {Learning with {Symmetric} {Label} {Noise}: {The} {Importance} of {Being} {Unhinged}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {van Rooyen, Brendan and Menon, Aditya and Williamson, Robert C},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{natarajan_learning_2013,
	title = {Learning with {Noisy} {Labels}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@article{frenay_classification_2014,
	title = {Classification in the {Presence} of {Label} {Noise}: {A} {Survey}},
	volume = {25},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Classification in the {Presence} of {Label} {Noise}},
	url = {http://ieeexplore.ieee.org/document/6685834/},
	doi = {10.1109/TNNLS.2013.2292894},
	number = {5},
	urldate = {2022-08-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Frenay, Benoit and Verleysen, Michel},
	month = may,
	year = {2014},
	pages = {845--869},
}

@misc{zhang_uberi_speechrecognition_nodate,
	title = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
	copyright = {BSD License},
	shorttitle = {{SpeechRecognition}},
	url = {https://github.com/Uberi/speech_recognition#readme},
	urldate = {2022-08-16},
	author = {Zhang (Uberi), Anthony},
	keywords = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\9RSTEWBT\\SpeechRecognition.html:text/html},
}

@inproceedings{panayotov_librispeech_2015,
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	shorttitle = {Librispeech},
	doi = {10.1109/ICASSP.2015.7178964},
	abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {Bioinformatics, Blogs, Corpus, Electronic publishing, Genomics, Information services, LibriVox, Resource description framework, Speech Recognition},
	pages = {5206--5210},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\2AK7VZ3R\\7178964.html:text/html},
}

@article{backstrom_introduction_2022,
	title = {Introduction to {Speech} {Processing}: 2nd {Edition}},
	copyright = {Open Access},
	shorttitle = {Introduction to {Speech} {Processing}},
	url = {https://zenodo.org/record/6821775},
	doi = {10.5281/ZENODO.6821775},
	abstract = {This release is primarily about migrating all content to jupyter-books and git. The published version is now hosted at https://speechprocessingbook.aalto.fi. In addition to github, the release has long-term storage location at Zenodo, which also assigns a DOI to the release. We have some entirely new sections, such as Forensic speaker recognition. There are also plenty of small improvements everywhere.},
	language = {en},
	urldate = {2022-08-16},
	author = {B√§ckstr√∂m, Tom and R√§s√§nen, Okko and Zewoudie, Abraham and Zarazaga, Pablo P√©rez and Koivusalo, Liisa and Das, Sneha and Mellado, Esteban G√≥mez and Mariem Bouafif Mansali and Ramos, Daniel},
	month = jul,
	year = {2022},
	note = {Publisher: Zenodo
Version Number: v2},
	keywords = {speech processing},
}

@book{yu_automatic_2015,
	address = {London},
	series = {Signals and {Communication} {Technology}},
	title = {Automatic {Speech} {Recognition}},
	isbn = {978-1-4471-5778-6 978-1-4471-5779-3},
	url = {http://link.springer.com/10.1007/978-1-4471-5779-3},
	urldate = {2022-08-16},
	publisher = {Springer London},
	author = {Yu, Dong and Deng, Li},
	year = {2015},
	doi = {10.1007/978-1-4471-5779-3},
}

@article{park_review_2021,
	title = {A {Review} of {Speaker} {Diarization}: {Recent} {Advances} with {Deep} {Learning}},
	volume = {72},
	copyright = {arXiv.org perpetual, non-exclusive license},
	issn = {101317},
	shorttitle = {A {Review} of {Speaker} {Diarization}},
	url = {https://arxiv.org/abs/2101.09624},
	doi = {10.48550/ARXIV.2101.09624},
	abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
	urldate = {2022-08-17},
	journal = {Computer Speech \& Language},
	author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@misc{juang_automatic_nodate,
	title = {Automatic {Speech} {Recognition} ‚Äì {A} {Brief} {History} of the {Technology} {Development} {Abstract}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis [1, 2], the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. H. and Rabiner, Lawrence R.},
	file = {Citeseer - Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\3YU9F3ID\\Juang and Rabiner - Automatic Speech Recognition ‚Äì A Brief History of .pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\I34GUPHY\\summary.html:text/html},
}

@incollection{maglogiannis__2020,
	address = {Cham},
	title = {Œë {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4 978-3-030-49161-1},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\K3RKXDZZ\\Filippidou and Moussiades - 2020 - Œë Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@inproceedings{morris_wer_2004,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{anusuya_speech_2010,
	title = {Speech {Recognition} by {Machine}, {A} {Review}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1001.2267},
	doi = {10.48550/ARXIV.1001.2267},
	abstract = {This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.},
	urldate = {2022-08-17},
	author = {Anusuya, M. A. and Katti, S. K.},
	year = {2010},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{upton_speech_1984,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages ‚Äî this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-17},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@article{davis_automatic_1952,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@inproceedings{maja_popovic_hjerson_2011,
	title = {Hjerson: {An} {Open} {Source} {Tool} for {Automatic} {Error} {Classification} of {Machine} {Translation} {Output}},
	author = {Maja Popovic},
	year = {2011},
}

@article{errattahi_automatic_2018,
	title = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}: {A} {Review}},
	volume = {128},
	issn = {18770509},
	shorttitle = {Automatic {Speech} {Recognition} {Errors} {Detection} and {Correction}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918302187},
	doi = {10.1016/j.procs.2018.03.005},
	language = {en},
	urldate = {2022-08-17},
	journal = {Procedia Computer Science},
	author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
	year = {2018},
	pages = {32--37},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\YWBD74CJ\\Errattahi et al. - 2018 - Automatic Speech Recognition Errors Detection and .pdf:application/pdf},
}

@inproceedings{povey_daniel_and_ghoshal_arnab_and_boulianne_gilles_and_burget_lukas_and_glembek_ondrej_and_goel_nagendra_and_hannemann_mirko_and_motlicek_petr_and_qian_yanmin_and_schwarz_petr__and_others_kaldi_2011,
	title = {The {Kaldi} speech recognition toolkit},
	booktitle = {Workshop on automatic speech recognition and understanding},
	author = {Povey, Daniel {and} Ghoshal, Arnab {and} Boulianne, Gilles {and} Burget, Lukas {and} Glembek, Ondrej {and} Goel, Nagendra {and} Hannemann, Mirko {and} Motlicek, Petr {and} Qian, Yanmin {and} Schwarz, Petr  {and} others},
	year = {2011},
}

@misc{sutherland_short_2017,
	title = {A short history of speech recognition},
	url = {https://medium.com/@sutherlandjamie/a-short-history-of-speech-recognition-9b8e78ad086a},
	abstract = {There has been more progress in speech recognition technology in the last 3 years than in the first 30 years. Computing power and‚Ä¶},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Sutherland, Jamie},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\LAELJJKS\\a-short-history-of-speech-recognition-9b8e78ad086a.html:text/html},
}

@inproceedings{xiong_microsoft_2017,
	address = {New Orleans, LA},
	title = {The microsoft 2016 conversational speech recognition system},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953159/},
	doi = {10.1109/ICASSP.2017.7953159},
	urldate = {2022-08-17},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Xiong, W. and Droppo, J. and Huang, X. and Seide, F. and Seltzer, M. and Stolcke, A. and Yu, D. and Zweig, G.},
	month = mar,
	year = {2017},
	pages = {5255--5259},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\ZVTCMYVF\\Xiong et al. - 2017 - The microsoft 2016 conversational speech recogniti.pdf:application/pdf},
}

@misc{qi_benchmarking_2021,
	title = {Benchmarking {Commercial} {Intent} {Detection} {Services} with {Practice}-{Driven} {Evaluations}},
	url = {http://arxiv.org/abs/2012.03929},
	abstract = {Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users' text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the intent detection models can see a different distribution of test data when being deployed in the real world, leading to poor accuracy. Finally, a practical intent detection model must be computationally efficient in both training and single query inference so that it can be used continuously and re-trained frequently. We benchmark intent detection methods on a variety of datasets. Our results show that Watson Assistant's intent detection model outperforms other commercial solutions and is comparable to large pretrained language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ.},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Qi, Haode and Pan, Lin and Sood, Atin and Shah, Abhishek and Kunc, Ladislav and Yu, Mo and Potdar, Saloni},
	month = jun,
	year = {2021},
	note = {arXiv:2012.03929 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\XMUN9XIG\\Qi et al. - 2021 - Benchmarking Commercial Intent Detection Services .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5JX39X9L\\2012.html:text/html},
}

@inproceedings{arora_hint3_2020,
	address = {Online},
	title = {{HINT3}: {Raising} the bar for {Intent} {Detection} in the {Wild}},
	shorttitle = {{HINT3}},
	url = {https://www.aclweb.org/anthology/2020.insights-1.16},
	doi = {10.18653/v1/2020.insights-1.16},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {Proceedings of the {First} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Arora, Gaurav and Jain, Chirag and Chaturvedi, Manas and Modi, Krupal},
	year = {2020},
	pages = {100--105},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\SYABT9YP\\Arora et al. - 2020 - HINT3 Raising the bar for Intent Detection in the.pdf:application/pdf},
}

@misc{patel_data-centric_2021,
	title = {Data-{Centric} {Approach} vs {Model}-{Centric} {Approach} in {Machine} {Learning}},
	url = {https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning},
	abstract = {Code and data are the foundations of the AI system. Both of these components play an important role in the development of a robust model but which one should you focus on more? In this article, we‚Äôll go through the data-centric vs model-centric approaches, and see which one is better, we would also talk about [‚Ä¶]},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {neptune.ai},
	author = {Patel, Harshil},
	month = dec,
	year = {2021},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\ZT7NNEUA\\data-centric-vs-model-centric-machine-learning.html:text/html},
}

@misc{radecic_data-centric_2022,
	title = {Data-centric vs. {Model}-centric {AI}? {The} {Answer} is {Clear}},
	shorttitle = {Data-centric vs. {Model}-centric {AI}?},
	url = {https://towardsdatascience.com/data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67},
	abstract = {There‚Äôs something wrong with the current approach to AI. But there‚Äôs a solution.},
	language = {en},
	urldate = {2022-08-17},
	journal = {Medium},
	author = {Radeƒçiƒá, Dario},
	month = mar,
	year = {2022},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\X4DU4GPV\\data-centric-vs-model-centric-ai-the-answer-is-clear-4b607c58af67.html:text/html},
}

@misc{noauthor_chat_nodate,
	title = {A {Chat} with {Andrew} on {MLOps}: {From} {Model}-centric to {Data}-centric {AI} - {YouTube}},
	url = {https://www.youtube.com/watch?v=06-AZXmwHjo&t=69s},
	urldate = {2022-08-17},
}

@misc{noauthor_data-centric_nodate,
	title = {Data-centric {Machine} {Learning}: {Making} customized {ML} solutions production-ready},
	shorttitle = {Data-centric {Machine} {Learning}},
	url = {https://dida.do/blog/data-centric-machine-learning},
	abstract = {In this article, we will see why many ML Projects do not make it into production, introduce the concepts of model- and data-centric ML, and give examples how we at dida improve projects by applying data-centric techniques.},
	language = {en},
	urldate = {2022-08-17},
	journal = {dida Machine Learning},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\F2569RZH\\data-centric-machine-learning.html:text/html},
}

@misc{noauthor_significance_nodate,
	title = {The {Significance} of {Data}-centric {AI}},
	url = {https://www.kdnuggets.com/the-significance-of-data-centric-ai.html/},
	abstract = {How a systematic way of maintaining data quality can do wonders to your model performance.},
	language = {en-US},
	urldate = {2022-08-17},
	journal = {KDnuggets},
	note = {Section: KDnuggets Originals},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\SGUDCHX6\\significance-data-centric-ai.html:text/html},
}

@misc{deeplearningai_data-centric_2021,
	title = {Data-centric {AI}: {Real} {World} {Approaches}},
	shorttitle = {Data-centric {AI}},
	url = {https://www.youtube.com/watch?v=Yqj7Kyjznh4},
	urldate = {2022-08-17},
	author = {{DeepLearningAI}},
	month = aug,
	year = {2021},
}

@article{creutz_morph-based_2007,
	title = {Morph-based speech recognition and modeling of out-of-vocabulary words across languages},
	volume = {5},
	issn = {1550-4875, 1550-4883},
	url = {https://dl.acm.org/doi/10.1145/1322391.1322394},
	doi = {10.1145/1322391.1322394},
	abstract = {We explore the use of morph-based language models in large-vocabulary continuous-speech recognition systems across four so-called morphologically rich languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. The morphs are subword units discovered in an unsupervised, data-driven way using the
              Morfessor
              algorithm. By estimating
              n
              -gram language models over sequences of morphs instead of words, the quality of the language model is improved through better vocabulary coverage and reduced data sparsity. Standard word models suffer from high out-of-vocabulary (OOV) rates, whereas the morph models can recognize previously unseen word forms by concatenating morphs. It is shown that the morph models do perform fairly well on OOVs without compromising the recognition accuracy on in-vocabulary words. The Arabic experiment constitutes the only exception since here the standard word model outperforms the morph model. Differences in the datasets and the amount of data are discussed as a plausible explanation.},
	language = {en},
	number = {1},
	urldate = {2022-08-18},
	journal = {ACM Transactions on Speech and Language Processing},
	author = {Creutz, Mathias and Hirsim√§ki, Teemu and Kurimo, Mikko and Puurula, Antti and Pylkk√∂nen, Janne and Siivola, Vesa and Varjokallio, Matti and Arisoy, Ebru and Sara√ßlar, Murat and Stolcke, Andreas},
	month = dec,
	year = {2007},
	pages = {1--29},
}

@misc{noauthor_mfcc_nodate,
	title = {{MFCC} vs {FBANK} for chain models ?},
	url = {https://groups.google.com/g/kaldi-help/c/_7hB74HKhC4},
	urldate = {2022-08-18},
	file = {MFCC vs FBANK for chain models ?:C\:\\Users\\DELL\\Zotero\\storage\\5FKKZNEQ\\_7hB74HKhC4.html:text/html},
}

@book{reithaug_orchestrating_2002,
	title = {Orchestrating {Success} in {Reading}},
	isbn = {978-0-9694974-4-8},
	url = {https://books.google.com.pk/books?id=\_YGZMQAACAAJ},
	publisher = {Stirling Head Enterprises},
	author = {Reithaug, D.},
	year = {2002},
}

@article{zia_pronouncur_2018,
	title = {{PronouncUR}: {An} {Urdu} {Pronunciation} {Lexicon} {Generator}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{PronouncUR}},
	url = {https://arxiv.org/abs/1801.00409},
	doi = {10.48550/ARXIV.1801.00409},
	abstract = {State-of-the-art speech recognition systems rely heavily on three basic components: an acoustic model, a pronunciation lexicon and a language model. To build these components, a researcher needs linguistic as well as technical expertise, which is a barrier in low-resource domains. Techniques to construct these three components without having expert domain knowledge are in great demand. Urdu, despite having millions of speakers all over the world, is a low-resource language in terms of standard publically available linguistic resources. In this paper, we present a grapheme-to-phoneme conversion tool for Urdu that generates a pronunciation lexicon in a form suitable for use with speech recognition systems from a list of Urdu words. The tool predicts the pronunciation of words using a LSTM-based model trained on a handcrafted expert lexicon of around 39,000 words and shows an accuracy of 64\% upon internal evaluation. For external evaluation on a speech recognition task, we obtain a word error rate comparable to one achieved using a fully handcrafted expert lexicon.},
	urldate = {2022-08-18},
	author = {Zia, Haris Bin and Raza, Agha Ali and Athar, Awais},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@book{jurafsky_speech_2009,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-187321-6},
	shorttitle = {Speech and language processing},
	publisher = {Pearson Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
	note = {OCLC: 213375806},
	keywords = {Automatic speech recognition, Computational linguistics},
}

@article{likhomanenko_rethinking_2020,
	title = {Rethinking {Evaluation} in {ASR}: {Are} {Our} {Models} {Robust} {Enough}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Rethinking {Evaluation} in {ASR}},
	url = {https://arxiv.org/abs/2010.11745},
	doi = {10.48550/ARXIV.2010.11745},
	abstract = {Is pushing numbers on a single benchmark valuable in automatic speech recognition? Research results in acoustic modeling are typically evaluated based on performance on a single dataset. While the research community has coalesced around various benchmarks, we set out to understand generalization performance in acoustic modeling across datasets - in particular, if models trained on a single dataset transfer to other (possibly out-of-domain) datasets. We show that, in general, reverberative and additive noise augmentation improves generalization performance across domains. Further, we demonstrate that when a large enough set of benchmarks is used, average word error rate (WER) performance over them provides a good proxy for performance on real-world noisy data. Finally, we show that training a single acoustic model on the most widely-used datasets - combined - reaches competitive performance on both research and real-world benchmarks.},
	urldate = {2022-08-19},
	author = {Likhomanenko, Tatiana and Xu, Qiantong and Pratap, Vineel and Tomasello, Paden and Kahn, Jacob and Avidov, Gilad and Collobert, Ronan and Synnaeve, Gabriel},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD), 68T07, 68T10, I.2.6; I.5.4},
}

@incollection{hutchison_illustrated_2013,
	address = {Berlin, Heidelberg},
	title = {An {Illustrated} {Methodology} for {Evaluating} {ASR} {Systems}},
	volume = {7836},
	isbn = {978-3-642-37424-1 978-3-642-37425-8},
	url = {http://link.springer.com/10.1007/978-3-642-37425-8_3},
	urldate = {2022-08-19},
	booktitle = {Adaptive {Multimedia} {Retrieval}. {Large}-{Scale} {Multimedia} {Retrieval} and {Evaluation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gonz√°lez, Mar√≠a and Moreno, Juli√°n and Mart√≠nez, Jos√© Luis and Mart√≠nez, Paloma},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Detyniecki, Marcin and Garc√≠a-Serrano, Ana and N√ºrnberger, Andreas and Stober, Sebastian},
	year = {2013},
	doi = {10.1007/978-3-642-37425-8_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {33--42},
	file = {Accepted Version:C\:\\Users\\DELL\\Zotero\\storage\\XVRNLJ7F\\Gonz√°lez et al. - 2013 - An Illustrated Methodology for Evaluating ASR Syst.pdf:application/pdf},
}

@phdthesis{zhang_strategies_2019,
	type = {Thesis},
	title = {Strategies for {Handling} {Out}-of-{Vocabulary} {Words} in {Automatic} {Speech} {Recognition}},
	url = {https://jscholarship.library.jhu.edu/handle/1774.2/62275},
	abstract = {Nowadays, most ASR (automatic speech recognition) systems deployed in  industry are closed-vocabulary systems, meaning we have a limited vocabulary of words the system can recognize, and where pronunciations are provided to the system. Words out of this vocabulary are called out-of-vocabulary (OOV) words, for which either pronunciations or both spellings and pronunciations are not known to the system. The basic motivations of developing strategies to handle OOV words are: First, in the training phase, missing or wrong pronunciations of words in training data results in poor acoustic models. Second, in the test phase, words out of the vocabulary cannot be recognized at all, and mis-recognition of OOV words may affect recognition performance of its in-vocabulary neighbors as well. Therefore, this dissertation is dedicated to exploring strategies of handling OOV words in closed-vocabulary ASR. 

First, we investigate dealing with OOV words in ASR training data, by introducing an acoustic-data driven pronunciation learning framework using a likelihood-reduction based criterion for selecting pronunciation candidates from multiple sources, i.e. standard grapheme-to-phoneme algorithms (G2P) and phonetic decoding, in a greedy fashion. This framework effectively expands a small hand-crafted pronunciation lexicon to cover OOV words, for which the learned pronunciations have higher quality than approaches using G2P alone or using other baseline pruning criteria. Furthermore, applying the proposed framework to generate alternative pronunciations for in-vocabulary (IV) words improves both recognition performance on relevant words and overall acoustic model performance.

Second, we investigate dealing with OOV words in ASR test data, i.e. OOV detection and recovery. We first conduct a comparative study of a hybrid lexical model (HLM) approach for OOV detection, and several baseline approaches, with the conclusion that the HLM approach outperforms others in both OOV detection and first pass OOV recovery performance. Next, we introduce a grammar-decoding framework for efficient second pass OOV recovery, showing that with properly designed schemes of estimating OOV unigram probabilities, the framework significantly improves OOV recovery and overall decoding performance compared to first pass decoding.

Finally we propose an open-vocabulary word-level recurrent neural network language model (RNNLM) re-scoring framework, making it possible to re-score lattices containing recovered OOVs using a single word-level RNNLM, that was ignorant of OOVs when it was trained. Above all, the whole OOV recovery pipeline shows the potential of a highly efficient open-vocabulary word-level ASR decoding framework, tightly integrated into a standard WFST decoding pipeline.},
	language = {en\_US},
	urldate = {2022-08-19},
	school = {Johns Hopkins University},
	author = {Zhang, Xiaohui},
	month = oct,
	year = {2019},
	note = {Accepted: 2020-02-06T04:08:11Z},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\6E6BV6R6\\Zhang - 2019 - Strategies for Handling Out-of-Vocabulary Words in.pdf:application/pdf;Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\QLFGJMPH\\62275.html:text/html},
}

@article{zhang_hello_2017,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Sound (cs.SD)},
}

@article{morgan_continuous_1995,
	title = {Continuous speech recognition},
	volume = {12},
	issn = {1558-0792},
	doi = {10.1109/79.382443},
	abstract = {The authors focus on a tutorial description of the hybrid HMM/ANN method. The approach has been applied to large vocabulary continuous speech recognition, and variants are in use by many researchers, The method provides a mechanism for incorporating a range of sources of evidence without strong assumptions about their joint statistics, and may have applicability to much more complex systems that can incorporate deep acoustic and linguistic context. The method is inherently discriminant and conservative of parameters. Despite these potential advantages, the hybrid method has focused on implementing fairly simple systems, which do surprisingly well on large continuous speech recognition tasks, Researchers are only beginning to explore the use of more complex structures with this paradigm. In particular, they are just beginning to look at the connectionist inference of language models (including phonology) from data, which may be required in order to take advantage of locally discriminant probabilities rather than simply translating to likelihoods. Finally, the authors' current intuition is that more advanced versions of the hybrid method can greatly benefit from a perceptual perspective.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Signal Processing Magazine},
	author = {Morgan, N. and Bourlard, H.},
	month = may,
	year = {1995},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Speech recognition, Hidden Markov models, Statistics, Vocabulary},
	pages = {24--42},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\SIWUUI5M\\382443.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\99KFFZGH\\Morgan and Bourlard - 1995 - Continuous speech recognition.pdf:application/pdf},
}

@inproceedings{nautsch_gdpr_2019,
	title = {The {GDPR} \& {Speech} {Data}: {Reflections} of {Legal} and {Technology} {Communities}, {First} {Steps} {Towards} a {Common} {Understanding}},
	shorttitle = {The {GDPR} \& {Speech} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html},
	doi = {10.21437/Interspeech.2019-2647},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Nautsch, Andreas and Jasserand, Catherine and Kindt, Els and Todisco, Massimiliano and Trancoso, Isabel and Evans, Nicholas},
	month = sep,
	year = {2019},
	pages = {3695--3699},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\V8ZYWF7E\\Nautsch et al. - 2019 - The GDPR & Speech Data Reflections of Legal and T.pdf:application/pdf},
}

@misc{noauthor_spsc_nodate,
	title = {{SPSC} {\textbar} {ISCA} {SIG}-{SPSC}},
	url = {https://www.spsc-sig.org/},
	urldate = {2022-08-19},
}

@misc{noauthor_mozilla_nodate,
	title = {Mozilla {Common} {Voice}},
	url = {https://commonvoice.mozilla.org/},
	language = {en},
	urldate = {2022-08-19},
	file = {Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\2ZVA6IM8\\datasets.html:text/html},
}

@article{schertz_acoustic_2020,
	title = {Acoustic cues in production and perception of the four-way stop laryngeal contrast in {Hindi} and {Urdu}},
	volume = {81},
	issn = {00954470},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009544702030070X},
	doi = {10.1016/j.wocn.2020.100979},
	language = {en},
	urldate = {2022-08-19},
	journal = {Journal of Phonetics},
	author = {Schertz, Jessamyn and Khan, Sarah},
	month = jul,
	year = {2020},
	pages = {100979},
}

@incollection{dua_urdu_2006,
	title = {Urdu},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022446},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02244-6},
	pages = {269--275},
}

@incollection{dua_hindustani_2006,
	title = {Hindustani},
	isbn = {978-0-08-044854-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542022203},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Dua, H.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/02220-3},
	pages = {309--312},
}

@incollection{annamalai_india_2006,
	title = {India: {Language} {Situation}},
	isbn = {978-0-08-044854-1},
	shorttitle = {India},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0080448542046113},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics}},
	publisher = {Elsevier},
	author = {Annamalai, E.},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/04611-3},
	pages = {610--613},
}

@misc{noauthor_morfessor_nodate,
	title = {Morfessor 2.0 documentation ‚Äî {Morfessor} 2.0.4 documentation},
	url = {https://morfessor.readthedocs.io/en/latest/},
	urldate = {2022-08-19},
}

@misc{noauthor_morpho_nodate,
	title = {Morpho project},
	url = {http://morpho.aalto.fi/projects/morpho/},
	urldate = {2022-08-19},
	file = {Morpho project:C\:\\Users\\DELL\\Zotero\\storage\\435PUDQA\\morpho.html:text/html},
}

@inproceedings{farooq_enhancing_2020,
	title = {Enhancing {Large} {Vocabulary} {Continuous} {Speech} {Recognition} {System} for {Urdu}-{English} {Conversational} {Code}-{Switched} {Speech}},
	doi = {10.1109/O-COCOSDA50338.2020.9295036},
	abstract = {This paper presents first step towards Large Vocabulary Continuous Speech Recognition (LVCSR) system for Urdu-English code-switched conversational speech. Urdu is the national language and lingua franca of Pakistan, with 100 million speakers worldwide. English, on the other hand, is official language of Pakistan and commonly mixed with Urdu in daily communication. Urdu, being under-resourced language, have no substantial Urdu-English code-switched corpus in hand to develop speech recognition system. In this research, readily available spontaneous Urdu speech corpus (25 hours) is revised to use it for enhancement of read speech Urdu LVCSR to recognize code-switched speech. This data set is split into 20 hours of train and 5 hours of test set. 10 hours of Urdu BroadCast (BC) data are collected and annotated in a semi-supervised way to enhance the system further. For acoustic modeling, state-of-the-art DNN-HMM modeling technique is used without any prior GMM-HMM training and alignments. Various techniques to improve language model using monolingual data are investigated. The overall percent Word Error Rate (WER) is reduced from 40.71\% to 26.95\% on test set.},
	booktitle = {2020 23rd {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	author = {Farooq, Muhammad Umar and Adeeba, Farah and Hussain, Sarmad and Rauf, Sahar and Khalid, Maryam},
	month = nov,
	year = {2020},
	note = {ISSN: 2472-7695},
	keywords = {Speech recognition, Data models, Vocabulary, Acoustics, Speech coding, Speech enhancement, Switches, under-resourced language, Urdu speech recognition, Urdu-English code-switching},
	pages = {155--159},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\88SVPLUM\\9295036.html:text/html},
}

@inproceedings{watanabe_-line_2009,
	address = {Taipei, Taiwan},
	title = {On-line adaptation and {Bayesian} detection of environmental changes based on a macroscopic time evolution system},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4960598/},
	doi = {10.1109/ICASSP.2009.4960598},
	urldate = {2022-08-19},
	booktitle = {2009 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Watanabe, Shinji and Nakamura, Atsushi},
	month = apr,
	year = {2009},
	pages = {4373--4376},
}

@article{davis_automatic_1952-1,
	title = {Automatic {Recognition} of {Spoken} {Digits}},
	volume = {24},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1906946},
	doi = {10.1121/1.1906946},
	language = {en},
	number = {6},
	urldate = {2022-08-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
	month = nov,
	year = {1952},
	pages = {637--642},
}

@article{s_review_2016,
	title = {A {Review} on {Automatic} {Speech} {Recognition} {Architecture} and {Approaches}},
	volume = {9},
	issn = {20054254, 20054254},
	url = {http://article.nadiapub.com/IJSIP/vol9_no4/34.pdf},
	doi = {10.14257/ijsip.2016.9.4.34},
	number = {4},
	urldate = {2022-08-19},
	journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
	author = {S, Karpagavalli and E, Chandra},
	month = apr,
	year = {2016},
	pages = {393--404},
}

@incollection{maglogiannis__2020-1,
	address = {Cham},
	title = {Œë {Benchmarking} of {IBM}, {Google} and {Wit} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {583},
	isbn = {978-3-030-49160-4 978-3-030-49161-1},
	url = {http://link.springer.com/10.1007/978-3-030-49161-1_7},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Filippidou, Foteini and Moussiades, Lefteris},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	doi = {10.1007/978-3-030-49161-1_7},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\79KCL3R7\\Filippidou and Moussiades - 2020 - Œë Benchmarking of IBM, Google and Wit Automatic Sp.pdf:application/pdf},
}

@misc{amodei_deep_2015,
	title = {Deep {Speech} 2: {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	shorttitle = {Deep {Speech} 2},
	url = {http://arxiv.org/abs/1512.02595},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
	month = dec,
	year = {2015},
	note = {arXiv:1512.02595 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\DELL\\Zotero\\storage\\T5Y77XAY\\Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\5HBNU4V7\\1512.html:text/html},
}

@article{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Deep {Speech}},
	url = {https://arxiv.org/abs/1412.5567},
	doi = {10.48550/ARXIV.1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	urldate = {2022-08-19},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{xiong_microsoft_2018,
	title = {The {Microsoft} 2017 {Conversational} {Speech} {Recognition} {System}},
	url = {https://www.microsoft.com/en-us/research/publication/conference-paper-microsoft-2017-conversational-speech-recognition-system/},
	abstract = {We describe the latest version of Microsoft's conversational speech recognition system for the Switchboard and CallHome domains. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby acoustic model posteriors are first combined at the senone/frame level,followed by a word-level voting via confusion networks. We also added another language model rescoring step following the confusion network combination. The resulting system yields a 5.1\% word error rate on the NIST 2000 Switchboard test set, and 9.8\% on the CallHome subset.},
	booktitle = {Proc. {IEEE} {ICASSP}},
	publisher = {IEEE},
	author = {Xiong, Wayne and Wu, Lingfeng and Droppo, Jasha and Huang, Xuedong and Stolcke, Andreas},
	month = apr,
	year = {2018},
	pages = {5934--5938},
}

@article{juang_automatic_2005,
	title = {Automatic {Speech} {Recognition} - {A} {Brief} {History} of the {Technology} {Development}},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (1, 2), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language and takes into account the varying statistics of the language in which the speech is produced. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface, such as automatic call processing in the telephone network and query-based information systems that do things like provide updated travel information, stock price quotations, weather reports, etc. In this article, we review some major highlights in the research and development of automatic speech recognition during the last few decades so as to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	author = {Juang, B. and Rabiner, Lawrence},
	month = jan,
	year = {2005},
}

@incollection{juang_speech_2006,
	address = {Oxford},
	title = {Speech {Recognition}, {Automatic}: {History}},
	isbn = {978-0-08-044854-1},
	shorttitle = {Speech {Recognition}, {Automatic}},
	url = {https://www.sciencedirect.com/science/article/pii/B0080448542009068},
	abstract = {Designing a machine that mimics human behavior, particularly the capability of speaking naturally and responding properly to spoken language, has intrigued engineers and scientists for centuries. Since the 1930s, when Homer Dudley of Bell Laboratories proposed a system model for speech analysis and synthesis (Dudley, 1939; Dudley et al., 1939), the problem of automatic speech recognition has been approached progressively, from a simple machine that responds to a small set of sounds to a sophisticated system that responds to fluently spoken natural language. Based on major advances in statistical modeling of speech in the 1980s, automatic speech recognition systems today find widespread application in tasks that require a human-machine interface. Examples are automatic call processing in the telephone network and query-based information systems that provide updated travel information, stock price quotations, weather reports, etc. In this article we review some major highlights in the research and development of automatic speech recognition during the last few decades to provide a technological perspective and an appreciation of the fundamental progress that has been made in this important area of information and communication technology.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Encyclopedia of {Language} \& {Linguistics} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Juang, B. -H. and Rabiner, L. R.},
	editor = {Brown, Keith},
	month = jan,
	year = {2006},
	doi = {10.1016/B0-08-044854-2/00906-8},
	keywords = {Speech recognition, acoustic modeling, automatic transcription, dialog systems, finite state network, hidden Markov models, keyword spotting, language modeling, neural networks, office automation, pattern recognition, spectral analysis, speech understanding, statistical modeling, time normalization},
	pages = {806--819},
	file = {ScienceDirect Snapshot:C\:\\Users\\DELL\\Zotero\\storage\\VWPXBBVS\\B0080448542009068.html:text/html},
}

@book{brown_encyclopedia_2006,
	address = {Amsterdam},
	edition = {2nd ed},
	title = {Encyclopedia of language \& linguistics},
	isbn = {978-0-08-044854-1},
	language = {eng},
	publisher = {Elsevier},
	author = {Brown, E. K. and Anderson, Anne},
	year = {2006},
}

@article{upton_speech_1984-1,
	title = {Speech recognition for computers in industry},
	volume = {4},
	issn = {0260-2288},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb007649/full/html},
	doi = {10.1108/eb007649},
	abstract = {Speech data entry for computers offers many advantages ‚Äî this article considers the vocabulary and training needed for its potential to be reached.},
	language = {en},
	number = {4},
	urldate = {2022-08-19},
	journal = {Sensor Review},
	author = {Upton, Jon},
	month = apr,
	year = {1984},
	pages = {177--178},
}

@inproceedings{morris_wer_2004-1,
	title = {From {WER} and {RIL} to {MER} and {WIL}: improved evaluation measures for connected speech recognition},
	shorttitle = {From {WER} and {RIL} to {MER} and {WIL}},
	url = {https://www.isca-speech.org/archive/interspeech_2004/morris04_interspeech.html},
	doi = {10.21437/Interspeech.2004-668},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Interspeech 2004},
	publisher = {ISCA},
	author = {Morris, Andrew Cameron and Maier, Viktoria and Green, Phil},
	month = oct,
	year = {2004},
	pages = {2765--2768},
}

@article{zhang_hello_2017-1,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-19},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Sound (cs.SD)},
}

@inproceedings{wu_deep_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Deep neural networks employing {Multi}-{Task} {Learning} and stacked bottleneck features for speech synthesis},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178814/},
	doi = {10.1109/ICASSP.2015.7178814},
	urldate = {2022-08-19},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Zhizheng and Valentini-Botinhao, Cassia and Watts, Oliver and King, Simon},
	month = apr,
	year = {2015},
	pages = {4460--4464},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\GFDVBE7Z\\Wu et al. - 2015 - Deep neural networks employing Multi-Task Learning.pdf:application/pdf},
}

@article{pohjalainen_feature_2015,
	title = {Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits},
	volume = {29},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230813001113},
	doi = {10.1016/j.csl.2013.11.004},
	language = {en},
	number = {1},
	urldate = {2022-08-19},
	journal = {Computer Speech \& Language},
	author = {Pohjalainen, Jouni and R√§s√§nen, Okko and Kadioglu, Serdar},
	month = jan,
	year = {2015},
	pages = {145--171},
}

@inproceedings{eyben_opensmile_2010,
	address = {Firenze, Italy},
	title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
	isbn = {978-1-60558-933-6},
	shorttitle = {Opensmile},
	url = {http://dl.acm.org/citation.cfm?doid=1873951.1874246},
	doi = {10.1145/1873951.1874246},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the international conference on {Multimedia} - {MM} '10},
	publisher = {ACM Press},
	author = {Eyben, Florian and W√∂llmer, Martin and Schuller, Bj√∂rn},
	year = {2010},
	pages = {1459},
	file = {Full Text:C\:\\Users\\DELL\\Zotero\\storage\\4F8ACZKL\\Eyben et al. - 2010 - Opensmile the munich versatile and fast open-sour.pdf:application/pdf},
}

@article{chung_unsupervised_2019,
	title = {An {Unsupervised} {Autoregressive} {Model} for {Speech} {Representation} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1904.03240},
	doi = {10.48550/ARXIV.1904.03240},
	abstract = {This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.},
	urldate = {2022-08-19},
	author = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@inproceedings{boser_training_1992,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {A training algorithm for optimal margin classifiers},
	isbn = {978-0-89791-497-0},
	url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
	doi = {10.1145/130385.130401},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Proceedings of the fifth annual workshop on {Computational} learning theory  - {COLT} '92},
	publisher = {ACM Press},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	year = {1992},
	pages = {144--152},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\29LCGTE4\\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf:application/pdf},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {wav2vec 2.0},
	url = {https://arxiv.org/abs/2006.11477},
	doi = {10.48550/ARXIV.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2022-08-19},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@article{soldz_big_1999,
	title = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}: {A} 45-{Year} {Longitudinal} {Study}},
	volume = {33},
	issn = {00926566},
	shorttitle = {The {Big} {Five} {Personality} {Traits} and the {Life} {Course}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656699922432},
	doi = {10.1006/jrpe.1999.2243},
	language = {en},
	number = {2},
	urldate = {2022-08-19},
	journal = {Journal of Research in Personality},
	author = {Soldz, Stephen and Vaillant, George E.},
	month = jun,
	year = {1999},
	pages = {208--232},
}

@inproceedings{zhou_security_2010,
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	abstract = {Cloud Computing is becoming a well-known buzzword nowadays. Many companies, such as Amazon, Google, Microsoft and so on, accelerate their paces in developing Cloud Computing systems and enhancing their services to provide for a larger amount of users. However, security and privacy issues present a strong barrier for users to adapt into Cloud Computing systems. In this paper, we investigate several Cloud Computing system providers about their concerns on security and privacy issues. We find those concerns are not adequate and more should be added in terms of five aspects (i.e., availability, confidentiality, data integrity, control, audit) for security. Moreover, released acts on privacy are out of date to protect users' private information in the new environment (i.e., Cloud Computing system environment) since they are no longer applicable to the new relationship between users and providers, which contains three parties (i.e., Cloud service user, Cloud service provider/Cloud user, Cloud provider). Multi located data storage and services (i.e., applications) in the Cloud make privacy issues even worse. Hence, adapting released acts for new scenarios in the Cloud, it will result in more users to step into Cloud. We claim that the prosperity in Cloud Computing literature is to be coming after those security and privacy issues having be resolved.},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = dec,
	year = {2010},
	note = {Journal Abbreviation: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
Pages: 112
Publication Title: Semantics Knowledge and Grid (SKG), 2010 Sixth International Conference On
DOI:},
}

@inproceedings{zhou_security_2010-1,
	address = {Beijing, China},
	title = {Security and {Privacy} in {Cloud} {Computing}: {A} {Survey}},
	isbn = {978-1-4244-8125-5},
	shorttitle = {Security and {Privacy} in {Cloud} {Computing}},
	url = {http://ieeexplore.ieee.org/document/5663489/},
	doi = {10.1109/SKG.2010.19},
	urldate = {2022-08-19},
	booktitle = {2010 {Sixth} {International} {Conference} on {Semantics}, {Knowledge} and {Grids}},
	publisher = {IEEE},
	author = {Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
	month = nov,
	year = {2010},
	pages = {105--112},
}

@inproceedings{karimov_cloud_2021,
	title = {Cloud {Computing} {Security} {Challenges} and {Solutions}},
	doi = {10.1109/ICISCT52966.2021.9670220},
	abstract = {in this paper focuses on development of cloud computing, privacy security issues have become increasingly prominent, which is of concern to industry and academia. We review the research progress on privacy security issues from the perspective of several privacy security protection technologies in cloud computing. First, we introduce some privacy security risks of cloud computing and propose a comprehensive privacy security protection framework.},
	booktitle = {2021 {International} {Conference} on {Information} {Science} and {Communications} {Technologies} ({ICISCT})},
	author = {Karimov, Abdukodir and Olimov, Iskandar and Berdiyev, Khusniddin and Tojiakbarova, Umida and Tursunov, Otabek},
	month = nov,
	year = {2021},
	keywords = {Computational modeling, Access control, Attribute-based encryption, Cloud computing, Communications technology, Industries, Information science, Privacy, Privacy security},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\6B73HGPD\\9670220.html:text/html},
}

@incollection{takabi_introduction_2019,
	title = {Introduction to the {Cloud} and {Fundamental} {Security} and {Privacy} {Issues} of the {Cloud}},
	isbn = {978-1-119-05340-8},
	url = {https://ieeexplore.ieee.org/document/9821151},
	abstract = {Cloud Computing is the most important solution to extend Information Technology's (IT) capabilities. However, Cloud is still vulnerable to a variety of threats and attacks that affects the growth of cloud computing in recent years. Therefore, the security concerns should be considered to improve the assurance of required security for the cloud customers. The cloud security consists of different aspects such as: infrastructure, information, and identity. In this chapter, we provide an introduction to the Cloud and its fundamental security and privacy issues, investigate security issues in different cloud services delivery models and introduce Cloud security standards.},
	urldate = {2022-08-19},
	booktitle = {Security, {Privacy}, and {Digital} {Forensics} in the {Cloud}},
	publisher = {Wiley},
	author = {Takabi, Hassan and GhasemiGol, Mohammad},
	year = {2019},
	doi = {10.1002/9781119053385.ch1},
	note = {Conference Name: Security, Privacy, and Digital Forensics in the Cloud},
	keywords = {Computational modeling, Cloud computing, Privacy, Operating systems, Organizations, Security, Software as a service},
	pages = {1--22},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\NNG85SBV\\9821151.html:text/html},
}

@inproceedings{kumar_systematic_2020,
	title = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}: {Data} {Integrity}, {Confidentiality} and {Availability}},
	shorttitle = {A {Systematic} {Review} of the {Security} in {Cloud} {Computing}},
	doi = {10.1109/GUCON48875.2020.9231255},
	abstract = {The cloud computing plays the prominent role in many organizations and researchers were focus on securing the cloud computing. The privacy preserving is the major challenge that grows exponentially with increases in user. In this paper, the depth survey is conducted on the recent methodologies of the cloud storage security related with the cloud computing. The overview of the cloud computing and security issues is analyzed in this paper. The key security requirements such as data integrity, availability and confidentiality. Security issues in the recent methodologies of cloud security is analyzed. The challenges in the cloud security is analyzed and possible future scope of the method is discussed. The paper involves in analyzing the state-of-art method to investigate the advantages and limitations.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Computing}, {Power} and {Communication} {Technologies} ({GUCON})},
	author = {Kumar, Rajeev and Bhatia, M P S},
	month = oct,
	year = {2020},
	keywords = {Systematics, Privacy, Organizations, Cloud Computing, Cloud computing security, Cloud Storage Security, Conferences, Confidentiality, Data integrity, Data Integrity, Data transfer, Privacy Preserving},
	pages = {334--337},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DELL\\Zotero\\storage\\MT8C247U\\9231255.html:text/html},
}

@inproceedings{godfrey_switchboard_1992,
	address = {San Francisco, CA, USA},
	title = {{SWITCHBOARD}: telephone speech corpus for research and development},
	isbn = {978-0-7803-0532-8},
	shorttitle = {{SWITCHBOARD}},
	url = {http://ieeexplore.ieee.org/document/225858/},
	doi = {10.1109/ICASSP.1992.225858},
	urldate = {2022-08-20},
	booktitle = {[{Proceedings}] {ICASSP}-92: 1992 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
	year = {1992},
	pages = {517--520 vol.1},
}

@article{zhang_hello_2017-2,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128},
	doi = {10.48550/ARXIV.1711.07128},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	urldate = {2022-08-20},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Sound (cs.SD)},
}

@inproceedings{lin_self-attentive_2020,
	title = {Self-{Attentive} {Similarity} {Measurement} {Strategies} in {Speaker} {Diarization}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1908},
	language = {en},
	urldate = {2022-08-20},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Lin, Qingjian and Hou, Yu and Li, Ming},
	month = oct,
	year = {2020},
	pages = {284--288},
}

@article{schuller_interspeech_2013,
	title = {The interspeech 2013 computational paralinguistics challenge: {Social} signals, conflict, emotion, autism},
	shorttitle = {The interspeech 2013 computational paralinguistics challenge},
	journal = {Proceedings of Interspeech},
	author = {Schuller, Bj√∂rn and Steidl, S. and Batliner, Anton and Vinciarelli, Alessandro and Scherer, K. and Ringeval, Fabien and Chetouani, Mohamed and Weninger, F. and Eyben, Florian and Marchi, Erik and Mortillaro, Marcello and Salamin, H. and Polychroniou, Anna and Valente, F. and Kim, S.},
	month = jan,
	year = {2013},
	pages = {148--152},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\IJ9QFIG3\\Schuller et al. - 2013 - The interspeech 2013 computational paralinguistics.pdf:application/pdf},
}

@inproceedings{misra_spectral_2004,
	address = {Montreal, Que., Canada},
	title = {Spectral entropy based feature for robust {ASR}},
	volume = {1},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1325955/},
	doi = {10.1109/ICASSP.2004.1325955},
	urldate = {2022-08-20},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Misra, H. and Ikbal, S. and Bourlard, H. and Hermansky, H.},
	year = {2004},
	pages = {I--193--6},
	file = {Submitted Version:C\:\\Users\\DELL\\Zotero\\storage\\YLKP828K\\Misra et al. - 2004 - Spectral entropy based feature for robust ASR.pdf:application/pdf},
}

@article{hunt_spectral_2000,
	title = {Spectral {Signal} {Processing} for {ASR}},
	abstract = {The paper begins by discussing the difficulties in obtaining repeatable results in speech recognition. Theoretical arguments are presented for and against copying human auditory properties in automatic speech recognition. The "standard" acoustic analysis for automatic speech recognition, consisting of melscale cepstrum coefficients and their temporal derivatives, is described. Some variations and extensions of the standard analysis --- PLP, cepstrum correlation methods, LDA, and variants on log power --- are then discussed. These techniques pass the test of having been found useful at multiple sites, especially with noisy speech. The extent to which auditory properties can account for the advantage found for particular techniques is considered. It is concluded that the advantages do not in fact stem from auditory properties, and that there is so far little or no evidence that the study of the human auditory system has contributed to advances in automatic speech recognition. Contributio...},
	author = {Hunt, Melvyn},
	month = aug,
	year = {2000},
	file = {Full Text PDF:C\:\\Users\\DELL\\Zotero\\storage\\Z9XRFNJG\\Hunt - 2000 - Spectral Signal Processing for ASR.pdf:application/pdf},
}

@article{biswas_speaker_2021,
	title = {Speaker recognition: an enhanced approach to identify singer voice using neural network},
	volume = {24},
	issn = {1381-2416, 1572-8110},
	shorttitle = {Speaker recognition},
	url = {http://link.springer.com/10.1007/s10772-020-09698-8},
	doi = {10.1007/s10772-020-09698-8},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {International Journal of Speech Technology},
	author = {Biswas, Sharmila and Solanki, Sandeep Singh},
	month = mar,
	year = {2021},
	pages = {9--21},
}
